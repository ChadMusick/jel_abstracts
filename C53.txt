C53|In Search of a Job: Forecasting Employment Growth in the US using Google Trends|We show that Google search activity on relevant terms is a strong out-of-sample predictor of future employment growth in the US and that it greatly outperforms benchmark predictive models based on macroeconomic, financial, and sentiment variables. Using a subset of ten keywords, we construct a panel with 211 variables using Google’s own algorithms to find related search queries. We use Elastic Net variable selection in combination with Partial Least Squares to extract the most important information from a large set of search terms. Our forecasting model, which can be constructed in real time and is free from revisions, delivers an out-of-sample R^2 statistic of 65% to 88% for horizons between one month and one year ahead over the period 2008-2017, which compares to between roughly 30% and 60% for the benchmark models.
C53|Consistent Inference for Predictive Regressions in Persistent VAR Economies|This paper studies the properties of standard predictive regressions in model economies, characterized through persistent vector autoregressive dynamics for the state variables and the associated series of interest. In particular, we consider a setting where all, or a subset, of the variables may be fractionally integrated, and note that this induces a spurious regression problem. We then propose a new inference and testing procedure - the local spectrum (LCM) approach - for the joint significance of the regressors, which is robust against the variables having different integration orders. The LCM procedure is based on (semi-)parametric fractional-filtering and band spectrum regression using a suitably selected set of frequency ordinates. We establish the asymptotic properties and explain how they differ from and extend existing procedures. Using these new inference and testing techniques, we explore the implications of assuming VAR dynamics in predictive regressions for the realized return variation. Standard least squares predictive regressions indicate that popular financial and macroeconomic variables carry valuable information about return volatility. In contrast, we find no significant evidence using our robust LCM procedure, indicating that prior conclusions may be premature. In fact, if anything, our results suggest the reverse causality, i.e., rising volatility predates adverse innovations to key macroeconomic variables. Simulations are employed to illustrate the relevance of the theoretical arguments for finite-sample inference.
C53|Forecaster’s utility and forecasts coherence|I provide general frequentist framework to elicit the forecaster’s expected utility based on a Lagrange Multiplier-type test for the null of locality of the scoring rules associated to the probabilistic forecast. These are assumed to be observed transition variables in a nonlinear autoregressive model to ease the statistical inference. A simulation study reveals that the test behaves consistently with the requirements of the theoretical literature. The locality of the scoring rule is fundamental to set dating algorithms to measure and forecast probability of recession in US business cycle. An investigation of Bank of Norway’s forecasts on output growth leads us to conclude that forecasts are often suboptimal with respect to some simplistic benchmark if forecaster’s reward is not properly evaluated.
C53|Assessing predictive accuracy in panel data models with long-range dependence|This paper proposes tests of the null hypothesis that model-based forecasts are uninformative in panels, allowing for individual and interactive fixed effects that control for cross-sectional dependence, endogenous predictors, and both short-range and long-range dependence. We consider a Diebold-Mariano style test based on comparison of the model-based forecast and a nested nopredictability benchmark, an encompassing style test of the same null, and a test of pooled uninformativeness in the entire panel. A simulation study shows that the encompassing style test is reasonably sized in finite samples, whereas the Diebold-Mariano style test is oversized. Both tests have non-trivial local power. The methods are applied to the predictive relation between economic policy uncertainty and future stock market volatility in a multi-country analysis.
C53|Forecasting Causes of Death using Compositional Data Analysis: the Case of Cancer Deaths|Cause-specific mortality forecasting is often based on predicting cause-specific death rates independently. Only a few methods have been suggested that incorporate dependence among causes. An attractive alternative is to model and forecast cause-specific death distributions, rather than mortality rates, as dependence among the causes can be incorporated directly. We follow this idea and propose two new models which extend the current research on mortality forecasting using death distributions. We find that adding age, time, and cause-specific weights and decomposing both joint and individual variation among different causes of death increased the forecast accuracy of cancer deaths using data for French and Dutch populations
C53|Longevity forecasting by socio-economic groups using compositional data analysis|Several OECD countries have recently implemented an automatic link between the statutory retirement age and life expectancy for the total population to insure sustainability in their pension systems when life expectancy is increasing. Significant mortality differentials are observed across socio-economic groups and future changes in these differentials will determine whether some socio-economic groups drive increases in the retirement age leaving other groups with fewer years in receipt of pensions. We forecast life expectancy by socio-economic groups and compare the forecast performance of competing models using Danish mortality data and find that the most accurate model assumes a common mortality trend. Life expectancy forecasts are used to analyse the consequences of a pension system where the statutory retirement age is increased when total life expectancy is increasing
C53|In search of a job: Forecasting employment growth using Google Trends|We show that Google search activity on relevant terms is a strong out-of-sample predictor for future employment growth in the US over the period 2004-2018 at both short and long horizons. Using a subset of ten keywords associated with “jobs”, we construct a large panel of 173 variables using Google’s own algorithms to find related search queries. We find that the best Google Trends model achieves an out-of-sample R2 between 26% and 59% at horizons spanning from one month to a year ahead, strongly outperforming benchmarks based on a large set of macroeconomic and financial predictors. This strong predictability extends to US state-level employment growth, using state-level specific Google search activity. Encompassing tests indicate that when the Google Trends panel is exploited using a non-linear model it fully encompasses the macroeconomic forecasts and provides significant information in excess of those.
C53|Improving Forecast Accuracy of Financial Vulnerability: PLS Factor Model Approach|We present a factor augmented forecasting model for assessing the financial vulnerability in Korea. Dynamic factor models often extract latent common factors from a large panel of time series data via the method of the principal components (PC). Instead, we employ the partial least squares (PLS) method that estimates target specific common factors, utilizing covariances between predictors and the target variable. Applying PLS to 198 monthly frequency macroeconomic time series variables and the Bank of Korea's Financial Stress Index (KFSTI), our PLS factor augmented forecasting models consistently outperformed the random walk benchmark model in out-of-sample prediction exercises in all forecast horizons we considered. Our models also outperformed the autoregressive benchmark model in short-term forecast horizons. We expect our models would provide useful early warning signs of the emergence of systemic risks in Korea's financial markets.
C53|Foreign aid, instability and governance in Africa|This study contributes to the attendant literature by bundling governance dynamics and focusing on foreign aid instability instead of foreign aid. We assess the role of foreign aid instability on governance dynamics in fifty three African countries for the period 1996-2010. An autoregressive endogeneity-robust Generalized Method of Moments is employed. Instabilities are measured in terms of variance of the errors and standard deviations. Three main aid indicators are used, namely: total aid, aid from multilateral donors and bilateral aid. Principal Component Analysis is used to bundle governance indicators, namely: political governance (voice & accountability and political stability/no violence), economic governance (regulation quality and government effectiveness), institutional governance (rule of law and corruption-control) and general governance (political, economic and institutional governance). Our findings show that foreign aid instability increases governance standards, especially political and general governance. Policy implications are discussed.
C53|Structural Factor Analysis of Interest Rate Pass Through In Four Large Euro Area Economies|In this paper we examine the influence of unconventional monetary policy at the ECB on mortgage and business lending rates offered by banks in the major euro area countries (Germany, France, Italy and Spain). Since there are many different policy measures that have been undertaken, we utilise a dynamic factor model based on the Bernanke Boivin and Eliasz (2005) approach, which allows examination of impulse responses to a policy rate conditioned by structurally identified latent factors. The distinct feature of this paper is that it explores the effects of all three phases of monetary policy to emphasize the transmission channels - through short-term rates, long-term yields and and perceived risk - ultimately directed towards bank lending rates. Further analysis of unconventional monetary policy is provided through rolling window impulse responses and variance decompositions of the identified financial factors on lending rates to demonstrate the changing influence of different policy measures on lending rates.
C53|Iassopack: Model Selection and Prediction with Regularized Regression in Stata|This article introduces lassopack, a suite of programs for regularized regression in Stata. lassopack implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso and post-estimation OLS. The methods are suitable for the high-dimensional setting where the number of predictors p may be large and possibly greater than the number of observations, n. We offer three different approaches for selecting the penalization ('tuning') parameters: information criteria (implemented in lasso2), K-fold cross-validation and h-step ahead rolling cross-validation for cross-section, panel and time-series data (cvlasso), and theory-driven ('rigorous') penalization for the lasso and square-root lasso for cross-section and panel data (rlasso). We discuss the theoretical framework and practical considerations for each approach. We also present Monte Carlo results to compare the performance of the penalization approaches.
C53|Bayesian MIDAS penalized regressions: estimation, selection, and prediction|We propose a new approach to mixed-frequency regressions in a high-dimensional environment that resorts to Group Lasso penalization and Bayesian techniques for estimation and inference. To improve the sparse recovery ability of the model, we also consider a Group Lasso with a spike-and-slab prior. Penalty hyper-parameters governing the model shrinkage are automatically tuned via an adaptive MCMC algorithm. Simulations show that the proposed models have good selection and forecasting performance, even when the design matrix presents high cross-correlation. When applied to U.S. GDP data, the results suggest that financial variables may have some, although limited, short-term predictive content.
C53|Multivariate Fractional Components Analysis|We propose a setup for fractionally cointegrated time series which is formulated in terms of latent integrated and short-memory components. It accommodates nonstationary processes with different fractional orders and cointegration of different strengths and is applicable in high-dimensional settings. In an application to realized covariance matrices, we find that orthogonal short- and long-memory components provide a reasonable fit and competitive out-of-sample performance compared to several competing methods.
C53|The Trend Unemployment Rate in Canada: Searching for the Unobservable|In this paper, we assess several methods that have been used to measure the Canadian trend unemployment rate (TUR). We also considerimprovements and extensions to some existing methods. The assessment is based on four criteria: (i) the extent to which methods provide explanations for changes in trend unemployment; (ii) whether revisions to unemployment gap (UGAP, the difference between the actual unemployment rate and TUR) estimates are well behaved; (iii) if UGAPs provide information about future inflation; and (iv) if UGAPs help explain historical data about wages and consumer price inflation. In our assessment of conformity to the second and third criteria, we use real-time data, i.e., the data available to policymakers at the time of making decisions. We find that while all methods we consider have both strengths and weaknesses, those based on variables thought to determine TUR provide better interpretation and tend to do at least as well as others against the other criteria. These are most promising for future work. Nevertheless, there is considerable uncertainty about the value of TUR, which suggests it would be prudent to use a range of models in research or policy work. While estimates of TUR have declined since the mid-1990s, it is assessed to range between 5.6 and 6.7 per cent in 2018Q4.
C53|El índice de precios de consumo: usos y posibles vías de mejora|En este documento se presentan de forma breve los principales usos de las estadísticas de precios de consumo, prestando especial atención a la predicción de la inflación mediante modelos econométricos. También se realizan algunas propuestas de cara a aumentar la utilidad del índice de precios de consumo para los usuarios finales.
C53|Forecasting the Colombian Unemployment Rate Using Labour Force Flows|Accurate predictions of future magnitudes of the unemployment rate are crucial for monetary policy. This paper investigates whether the use of disaggregated household survey data improves the forecasts of the Colombian 13 cities unemployment rate. We conduct an outof-sample forecast exercise to compare the performance of a model that incorporates flows of workers across different states of the labour market to that of various macroeconomic non-structural models. The paper follows the approach proposed by Barnichon & Nekarda (2013). Our results indicate that the two-state-flow model provides substantially better forecasts of the unemployment rate over longer horizons (more than five months ahead). Additionally, when forecasts are combined, significant gains in every forecasting horizon occurs. This combined forecast shows a 23% reduction in overall RMSE. **** ABSTRACT: En este documento se evalúan los pronósticos de la tasa de desempleo urbana en Colombia utilizando varias metodologías. La primera se basa en las propiedades estadísticas de la serie de tiempo de la tasa de desempleo. La segunda considera la relación entre el crecimiento del producto y los cambios en el desempleo, conocida como la Ley de Okun. Finalmente, con base en los microdatos de las encuestas de hogares se calculan los flujos de trabajadores del mercado laboral para pronosticar la tasa de desempleo de acuerdo con Barnichon y Nekarda (2013). La evaluación de los pronósticos fuera de muestra indica que el modelo de dos estados (ocupado-desocupado) es el mejor en horizontes superiores a cinco meses. Por su parte, los modelos ARIMA y la Ley de Okun compiten en precisión en horizontes de corto plazo. Cabe destacar que la combinación de los modelos de pronóstico genera ganancias significativas en todos los horizontes, alcanzando una reducción global de 23% en la raíz del error cuadrático medio. Classification-JEL: C53, E24, E27, E3, J64
C53|Indicadores de alerta temprana para el sector corporativo privado colombiano|Este documento valida la utilidad de algunas variables financieras en la identificación temprana de acumulación de vulnerabilidades para el sector corporativo privado en Colombia. Para esto, se estudia la evolución de varios indicadores para firmas que han entrado en distress financiero. Adicionalmente, se valida la capacidad predictiva de los indicadores in-sample y out-of-sample. Los resultados sugieren que la razón de endeudamiento y una medida de debt-to-cashflow son las que mejor información proveen. Asimismo, se encuentra que la desagregación de las firmas por sector económico y el uso de medidas conjuntas aumentan la capacidad de identificación de situaciones de vulnerabilidad en este sector. **** ABSTRACT: This paper assesses the usefulness of some financial variables in predicting episodes of vulnerability for the private corporate sector in Colombia. We analyse the evolution of several indicators for firms that have experienced episodes of distress. Additionally, we validate the predictive power of our indicators by using in-sample and out-of-sample tests. The results suggest that the ratio of financial obligations to assets, as well as a measure of debt-to-cashflow provide better information. Likewise, we find that classifying the firms by economic sector and the use of several variables at the same time improve the ability to detect changes in the financial health of firms.
C53|Bayesian MIDAS Penalized Regressions: Estimation, Selection, and Prediction|We propose a new approach to mixed-frequency regressions in a high-dimensional environment that resorts to Group Lasso penalization and Bayesian techniques for estimation and inference. To improve the sparse recovery ability of the model, we also consider a Group Lasso with a spike-and-slab prior. Penalty hyper-parameters governing the model shrinkage are automatically tuned via an adaptive MCMC algorithm. Simulations show that the proposed models have good selection and forecasting performance, even when the design matrix presents high cross-correlation. When applied to U.S. GDP data, the results suggest that financial variables may have some, although limited, short-term predictive content.
C53|When are Google data useful to nowcast GDP? An approach via pre-selection and shrinkage|Nowcasting GDP growth is extremely useful for policy-makers to assess macroe-conomic conditions in real-time. In this paper, we aim at nowcasting euro area GDP with a large database of Google search data. Our objective is to check whether this specific type of information can be useful to increase GDP nowcasting accuracy, and when, once we control for official variables. In this respect, we estimate shrunk bridge regressions that integrate Google data optimally screened through a targeting method, and we empirically show that this approach provides some gain in pseudo-real-time nowcasting of euro area GDP quarterly growth. Especially, we get that Google data bring useful information for GDP nowcasting for the four first weeks of the quarter when macroeconomic information is lacking. However, as soon as official data become available, their relative nowcasting power vanishes. In addition, a true real-time anal-ysis confirms that Google data constitute a reliable alternative when official data are lacking.
C53|VAR-based Granger-causality test in the presence of instabilities|In this article, we review Granger-causality tests robust to the presence of instabilities in a Vector Autoregressive framework. We also introduce the gcrobustvar command, which illustrates the procedure in Stata. In the presence of instabilities, the Granger-causality robust test is more powerful than the traditional Granger-causality test.
C53|Bank intermediation activity in a low interest rate environment|This paper investigates how the prolonged period of low interest rates affects bank intermediation activity. We use data for 113 large international banks headquartered in 14 major advanced economies during the period 1994–2015. We find that low interest rates induce banks to shift their activities from interest-generating to fee-related and trading activities. This rebalancing is stronger for low capitalised banks. Banks also moderately adjust their funding structure, away from short-term market funding towards deposits. We observe a concomitant decline in the risk-weighted asset ratio and a reduction in loan-loss provisions, which is consistent with signs of evergreening.
C53|The finer points of model comparison in machine learning: forecasting based on russian banks’ data|We evaluate the forecasting ability of machine learning models to predict bank license withdrawal and the violation of statutory capital and liquidity requirements (capital adequacy ratio N1.0, common equity Tier 1 adequacy ratio N1.1, Tier 1 capital adequacy ratio N1.2, N2 instant and N3 current liquidity). On the basis of 35 series from the accounting reports of Russian banks, we form two data sets of 69 and 721 variables and use them to build random forest and gradient boosting models along with neural networks and a stacking model for different forecasting horizons (1, 2, 3, 6, 9 months). Based on the data from February 2014 to October 2018 we show that these models with fine-tuned architectures can successfully compete with logistic regression usually applied for this task. Stacking and random forest generally have the best forecasting performance comparing to the other models. We evaluate models with commonly used performance metrics (ROC-AUC and F1) and show that, depending on the task, F1-score could be better at defining the model’s performance. Comparison of the results depending on the metrics applied and types of cross-validation used illustrate the importance of choosing the appropriate metric for performance evaluation and the cross-validation procedure, which accounts for the characteristics of the data set and the task under consideration. The developed approach shows the advantages of non-linear methods for bank regulation tasks and provides the guidelines for the application of machine learning algorithms to these tasks.
C53|Multivariate Bayesian Predictive Synthesis in Macroeconomic Forecasting|We present new methodology and a case study in use of a class of Bayesian predictive synthesis (BPS) models for multivariate time series forecasting. This extends the foundational BPS framework to the multivariate setting, with detailed application in the topical and challenging context of multi-step macroeconomic forecasting in a monetary policy setting. BPS evaluates– sequentially and adaptively over time– varying forecast biases and facets of miscalibration of individual forecast densities for multiple time series, and– critically– their time-varying interdependencies. We define BPS methodology for a new class of dynamic multivariate latent factor models implied by BPS theory. Structured dynamic latent factor BPS is here motivated by the application context– sequential forecasting of multiple US macroeconomic time series with forecasts generated from several traditional econometric time series models. The case study highlights the potential of BPS to improve of forecasts of multiple series at multiple forecast horizons, and its use in learning dynamic relationships among forecasting models or agents.
C53|News-driven inflation expectations and information rigidities|We investigate the role played by the media in the expectations formation process of households. Using a news-topic-based approach we show that news types the media choose to report on, e.g., (Internet) technology, health, and politics, are good predictors of households' stated in ation expectations. In turn, in a noisy information model setting, augmented with a simple media channel, we document that the underlying time series properties of relevant news topics explain the timevarying information rigidity among households. As such, we not only provide a novel estimate showing the degree to which information rigidities among households vary across time, but also provide, using a large news corpus and machine learning algorithms, robust and new evidence highlighting the role of the media for understanding infl ation expectations and information rigidities.
C53|Multivariate Bayesian Predictive Synthesis in Macroeconomic Forecasting|We present new methodology and a case study in use of a class of Bayesian predictive synthesis (BPS) models for multivariate time series forecasting. This extends the foundational BPS framework to the multivariate setting, with detailed application in the topical and challenging context of multi-step macroeconomic forecasting in a monetary policy setting. BPS evaluates - sequentially and adaptively over time – varying forecast biases and facets of miscalibration of individual forecast densities for multiple time series, and – critically – their time-varying interdependencies. We define BPS methodology for a new class of dynamic multivariate latent factor models implied by BPS theory. Structured dynamic latent factor BPS is here motivated by the application context–sequential forecasting of multiple US macroeconomic time series with forecasts generated from several traditional econometric time series models. The case study highlights the potential of BPS to improve of forecasts of multiple series at multiple forecast horizons, and its use in learning dynamic relationships among forecasting models or agents.
C53|News-driven inflation expectations and information rigidities|We investigate the role played by the media in the expectations formation process of households. Using a novel news-topic-based approach we show that news types the media choose to report on, e.g., fiscal policy, health, and politics, are good predictors of households' stated inflation expectations. In turn, in a noisy information model setting, augmented with a simple media channel, we document that the underlying time series properties of relevant news topics explain the time-varying information rigidity among households. As such, we not only provide a novel estimate showing the degree to which information rigidities among households varies across time, but also provide, using a large news corpus and machine learning algorithms, robust and new evidence highlighting the role of the media for understanding inflation expectations and information rigidities.
C53|Assessing reliability of aggregated inflation views in the European Commission consumer survey|Using a novel approach based on micro-level survey responses, we assess the reliability of aggregated inflation expectations estimates in the European Commission Consumer Survey. We identify the share of consumers, whose qualitative and quantitative views on expected increase of prices do not match each other. Then we consider the impact of inconsistent survey responses on balance statistics and mean values of quantitative inflation expectations. We also analyze expectations’ formation estimating the sticky-information models. The results, based on Finnish and Polish data, suggest that even if the fraction of inconsistent survey responses is non-negligible, it matters neither for the aggregated figures of inflation views, nor for understanding of the formation of inflation expectations by consumers. We conclude that micro-level inconsistencies do not reduce the reliability of the current EC Consumer Survey dataset. Our results also indicate that inconsistent responses are not important drivers of the inflation overestimation bias displayed in the data.
C53|Optimal monetary policy under bounded rationality|Optimal monetary policy under discretion, commitment, and optimal simple rules regimes is analyzed through a behavioral New Keynesian model. Flexible price level targeting dominates under discretion; flexible inflation targeting dominates under commitment; and strict price level targeting dominates when using optimal simple rules. The optimality of a particular regime is found to be independent of bounded rationality and only regime 's stabilizing properties condition its hierarchy. For every targeting regime, the policymaker 's knowledge of agents' myopia is decisive in terms of policy reactions. Welfare evaluation of different targeting regimes reveals that bounded rationality is not necessarily associated with decreased welfare. Several forms of economic inattention can increase welfare.
C53|Forecast Performance in Times of Terrorism|Governments, central banks, and private companies make extensive use of expert and market-based forecasts in their decision-making processes. These forecasts can be affected by terrorism, which should be considered by decision makers. We focus on terrorism, as a mostly endogenously driven form of political uncertainty, and use new econometric tests to assess the forecasting performance of market and professional inflation and exchange-rate forecasts in Israel. We show that expert forecasts are better than market-based forecasts, particularly during periods of terrorism. However, forecasting performance and abilities of both market-based and expert forecasts are significantly reduced during such periods. Thus, policymakers should be particularly attentive to terrorism when considering inflation and exchange-rate forecasts.
C53|High-Frequency Credit Spread Information and Macroeconomic Forecast Revision|We examine whether professional forecasters incorporate high-frequency information about credit conditions when revising their economic forecasts. Using Mixed Data Sampling regression approach, we find that daily credit spreads have significant predictive ability for monthly forecast revisions of output growth, at both aggregate and individual forecast levels. The relations are shown to be notably strong during ¡®bad¡¯ economic conditions, suggesting that forecasters anticipate more pronounced effects of credit tightening during economic downturns, indicating the amplification effect of financial developments on macroeconomic aggregates. Forecasts do not incorporate the totality of financial information received in equal measures, implying the presence of information rigidities in the incorporation of credit spread information.
C53|Density Forecasting|This paper reviews different methods to construct density forecasts and to aggregate forecasts from many sources. Density evaluation tools to measure the accuracy of density forecasts are reviewed and calibration methods for improving the accuracy of forecasts are presented. The manuscript provides some numerical simulation tools to approximate predictive densities with a focus on parallel computing on graphical process units. Some simple examples are proposed to illustrate the methods.
C53|Some Dynamic and Steady-State Properties of Threshold Autoregressions with Applications to Stationarity and Local Explosivity|The purpose of this paper is to investigate the dynamics and steady-state properties of threshold autoregressive models with exogenous states that follow Markovian processes; these processes are widely used in applied economics although their statistical properties have not been explored in detail. We use characteristic functions to carry out the analysis and this allows us to describe limiting distributions for processes not considered in the literature previously. We also calculate analytical expressions for some moments. Furthermore, we see that we can have locally explosive processes that are explosive in one regime whilst being strongly stationary overall. This is explored through simulation analysis where we also show how the distribution changes when the explosive state become more frequent although the overall process remains stationary. In doing so, we are able to relate our analysis to asset prices which exhibit similar distributional properties.
C53|How BLUE is the Sky? Estimating the Air Quality Data in Beijing During the Blue Sky Day Period (2008-2012) by the Bayesian LSTM Approach|Over the last three decades, air pollution has become a major environmental challenge in many of the fast growing cities in China, including Beijing. Given that any long-term exposure to high-levels of air pollution has devastating health consequences, accurately monitoring and reporting air pollution information to the public is critical for ensuring public health and safety and facilitating rigorous air pollution and health-related scientific research. Recent statistical research examining China’s air quality data has posed questions regarding data accuracy, especially data reported during the Blue Sky Day (BSD) period (2000 – 2012), though the accuracy of publicly available air quality data in China has improved gradually over the recent years (2013 – 2017). To the best of our understanding, no attempt has been made to re-estimate the air quality data during the BSD period. In this paper, we put forward a machine-learning model to re-estimate the official air quality data during the BSD period of 2008 – 2012, based on the PM2.5 data of the Beijing US Embassy, and the proxy data covering Aerosol Optical Depth (AOD) and meteorology. Results have shown that the average re-estimated daily air quality values are respectively 64% and 61% higher than the official values, for air quality index (AQI) and AQI equivalent PM2.5, during the BSD period of 2008 to 2012. Moreover, the re-estimated BSD air quality data exhibit reduced statistical discontinuity and irregularity, based on our validation tests. The results suggest that the proposed data re-estimation methodology has the potential to provide more justifiable historical air quality data for evidence-based environmental decision-making in China.<br><small>(This abstract was borrowed from another version of this item.)</small>
C53|Tracking the Course of the Economy (Nowcasting of basic macroeconomic indicators of Slovakia)|Real GDP and its structure are available within 70 days after the end of the reference quarter. By using leading indicators of higher frequency, it is possible to nowcast GDP in real-time. With an assumption of unobserved factor driving the business cycle we estimate dynamic factor models for real GDP, its demand components, inflation, wages and employment using statistically significant domestic and foreign indicators. To ensure the consistency of out-of-sample forecasts for GDP and its components, past forecast deviations and correlation coefficients are used to adjust the forecast, which helps to reduce the bias of individual models. Forecasts using real-time database are carried out since the 1st January of 2017 using daily data vintages. Real-time forecasts display a reduction of forecasting error with the arrival of new data in the last month of the quarter until the official publication. The main role of nowcasting in CBR is to track the actual positive and negative macroeconomic risks of the Slovak economy in relation to the latest official national macroeconomic forecast by the Macroeconomic Forecasting Committee. Additionally, the nowcast models help to improve precision of estimates of initial conditions of the economy by bridging the short-term forecast and mid-term forecast.
C53|An Evaluation of CBOâ€™s Past Deficit and Debt Projections|In this report, CBO analyzes its baseline projections of deficits and debt held by the public that were made each spring beginning in 1984. Each of those projections spanned the fiscal year already under way and five or 10 subsequent years.
C53|Global Value Chains, Trade Shocks and Jobs: An Application to Brexit|We develop a network trade model with country-sector level input-output linkages. It includes (1) domestic and global value chain linkages between all country-sectors, (2) direct as well as indirect shipments (via other sectors and countries) to a final destination, (3) value added rather than gross trade flows. The model is solved analytically and we use the sectoral World Input Output Database (WIOD) to predict the impact of Brexit for every individual EU country by aggregating up the country-sector effects. In contrast to other studies, we find EU-27 job losses to be substantially higher than hitherto believed as a result of the closely integrated EU network structure. Upstream country-sectors stand to lose more from Brexit due to their network centrality in Europe.
C53|A Theory of Scenario Generation|We show how distributions can be reduced to low-dimensional scenario trees. Applied to intertemporal distributions, the scenarios and their probabilities become time-varying factors. From S&P 500 options, two or three time-varying scenarios suffice to forecast returns, implied variance or skewness on par, or better, than extant multivariate stochastic volatility jump-diffusion models, while reducing the computational effort to fractions of a second.
C53|A Flexible Regime Switching Model for Asset Returns|A non-Gaussian multivariate regime switching dynamic correlation model for fi nancial asset returns is proposed. It incorporates the multivariate generalized hyperbolic law for the conditional distribution of returns. All model parameters are estimated consistently using a new two-stage expectation-maximization algorithm that also allows for incorporation of shrinkage estimation via quasi-Bayesian priors. It is shown that use of Markov switching correlation dynamics not only leads to highly accurate risk forecasts, but also potentially reduces the regulatory capital requirements during periods of distress. In terms of portfolio performance, the new regime switching model delivers consistently higher Sharpe ratios and smaller losses than the equally weighted portfolio and all competing models. Finally, the regime forecasts are employed in a dynamic risk control strategy that avoids most losses during the fi nancial crisis and vastly improves risk-adjusted returns.
C53|Market Impact and Performance of Arbitrageurs of Financial Bubbles in An Agent-Based Model|"We analyse the consequences of predicting and exploiting financial bubbles in an agent-based model, with a risky and a risk-free asset and three different trader types: fundamentalists, noise traders and ""dragon riders"" (DR). The DR exploit their ability to diagnose financial bubbles from the endogenous price history to determine optimal entry and exit trading times. We study the DR market impact as a function of their wealth fraction. With a proportion of up to 10%, DR are found to have a beneficial effect, reducing the volatility, value-at-risk and average bubble peak amplitudes. They thus reduce inefficiencies and stabilise the market by arbitraging the bubbles. At larger proportions, DR tend to destabilise prices, as their diagnostics of bubbles become increasingly self-referencing, leading to volatility amplification by the noise traders, which destroys the bubble characteristics that would have allowed them to predict bubbles at lower fraction of wealth. Concomitantly, bubble-based arbitrage opportunities disappear with large fraction of DR in the population of traders."
C53|A Non-Elliptical Orthogonal GARCH Model for Portfolio Selection under Transaction Costs|Covariance matrix forecasts for portfolio optimization have to balance sensitivity to new data points with stability in order to avoid excessive rebalancing. To achieve this, a new robust orthogonal GARCH model for a multivariate set of non-Gaussian asset returns is proposed. The conditional return distribution is multivariate generalized hyperbolic and the dispersion matrix dynamics are driven by the leading factors in a principle component decomposition. Each of these leading factors is endowed with a univariate GARCH structure, while the remaining eigenvalues are kept constant over time. Joint maximum likelihood estimation of all model parameters is performed via an expectation maximization algorithm, and is applicable in high dimensions. The new model generates realistic correlation forecasts even for large asset universes and captures rising pairwise correlations in periods of market distress better than numerous competing models. Moreover, it leads to improved forecasts of an eigenvalue-based financial systemic risk indicator. Crucially, it generates portfolios with much lower turnover and superior risk-adjusted returns net of transaction costs, outperforming the equally weighted strategy even under high transaction fees.
C53|Predicting criminal behavior with Lévy flights using real data from Bogotá|I use residential burglary data from Bogota, Colombia, to fit an agent-based model following truncated L´evy flights (Pan et al., 2018) elucidating criminal rational behavior and validating repeat/near-repeat victimization and broken windows effects. The estimated parameters suggest that if an average house or its neighbors have never been attacked, and it is suddenly burglarized, the probability of a new attack the next day increases, due to the crime event, in 79 percentage points. Moreover, the following day its neighbors will also face an increment in the probability of crime of 79 percentage points. This effect persists for a long time span. The model presents an area under the Cumulative Accuracy Profile (CAP) curve, of 0.8 performing similarly or better than state-of-the-art crime prediction models. Public policies seeking to reduce criminal activity and its negative consequences must take into account these mechanisms and the self-exciting nature of crime to effectively make criminal hotspots safer.
C53|Mind the gap: a multi-country BVAR benchmark for the Eurosystem projections|The Eurosystem staff forecasts are conditional on the financial markets, the global economy and fiscal policy outlook, and include expert judgement. We develop a multi-country BVAR for the four largest countries of the euro area and we show that it provides accurate conditional forecasts of policy relevant variables such as, for example, consumer prices and GDP. The forecasting accuracy and the ability to mimic the path of the Eurosystem projections suggest that the model is a valid benchmark to assess the consistency of the projections with the conditional assumptions. As such, the BVAR can be used to identify possible sources of judgement, based on the gaps between the Eurosystem projections and the historical regularities captured by the model. JEL Classification: C52, C53, E37
C53|Forecasting daily electricity prices with monthly macroeconomic variables|We analyse the importance of macroeconomic information, such as industrial production index and oil price, for forecasting daily electricity prices in two of the main European markets, Germany and Italy. We do that by means of mixed-frequency models, introducing a Bayesian approach to reverse unrestricted MIDAS models (RU-MIDAS). We study the forecasting accuracy for different horizons (from 1 day ahead to 28 days ahead) and by considering different specifications of the models. We find gains around 20% at short horizons and around 10% at long horizons. Therefore, it turns out that the macroeconomic low frequency variables are more important for short horizons than for longer horizons. The benchmark is almost never included in the model confidence set. JEL Classification: C11, C53, Q43, Q47
C53|An analysis of the Eurosystem/ECB projections|The Eurosystem/ECB staff macroeconomic projection exercises constitute an important input to the ECB's monetary policy. This work marks a thorough analysis of the Eurosystem/ECB projection errors by looking at criteria of optimality and rationality using techniques widely employed in the applied literature of forecast evaluation. In general, the results are encouraging and suggest that Eurosystem/ECB staff projections abide to the main characteristics that constitute them reliable as a policy input. Projections of GDP - up to one year - and inflation are optimal - in the case of inflation they are also rational. A main finding is that GDP forecasts can be substantially improved, especially at long horizons. JEL Classification: C53, E37, E58
C53|Preparing for the next MFF: Where did the money go in the past?|This paper presents the state of play of the preparations for the next Multiannual Financial Framework (MFF) of the EU for the period 2021-2027. It then turns to an analysis of the allocation of regional support funding over the last two MFFs, using a standard growth model to interpret the results. It finds that: First, the distribution of Cohesion spending across regions (as proportion of regional GDP) can be explained to a large extent by a few variables, namely income per capita, unemployment and the importance of agriculture. However, there are also important differences across different clusters of regions. Regions in Southern Europe received less funding than those in Central and Eastern Europe even accounting for differences in these determinants. Second, regions in Southern Europe have a relatively high capital/output ratio and thus a lower productivity of capital. Moreover, their investment rates do not seem to be affected by the Structural Funds they receive. These results suggest the need for a change in emphasis from infrastructure investment to measures that improve overall allocation of resources.
C53|Reference Forecasts for CO2 Emissions from Fossil-Fuel Combustion and Cement Production in Portugal|"We provide reference forecasts for CO2 emissions from burning fuel fossil and cement production in Portugal based on an ARFIMA model approach and using annual data from 1950 to 2017. Our ""business as usual"" projections suggest a pattern of decarbonization that will cause the reduction of 3.3 Mt until 2030 and 5.1 Mt between 2030 and 2050. This scenario allows us to assess effort required by the new IPCC goals to ensure carbon neutrality by 2050. For this objective to be achieved it is necessary for emissions to be reduced by 39.6 Mt by 2050. Our results suggest that of these, only 8.4 Mt will result from the inertia of the national emissions system. The remaining reduction on emissions of 31.2 Mt of CO2 will require additional policy efforts. Accordingly, our results suggest that about 79% of the reductions necessary to achieve IPCC goals require deliberate policy efforts. Finally, the presence in the data of long memory with mean reversion suggests that policies must be persistent to ensure that these reductions in emissions are also permanent."
C53|ARFIMA Reference Forecasts for Worldwide CO2 Emissions and the National Dimension of the Policy Efforts to Meet IPCC Targets|We use an ARFIMA approach to develop reference scenario projections for CO2 emissions worldwide and for seven different regions. Our objective is to determine the magnitude of the policy efforts necessary to achieve the IPCC emissions reductions goals. For worldwide emissions, the aggregate policy effort required to achieve the 2050 goals is equivalent to 97.4% of 2010 emissions. This policy effort is frontloaded as about 60% of such efforts would have to occur by 2030. In order to achieve the IPCC target the policy efforts in the cases of the USA, EU(28), Russia, and Japan - which account for 32% of worldwide emissions, are lower and less frontloaded than the IPCC goals themselves. In the case of China, India and the ROW, which account for 68% of worldwide emissions, additional policy efforts are necessary to achieve reductions in emissions of 105.0%, 156.0% and 111.4%, of the 2010 levels, respectively. In the case of India, policy efforts are not only rather severe but also rather dramatically frontloaded, as about 74% of the policy efforts would have to occur by 2030. Our results suggest that the policies toward decarbonization must consider the specific regional characteristics of emissions. Given the differences in the inertia of emissions in the different regions a one-size fits all approach is not the best approach.
C53|Good Carry, Bad Carry|"We distinguish between ""good"" and ""bad"" carry trades constructed from G-10 currencies. The good trades exhibit higher Sharpe ratios and sometimes positive return skewness, in contrast to the bad trades that have both substantially lower Sharpe ratios and highly negative return skewness. Surprisingly, good trades do not involve the most typical carry currencies like the Australian dollar and Japanese yen. The distinction between good and bad carry trades significantly alters our understanding of currency carry trade returns, and invalidates, for example, explanations invoking return skewness and crash risk."
C53|Currency Factors|"We examine the ability of existing and new factor models to explain the comovements of G10- currency changes, measured using the novel concept of ""currency baskets"", representing the overall movement of a particular currency. Using a clustering technique, we find a clear two-block structure in currency comovements with the first block containing mostly the dollar currencies, and the other the European currencies. A factor model incorporating this ""clustering"" factor and two additional factors, a commodity currency factor and a ""world"" factor based on trading volumes, fits currency basket correlations much better than extant factors, such as value and carry, do. In particular, it explains on average about 60% of currency variation and generates a root mean squared error relative to sample correlations of only 0.11. The model also fits comovements in emerging market currencies well. Economically, the correlations between currency baskets underlying the factor structure are inversely related to the physical distances between countries. The factor structure is also related to the exposure of the corresponding pricing kernels with respect to the global pricing kernel and is apparent in cross-country retail sales growth data."
C53|Average Crossing Time: An Alternative Characterization of Mean Aversion and Reversion|We evaluate the properties of mean reversion and mean aversion in asset prices and returns as commonly characterized in the finance literature. The study is undertaken within a class of well-known dynamic stochastic general equilibrium models and shows that the mean reversion/aversion distinction is largely artificial. We then propose an alternative measure, the ‘Average Crossing Time’ that both unifies these concepts and provides an alternative characterization. Ceteris paribus, mean reverting processes have a relatively shorter average crossing time as compared to mean averting processes.
C53|The Promise and Pitfalls of Conflict Prediction: Evidence from Colombia and Indonesia|"Policymakers can take actions to prevent local conflict before it begins, if such violence can be accurately predicted. We examine the two countries with the richest available sub-national data: Colombia and Indonesia. We assemble two decades of fine-grained violence data by type, alongside hundreds of annual risk factors. We predict violence one year ahead with a range of machine learning techniques. Models reliably identify persistent, high-violence hot spots. Violence is not simply autoregressive, as detailed histories of disaggregated violence perform best. Rich socio-economic data also substitute well for these histories. Even with such unusually rich data, however, the models poorly predict new outbreaks or escalations of violence. ""Best case"" scenarios with panel data fall short of workable early-warning systems."
C53|Improving the Accuracy of Economic Measurement with Multiple Data Sources: The Case of Payroll Employment Data|This paper combines information from two sources of U.S. private payroll employment to increase the accuracy of real-time measurement of the labor market. The sources are the Current Employment Statistics (CES) from BLS and microdata from the payroll processing firm ADP. We briefly describe the ADP-derived data series, compare it to the BLS data, and describe an exercise that benchmarks the data series to an employment census. The CES and the ADP employment data are each derived from roughly equal-sized samples. We argue that combining CES and ADP data series reduces the measurement error inherent in both data sources. In particular, we infer “true” unobserved payroll employment growth using a state-space model and find that the optimal predictor of the unobserved state puts approximately equal weight on the CES and ADP-derived series. Moreover, the estimated state contains information about future readings of payroll employment.<br><small>(This abstract was borrowed from another version of this item.)</small>
C53|Machine Learning for Solar Accessibility: Implications for Low-Income Solar Expansion and Profitability|The solar industry in the US typically uses a credit score such as the FICO score as an indicator of consumer utility payment performance and credit worthiness to approve customers for new solar installations. Using data on over 800,000 utility payment performance and over 5,000 demographic variables, we compare machine learning and econometric models to predict the probability of default to credit-score cutoffs. We compare these models across a variety of measures, including how they affect consumers of different socio-economic backgrounds and profitability. We find that a traditional regression analysis using a small number of variables specific to utility repayment performance greatly increases accuracy and LMI inclusivity relative to FICO score, and that using machine learning techniques further enhances model performance. Relative to FICO, the machine learning model increases the number of low-to-moderate income consumers approved for community solar by 1.1% to 4.2% depending on the stringency used for evaluating potential customers, while decreasing the default rate by 1.4 to 1.9 percentage points. Using electricity utility repayment as a proxy for solar installation repayment, shifting from a FICO score cutoff to the machine learning model increases profits by 34% to 1882% depending on the stringency used for evaluating potential customers.
C53|Predicting Returns With Text Data|We introduce a new text-mining methodology that extracts sentiment information from news articles to predict asset returns. Unlike more common sentiment scores used for stock return prediction (e.g., those sold by commercial vendors or built with dictionary-based methods), our supervised learning framework constructs a sentiment score that is specifically adapted to the problem of return prediction. Our method proceeds in three steps: 1) isolating a list of sentiment terms via predictive screening, 2) assigning sentiment weights to these words via topic modeling, and 3) aggregating terms into an article-level sentiment score via penalized likelihood. We derive theoretical guarantees on the accuracy of estimates from our model with minimal assumptions. In our empirical analysis, we text-mine one of the most actively monitored streams of news articles in the financial system—the Dow Jones Newswires—and show that our supervised sentiment model excels at extracting return-predictive signals in this context.
C53|Practising Subnational Public Finance in an Emerging Economy: Fiscal Marksmanship in Kerala|Abstract Our paper analyses the subnational public finance practices in one of the States in India –Kerala- and estimate the fiscal marksmanship. Fiscal marksmanship is the analysis of fiscal forecasting errors. Kerala, though well known for its achievements in human development outcomes, is facing fiscal stress within the rule-based fiscal framework and innovating policy tools to achieve a revenue-led fiscal consolidation. We have examined the Budget Estimates, Revised Estimates and Actuals for the macro-fiscal variables from the Kerala State Budgets, during the period 2011-12 to 2016-17 to analyse the significant deviation between the projections and realizations of the State finances. We found that the magnitude of forecasting errors was relatively significant in case of tax revenue. While partitioning the sources of errors in the budgetary forecasting in Kerala, we found that the random components of the error were larger than the systematic components for all the macro-fiscal variables, except for own revenue, grants and capital expenditure. This has three macro policy implications. One, the volatility in intergovernmental fiscal transfers can affect the stability of finances at subnational level. Two, the State needs to identify innovative policy tools in Additional Resource Mobilisation (ARM) to maintain the human development achievements. Three, within the rule-based fiscal framework, State has to innovate financing strategies for strengthening growth-inducing capital infrastructure formation.
C53|Budget Credibility of Subnational Governments: Analyzing the Fiscal Forecasting Errors of 28 States in India|Budget credibility, the ability of governments to accurately forecast the macro-fiscal variables, is crucial for effective Public Financial Management (PFM). Fiscal marksmanship analysis captures the extent of errors in the budgetary forecasting. The fiscal rules can determine fiscal marksmanship, as effective fiscal consolidation procedure affects the fiscal behaviour of the states in conducting the budgetary forecasts. Against this backdrop, applying Theil's technique, we analyse the fiscal forecasting errors for 28 States (except Telangana) in India for the period 2011-12 to 2015-16. There is a heterogeneity in the magnitude of errors across subnational governments in India. The forecast errors in revenue receipts have been greater than revenue expenditure. Within revenue receipts, the errors are pronounced more significantly in grants component. Within expenditure budgets, the errors in capital spending are found greater than revenue spending in all the States. Partitioning the sources of errors, we identified that the errors were more broadly random than systematic bias, except for a few crucial macro-fiscal variables where improving the forecasting techniques can provide better estimates.
C53|Variational Bayesian inference in large Vector Autoregressions with hierarchical shrinkage|Many recent papers in macroeconomics have used large Vector Autoregressions (VARs) involving a hundred or more dependent variables. With so many parameters to estimate, Bayesian prior shrinkage is vital in achieving reasonable results. Computational concerns currently limit the range of priors used and render difficult the addition of empirically important features such as stochastic volatility to the large VAR. In this paper, we develop variational Bayes methods for large VARs which overcome the computational hurdle and allow for Bayesian inference in large VARs with a range of hierarchical shrinkage priors and with time-varying volatilities. We demonstrate the computational feasibility and good forecast performance of our methods in an empirical application involving a large quarterly US macroeconomic data set.
C53|Measuring Data Uncertainty : An Application using the Bank of England’s “Fan Charts” for Historical GDP Growth|Historical economic data are often uncertain due to sampling and non-sampling errors. But data uncertainty is rarely communicated quantitatively. An exception are the “fan charts” for historical GDP growth published at the Bank of England. We propose a generic loss function based approach to extract from these ex ante density forecasts a quantitative measure of unforecastable data uncertainty. We find GDP data uncertainty in the UK rose sharply at the onset of the 2008/9 recession; and that data uncertainty is positively correlated with popular estimates of macroeconomic uncertainty.
C53|Health Spending Projections to 2030: New results based on a revised OECD methodology|To gain a better understanding of the financial sustainability of health systems, the OECD has produced a new set of health spending projections up to 2030 for all its member countries. Estimates are produced across a range of policy situations. Policy situations analysed include a “base” scenario – estimates of health spending growth in the absence of major policy changes – and a number of alternative scenarios that model the effect on health spending of policies that increase productivity or contribute to better lifestyles; or conversely, ineffective policies that contribute to additional cost pressures on health systems.
C53|Online Estimation of DSGE Models|This paper illustrates the usefulness of sequential Monte Carlo (SMC) methods in approximating DSGE model posterior distributions. We show how the tempering schedule can be chosen adaptively, explore the benefits of an SMC variant we call generalized tempering for “online” estimation, and provide examples of multimodal posteriors that are well captured by SMC methods. We then use the online estimation of the DSGE model to compute pseudo-out-of-sample density forecasts of DSGE models with and without financial frictions and document the benefits of conditioning DSGE model forecasts on nowcasts of macroeconomic variables and interest rate expectations. We also study whether the predictive ability of DSGE models changes when we use priors that are substantially looser than those commonly adopted in the literature.
C53|Power Industry Disruptors and Prospects of the Electricity Demand in the Greater Metro-Manila Area|The power industry is being severely disrupted globally and local industry stakeholders have every reason to be worried. The question is how stakeholder capital should henceforth be deployed to reduce the risk of stranded assets. This study is undertaken to assess the impact of power industry disruptors on the near-term prospect of the electricity demand in the most important submarket of the Philippine power market, the greater Metro-Manila area. The emphasis is on the impact of technology disruptors, especially of solar photovoltaic generation and storage, on top of and in conjunction with policy disruptors. Part One tackles firstly the risks to sustained economic and income growth which will, in turn, impact on the demand for electricity?the macro-economic risks, the global risks, and the policy risks; secondly the risks internal to the electricity industry itself?the technology disruptors especially coming from growing adoption of rooftop and mini-grid solar photovoltaic installations and battery storage. The challenge of solar distributed generation counsels a more sober outlook and a more inclusive portfolio diversification by centralized power generation capitalists. Part Two employs an error correction model to forecast the growth of aggregate and disaggregate (by customer types) demand in a distribution utility franchise, in this case, the Meralco franchise, over the next five years. This model can be adopted as benchmark and adapted by industry stakeholders especially other distribution utility franchises for their own forecasts which should inform the rate setting exercise between the distribution utilities and the regulator, the Energy Regulatory Commission.
C53|Is the United States of America (USA) really being made great again? witty insights from the Box-Jenkins ARIMA approach|Using annual time series data on GDP per capita in the United States of America (USA) from 1960 to 2017, I model and forecast GDP per capita using the Box – Jenkins ARIMA technique. My diagnostic tests such as the ADF tests show that US GDP per capita data is I (2). Based on the AIC, the study presents the ARIMA (0, 2, 2) model. The diagnostic tests further indicate that the presented model is stable and hence reliable. The results of the study reveal that living standards in the US are likely to sky-rocket over the next decade, especially if the current economic policy stance is to be at least maintained. Indeed, America is being made great again!!!
C53|Modeling and forecasting population in Bangladesh: a Box-Jenkins ARIMA approach|Employing annual time series data on total population in Bangladesh from 1960 to 2017, I model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that Bangladesh annual total population is neither I (1) nor I (2) but for simplicity purposes, the researcher has assumed it is I (2). Based on the AIC, the study presents the ARIMA (4, 2, 1) model. The diagnostic tests further indicate that the presented model is very stable and quite reliable. The results of the study reveal that total population in Bangladesh will continue to sharply rise in the next three decades. In order to deal with the threats posed by a large population, 3 policy recommendations have been suggested.
C53|Where is Kenya being headed to? Empirical evidence from the Box-Jenkins ARIMA approach|Using annual time series data on GDP per capita in Kenya from 1960 to 2017, the study analyzes GDP per capita using the Box – Jenkins ARIMA technique. The diagnostic tests such as the ADF tests show that Kenyan GDP per capita data is I (2). Based on the AIC, the study presents the ARIMA (3, 2, 1) model. The diagnostic tests further show that the presented parsimonious model is stable and reliable. The results of the study indicate that living standards in Kenya will improve over the next decade, as long as the prudent macroeconomic management continues in Kenya. Indeed, Kenya’s economy is growing. The study offers 3 policy prescriptions in an effort to help policy makers in Kenya on how to promote and maintain the much needed growth.
C53|Is Nigeria's economy progressing or backsliding? Implications from ARIMA models|Using annual time series data on GDP per capita in Nigeria from 1960 to 2017, I model and forecast GDP per capita using the Box – Jenkins ARIMA technique. My diagnostic tests such as the ADF tests show that Nigerian GDP per capita data is I (1). Based on the AIC, the study presents the ARIMA (2, 1, 0) model. The diagnostic tests further reveal that the presented optimal model is stable and hence reliable. The results of the study indicate that living standards in Nigeria will tumble over the next decade, as long as the current economic policy stance is not reviewed. Indeed, Nigeria’s economy is backsliding again!!! In order to improve the living standards of an ordinary Nigerian, this study has put forward four-fold policy prescriptions.
C53|Time series modeling and forecasting of the consumer price index in Japan|This research uses annual time series data on CPI in Japan from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the X series is I (1). The study presents the ARIMA (1, 1, 0) model for predicting CPI in Japan. The diagnostic tests further imply that the presented optimal model is actually stable and acceptable. The results of the study apparently show that CPI in Japan is likely to continue on an upwards trajectory in the next decade. The study basically encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in Japan.
C53|Forecasting UK consumer price index using Box-Jenkins ARIMA models|This research uses annual time series data on CPI in the UK from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the K series is I (2). The study presents the ARIMA (1, 2, 1) model for predicting CPI in the UK. The diagnostic tests further indicate that the presented optimal model is actually stable and acceptable. The results of the study apparently show that CPI in the UK is likely to continue on a sharp upwards trajectory in the next decade. The study basically encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in the UK.
C53|Forecasting consumer price index in Norway: An application of Box-Jenkins ARIMA models|This research uses annual time series data on CPI in Norway from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the N series is I (2). The study presents the ARIMA (2, 2, 2) model for predicting CPI in Norway. The diagnostic tests further imply that the presented optimal model is actually stable. The results of the study apparently show that CPI in Norway is likely to continue on an upwards trajectory in the next decade. The study basically encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in Norway.
C53|Forecasting Australian CPI using ARIMA models|This research uses annual time series data on CPI in Australia from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the A series is I (1). The study presents the ARIMA (1, 1, 0) model for predicting CPI in Australia. The diagnostic tests further imply that the presented optimal model is stable and acceptable. The results of the study apparently show that CPI in Australia is likely to continue on an upwards trend in the next decade. The study basically encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in Australia.
C53|Predicting CPI in Singapore: An application of the Box-Jenkins methodology|This research uses annual time series data on CPI in Singapore from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the S series is I (1). The study presents the ARIMA (1, 1, 2) model for predicting CPI in Singapore. The diagnostic tests further show that the presented optimal model is actually stable and acceptable. The results of the study apparently show that CPI in Singapore is likely to continue on an upwards trajectory in the next decade. The study basically encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in Singapore.
C53|Time series modeling and forecasting of the consumer price index in Belgium|This research uses annual time series data on CPI in Belgium from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the B series is I (2). The study presents the ARIMA (0, 2, 1) model for predicting CPI in Belgium. The diagnostic tests further imply that the presented optimal model is apparently stable and acceptable. The results of the study apparently show that CPI in Belgium is likely to continue on an upwards trajectory in the next decade. The study basically encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in Belgium.
C53|Understanding CPI dynamics in Canada|This research uses annual time series data on CPI in Canada from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the C series is I (1). The study presents the ARIMA (1, 1, 1) model for predicting CPI in Canada. The diagnostic tests further show that the presented parsimonious model is stable. The results of the study apparently show that CPI in Canada is likely to continue on a sharp upwards trajectory in the next decade. The study encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in Canada.
C53|Predicting CPI in France|This research uses annual time series data on CPI in France from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the F series is I (2). The study presents the ARIMA (1, 2, 0) model for predicting CPI in France. The diagnostic tests further imply that the presented model is stable and acceptable for predicting CPI in France. The results of the study apparently show that CPI in France is likely to continue on an upwards trajectory in the next ten years. The study encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in France.
C53|Forecasting CPI in Sweden|This research uses annual time series data on CPI in Sweden from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the W series is I (1). The study presents the ARIMA (1, 1, 0) model for predicting CPI in Sweden. The diagnostic tests further imply that the presented optimal model is stable as expected. The results of the study apparently show that CPI in Sweden is likely to continue on an upwards trajectory in the next ten years. The study encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in Sweden.
C53|Predicting CPI in Panama|This study uses annual time series data on CPI in Panama from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the P series is I (1). The study presents the ARIMA (1, 1, 0) model for predicting CPI in Panama. The diagnostic tests further imply that the presented optimal model is actually stable and acceptable for forecasting CPI in Panama. The results of the study apparently show that CPI in Panama is likely to continue on an upwards trajectory in the next 10 years. The study encourages policy makers to make use of tight monetary and fiscal policy measures in order to deal with inflation in Panama.
C53|Modeling and forecasting CPI in Myanmar: An application of ARIMA models|This research uses annual time series data on CPI in Myanmar from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the M series is I (2). The study presents the ARIMA (2, 2, 1) model for predicting CPI in Myanmar. The diagnostic tests further imply that the presented optimal model is stable and acceptable in modeling CPI in Myanmar. The results of the study apparently show that CPI in Myanmar is likely to continue on an upwards trajectory in the next decade. The study encourages policy makers to make use of tight monetary and fiscal policy measures in order to deal with inflation in Myanmar.
C53|Analyzing CPI dynamics in Italy|This research uses annual time series data on CPI in Italy from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the T series is I (2). The study presents the ARIMA (0, 2, 1) model for predicting CPI in Italy. The diagnostic tests further imply that the presented optimal model is actually stable and acceptable for predicting CPI in Italy over the period under study. The results of the study apparently show that CPI in Italy is likely to continue on an upwards trajectory in the next decade. The study basically encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in Italy.
C53|Predicting consumer price index in Saudi Arabia|This paper uses annual time series data on CPI in Japan from 1963 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the Y series is I (2). The study presents the ARIMA (0, 2, 1) model for predicting CPI in Saudi Arabia. The diagnostic tests further imply that the presented optimal model is actually stable and acceptable for predicting CPI in Saudi Arabia. The results of the study apparently show that CPI in Saudi Arabia is likely to be relatively high in the next decade. The study encourages policy makers to make use of tight monetary and fiscal policy measures in order to deal with inflation in Saudi Arabia.
C53|Modeling and forecasting CPI in Mauritius|This paper uses annual time series data on CPI in Mauritius from 1963 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the Z series is I (2). The study presents the ARIMA (0, 2, 3) model for predicting CPI in Mauritius. The diagnostic tests further imply that the presented optimal model is actually stable and acceptable for predicting CPI in Mauritius. The results of the study apparently show that CPI in Mauritius is likely to continue on a very sharp upwards trajectory in the next decade. The study basically encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in Mauritius.
C53|Can Algeria be the first African country to outsmart the Malthusian population trap? Insights from the ARIMA approach|Using annual time series data on total population in Algeria from 1960 to 2017, we model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that Algeria annual total population is I (2). Based on the AIC, the study presents the ARIMA (4, 2, 0) model as the optimal model. The diagnostic tests further show that the presented model is stable and that its residuals are integrated of order zero. The results of the study reveal that total population in Algeria will continue to rise gradually in the next three decades and in 2050 Algeria’s total population will be approximately 62 million people. In order to outsmart the Malthusian population trap, 4 policy prescriptions have been suggested for consideration by the government of Algeria.
C53|Prediction of Inflation in Algeria using ARIMA models|This research uses annual time series data on inflation rates in Algeria from 1970 to 2017, to model and forecast inflation using ARIMA models. Diagnostic tests indicate that A is I(1). The study presents the ARIMA (1, 1, 1). The diagnostic tests further imply that the presented optimal ARIMA (1, 1, 1) model is stable and acceptable for predicting inflation in Algeria. The results of the study apparently show that A will ranging between 4.9% and 5.2% over the out-of-sample period. Monetary authorities in Algeria are expected to tighten Algeria’s monetary policy in order to maintain price stability.
C53|Understanding inflation trends in Israel: A univariate approach|This paper uses annual time series data on inflation in Israel from 1960 to 2017, to model and forecast inflation using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that Q is I (1). The study presents the ARIMA (1, 1, 2) model for predicting inflation in Israel. The diagnostic tests further show that the presented parsimonious model is stable and acceptable for predicting inflation in Israel. The results of the study apparently show that inflation in Israel is likely to be hovering around 1.6% over the next decade. Basically, the study encourages the Bank of Israel to continue being transparent and independent in order to retain credibility and boost its ability to engineer successful macroeconomic policy actions.
C53|Modeling and forecasting inflation in Lesotho using Box-Jenkins ARIMA models|This research uses annual time series data on inflation rates in Lesotho from 1974 to 2017, to model and forecast inflation using ARIMA models. Diagnostic tests indicate that L is I(1). The study presents the ARIMA (0, 1, 2). The diagnostic tests further imply that the presented optimal ARIMA (0, 1, 2) model is stable and acceptable for predicting inflation in Lesotho. The results of the study apparently show that L will be approximately 5.2% over the out-of-sample forecast period. The CBL is expected to tighten Lesotho’s monetary policy in order to maintain price stability.
C53|Modeling and forecasting inflation in Philippines using ARIMA models|This research uses annual time series data on inflation rates in the Philippines from 1960 to 2017, to model and forecast inflation using ARIMA models. Diagnostic tests indicate that P is I(1). The study presents the ARIMA (1, 1, 3). The diagnostic tests further imply that the presented optimal ARIMA (1, 1, 3) model is stable and acceptable for predicting inflation in the Philippines. The results of the study apparently show that P will fall down from 5.6% in 2018 to approximately 0.3% in 2027. The Bangko Sentral ng Pilipinas is expected to continue implementing it inflation targeting policy framework since it proves to work well for the economy.
C53|Predicting inflation in Senegal: An ARMA approach|This research uses annual time series data on inflation rates in Senegal from 1968 to 2017, to model and forecast inflation using ARMA models. Diagnostic tests indicate that the inflation rate series is I(0). The study presents the ARMA (1, 0, 0) model, which is equivalent to an AR (1) model. The diagnostic tests further imply that the presented optimal ARMA (1, 0, 0) model is stable and acceptable for forecasting inflation rates in Senegal. The results of the study apparently show that inflation will be approximately 4.7% by 2020. Policy makers and the business community in Senegal are expected to take advantage of the anticipated stable inflation rates over the next decade.
C53|Predicting inflation in Sri Lanka using ARMA models|This research uses annual time series data on inflation rates in Sri Lanka from 1960 to 2017, to model and forecast inflation using ARMA models. Diagnostic tests indicate that S is I(0). The study presents the ARMA model (1, 0, 0) [or simply AR (1) process] for forecasting inflation rates in Sri Lanka. The diagnostic tests further imply that the presented optimal ARMA (1, 0, 0) model is not only stable but also suitable. The results of the study apparently show that S will be approximately 8.17% by 2020. Policy makers and the business community in Sri Lanka are expected to take advantage of the anticipated stable inflation rates over the next decade.
C53|Forecasting total population in Yemen using Box-Jenkins ARIMA models|Using annual time series data on total population in Yemen from 1960 to 2017, we model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that Yemen annual total population is neither I (1) nor I (2) but for simplicity purposes, the researcher has assumed it is I (2). Based on the AIC, the study presents the ARIMA (10, 2, 0) model as the best model. The diagnostic tests further indicate that the presented model is indeed stable and its residuals are stationary in levels. The results of the study reveal that total population in Yemen will continue to rise sharply in the next three decades and in 2050 Yemen’s total population will be approximately 52 million people. In order to benefit from an increase in total population in Yemen, 4 policy recommendations have been suggested for consideration by policy makers in Yemen.
C53|A Box-Jenkins ARIMA approach to the population question in Pakistan: A reliable prognosis|Employing annual time series data on total population in Pakistan from 1960 to 2017, we model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Based on the minimum AIC and Theil’s U, the study presents the ARIMA (3, 2, 1) model. The diagnostic tests indicate that the presented model is stable. The results of the study reveal that total population in Pakistan will continue to sharply rise within the next three decades, for up to approximately 324 million people by 2050. In order to address the threats posed by such a population explosion, 3 policy recommendations have been put forward.
C53|Where is Eritrea going in terms of population growth? Insights from the ARIMA approach|Employing annual time series data on total population in Eritrea from 1960 to 2011, we model and forecast total population over the next 39 years using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that Eritrea annual total population is I (2). Based on the AIC, the study presents the ARIMA (2, 2, 1) model as the best model. The diagnostic tests further indicate that the presented model is quite stable. The results of the study establishes that total population in Eritrea will gradually rise in the next 39 years and in 2050 Eritrea’s total population will be approximately 7.6 million people. In order to take advantage of the expected increase in total population in Eritrea, 3 policy recommendations have been proposed.
C53|Predicting total population in India: A Box-Jenkins ARIMA approach|Employing annual time series data on total population in India from 1960 to 2017, we model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests show that Indian annual total population data is I (2). Based on both the AIC and Theil’s U, the study presents the ARIMA (1, 2, 3) model. The diagnostic tests further confirm that the presented model is stable and quite acceptable. The results of the study reveal that total population in India will continue to sharply rise in the next three decades, thereby posing a threat to both natural and non-renewable resources. In order to deal with the threats posed by a large population in India, the study recommends family planning practices amongst other policy prescriptions.
C53|Forecasting the population of Brazil using the Box-Jenkins ARIMA approach|Employing annual time series data on total population in Brazil from 1960 to 2017, we model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that Brazil annual total population is non-stationary in all levels; for simplicity purposes, the study has assumed that the POP series is I (2). Based on the AIC, the study presents the ARIMA (6, 2, 0) model as the optimal model. The diagnostic tests further indicate that the presented model is stable and that its residuals are stationary. The results of the study reveal that total population in Brazil will continue to rise in the next three decades and in 2050 Brazil’s total population will be approximately 256 million people. Four policy prescriptions have been suggested for consideration by the government of Brazil.
C53|What will be China's population in 2050? Evidence from the Box-Jenkins approach|Employing annual time series data on total population in China from 1960 to 2017, we model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that China annual total population is I (2). Based on the AIC, the study presents the ARIMA (0, 2, 1) model as the optimal model. The diagnostic tests further indicate that the presented model is indeed stable although its residuals are non-stationary. The results of the study reveal that total population in China will continue to rise in the next three decades and in 2050 China’s total population will be approximately 1.6 billion people. Three policy recommendations have been suggested for consideration by the Chinese government.
C53|Addressing the population question in Mexico: A Box-Jenkins ARIMA approach|Employing annual time series data on total population in Mexico from 1960 to 2017, we model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that Mexico annual total population is I (2). Based on the AIC, the study presents the ARIMA (3, 2, 1) model as the optimal model. The diagnostic tests further reveal that the presented model is stable and that its residuals are stationary. The results of the study show that total population in Mexico will continue to rise in the next three decades and in 2050 Mexico’s total population will be approximately 180 million people. Three policy prescriptions have been proposed for consideration by the government of Mexico.
C53|Is South Africa the South Africa we all desire? Insights from the Box-Jenkins ARIMA approach|Using annual time series data on GDP per capita in South Africa from 1960 to 2017, the study investigates GDP per capita using the Box – Jenkins ARIMA technique. The diagnostic tests such as the ADF tests show that South African GDP per capita data is I (1). Based on the AIC, the study presents the ARIMA (0, 1, 1) model. The diagnostic tests further show that the presented parsimonious model is indeed stable and quite reliable. The results of the study indicate that living standards in South Africa may improve but very slowly over the next decade, unless prudent macroeconomic management practices are exercised. The paper offers 5 main policy prescriptions in an effort to help policy makers in South Africa on how to promote and maintain the much awaited growth and development.
C53|ARIMA modeling and forecasting of Consumer Price Index (CPI) in Germany|This paper uses annual time series data on CPI in Germany from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the GC series is I (1). The study presents the ARIMA (1, 1, 1) model for predicting CPI in Germany. The diagnostic tests further show that the presented parsimonious model is stable and acceptable for predicting CPI in Germany. The results of the study apparently show that CPI in Germany is likely to continue on an upwards trajectory in the next decade. The study encourages policy makers to make use of tight monetary and fiscal policy measures in order to deal with inflation in Germany.
C53|Forecasting inflation in Burkina Faso using ARMA models|This research uses annual time series data on inflation rates in Burkina Faso from 1960 to 2017, to model and forecast inflation using ARMA models. Diagnostic tests indicate that B is I(0). The study presents the ARMA (2, 0, 0) model, which is nothing but an AR (2) model. The diagnostic tests further imply that the presented optimal ARMA (2, 0, 0) model is stable and acceptable. The results of the study apparently show that W will be approximately 4% by 2020. Policy makers and the business community in Burkina Faso are expected to take advantage of the anticipated stable inflation rates over the next decade.
C53|Modeling and forecasting inflation in Burundi using ARIMA models|This research uses annual time series data on inflation rates in Burundi from 1966 to 2017, to model and forecast inflation using ARIMA models. Diagnostic tests indicate that B is I(1). The study presents the ARIMA (0, 1, 1). The diagnostic tests further imply that the presented optimal ARIMA (0, 1, 1) model is stable and acceptable for predicting inflation in Burundi. The results of the study apparently show that B will be approximately 9.4% by 2020. Policy makers, particulary, monetary authorities in Burundi are expected to tighten Burundi’s monetary policy in order to restore price stability.
C53|ARIMA modeling and forecasting of inflation in Egypt (1960-2017)|This research uses annual time series data on inflation rates in Egypt from 1960 to 2017, to model and forecast inflation using ARIMA models. Diagnostic tests indicate that E is I(1). The study presents the ARIMA (0, 1, 1). The diagnostic tests further imply that the presented optimal ARIMA (0, 1, 1) model is stable and acceptable for predicting inflation in Egypt. The results of the study apparently show that E will be approximately 23.3% over the out-of-sample forecast period. The CBE is expected to continue tightening Egypt’s monetary policy in order to restore price stability.
C53|Understanding inflation trends in Finland: A univariate approach|This research uses annual time series data on inflation rates in Finland from 1960 to 2017, to model and forecast inflation using ARIMA models. Diagnostic tests indicate that F is I(1). The study presents the ARIMA (1, 1, 3) model. The diagnostic tests further imply that the presented optimal ARIMA (1, 1, 3) model is stable and acceptable in predicting Finnish inflation. The results of the study apparently show that F will be hovering around 1% over the next 10 years. Policy makers and the business community in Finland are expected to take advantage of the anticipated stable inflation rates over the next decade.
C53|Inflation dynamics in Jamaica: Evidence from the ARMA methodology|This research uses annual time series data on inflation rates in Jamaica from 1968 to 2017, to model and forecast inflation using ARMA models. Diagnostic tests indicate that JINF is I(0). The study presents the ARMA (1, 0, 0) model, which is the same as an AR (1) process. The diagnostic tests further imply that the presented optimal ARMA (1, 0, 0) model is stable and acceptable for forecasting inflation rates in Jamaica. The results of the study apparently show that JINF will be approximately 11.42% by 2020. Policy makers in Jamaica are expected to the take the necessary action with regards to maintaining a low and stable inflation rate over the next decade and even beyond.
C53|Inflation dynamics in Niger unlocked: An ARMA approach|This research uses annual time series data on inflation rates in Niger from 1964 to 2017, to model and forecast inflation using ARMA models. Diagnostic tests indicate that N is I(0). The study presents the ARMA (1, 0, 0) model, which is simply an AR (1) model. The diagnostic tests further imply that the presented optimal ARMA (1, 0, 0) model is stable. The results of the study apparently show that N will be approximately 4.3% by 2020. Policy makers and the business community in Niger are expected to take advantage of the anticipated stable inflation rates over the next decade.
C53|Understanding inflation patterns in Thailand: An ARMA approach|This research uses annual time series data on inflation rates in Thailand from 1960 to 2017, to model and forecast inflation using ARMA models. Diagnostic tests indicate that T is I(0). The study presents the ARMA (0, 0, 1) model, which is nothing but an MA (1) process. The diagnostic tests further imply that the presented optimal ARMA (0, 0, 1) model is stable and acceptable. The results of the study apparently show that T will be approximately 4.2% by 2020. Policy makers and the business community in Thailand are expected to take advantage of the anticipated stable inflation rates over the next decade.
C53|Predicting inflation in the Kingdom of Bahrain using ARIMA models|This research uses annual time series data on inflation rates in the Kingdom of Bahrain from 1966 to 2017, to model and forecast inflation using ARIMA models. Diagnostic tests indicate that Bahrain inflation series is I(1). The study presents the ARIMA (0, 1, 1). The diagnostic tests further imply that the presented optimal ARIMA (0, 1, 1) model is stable and acceptable for predicting inflation in the Kingdom of Bahrain. The results of the study apparently show that predicted inflation will be approximately 1.5% by 2020. Policy makers and the business community in the Kingdom of Bahrain are expected to take advantage of the anticipated stable inflation rates over the next decade.
C53|Modeling and forecasting CPI in Iran: A univariate analysis|This paper uses annual time series data on CPI in Iran from 1960 to 2017, to model and forecast CPI using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the I series is I (2). The study presents the ARIMA (1, 2, 1) model for predicting CPI in Iran. The diagnostic tests further imply that the presented optimal model is actually stable and acceptable for predicting CPI in Iran. The results of the study apparently show that CPI in Iran is likely to continue on an upwards trajectory in the next ten years. The study basically encourages Iranian policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in Iran.
C53|Uncovering inflation dynamics in Morocco: An ARIMA approach|This research uses annual time series data on inflation rates in Morocco from 1960 to 2017, to model and forecast inflation using ARIMA models. Diagnostic tests indicate that M is I(1). The study presents the ARIMA (0, 1, 1) model. The diagnostic tests further imply that the presented optimal ARIMA (0, 1, 1) model is stable and acceptable in forecasting inflation in Morocco. The results of the study apparently show that M will be hovering somewhere around 1.1% over the next decade. Policy makers and the business community in Morocco are expected to take advantage of the anticipated stable inflation rates over the next decade.
C53|Box-Jenkins ARIMA approach to predicting total population in Russia|Employing annual time series data on total population in Russia from 1960 to 2017, we model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that Russia annual total population is I (2). Based on the AIC, the study presents the ARIMA (1, 2, 1) model as the optimal model. The diagnostic tests further indicate that the presented model is quite stable and that its residuals are stationary as well. The results of the study reveal that total population in Russia will continue to rise, but slowly, in the next three decades and in 2050 Russia’s total population will be approximately 147 million people. Three policy prescriptions have been suggested for consideration by the government of the federation of Russia.
C53|Somalia population dynamics versus the Malthusian population trap: What does the ARIMA approach tell us?|Using annual time series data on total population in Somalia from 1960 to 2017, we model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that Somalia annual total population is basically I (2). Based on the AIC, the study presents the ARIMA (7, 2, 1) model as the most parsimonious model. The diagnostic tests further show that the presented model is really stable and that its residuals are I (0). The results of the study reveal that total population in Somalia will continue to rise sharply in the next three decades and in 2050 Somalia’s total population will be approximately 28 million people. In order to circumvent the chances of being a victim of the Malthusian population trap, 4 policy recommendations have been put forward for consideration by the government of Somalia.
C53|Modeling and forecasting inflation in Tanzania using ARIMA models|This research uses annual time series data on inflation rates in Tanzania from 1966 to 2017, to model and forecast inflation using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the T series is I (1). The study presents the ARIMA (1, 1, 2) model for predicting inflation in Tanzania. The diagnostic tests further imply that the presented optimal model is actually stable and acceptable for predicting inflation in Tanzania. The results of the study apparently show that inflation in Tanzania is likely to continue on an upwards trajectory in the next decade. The study basically encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in Tanzania.
C53|Will the United States of America (USA) be a beneficiary of the Alburg (1998) and Becker et al (1999) prophecies? Recent insights from the Box-Jenkins ARIMA approach|Employing annual time series data on total population in the USA from 1960 to 2017, we model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA approach. Diagnostic tests show that USA annual total population data is I (2). Based on the AIC, the study presents the ARIMA (0, 2, 3) model. The diagnostic tests indicate that the presented model is very stable and quite suitable. The results of the study reveal that total population in USA will continue to sharply rise in the next three decades. Considering a highly educated labor force, coupled with latest technological advancements, USA is likely to be one of the first beneficiaries of the Ahlburg (1998) and Becker et al (1999) prophecies. In order to stay in the realm of the aforementioned prophecies, USA should take note of the 3-fold policy recommendations put forward.
C53|Understanding inflation dynamics in the United States of America (USA): A univariate approach|This paper uses annual time series data on inflation rates in the USA from 1960 to 2016, to model and forecast inflation using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the US inflation series is I (1). The study presents the ARIMA (2, 1, 1) model for predicting inflation in the US. The diagnostic tests further show that the presented parsimonious model is stable and acceptable for predicting annual inflation rates in the US. The results of the study apparently show that inflation in the US is likely to be less than 2% over the out-of-sample forecast period (i.e 10 years). The study encourages policy makers to make use of tight monetary policy measures in order to maintain price stability in the US.
C53|Modeling and forecasting remittances in Bangladesh using the Box-Jenkins ARIMA methodology|This paper uses annual time series data on remittances into Bangladesh from 1976 to 2017, to model and forecast remittances using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that REM is I (2). The study presents the ARIMA (2, 2, 0) model for predicting remittances in Bangladesh. The diagnostic tests further show that the presented parsimonious model is stable and acceptable for predicting remittances in Bangladesh. The results of the study apparently show that remittances inflows into Bangladesh are on a downwards trajectory. The paper suggests the need for strengthening Bangladesh’s emigration policy in order to improve remittances inflows into Bangladesh.
C53|Forecasting the capacity of mobile networks|The optimization of mobile network capacity usage is an essential operation to promote positive returns on network investments, prevent capacity bottlenecks, and deliver good end user experience. This study examines the performance of several statistical models to predict voice and data traffic in a mobile network. While no method dominates the others across all time series and prediction horizons, exponential smoothing and ARIMA models are good alternatives to forecast both voice and data traffic. This analysis shows that network managers have at their disposal a set of statistical tools to plan future capacity upgrades with the most effective solution, while optimizing their investment and maintaining good network quality.
C53|Forecasting bubbles with mixed causal-noncausal autoregressive models|This paper investigates one-step ahead density forecasts of mixed causal-noncausal models. We compare the sample-based and the simulations-based approaches respectively developed by Gouriéroux and Jasiak (2016) and Lanne, Luoto, and Saikkonen (2012). We focus on explosive episodes and therefore on predicting turning points of bubbles bursts. We suggest the use of both methods to construct investment strategies based on how much probabilities are induced by the assumed model and by past behaviours. We illustrate our analysis on Nickel prices series.
C53|TF-MIDAS: a new mixed-frequency model to forecast macroeconomic variables|This paper tackles the mixed-frequency modeling problem from a new perspective. Instead of drawing upon the common distributed lag polynomial model, we use a transfer function representation to develop a new type of models, named TF-MIDAS. We derive the theoretical TF-MIDAS implied by the high-frequency VARMA family models and as a function of the aggregation scheme (flow and stock). This exact correspondence leads to potential gains in terms of nowcasting and forecasting performance against the current alternatives. A Monte Carlo simulation exercise confirms that TF-MIDAS beats UMIDAS models in terms of out-of-sample nowcasting performance for several data generating high-frequency processes.
C53|Monotonic Estimation for the Survival Probability over a Risk-Rated Portfolio by Discrete-Time Hazard Rate Models|Monotonic estimation for the survival probability of a loan in a risk-rated portfolio is based on the observation arising, for example, from loan pricing that a loan with a lower credit risk rating is more likely to survive than a loan with a higher credit risk rating, given the same additional risk covariates. Two probit-type discrete-time hazard rate models that generate monotonic survival probabilities are proposed in this paper. The first model calculates the discrete-time hazard rate conditional on systematic risk factors. As for the Cox proportion hazard rate model, the model formulates the discrete-time hazard rate by including a baseline component. This baseline component can be estimated outside the model in the absence of model covariates using the long-run average discrete-time hazard rate. This results in a significant reduction in the number of parameters to be otherwise estimated inside the model. The second model is a general form model where loan level factors can be included. Parameter estimation algorithms are also proposed. The models and algorithms proposed in this paper can be used for loan pricing, stress testing, expected credit loss estimation, and modeling of the probability of default term structure.
C53|Monotonic Estimation for Probability Distribution and Multivariate Risk Scales by Constrained Minimum Generalized Cross-Entropy|Minimum cross-entropy estimation is an extension to the maximum likelihood estimation for multinomial probabilities. Given a probability distribution {r_i }_(i=1)^k, we show in this paper that the monotonic estimates {p_i }_(i=1)^k for the probability distribution by minimum cross-entropy are each given by the simple average of the given distribution values over some consecutive indexes. Results extend to the monotonic estimation for multivariate outcomes by generalized cross-entropy. These estimates are the exact solution for the corresponding constrained optimization and coincide with the monotonic estimates by least squares. A non-parametric algorithm for the exact solution is proposed. The algorithm is compared to the “pool adjacent violators” algorithm in least squares case for the isotonic regression problem. Applications to monotonic estimation of migration matrices and risk scales for multivariate outcomes are discussed.
C53|Forecasting Realized Volatility of Russian stocks using Google Trends and Implied Volatility|This work proposes to forecast the Realized Volatility (RV) and the Value-at-Risk (VaR) of the most liquid Russian stocks using GARCH, ARFIMA and HAR models, including both the implied volatility computed from options prices and Google Trends data. The in-sample analysis showed that only the implied volatility had a significant effect on the realized volatility across most stocks and estimated models, whereas Google Trends did not have any significant effect. The outof-sample analysis highlighted that models including the implied volatility improved their forecasting performances, whereas models including internet search activity worsened their performances in several cases. Moreover, simple HAR and ARFIMA models without additional regressors often reported the best forecasts for the daily realized volatility and for the daily Value-at-Risk at the 1 % probability level, thus showing that efficiency gains more than compensate any possible model misspecifications and parameters biases. Our empirical evidence shows that, in the case of Russian stocks, Google Trends does not capture any additional information already included in the implied volatility.
C53|IFRS9 Expected Credit Loss Estimation: Advanced Models for Estimating Portfolio Loss and Weighting Scenario Losses|Estimation of portfolio expected credit loss is required for IFRS9 regulatory purposes. It starts with the estimation of scenario loss at loan level, and then aggregated and summed up by scenario probability weights to obtain portfolio expected loss. This estimated loss can vary significantly, depending on the levels of loss severity generated by the IFSR9 models, and the probability weights chosen. There is a need for a quantitative approach for determining the weights for scenario losses. In this paper, we propose a model to estimate the expected portfolio losses brought by recession risk, and a quantitative approach for determining the scenario weights. The model and approach are validated by an empirical example, where we stress portfolio expected loss by recession risk, and calculate the scenario weights accordingly.
C53|Heterogeneous component multiplicative error models for forecasting trading volumes|We propose a novel approach to modelling and forecasting high frequency trading volumes. The new model extends the Component Multiplicative Error Model of Brownlees et al. (2011) by introducing a more flexible specification of the long-run component. This uses an additive cascade of MIDAS polynomial filters, moving at different frequencies, in order to reproduce the changing long-run level and the persistent autocorrelation structure of high frequency trading volumes. After investigating its statistical properties, the merits of the proposed approach are illustrated by means of an application to six stocks traded on the XETRA market in the German Stock Exchange.
C53|What will be Botswana's population in 2050? Evidence from the Box-Jenkins approach|Employing annual time series data on total population in Botswana from 1960 to 2017, I model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that Botswana annual total population is neither I (1) nor I (2) but for simplicity purposes, the researcher has assumed it is I (2). Based on the AIC, the study presents the ARIMA (3, 2, 1) model as the optimal model. The diagnostic tests further indicate that the presented model is indeed stable. The results of the study reveal that total population in Botswana will continue to rise in the next three decades and in 2050 Botswana’s total population will be approximately 3 665 140 people. In order to benefit from an increase in total population in Botswana, 3 policy recommendations have been suggested.
C53|Can Afghanistan be a victim of the Malthusian population trap? what does the ARIMA approach tell us?|Using annual time series data on total population in Afghanistan from 1960 to 2017, we model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that Afghanistan annual total population is I (1). Based on the AIC, the study presents the ARIMA (1, 1, 2) model as the best model. The diagnostic tests further show that the presented model is stable and that its residuals are I (0). The results of the study reveal that total population in Afghanistan will continue to rise gradually in the next three decades and in 2050 Afghanistan’s total population will be approximately 51 million people. In order to circumvent the chances of being a victim of the Malthusian population trap, 4 policy prescriptions have been suggested for consideration by the government of Afghanistan.
C53|Understanding inflation dynamics in the Kingdom of Eswatini: a univariate approach|This research uses annual time series data on inflation rates in the Kingdom of Eswatini from 1966 to 2017, to model and forecast inflation using the Box – Jenkins ARIMA technique. Diagnostic tests indicate that the H series is I (1). The study presents the ARIMA (0, 1, 1) model for predicting inflation in the Kingdom of Eswatini. The diagnostic tests further imply that the presented optimal model is actually stable and acceptable for predicting inflation in the Kingdom of Eswatini. The results of the study apparently show that inflation in the Kingdom of Eswatini is likely to continue on an upwards trajectory in the next decade. The study basically encourages policy makers to make use of tight monetary and fiscal policy measures in order to control inflation in the Kingdom of Eswatini.
C53|Modeling and forecasting inflation in The Gambia: an ARMA approach|This research uses annual time series data on inflation rates in The Gambia from 1962 to 2016, to model and forecast inflation using ARMA models. Diagnostic tests indicate that G is I(0). The study presents the ARMA (1, 0, 0) model [which is nothing but an AR (1) model]. The diagnostic tests further imply that the presented optimal ARMA (1, 0, 0) model is stable and indeed acceptable. The results of the study apparently show that G will be approximately 7.88% by 2020. Policy makers and the business community in The Gambia are expected to take advantage of the anticipated stable inflation rates over the next decade.
C53|Demystifying inflation dynamics in Rwanda: an ARMA approach|This research uses annual time series data on inflation rates in Rwanda from 1967 to 2017, to model and forecast inflation over the next decade using ARMA models. Diagnostic tests indicate that W is I(0). The study presents the ARMA (3, 0, 0) model [which is nothing but an AR (3) model]. The diagnostic tests further imply that the presented optimal ARMA (3, 0, 0) model is stable and acceptable. The results of the study apparently show that W will be approximately 7.45% by 2020. Policy makers and the business community in Rwanda are expected to take advantage of the anticipated stable inflation rates over the next decade.
C53|Prediction of total population in Togo using ARIMA models|Using annual time series data on total population in Togo from 1960 to 2017, we model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that Togo annual total population is neither I (1) nor I (2) but for simplicity purposes, the researcher has assumed it is I (2). Based on the AIC, the study presents the ARIMA (3, 2, 0) model as the best model. The diagnostic tests further indicate that the presented model is stable. The results of the study reveal that total population in Togo will continue to rise in the next three decades and in 2050 Togo’s total population will be approximately 14.2 million people. In order to benefit from an increase in total population in Togo, 3 policy recommendations have been suggested for consider by the government of Togo.
C53|Modeling and forecasting carbon dioxide emissions in China using Autoregressive Integrated Moving Average (ARIMA) models|This research uses annual time series data on CO2 emissions in China from 1960 to 2017, to model and forecast CO2 using the Box – Jenkins ARIMA approach. Diagnostic tests indicate that China CO2 emission data is I (2). The study presents the ARIMA (1, 2, 1) model. The diagnostic tests further imply that the presented best model is stable and hence acceptable for predicting carbon dioxide emissions in China. The results of the study reveal that CO2 emissions in China are likely to increase and thereby exposing China to a plethora of climate change related challenges. 4 main policy prescriptions have been put forward for consideration by the Chinese government.
C53|Population dynamics in Gambia: an ARIMA approach|Employing annual time series data on total population in Gambia from 1960 to 2017, I model and forecast total population over the next 3 decades using the Box – Jenkins ARIMA technique. Diagnostic tests such as the ADF tests show that Gambia annual total population is I (2). Based on the AIC, the study presents the ARIMA (3, 2, 1) model and our diagnostic tests also indicate that the presented model is stable. The results of the study reveal that total population in Gambia will continue to gradually rise in the next three decades. In order to take advantage of the expected increase in total population in Gambia, 4 policy recommendations have been proposed for consideration by the Gambian policy makers.
C53|Foreign Direct Investment (FDI) dynamics in India: what do ARIMA models tell us?|Using annual time series data on net FDI inflows in India from 1960 to 2017, the study examines net FDI inflows using the Box – Jenkins ARIMA methodology. The ADF tests reflect that India FDI net FDI inflows data is I (1). Based on the AIC, the study presents the ARIMA (1, 1, 0) model. The diagnostic tests further show that the presented parsimonious model is not only stable but also suitable for explaining net FDI dynamics in India. The results of the study indicate that, net FDI inflows in India are likely to weaken over the next 10 years. The study identifies two (2) significant policy recommendations in an effort to aid policy makers on how to promote and stimulate the much expected net FDI inflows in India.
C53|Modeling and forecasting Botswana's Growth Domestic Product (GDP) per capita|Using annual time series data on GDP per capita in Botswana from 1960 to 2017, the study analyzes GDP per capita using the Box – Jenkins ARIMA methodology. The diagnostic tests such as the ADF tests show that Botswana GDP per capita data is I (1). Based on the AIC, the study presents the ARIMA (3, 2, 3) model. The diagnostic tests further show that the presented model is not only stable but also suitable. The results of the study indicate that living standards in Botswana will definitely continue to improve over the next decade. Indeed, Botswana’s success story is a reality. The study offers 4 policy recommendations in an effort to help policy makers in Botswana on how to promote and maintain the much needed better living standards for all Batswana.
C53|Short-term forecasting of the US unemployment rate|This paper aims to assess whether Google search data is useful when predicting the US unemployment rate among other more traditional predictor variables. A weekly Google index is derived from the keyword “unemployment” and is used in diffusion index variants along with the weekly number of initial claims and monthly estimated latent factors. The unemployment rate forecasts are generated using MIDAS regression models that take into account the actual frequencies of the predictor variables. The forecasts are made in real-time and the forecasts of the best forecasting models exceed, for the most part, the root mean squared forecast error of two benchmarks. However, as the forecasting horizon increases, the forecasting performance of the best diffusion index variants decreases over time, which suggests that the forecasting methods proposed in this paper are most useful in the short-term.
C53|On Trust Dynamics of Economic Growth|Trust among individuals in society may have various economic and social implications. Though, worldwide data on economic growth rarely consider trust as an ingredient in manipulating economic outcomes. Thus, we include trust instigating from individual, affecting community and state thus, forming trust-based economy. In order to explore the relationship of trust with growth and its benefits implications, this study suggests a model which is validated by Markov process. Consequently, results indicate significant impact of trust on economic growth by achieving convergence in very few iterations in the case of trust-based economy. On the other hand, economy with lowest trust level shows delayed convergence and takes around 4 times more iterations to attain equilibrium. Additionally, socio-economic benefits are more visible in a trust-based economy.
C53|A Consumption-Based Approach to Exchange Rate Predictability|This paper provides evidence of short-run predictability for the real exchange rate by performing out-of-sample tests of a forecasting equation which is derived from a consumption-based asset pricing model. In this model, the real exchange rate is predictable as a result of the implications of preferences with habit persistence on the pricing of international assets. The implied predictors are: domestic, US and world consumption growth. Empirical exercises show evidence of short-term predictability on the bilateral rates of 15 out of 17 countries vis-à-vis the US over the post Bretton-Woods float. A GMM estimation of the parameters of the model also finds evidence of the presence of habits in consumers’ preferences.
C53|Oil price volatility forecasts: What do investors need to know?|Contrary to the current practice that mainly considers stand-alone statistical loss functions, the aim of the paper is to assess oil price volatility forecasts based on objective-based evaluation criteria, given that different forecasting models may exhibit superior performance at different applications. To do so, we forecast implied and several intraday volatilities and we evaluate them based on financial decisions for which these forecasts are used. In this study we confine our interest on the use of such forecasts from financial investors. More specifically, we consider four well established trading strategies, which are based on volatility forecasts, namely (i) trading the implied volatility based on the implied volatility forecasts, (ii) trading implied volatility based on intraday volatility forecasts, (iii) trading straddles in the United States Oil Fund ETF and finally (iv) trading the United States Oil Fund ETF based on implied and intraday volatility forecasts. We evaluate the after-cost profitability of each forecasting model for 1-day up to 66-days ahead. Our results convincingly show that our forecasting framework is economically useful, since different models provide superior after-cost profits depending on the economic use of the volatility forecasts. Should investors evaluate the forecasting models based on statistical loss functions, then their financial decisions would be sub-optimal. Thus, we maintain that volatility forecasts should be evaluated based on their economic use, rather than statistical loss functions. Several robustness tests confirm these findings.
C53|Counter sanctions and well-being population of Russia: econometric analyses|This article examines the impact of counter-sanctions on the welfare of Russia’s population. We build a multiple-choice model and calculate the probability of being in a particular group of well-being based on the price (cost) of consumed counter-sanctions goods. The next step is the construction of a structural demand-supply system for estimating similar domestic good’s production elasticities. By knowing elasticity estimates we determine the price response to particular import closure. According to our estimates Russia's counter-sanctions led to an increase in poverty by 2.64 %.
C53|Day-ahead electricity price forecasting with emphasis on its volatility in Iran (GARCH combined with ARIMA models)|This paper provides a method to forecast day-ahead electricity prices based on autoregressive integrated moving average (ARIMA) and generalized autoregressive conditional heteroskedastic (GARCH) models. In the competitive power market environment, electricity price forecasting is an essential task for market participants. However, time series of electricity price has complex behavior such as nonlinearity, nonstationarity, and high volatility. ARIMA is suitable in forecasting, but it is not able to handle nonlinearity and volatility are existent in time series. Therefore, GARCH models are used to handle volatility in the in time series forecasting. The proposed method is computed using the daily electricity price data of Iran market for a five-year period from March 2013 to February 2018. The results reported in this paper illustrate the potential of the proposed ARMA-GARCH model and this combined model has been successfully applied to real prices in the Iranian power market.
C53|Cholesky-ANN models for predicting multivariate realized volatility|Accurately forecasting multivariate volatility plays a crucial role for the financial industry. The Cholesky-Artificial Neural Networks specification here presented provides a twofold advantage for this topic. On the one hand, the use of the Cholesky decomposition ensures positive definite forecasts. On the other hand, the implementation of artificial neural networks allows to specify nonlinear relations without any particular distributional assumption. Out-of-sample comparisons reveal that Artificial neural networks are not able to strongly outperform the competing models. However, long-memory detecting networks, like Nonlinear Autoregressive model process with eXogenous input and long shortterm memory, show improved forecast accuracy respect to existing econometric models.
C53|Ferndiagnose des RWI-Konjunkturmodells<BR>[Remote diagnosis of the RWI business cycle model]|The German Institute for Economic Research Berlin (DIW), the Halle Institute for Economic Research (IWH) and the Rheinisch-Westfälisches Institut Essen (RWI) have used their own models to calculate the macroeconomic effects of the measures under the coalition agreement for the 19th parliamentary term and published them in the spring report 2018 of the Joint Economic Forecast project group. A comparison with the simulations of two other econometric models, including an older version of the RWI business cycle model (KM), suggests that these effects have been overestimated. The study focuses on the results of the RWI, which least overestimates those effects. To test the overestimation hypothesis, (1) a black box method is used to show that the implicit multipliers of the simulation results are not plausible. In a further attempt (2), the problematic effects are calculated on the basis of multipliers published in 2012 from an older version of the KM and compared with those of the new version. The hypothesis can be confirmed. Finally, (3) the latest documentation of the new versions of the KM shows that the price development indices are not calculated correctly. This would explain why the simulation results of the KM have a bias. In view of the importance of the KM for the Rheinisch-Westfälisches Institut and beyond that for the project group Gemeinschaftsdiagnose, which informs the public twice a year about the expected economic development, but also because other models may also be affected by this problem, the experts should be informed and encouraged to check their own models. The public should be warned against overly optimistic forecasts. It goes without saying that the wrong calculation method, if it is still used, must be eliminated before further simulations and forecasts are published. For a publicly funded model, it would also be necessary to document which of the technically possible methods for implementing the annual overlap method has been implemented.
C53|Realized Volatility Forecasting with Neural Networks|In the last few decades, a broad strand of literature in finance has implemented artificial neural networks as forecasting method. The major advantage of this approach is the possibility to approximate any linear and nonlinear behaviors without knowing the structure of the data generating process. This makes it suitable for forecasting time series which exhibit long memory and nonlinear dependencies, like conditional volatility. In this paper, I compare the predictive performance of feed-forward and recurrent neural networks (RNN), particularly focusing on the recently developed Long short-term memory (LSTM) network and NARX network, with traditional econometric approaches. The results show that recurrent neural networks are able to outperform all the traditional econometric methods. Additionally, capturing long-range dependence through Long short-term memory and NARX models seems to improve the forecasting accuracy also in a highly volatile framework.
C53|Nowcasting US GDP with artificial neural networks|We use a machine learning approach to forecast the US GDP value of the current quarter and several quarters ahead. Within each quarter, the contemporaneous value of GDP growth is unavailable but can be estimated using higher-frequency variables that are published in a more timely manner. Using the monthly FRED-MD database, we compare the feedforward artificial neural network forecasts of GDP growth to forecasts of state of the art dynamic factor models and the Survey of Professional Forecasters, and we evaluate the relative performance. The results indicate that the neural network outperforms the dynamic factor model in terms of now- and forecasting, while it generates at least as good now- and forecasts as the Survey of Professional Forecasters.
C53|From Twitter to GDP: Estimating Economic Activity From Social Media|Using all geo-located image tweets shared on Twitter in 2012-2013, I find that the volume of tweets is a valid proxy for estimating current GDP in USD at the country level. Residuals from my preferred model are negatively correlated to a data quality index, indicating that my estimates of GDP are more accurate for countries with more reliable GDP data. Comparing Twitter with more commonly-used proxy of night-light data, I find that variation in Twitter activity explains slightly more of the cross-country variance in GDP. I also exploit the continuous time and geographic granularity of social media posts to create monthly and weekly estimates of GDP for the US, as well as sub- national estimates, including those economic areas that span national borders. My findings suggest that Twitter can be used to measure economic activity in a more timely and more spatially disaggregate way than conventional data and that governments’ statistical agencies could incorporate social media data to complement and further reduce measurement error in their official GDP estimates.
C53|Budget Credibility of Subnational Governments: Analyzing the Fiscal Forecasting Errors of 28 States in India|Budget credibility, the ability of governments to accurately forecast the macro-fiscal variables, is crucial for effective Public Finance Management (PFM). Fiscal marksmanship analysis captures the extent of errors in the budgetary forecasting. The fiscal rules can determine fiscal marksmanship, as effective fiscal consolidation procedure affects the fiscal behaviour of the states in conducting the budgetary forecasts. Against this backdrop, applying Theil’s technique, we analyse the fiscal forecasting errors for 28 States (except Telengana) in India for the period 2011-12 to 2015-16. There is a heterogeneity in the magnitude of errors across subnational governments in India. The forecast errors in revenue receipts have been greater than revenue expenditure. Within revenue receipts, the errors are pronounced more significantly in grants component. Within expenditure budgets, the errors in capital spending are found greater than revenue spending in all the States. Partitioning the sources of errors, we identified that the errors were more broadly random than systematic bias, except for a few crucial macro-fiscal variables where improving the forecasting techniques can provide better estimates.
C53|A multivariate approach for the simultaneous modelling of market risk and credit risk for cryptocurrencies|This paper proposes a set of models which can be used to estimate the market risk for a portfolio of crypto-currencies, and simultaneously to estimate also their credit risk using the Zero Price Probability (ZPP) model by Fantazzini et al (2008), which is a methodology to compute the probabilities of default using only market prices. For this purpose, both univariate and multivariate models with different specifications are employed. Two special cases of the ZPP with closed-form formulas in case of normally distributed errors are also developed using recent results from barrier option theory. A backtesting exercise using two datasets of 5 and 15 coins for market risk forecasting and a dataset of 42 coins for credit risk forecasting was performed. The Value-at-Risk and the Expected Shortfall for single coins and for an equally weighted portfolio were calculated and evaluated with several tests. The ZPP approach was used for the estimation of the probability of default/death of the single coins and compared to classical credit scoring models (logit and probit) and to a machine learning algorithm (Random Forest). Our results reveal the superiority of the t-copula/skewed-t GARCH model for market risk, and the ZPP-based models for credit risk.
C53|The importance of being informed: forecasting market risk measures for the Russian RTS index future using online data and implied volatility over two decades|This paper focuses on the forecasting of market risk measures for the Russian RTS index future, and examines whether augmenting a large class of volatility models with implied volatility and Google Trends data improves the quality of the estimated risk measures. We considered a time sample of daily data from 2006 till 2019, which includes several episodes of large-scale turbulence in the Russian future market. We found that the predictive power of several models did not increase if these two variables were added, but actually decreased. The worst results were obtained when these two variables were added jointly and during periods of high volatility, when parameters estimates became very unstable. Moreover, several models augmented with these variables did not reach numerical convergence. Our empirical evidence shows that, in the case of Russian future markets, T-GARCH models with implied volatility and student’s t errors are better choices if robust market risk measures are of concern.
C53|Nowcasting GDP Growth Using a Coincident Economic Indicator for India|In India, the first official estimate of quarterly GDP is released approximately 7-8 weeks after the end of the reference quarter. To provide an early estimate of the current quarter GDP growth, we construct a Coincident Economic Indicator for India (CEII) using 6, 9 and 12 high-frequency indicators. These indicators represent various sectors, display high contemporaneous correlation with GDP, and track GDP turning points well. While CEII-6 includes domestic economic activity indicators, CEII-9 combines indicators on trade and services along with the indicators used in CEII-6. Finally, CEII-12 adds financial indicators to the indicators used in CEII-9. In addition to the conventional economic activity indicators, we include a financial block in CEII-12 to reflect the growing influence of the financial sector on economic activity. CEII is estimated using a dynamic factor model to extract a common trend underlying the highfrequency indicators. We use the underlying trend to gauge the state of the economy and to identify sectors contributing to economic fluctuations. Further, CEIIs are used to nowcast GDP growth, which closely tracks the actual GDP growth, both in-sample and out-of-sample.
C53|Forecasting with many predictors using message passing algorithms|Machine learning methods are becoming increasingly popular in economics, due to the increased availability of large datasets. In this paper I evaluate a recently proposed algorithm called Generalized Approximate Message Passing (GAMP) , which has been very popular in signal processing and compressive sensing. I show how this algorithm can be combined with Bayesian hierarchical shrinkage priors typically used in economic forecasting, resulting in computationally efficient schemes for estimating high-dimensional regression models. Using Monte Carlo simulations I establish that in certain scenarios GAMP can achieve estimation accuracy comparable to traditional Markov chain Monte Carlo methods, at a tiny fraction of the computing time. In a forecasting exercise involving a large set of orthogonal macroeconomic predictors, I show that Bayesian shrinkage estimators based on GAMP perform very well compared to a large set of alternatives.
C53|Will the real eigensystem VAR please stand up? A univariate primer|I introduce the essential aspects of the eigensystem vector autoregression (EVAR), which allows VARs to be specified and estimated directly in terms of their eigensystem, using univariate examples for clarity. The EVAR guarantees non-explosive dynamics and, if included, non-redundant moving-average components. In the empirical application, constraining the EVAR eigenvalues to be real and positive leads to “desirable” impulse response functions and improved out-of-sample forecasts.
C53|Variational Bayesian Inference in Large Vector Autoregressions with Hierarchical Shrinkage|Many recent papers in macroeconomics have used large Vector Autoregressions (VARs) involving a hundred or more dependent variables. With so many parameters to estimate, Bayesian prior shrinkage is vital in achieving reasonable results. Computational concerns currently limit the range of priors used and render difficult the addition of empirically important features such as stochastic volatility to the large VAR. In this paper, we develop variational Bayes methods for large VARs which overcome the computational hurdle and allow for Bayesian inference in large VARs with a range of hierarchical shrinkage priors and with time-varying volatilities. We demonstrate the computational feasibility and good forecast performance of our methods in an empirical application involving a large quarterly US macroeconomic data set.
C53|An automated prior robustness analysis in Bayesian model comparison|The marginal likelihood is the gold standard for Bayesian model comparison although it is well-known that the value of marginal likelihood could be sensitive to the choice of prior hyperparameters. Most models require computationally intense simulation-based methods to evaluate the typically high-dimensional integral of the marginal likelihood expression. Hence, despite the recognition that prior sensitivity analysis is important in this context, it is rarely done in practice. In this paper we develop efficient and feasible methods to compute the sensitivities of marginal likelihood, obtained via two common simulation-based methods, with respect to any prior hyperparameter alongside the MCMC estimation algorithm. Our approach builds on Automatic Differentiation (AD), which has only recently been introduced to the more computationally intensive setting of Markov chain Monte Carlo simulation. We illustrate our approach with two empirical applications in the context of widely used multivariate time series models.
C53|Efficient selection of hyperparameters in large Bayesian VARs using automatic differentiation|Large Bayesian VARs with the natural conjugate prior are now routinely used for forecasting and structural analysis. It has been shown that selecting the prior hyperparameters in a data-driven manner can often substantially improve forecast performance. We propose a computationally efficient method to obtain the optimal hyperparameters based on Automatic Differentiation, which is an efficient way to compute derivatives. Using a large US dataset, we show that using the optimal hyperparameter values leads to substantially better forecast performance. Moreover, the proposed method is much faster than the conventional grid-search approach, and is applicable in high-dimensional optimization problems. The new method thus provides a practical and systematic way to develop better shrinkage priors for forecasting in a data-rich environment.
C53|Simulating the impact of transport infrastructure investment on wages: A dynamic spatial panel model approach|This paper estimates the impact of a multi-billion pound investment in Britain's rail transport infrastructure, in the form of high-speed rail links, on wage levels across each of 347 districts of England and Wales. The impacts are based on a dynamic spatial panel model adaptation of standard urban economics based on employment density and commuting patterns. This allows estimation of these global impacts operating via improved commuting times. We demonstrate that while estimates of a traditional market potential approach with fixed effects are to some extent qualitatively and quantitatively similar to our predictions, our predictions allow more heterogeneous effects and give more accurate forecasts. The study finds that on average wages increase by around 2% as employment centres gain improved access to more skilled workers and as spillover effects become spatially more extensive. While most areas see modest positive effects, some locations are negatively affected, in the extreme case by as much as 7%.
C53|Evaluating heterogeneous forecasts for vintages of macroeconomic variables|There are various reasons why professional forecasters may disagree in their quotes for macroeconomic variables. One reason is that they target at different vintages of the data. We propose a novel method to test forecast bias in case of such heterogeneity. The method is based on Symbolic Regression, where the variables of interest become interval variables. We associate the interval containing the vintages of data with the intervals of the forecasts. An illustration to 18 years of forecasts for annual USA real GDP growth, given by the Consensus Economics forecasters, shows the relevance of the method.
C53|IMA(1,1) as a new benchmark for forecast evaluation|Many forecasting studies compare the forecast accuracy of new methods or models against a benchmark model. Often, this benchmark is the random walk model. In this note I argue that for various reasons an IMA(1,1) model is a better benchmark in many cases.
C53|Professional Forecasters and January|Each month various professional forecasters give forecasts for next year's real GDP growth and many other variables. In terms of forecast updates, January is a special month, as then the forecast horizon moves to the following calendar year, and as such the observation is not a revision. Instead of deleting the January data when analyzing forecast updates, this paper proposes a periodic version of an often considered test regression, to explicitly include and model the January data. An application of this periodic model for many forecasts across a range of countries learns that apparently there is a January optimism effect. In fact, in January, GDP forecast updates are suddenly positive, and at the same time the forecast updates for unemployment are likewise negative. This optimism about the new year of the professional forecasters is however found to be detrimental to forecast accuracy. The main conclusion is that forecasts created in January for the next year need to be treated with care.
C53|Estimates of quarterly GDP growth using MIDAS regressions|This paper provides new estimates of year-to-year quarterly real GDP growth in Suriname for 2013Q1 to 2018Q4. The methodology to arrive at these estimates consists of the following steps. Using the familiar Chow and Lin method, the available annual data are disaggregated into a first round of quarterly data. The quarterly data are then included in a MIDAS model, which links the quarterly observations with a new but well established monthly observed indicator of economic activity. The best-performing MIDAS model is then used to update the initial estimates of quarterly GDP growth to final estimates, which in turn can be used in macro-economic modelling and analysis.
C53|Time-Varying Risk Aversion and the Predictability of Bond Premia|We show that time-varying risk aversion captures significant predictive information over excess returns on U.S. government bonds even after controlling for a large number of financial and macro factors. Including risk aversion improves the predictive accuracy at all horizons (one- to twelve-months ahead) for shorter maturity bonds and at shorter forecast horizons (one- to three-months ahead) for longer maturity bonds. Given the role of Treasury securities in economic forecasting models and portfolio allocation decisions, our findings have significant implications for investors, policy makers and researchers interested in accurately forecasting return dynamics for these assets.
C53|Forecasting the Term Structure of Interest Rates of the BRICS: Evidence from a Nonparametric Functional Data Analysis|In this paper, we develop a non-parametric functional data analysis (NP-FDA) model to forecast the term-structure of Brazil, Russia, India, China and South Africa (BRICS). We use daily data over the period of January 1, 2010 to December 31, 2016. We find that, while it is in general difficult to beat the random-walk model in the shorter-horizons, at longer-runs our proposed NP-FDA approach outperforms not only the random-walk model, but also other popular competitors used in term-structure forecasting literature. Our results have important implications for both policymakers aiming to stabilize the economy, and for optimal portfolio allocation decisions of financial market agents.
C53|Variants of Consumption-Wealth Ratios and Predictability of U.S. Government Bond Risk Premia: Old is still Gold|This paper compares the ability of alternative consumption-wealth ratios, based on constant parameter (cay), Markov-switching (cayMS ) and time-varying parameter (cayTVP) cointegration estimation of the consumption function, for predicting in- and out-of-sample movements of quarterly excess returns of U.S. government bonds over 1953:Q2 to 2015:Q3. Our findings show that after controlling for standard financial and macroeconomic factors, cay outperforms the cayMS and cayTVP in predicting the path of excess returns on bonds. Implications of our results for academics, investors and policymakers are discussed.
C53|The Role of Real Estate Uncertainty in Predicting US Home Sales Growth: Evidence from a Quantiles-Based Bayesian Model Averaging Approach|This paper investigates the role of real estate-specific uncertainty in predicting the conditional distribution of US home sales growth over the monthly period of 1970:07 to 2017:12, based on Bayesian Model Averaging (BMA) to account for model uncertainty. After controlling for standard predictors of home sales (housing price, mortgage rate, personal disposable income, unemployment rate, building permits, and housing starts), and macroeconomic and financial uncertainties, our results from the quantile BMA (QBMA) model show that real estate uncertainty has predictive content for the lower and upper quantiles of the conditional distribution of home sales growth.
C53|The Predictability of Stock Market Volatility in Emerging Economies: Relative Roles of Local, Regional and Global Business Cycles|This paper explores the role of business cycle proxies, measured by the output gap at the global, regional and local levels, as potential predictors of stock market volatility in the emerging BRICS nations. We observe that the emerging BRICS nations display a rather heterogeneous pattern when it comes to the relative role of idiosyncratic factors as a predictor of stock market volatility. While domestic output gap is found to capture significant predictive information for India and China particularly, the business cycles associated with emerging economies and the world in general are strongly important for the BRIC countries and weakly for South Africa, especially in the post-global financial crisis era. The findings suggest that despite the increase in the financial integration of world capital markets, emerging economies can still bear significant exposures to idiosyncratic risk factors, an issue of high importance for the profitability of global diversification strategies.
C53|Forecasting Local Currency Bond Risk Premia of Emerging Markets: The Role of Cross-Country Macro-Financial Linkages|In this paper, we forecast local currency debt of five major emerging market countries (Brazil, Indonesia, Mexico, South Africa, and Turkey) over the period of January 2010 to January 2019 (with an in-sample: March 2005 to December 2018). We exploit information from a large set of economic and financial time series to assess the importance of not only “own-country” factors (derived from principal component and partial least squares approach), but also create “global” predictors by combining the country-specific variables across the five emerging economies. We find that while information on own-country factors can outperform the historical average model, global factors tend to produce not only greater statistical and economic gains, but also enhances market timing ability of investors, especially when we use the target-variable (bond premium) approach under the partial least squares method to extract our factors. Our results have important implications for not only fund managers, but also policymakers.
C53|Gold, Platinum and the Predictability of Bond Risk Premia|We show that the ratio of gold to platinum prices (GP) contains significant predictive information for excess U.S. government bond returns, even after controlling for a large number of financial and macro factors. Including GP in the model improves the predictive accuracy, over and above the standard macroeconomic and financial predictors, at all forecasting horizons for the shortest maturity bonds and at longer forecasting horizons for bonds with longer maturities beyond 2 years. The findings highlight the predictive information captured by commodity prices on bond market excess returns with significant investment and policy making implications.
C53|The Role of an Aligned Investor Sentiment Index in Predicting Bond Risk Premia of the United States|In this paper, we develop a new investor sentiment index that is aligned with the purpose of predicting the excess returns on government bonds of the United States (US) of maturities of 2-, 3-, 4-, 5-year. By eliminating a common noise component in underlying sentiment proxies using the partial least squares (PLS) approach, the new index is shown to have much greater predictive power than the original principal component analysis (PCA)-based sentiment index both in- and out-of-sample, with the predictability being statistically significant, especially for bond premia with shorter maturities, even after controlling for a large number of financial and macro factors, as well as investor attention and manager sentiment indexes. Given the role of Treasury securities in forecasting of output and inflation, and portfolio allocation decisions, our findings have significant implications for investors, policymakers and researchers interested in accurately forecasting return dynamics for these assets.
C53|Monthly Forecasting of GDP with Mixed Frequency Multivariate Singular Spectrum Analysis|The literature on mixed-frequency models is relatively recent and has found applications across economics and finance. The standard application in economics considers the use of (usually) monthly variables (e.g. industrial production) in predicting/fitting quarterly variables (e.g. real GDP). In this paper we propose a Multivariate Singular Spectrum Analysis (MSSA) based method for mixed frequency interpolation and forecasting, which can be used for any mixed frequency combination. The novelty of the proposed approach rests on the grounds of simplicity within the MSSA framework. We present our method using a combination of monthly and quarterly series and apply MSSA decomposition and reconstruction to obtain monthly estimates and forecasts for the quarterly series. Our empirical application shows that the suggested approach works well, as it offers forecasting improvements on a dataset of eleven developed countries over the last 50 years. The implications for mixed frequency modelling and forecasting, and useful extensions of this method, are also discussed.
C53|SVARs Identification through Bounds on the Forecast Error Variance| Sign-restricted Structural Vector Autoregressions (SVARs) are increasingly common. However, they usually result in a set of structural parameters that have very different implications in terms of impulse responses, elasticities, historical decomposition and forecast error variance decomposition (FEVD). This makes it difficult to derive meaningful economic conclusions, and there is always the risk of retaining structural parameters with implausible implications. This paper imposes bounds on the FEVD as a way of sharpening set-identification induced by sign restrictions. Firstly, in a bivariate and trivariate setting, this paper analytically proves that bounds on the FEVD reduce the identified set. For higher dimensional SVARs, I establish the conditions in which the placing of bounds on the FEVD delivers a non-empty set and sharpens inference; algorithms to detect non-emptiness and reduction are also provided. Secondly, under a convexity criterion, a prior-robust approach is proposed to construct estimation and inference. Thirdly, this paper suggests a procedure to derive theory-driven bounds that are consistent with the implications of a variety of popular, but different, DSGE models, with real, nominal, and financial frictions, and with sufficiently wide ranges for their parameters. The methodology is generalized to incorporate uncertainty about the bounds themselves. Fourthly, a Monte-Carlo exercise verifies the effectiveness of those bounds in identifying the data-generating process relative to sign restrictions. Finally, a monetary policy application shows that bounds on the FEVD tend to remove unreasonable implications, increase estimation precision, sharpen and also alter the inference of models identified through sign restrictions.
C53|A Practical Guide to Harnessing the HAR Volatility Model|The standard heterogeneous autoregressive (HAR) model is perhaps the most popular benchmark model for forecasting return volatility. It is often estimated using raw realized variance (RV) and ordinary least squares (OLS). However, given the stylized facts of RV and wellknown properties of OLS, this combination should be far from ideal. One goal of this paper is to investigate how the predictive accuracy of the HAR model depends on the choice of estimator, transformation, and forecasting scheme made by the market practitioner. Another goal is to examine the effect of replacing its high-frequency data based volatility proxy (RV) with a proxy based on free and publicly available low-frequency data (logarithmic range). In an out-of-sample study, covering three major stock market indices over 16 years, it is found that simple remedies systematically outperform not only standard HAR but also state of the art HARQ forecasts, and that HAR models using logarithmic range can often produce forecasts of similar quality to those based on RV.
C53|MARTIN Has Its Place: A Macroeconometric Model of the Australian Economy|This paper introduces MARTIN – the Reserve Bank of Australia's (RBA) current model of the Australian economy. MARTIN is an economy-wide model used to produce forecasts and conduct counterfactual scenario analysis. In contrast to other large-scale models used at the RBA – and at many other central banks – which adhere to a narrow theoretical view of how the economy operates, MARTIN is a macroeconometric model that consists of a system of reduced form equations built to strike a balance between theoretical rigour and empirical realism. Most of the model's equations align closely with the way RBA staff typically interpret the behaviour of individual economic variables. However, combining these individual equations in a system can bring fresh insights that are not possible without model-based analysis. In the paper we provide an overview of the model, outline its core behavioural equations and describe its empirical properties. The Online Appendix presents the full set of model equations.
C53|Going with your Gut: The (In)accuracy of Forecast Revisions in a Football Score Prediction Game|This paper studies 150 individuals who each chose to forecast the outcome of 380 fixed events, namely all football matches during the 2017/18 season of the English Premier League. The focus is on whether revisions to these forecasts before the matches began improved the likelihood of predicting correct scorelines and results. Against what theory might expect, we show how these revisions tended towards significantly worse forecasting performance, suggesting that individuals should have stuck with their initial judgements, or their ‘gut instincts’. This result is robust to both differences in the average forecasting ability of individuals and the predictability of matches. We find evidence this is because revisions to the forecast number of goals scored in football matches are generally excessive, especially when these forecasts were increased rather than decreased.
C53|Information, prices and efficiency in an online betting market|We study the odds (or prices) set by fifty-one online bookmakers for the result outcomes in over 16,000 association football matches in England since 2010. Adapting a methodology typically used to evaluate forecast efficiency, we test the Efficient Market Hypothesis in this context. We find odds are generally not biased when compared against actual match outcomes, both in terms of favourite-longshot or outcome types. But individual bookmakers are not efficient. Their own odds do not appear to use fully the information contained in their competitors' odds.
C53|Macroeconomic Indicator Forecasting with Deep Neural Networks|Economic policymaking relies upon accurate forecasts of economic conditions. Current methods for unconditional forecasting are dominated by inherently linear models {{p}} that exhibit model dependence and have high data demands. {{p}} We explore deep neural networks as an {{p}} opportunity to improve upon forecast accuracy with limited data and while remaining agnostic as to {{p}} functional form. We focus on predicting civilian unemployment using models based on four different neural network architectures. Each of these models outperforms bench- mark models at short time horizons. One model, based on an Encoder Decoder architecture outperforms benchmark models at every forecast horizon (up to four quarters).
C53|Bayesian combination for inflation forecasts: The effects of a prior based on central banks’ estimates|Typically, central banks use a variety of individual models (or a combination of models) when forecasting inflation rates. Most of these require excessive amounts of data, time, and computational power, all of which are scarce when monetary authorities meet to decide over policy interventions. In this paper we use a rolling Bayesian combination technique that considers inflation estimates by the staff of the Central Bank of Colombia during 2002–2011 as prior information. Our results show that: (1) the accuracy of individual models is improved by using a Bayesian shrinkage methodology, and (2) priors consisting of staff estimates outperform all other priors that comprise equal or zero vector weights. Consequently, our model provides readily available forecasts that exceed all individual models in terms of forecasting accuracy at every evaluated horizon.
C53|Cadenas globales de valor, crecimiento y protección arancelaria en Colombia|Este documento realiza un análisis detallado de la dispersión del arancel en Colombia, evaluando su evolución desde la implementación de la apertura económica en la década de los noventa, y presenta un marco comparativo de los niveles de dispersión del arancel nacional frente al de los países de la región. El análisis de la dispersión del arancel en Colombia, junto con los niveles de este en cada uno de los sectores de la economía nacional, servirán de insumo para la estimación de un arancel menos disperso que permita la incursión de la economía colombiana en las cadenas globales de valor. **** ABSTRACT: The present paper makes a detailed analysis of the tariff dispersion in Colombia, considering its evolution since the implementation of the economic opening in the nineties. The document presents a comparative framework between dispersion levels of the national tariff against that of the countries in the region. The analysis of tariff dispersion in Colombia and the levels of this in each sector of the national economy contributes as input for the estimation of a less dispersed tariff that allows the incursion of the Colombian economy in the global value chains.
C53|Quarterly Forecasting Model for India’s Economic Growth: Bayesian Vector Autoregression Approach|This study develops a framework to forecast India’s gross domestic product growth on a quarterly frequency from 2004 to 2018. The models, which are based on real and monetary sector descriptions of the Indian economy, are estimated using Bayesian vector autoregression (BVAR) techniques. The real sector groups of variables include domestic aggregate demand indicators and foreign variables, while the monetary sector groups specify the underlying inflationary process in terms of the consumer price index (CPI) versus the wholesale price index given India’s recent monetary policy regime switch to CPI inflation targeting. The predictive ability of over 3,000 BVAR models is assessed through a set of forecast evaluation statistics and compared with the forecasting accuracy of alternate econometric models including unrestricted and structural VARs. Key findings include that capital flows to India and CPI inflation have high informational content for India’s GDP growth. The results of this study provide suggestive evidence that quarterly BVAR models of Indian growth have high predictive ability.
C53|Information Environments and High Price Impact Trades: Implication for Volatility and Price Efficiency|Using high-frequency transaction and Limit Order Book (LOB) data, we extend the identification dimensions of High Price Impact Trades (HPITs) by using LOB matchedness. HPITs are trades associated with disproportionately large price changes relative to their proportion of volume. We nd that a higher presence of HPITs leads to a decline in volatility due to more contrarian trades against uninformed traders, but this decline varies with information environments and liquidity levels. Further, we show that more HPITs lead to higher price eciency for stocks with greater public disclosure and higher liquidity. Our empirical results provide evidence that HPITs mainly reect fundamental-based information in a high public information environment, and belief-based information in a low public information environment.
C53|Forecasting ECB Policy Rates with Different Monetary Policy Rules|This article compares two types of monetary policy rules – the Taylor-Rule and the Orphanides-Rule – with respect to their forecasting properties for the European Central Bank. In this respect the basic rules, results from estimates models and augmented rules are compared. Using quarterly real-time data from 1999 to the beginning of 2019, we find that an estimated Orphanides-Rule performs best in nowcasts, while it is outperformed by an augmented Taylor-Rule when it comes to forecasts. However, also a no-change rule delivers good results for forecasts, which is hard to beat for most policy rules.
C53|A Horse Race in High Dimensional Space|In this paper, we study the predictive power of dense and sparse estimators in a high dimensional space. We propose a new forecasting method, called Elastically Weighted Principal Components Analysis (EWPCA) that selects the variables, with respect to the target variable, taking into account the collinearity among the data using the Elastic Net soft thresholding. Then, we weight the selected predictors using the Elastic Net regression coefficient, and we finally apply the principal component analysis to the new “elastically” weighted data matrix. We compare this method to common benchmark and other methods to forecast macroeconomic variables in a data-rich environment, dived into dense representation, such as Dynamic Factor Models and Ridge regressions and sparse representations, such as LASSO regression. All these models are adapted to take into account the linear dependency of the macroeconomic time series. Moreover, to estimate the hyperparameters of these models, including the EWPCA, we propose a new procedure called “brute force”. This method allows us to treat all the hyperparameters of the model uniformly and to take the longitudinal feature of the time-series data into account. Our findings can be summarized as follows. First, the “brute force” method to estimate the hyperparameters is more stable and gives better forecasting performances, in terms of MSFE, than the traditional criteria used in the literature to tune the hyperparameters. This result holds for all samples sizes and forecasting horizons. Secondly, our two-step forecasting procedure enhances the forecasts’ interpretability. Lastly, the EWPCA leads to better forecasting performances, in terms of mean square forecast error (MSFE), than the other sparse and dense methods or naïve benchmark, at different forecasts horizons and sample sizes.
C53|Predictability, Real Time Estimation, and the Formulation of Unobserved Components Models|The formulation of unobserved components models raises some relevant interpretative issues, owing to the existence of alternative observationally equivalent specifications, differing for the timing of the disturbances and their covariance matrix. We illustrate them with reference to unobserved components models with ARMA(m;m) reduced form, performing the decomposition of the series into an ARMA(m; q) signal, q m, and a noise component. We provide a characterization of the set of covariance structures that are observationally equivalent, when the models are formulated both in the future and the contemporaneous forms. Hence, we show that, while the point predictions and the contemporaneous real time estimates are invariant to the specification of the disturbances covariance matrix, the reliability cannot be identified, except for special cases requiring q
C53|Estimation and Forecasting of Industrial Production Index|It is essential for policy makers to timely consider the cyclical changes in output. Monthly industrial production is one of the most important and commonly used macroeconomic indicators for this purpose. In Pakistan monthly estimates of industrial production are not available. Alternatively, policy makers rely on Large Scale Manufacturing (LSM) index which accounts for only 10% of the GDP. Another limitation of LSM is that it mainly accounts for private sector industry thus leaving out direct public sector presence in industrial production. LSM is relied upon heavily by economic policy makers to gauge economic activity in Pakistan. In this paper, we present a new Industrial Production Index (IPI), which covers whole of industrial sector in Pakistan. The advantage of this IPI index is that it provides additional information that LSM misses out. Post estimation, we built seven econometrics models reflecting conditions in real, financial and external sectors to estimate YoY changes in the proposed Instrial Production Index (IPI). Our results show that the root mean square error of the ARDL model reflecting financial conditions is lowest across all horizons
C53|Machine Learning vs Traditional Forecasting Methods: An Application to South African GDP|This study employs traditional autoregressive and vector autoregressive forecasting models, as well as machine learning methods of forecasting, in order to compare the performance of each of these techniques. Each technique is used to forecast the percentage change of quarterly South African Gross Domestic Product, quarter-on-quarter. It is found that machine learning methods outperform traditional methods according to the chosen criteria of minimising root mean squared error and maximising correlation with the actual trend of the data. Overall, the outcomes suggest that machine learning methods are a viable option for policy-makers to use, in order to aid their decision-making process regarding trends in macroeconomic data. As this study is limited by data availability, it is recommended that policy-makers consider further exploration of these techniques.
C53|Measuring inflation uncertainty in Turkey|Measuring and monitoring inflation uncertainty is an essential ingredient of monetary policy analysis. This study constructs survey measures of inflation uncertainty for the Turkish economy. Using density and point inflation forecasts in the CBRT Survey of Expectations, we derive various uncertainty measures through standard deviation, entropy, and disagreement among forecasters. Our results suggest that survey-based inflation uncertainty measures are broadly consistent with market-implied indicators of inflation risk. Moreover, we find that an increase in observed inflation is associated with higher inflation uncertainty across all empirical specifications.
C53|Monitoring and Forecasting Cyclical Dynamics in Bank Credits: Evidence from Turkish Banking Sector|Credit growth rate deviating from its long-run trend or equilibrium value holds importance for policymakers given the implications on economic activity and macro-financial interactions. In the first part of this study, the main aim is to construct indicators for determining the episodes of moderate-to-excessive credit slowdown and expansion by utilizing time-series filtering methods such as Hodrick-Prescott filter, Butterworth filter, Christiano-Fitzgerald filter and Hamilton filter over the time period 2007-2019. In addition to filtering choices, four different credit ratios (which are credit-to-GDP ratio, real credit growth, logarithm of real credit, credit impulse ratio) are included in the methodology to ensure the robustness. This framework enables one to generate monitoring tools for not only total loans, but also for financial intermediation activities with different loan breakdowns regarding type, sector and currency denomination. Moreover, industry-based dynamics of commercial loans are examined by using micro-level Credit Registry data set. In the following part, the credit cycle implied by macroeconomic dynamics are investigated by using factor-augmented predictive regression models. In this context, factors representing the global economic developments, banking sector outlook, local financial conditions and economic growth tendencies are created from large data set of 107 time series by utilizing principal component analysis. Analysis conducted for January 2009-April 2019 interval seems to be in line with exogenous shocks affecting the credit market in the corresponding period. To gain more knowledge about the predictive power of factor-augmented regression models, out-of-sample forecasting exercises are performed. It is found that global forces and economic activity provide substantial improvement in terms of predictive power over simple autoregressive benchmark models given low level of relative forecast errors.
C53|Partially Censored Posterior for Robust and Efficient Risk Evaluation|A novel approach to inference for a specific region of the predictive distribution is introduced. An important domain of application is accurate prediction of financial risk measures, where the area of interest is the left tail of the predictive density of logreturns. Our proposed approach originates from the Bayesian approach to parameter estimation and time series forecasting, however it is robust in the sense that it provides a more accurate estimation of the predictive density in the region of interest in case of misspecification. The first main contribution of the paper is the novel concept of the Partially Censored Posterior (PCP), where the set of model parameters is partitioned into two subsets: for the first subset of parameters we consider the standard marginal posterior, for the second subset of parameters (that are particularly related to the region of interest) we consider the conditional censored posterior. The censoring means that observations outside the region of interest are censored: for those observations only the probability of being outside the region of interest matters. This quasi-Bayesian approach yields more precise parameter estimation than a fully censored posterior for all parameters, and has more focus on the region of interest than a standard Bayesian approach. The second main contribution is that we introduce two novel methods for computationally efficient simulation: Conditional MitISEM, a Markov chain Monte Carlo method to simulate model parameters from the Partially Censored Posterior, and PCP-QERMit, an Importance Sampling method that is introduced to further decrease the numerical standard errors of the Value-at-Risk and Expected Shortfall estimators. The third main contribution is that we consider the effect of using a time-varying boundary of the region of interest, which may provide more information about the left tail of the distribution of the standardized innovations. Extensive simulation and empirical studies show the ability of the introduced method to outperform standard approaches.
C53|Backtesting Value-at-Risk and Expected Shortfall in the Presence of Estimation Error|We investigate the effect of estimation error on backtests of (multi-period) expected shortfall (ES) forecasts. These backtests are based on first order conditions of a recently introduced family of jointly consistent loss functions for Value-at-Risk (VaR) and ES. We provide explicit expressions for the additional terms in the asymptotic covariance matrix that result from estimation error, and propose robust tests that account for it. Monte Carlo experiments show that the tests that ignore these terms suffer from size distortions, which are more pronounced for higher ratios of out-of-sample to in-sample observations. Robust versions of the backtests perform well, although this also depends on the choice of conditioning variables. In an application to VaR and ES forecasts for daily FTSE 100 index returns as generated by AR-GARCH, AR-GJR-GARCH, and AR-HEAVY models, we find that estimation error substantially impacts the outcome of the backtests.
C53|Uncertainty in Electricity Markets from a seminonparametric Approach|No abstract is available for this item.
C53|Efectos de las variaciones del IPC en las decisiones financieras|En este documento se desarrolló un análisis de mercado que derivó en un modelo econométrico con miras a determinar el comportamiento del Índice de Precios al Consumidor en un horizonte de tiempo de 2 años, para dar apoyo a la toma de decisiones financieras de inversión y financiamiento. En el análisis se tuvieron en cuenta las Encuestas a expertos y los Pronósticos a entidades financieras. Sin embargo, al enfrentar dicha información con el IPC observado, se concluyó que los pronósticos y encuestas mencionadas no tenían una capacidad de predicción a dos años confiable. Debido a que el mercado no permitió cumplir con el objetivo propuesto, fue necesario desarrollar un modelo econométrico de tipo ARIMA con datos mensuales desde entre enero de 2010 y diciembre de 2018. En la construcción del modelo se determinó que la volatilidad del IPC estaba fuertemente influida por el precio de los alimentos, y por ende serían el fenómeno del niño y los paros de transporte las variables idóneas en la conformación del modelo. Como resultado, se obtuvo una proyección del IPC a dos años. No obstante, el pronóstico presentó una desviación estándar considerable y creciente en el tiempo que redujo la efectividad del modelo a un año. En el desarrollo del modelo como paso a seguir, se plantea necesario realizar una función impulso respuesta de las variables Dummy y adaptar el modelo a la nueva metodología del IPC propuesta por el DANE para 2019. *** In this document, a market analysis was developed which derived in an econometric model with a view to determining the behaviour of the Consumer Price Index in a time horizon of 2 years, to give support to the financial decision making of investment and financing. The analysis took into account the Surveys to experts and the Forecasts to financial entities. However, when facing said information with the observed CPI, it was concluded that the forecasts and surveys mentioned did not have a reliable two-year prediction capacity. Since the market did not comply with the proposed objective, it was necessary to develop an ARIMA-type econometric model with monthly data from January 2010 to December 2018. In the construction of the model, it was determined that the volatility of the CPI was strongly influenced by the food price, and therefore the El Niño phenomenon and transport stoppages would be the ideal variables in shaping the model. As a result, a two-year CPI projection was obtained. However, the forecast presented a considerable and increasing standard deviation over time that reduced the effectiveness of the model to one year. In the development of the model as a step to follow, it is necessary to carry out a response impulse function of the Dummy variables and to adapt the model to the new CPI methodology proposed by DANE for 2019.
C53|Una breve aplicación a la predicción de la fragilidad de empresas colombianas, mediante el uso de modelos estadísticos|Resumen: Este trabajo estima diferentes modelos estadísticos para medir la probabilidad de riesgo de quiebra empresarial e identificar cuál de ellos presenta un mejor desempeño predictivo. Para lograr este objetivo, se emplean los estados financieros de las empresas colombianas para el 2015 con el fin construir indicadores financieros como variables explicativas en los modelos empleados. Las variables más relevantes para medir la probabilidad de quiebra fueron la rentabilidad del patrimonio y el nivel de endeudamiento. Entre los modelos estimados (logístico, logístico heterocedástico, logístico robusto y logístico mixto), el logístico mixto fue el que presentó el mejor desempeño para predecir la fragilidad empresarial. / Abstract : This manuscript estimates different statistical models to measure the probability of risk business failure, and identifies which one has better predicting performance. In order to achieve this objective, the financial statements of Colombian companies in 2015 are used in order to build financial indicators as explanatory variables in the used models. The most relevant variables to measure the probability of business failure were the return on equity and debt ratio. Among the estimated models (logistic, heteroscedastic logistic, robust logistic and mixed logistic), the mixed logistic has the best performance in predicting the corporate fragility.
C53|Predicting criminal behavior with Levy flights using real data from Bogota|I use residential burglary data from Bogota, Colombia, to fit an agent-based modelfollowing truncated Lévy flights (Pan et al., 2018) elucidating criminal rational behaviorand validating repeat/near-repeat victimization and broken windows effects. The estimatedparameters suggest that if an average house or its neighbors have never been attacked,and it is suddenly burglarized, the probability of a new attack the next day increases, dueto the crime event, in 79 percentage points. Moreover, the following day its neighborswill also face an increment in the probability of crime of 79 percentage points. This effectpersists for a long time span. The model presents an area under the Cumulative AccuracyProfile (CAP) curve, of 0.8 performing similarly or better than state-of-the-art crimeprediction models. Public policies seeking to reduce criminal activity and its negativeconsequences must take into account these mechanisms and the self-exciting nature ofcrime to effectively make criminal hotspots safer
C53|Good Carry, Bad Carry|We distinguish between ”good” and ”bad” carry trades constructed from G-10 currencies. The good trades exhibit higher Sharpe ratios and sometimes positive return skewness, in contrast to the bad trades that have both substantially lower Sharpe ratios and highly negative return skewness. Surprisingly, good trades do not involve the most typical carry currencies like the Australian dollar and Japanese yen. The distinction between good and bad carry trades significantly alters our understanding of currency carry trade returns, and invalidates, for example, explanations invoking return skewness and crash risk.
C53|Currency Factors|We examine the ability of existing and new factor models to explain the comovements of G10- currency changes, measured using the novel concept of “currency baskets”, representing the overall movement of a particular currency. Using a clustering technique, we find a clear two-block structure in currency comovements with the first block containing mostly the dollar currencies, and the other the European currencies. A factor model incorporating this “clustering” factor and two additional factors, a commodity currency factor and a “world” factor based on trading volumes, fits currency basket correlations much better than extant factors, such as value and carry, do. In particular, it explains on average about 60% of currency variation and generates a root mean squared error relative to sample correlations of only 0.11. The model also fits comovements in emerging market currencies well. Economically, the correlations between currency baskets underlying the factor structure are inversely related to the physical distances between countries. The factor structure is also related to the exposure of the corresponding pricing kernels with respect to the global pricing kernel and is apparent in cross-country retail sales growth data.
C53|The global component of inflation volatility|Global developments play an important role in domestic inflation rates. Previous literature has found that a substantial amount of the variation in a large set of national inflation rates can be explained by a single global factor. However, inflation volatility has been typically neglected, while it is clearly relevant both from a policy point of view and for structural analysis and forecasting purposes. We study the evolution of inflation rates in several countries, using a novel model that allows for commonality in both levels and volatilities, in addition to country-specific components. We find that inflation stochastic volatility is indeed important, and a substantial share of it can be attributed to a global factor that also drives the levels and persistence of inflation. While various phenomena may contribute to global inflation dynamics, it turns out that since the early 1990s, the estimated global factor is correlated with China’s PPI and with oil inflation levels and volatilities. The extent of commonality among core inflation rates and volatilities is substantially smaller than for overall inflation, which leaves scope for national monetary policies.
C53|The Promise and Pitfalls of Conflict Prediction: Evidence from Colombia and Indonesia|"Policymakers can take actions to prevent local conflict before it begins, if such violence can be accurately predicted. We examine the two countries with the richest available sub-national data: Colombia and Indonesia. We assemble two decades of finegrained violence data by type, alongside hundreds of annual risk factors. We predict violence one year ahead with a range of machine learning techniques. Models reliably identify persistent, high-violence hot spots. Violence is not simply autoregressive, as detailed histories of disaggregated violence perform best. Rich socio-economic data also substitute well for these histories. Even with such unusually rich data, however, the models poorly predict new outbreaks or escalations of violence. ""Best case"" scenarios with panel data fall short of workable early-warning systems."
C53|Bank intermediation activity in a low interest rate environment|This paper investigates how the prolonged period of low interest rates affects bank intermediation activity. We use data for 113 large international banks headquartered in 14 major advanced economies during the period 1994â??2015. We find that low interest rates induce banks to shift their activities from interest-generating to fee-related and trading activities. This rebalancing is stronger for low capitalised banks. Banks also moderately adjust their funding structure, away from short-term market funding towards deposits. We observe a concomitant decline in the risk-weighted asset ratio and a reduction in loan-loss provisions, which is consistent with signs of evergreening.
C53|Forecasting Volatility in Cryptocurrency Markets|In this paper, we revisit the stylized facts of cryptocurrency markets and propose various approaches for modeling the dynamics governing the mean and variance processes. We first provide the statistical properties of our proposed models and study in detail their forecasting performance and adequacy by means of point and density forecasts. We adopt two loss functions and the model confidence set (MSC) test to evaluate the predictive ability of the models and the likelihood ratio test to assess their adequacy. Our results confirm that cryptocurrency markets are characterized by regime shifting, long memory and multifractality. We find that the Markov switching multifractal (MSM) and FIGARCH models outperform other GARCH-type models in forecasting bitcoin returns volatility. Furthermore, combined forecasts improve upon forecasts from individual models.
C53|When are Google data useful to nowcast GDP? An approach via pre-selection and shrinkage|Nowcasting GDP growth is extremely useful for policy-makers to assess macroeconomic conditions in real-time. In this paper, we aim at nowcasting euro area GDP with a large database of Google search data. Our objective is to check whether this specific type of information can be useful to increase GDP nowcasting accuracy, and when, once we control for official variables. In this respect, we estimate shrunk bridge regressions that integrate Google data optimally screened through a targeting method, and we empirically show that this approach provides some gain in pseudo-real-time nowcasting of euro area GDP quarterly growth. Especially, we get that Google data bring useful information for GDP nowcasting for the four first weeks of the quarter when macroeconomic information is lacking. However, as soon as official data become available, their relative nowcasting power vanishes. In addition, a true real-time analysis confirms that Google data constitute a reliable alternative when official data are lacking.
C53|The Jolly Ride of International Reserves and Commodity Prices: Evidence from Predictive Models|This study offers new insight into the dynamics of international reserves (IR). We argue that commodity prices play importance role in the accumulation of IR. We test this hypothesis by specifying a predictive model, in which commodity prices serve as predictors of IR. We essentially examine the extent to which the former predicts the later. Building a dataset for the BRICS nations, we found that a number of interesting results were obtained: first, commodity prices resoundingly predict the level of IR. Second, accounting for asymmetry helps improve the level of predictability.
C53|Revisiting the accuracy of inflation forecasts in Nigeria: the oil price â€“exchange rate perspectives|Motivated by the dual characteristic of the Nigeria economy as exporter and importer of oil, we extend the Tule et al. (2018) oil price â€“based predictive model to include the role of exchange rates in the predictability of inflation in Nigeria. Using the FQGLS estimator developed by Westerlund and Narayan (2012, 2015), we account for endogeneity, persistence and conditional heterosecedatcity effects in the forecasting analyses of inflation in Nigeria. We use both the single and pairwise measures to compare the forecast results of oil prices and/or exchange rates based augmented Phillips curve with the variant that contain them both simultaneously. The result suggests that augmenting the Phillips curve with oil prices and exchange rates in the same model set up is the more accurate to forecasting inflation in Nigeria relative to having them individually in a predictive model. More so, the augmented Phillips curve with oil prices and exchange rate consistently outperforms time series models such as ARMA and ARFIMA. However, we find that accounting for structural breaks will largely improve the accuracy of CMB-APC for forecasting inflation in Nigeria. We find the robustness of these findings evident for in-sample and out-of-sample forecasts.
C53|Statistical Learning and Exchange Rate Forecasting|his study uses the most innovative tools recently proposed in the statistical learning literature to assess the ability of standard exchange rate models to predict the exchange rate in the short and long run. Our results show that statistical learning methods display impressive performances, consistently outperforming the random walk in forecasting the exchange rate at different forecasting horizons, with the exception of the very short term (a period of 1-2 months). We use these tools to compare the predictive ability of different exchange rate models and model specifications. We find that sticky price versions of the monetary model with the error correction specification exhibit the best performance. We also explore the functioning of statistical learning models by developing measures of variable importance and by analyzing the kind of relationship that links each variable with the outcome. This allows us to improve our understanding of the relationship between the exchange rate and economic fundamentals, which appears complex and characterized by strong non-linearities.
C53|Euro area sovereign risk spillovers before and after the ECB's OMT announcement|We study the dynamics of sovereign risk spillovers from (and between) Spain and Italy, before and after the ECB's announcement of the OMT program. We identify domestic Italian and Spanish sovereign risk shocks through an intraday event study. The shocks are used as external instruments in bilateral, daily, local projection regressions. Prior to the announcement of the OMT, changes in the Spanish and, to a lesser extent, Italian spread spilled over to many other euro area member states, and also affected the euro-dollar exchange rate. Peak effects generally materialized after 2-3 days. Since the OMT announcement, spillovers to non-crisis, non-safe haven countries have disappeared. Some spillovers among crisis countries persist, but are smaller and shorter-lived than before. Overall, our results are consistent with the view that the OMT, through eliminating equilibrium multiplicity, has largely stopped contagion.
C53|How BLUE is the Sky? Estimating the Air Quality Data in Beijing During the Blue Sky Day Period (2008-2012) by the Bayesian LSTM Approach|Over the last three decades, air pollution has become a major environmental challenge in many of the fast growing cities in China, including Beijing. Given that any long-term exposure to high-levels of air pollution has devastating health consequences, accurately monitoring and reporting air pollution information to the public is critical for ensuring public health and safety and facilitating rigorous air pollution and health-related scientific research. Recent statistical research examining China’s air quality data has posed questions regarding data accuracy, especially data reported during the Blue Sky Day (BSD) period (2000 – 2012), though the accuracy of publicly available air quality data in China has improved gradually over the recent years (2013 – 2017). To the best of our understanding, no attempt has been made to re-estimate the air quality data during the BSD period. In this paper, we put forward a machine-learning model to re-estimate the official air quality data during the BSD period of 2008 – 2012, based on the PM2.5 data of the Beijing US Embassy, and the proxy data covering Aerosol Optical Depth (AOD) and meteorology. Results have shown that the average re-estimated daily air quality values are respectively 64% and 61% higher than the official values, for air quality index (AQI) and AQI equivalent PM2.5, during the BSD period of 2008 to 2012. Moreover, the re-estimated BSD air quality data exhibit reduced statistical discontinuity and irregularity, based on our validation tests. The results suggest that the proposed data re-estimation methodology has the potential to provide more justifiable historical air quality data for evidence-based environmental decision-making in China.
C53|A Model-Based Assessment of the Distributional Impact of Structural Reforms|This paper studies the effects of structural reforms on the functional distribution of income in EU Member States. To study this mechanism we use a DSGE model (Roeger et al. 2008) with households supplying three types of labour, low-, medium- and high-skilled. We assume that households receive income from labour, tangible capital, intangible capital, financial wealth and transfers and we trace how structural reforms affect these types of incomes. The quantification of structural reforms is based on changes in structural indicators that can significantly close the gap of a country’s average income towards the best performing countries in the EU. We find a general trade-off between an increase in employment of a particular group and the income of the average group member relative to income per capita. In general, reforms which aim at increasing employment of low skilled workers are associated with a fall in wages relative to income per capita. Capital owners generally benefit from labour market reforms, with an increasing share in total income, due to limited entry into the final goods production sector. This suggests that labour market re-forms may lead to suboptimal distributional effects if there are rigidities in goods markets present, a finding which confirms the importance of ensuring that such reforms are accompanied or preceded by product market reforms.
