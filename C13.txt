C13|Consistent Inference for Predictive Regressions in Persistent VAR Economies|This paper studies the properties of standard predictive regressions in model economies, characterized through persistent vector autoregressive dynamics for the state variables and the associated series of interest. In particular, we consider a setting where all, or a subset, of the variables may be fractionally integrated, and note that this induces a spurious regression problem. We then propose a new inference and testing procedure - the local spectrum (LCM) approach - for the joint significance of the regressors, which is robust against the variables having different integration orders. The LCM procedure is based on (semi-)parametric fractional-filtering and band spectrum regression using a suitably selected set of frequency ordinates. We establish the asymptotic properties and explain how they differ from and extend existing procedures. Using these new inference and testing techniques, we explore the implications of assuming VAR dynamics in predictive regressions for the realized return variation. Standard least squares predictive regressions indicate that popular financial and macroeconomic variables carry valuable information about return volatility. In contrast, we find no significant evidence using our robust LCM procedure, indicating that prior conclusions may be premature. In fact, if anything, our results suggest the reverse causality, i.e., rising volatility predates adverse innovations to key macroeconomic variables. Simulations are employed to illustrate the relevance of the theoretical arguments for finite-sample inference.
C13|Edgeworth expansion for Euler approximation of continuous diffusion processes|In this paper we present the Edgeworth expansion for the Euler approximation scheme of a continuous diffusion process driven by a Brownian motion. Our methodology is based upon a recent work [22], which establishes Edgeworth expansions associated with asymptotic mixed normality using elements of Malliavin calculus. Potential applications of our theoretical results include higher order expansions for weak and strong approximation errors associated to the Euler scheme, and for studentized version of the error process.
C13|A meta-analysis of the price and income elasticities of food demand|Food demand elasticities are crucial parameters in the calibration of simulation models used to assess the impacts of political reforms or to analyse long-term projections, notably in agricultural sectors. Numerous estimates of these parameters are now available in the economic literature. The main objectives of this work are twofold: we seek first to identify general patterns characterizing the demand elasticities of food products and second to identify the main sources of heterogeneity between the elasticity estimates available in the literature. To achieve these objectives, we conduct a broad literature review of food demand elasticity estimates and perform a meta-regression analysis. Our results reveal the important impacts of income levels on income and price elasticities both at the country (gross domestic product-GDP) and household levels: the higher the income is, the lower the level of elasticities. Food demand responses to changes in income and prices appear to follow different patterns depending on the global regions involved apart from any income level consideration. From a methodological viewpoint, the functional forms used to represent food demand are found to significantly affect elasticity estimates. This result sheds light on the importance of the specification of demand functions, and particularly of their flexibility, in simulation models.
C13|Fertility Response to Climate Shocks|In communities highly dependent on rainfed agriculture for their livelihoods, the common oc- currence of climatic shocks such as droughts can lower the opportunity cost of having children, and raise fertility. Using longitudinal household data from Madagascar, we estimate the causal effect of drought occurrences on fertility, and explore the nature of potential mechanisms driving this effect. We exploit exogenous within-district year-to-year variation in rainfall deficits, and find that droughts occurring during the agricultural season significantly increase the number of children born to women living in agrarian communities. This effect is long lasting, as it is not reversed within four years fol- lowing the drought occurrence. Analyzing the mechanism, we find that droughts have no effect on common underlying factors of high fertility such as marriage timing and child mortality. Further- more, droughts have no significant effect on fertility if they occur during the non-agricultural season or in non-agrarian communities, and their positive effect in agrarian communities is mitigated by irrigation. These findings provide evidence that a low marginal price of having children is the main channel driving the fertility effect of drought in agrarian communities.
C13|Weather Shocks|How much do weather shocks matter? The literature addresses this question in two isolated ways: either by looking at long-term effects through the prism of theoretical models, or by focusing on short-term effects using empirical analysis. We propose a framework to bring together both the short and long-term effects through the lens of an estimated DSGE model with a weather-dependent agricultural sector. The model is estimated using Bayesian methods and quarterly data for New Zealand using the weather as an observable variable. In the short-run, our analysis underlines the key role of weather as a driver of business cycles over the sample period. An adverse weather shock generates a recession, boosts the non-agricultural sector and entails a domestic currency depreciation. Taking a long-term perspective, a welfare analysis reveals that weather shocks are not a free lunch: the welfare cost of weather is currently estimated at 0.19% of permanent consumption. Climate change critically increases the variability of key macroeconomic variables (such as GDP, agricultural output or the real exchange rate) resulting in a higher welfare cost peaking to 0.29% in the worst case scenario.
C13|Robust Multivariate Local Whittle Estimation and Spurious Fractional Cointegration|This paper derives a multivariate local Whittle estimator for the memory parameter of a possibly long memory process and the fractional cointegration vector robust to low frequency contaminations. This estimator as many other local Whittle based procedures requires a priori knowledge of the cointegration rank. Since low frequency contaminations bias inference on the cointegration rank, we also provide a robust estimator of the cointegration rank. As both estimators are based on the trimmed periodogram we further derive some insights in the behaviour of the periodogram of a process under very general types of low frequency contaminations. An extensive Monte Carlo exercise shows the applicability of our estimators in finite samples. Our procedures are applied to realized betas of two American energy companies discovering that the series are fractionally cointegrated. As the series exhibit low frequency contaminations, standard procedures are unable to detect this relation.
C13|Do Portfolio Investors Need To Consider The Asymmetry Of Returns On The Russian Stock Market?|This paper uses the parsimonious method of embedding skewness in asset allocation based on the Taylor expansion of the investor utility function up to the third term and maximizing it by portfolio weights. This approach also enables us to consider investor risk aversion. Time-dependent multivariate asset moments are obtained via the GOGARCH volatility model with a normal-inverse Gaussian distribution for the error term. We explore the performance of the usual 2 moment utility and its 3 moment counterpart for a portfolio consisted of twenty assets traded on the Russian stock market. The results demonstrate that the 3 moment utility significantly outperforms the 2 moment utility by SD, MAD and CVaR for low levels of absolute risk aversion and by portfolio returns and investor utility level during the whole forecast period.
C13|Multilateral Sato-Vartia Index for International Comparisons of Prices and Real Expenditures|The Sato-Vartia (SV) index for bilateral price comparisons has impressive analytical properties and is used intensively in recent international trade and macroeconomic analyses. In this paper we propose several ways of constructing transitive multilateral version of the SV index. We show that the SV index is only one of many logarithmic indices that satisfy the factor reversal test discussed in index number theory. We derive closed form expressions for the generalized SV indices and empirically implement the new indices for making cross-country price comparison using World Bank data from the 2011 International Comparison Program.
C13|Breaking Ties: Regression Discontinuity Design Meets Market Design|Centralized school assignment algorithms must distinguish between applicants with the same preferences and priorities. This is done with randomly assigned lottery numbers, nonlottery tie-breakers like test scores, or both. The New York City public high school match illustrates the latter, using test scores, grades, and interviews to rank applicants to screened schools, combined with lottery tie-breaking at unscreened schools. We show how to identify causal effects of school attendance in such settings. Our approach generalizes regression discontinuity designs to allow for multiple treatments and multiple running variables, some of which are randomly assigned. Lotteries generate assignment risk at screened as well as unscreened schools. Centralized assignment also identifies screened school effects away from screened school cutoffs. These features of centralized assignment are used to assess the predictive value of New York City’s school report cards. Grade A schools improve SAT math scores and increase the likelihood of graduating, though by less than OLS estimates suggest. Selection bias in OLS estimates is egregious for Grade A screened schools.
C13|Generalised Anderson-Rubin statistic based inference in the presence of a singular moment variance matrix| The particular concern of this paper is the construction of a confidence region with pointwise asymptotically correct size for the true value of a parameter of interest based on the generalized Anderson-Rubin (GAR) statistic when the moment variance matrix is singular. The large sample behaviour of the GAR statistic is analysed using a Laurent series expansion around the points of moment variance singularity. Under a condition termed first order moment singularity the GAR statistic is shown to possess a limiting chi-square distribution on parameter sequences converging to the true parameter value. Violation, however, of this condition renders the GAR statistic unbounded asymptotically. The paper details an appropriate discretisation of the parameter space to implement a feasible GAR-based confidence region that contains the true parameter value with pointwise asymptotically correct size. Simulation evidence is provided that demonstrates the efficacy of the GAR-based approach to moment-based inference described in this paper.
C13|Breaking Ties: Regression Discontinuity Design Meets Market Design|Centralized school assignment algorithms must distinguish between applicants with the same preferences and priorities. This is done with randomly assigned lottery numbers, non-lottery tie-breakers like test scores, or both. The New York City public high school match illustrates the latter, using test scores, grades, and interviews to rank applicants to screened schools, combined with lottery tie-breaking at unscreened schools. We show how to identify causal effects of school attendance in such settings. Our approach generalizes regression discontinuity designs to allow for multiple treatments and multiple running variables, some of which are randomly assigned. Lotteries generate assignment risk at screened as well as unscreened schools. Centralized assignment also identifies screened school effects away from screened school cutoffs. These features of centralized assignment are used to assess the predictive value of New York City’s school report cards. Grade A schools improve SAT math scores and increase the likelihood of graduating, though by less than OLS estimates suggest. Selection bias in OLS estimates is egregious for Grade A screened schools.
C13|Practical Significance, Meta-Analysis and the Credibility of Economics|Recently, there has been much discussion about replicability and credibility. By integrating the full research record, increasing statistical power, reducing bias and enhancing credibility, meta-analysis is widely regarded as 'best evidence'. Through Monte Carlo simulation, closely calibrated on the typical conditions found among 6,700 economics research papers, we find that large biases and high rates of false positives will often be found by conventional meta-analysis methods. Nonetheless, the routine application of meta-regression analysis and considerations of practical significance largely restore research credibility.
C13|Publication Bias and Editorial Statement on Negative Findings|"In February 2015, the editors of eight health economics journals sent out an editorial statement which aims to reduce the extent of specification searching and reminds referees to accept studies that: ""have potential scientific and publication merit regardless of whether such studies' empirical findings do or do not reject null hypotheses"". Guided by a pre-analysis, we test whether the editorial statement decreased the extent of publication bias. Our differences-in-differences estimates suggest that the statement decreased the proportion of tests rejecting the null hypothesis by 18 percentage points. Our findings suggest that incentives may be aligned to promote more transparent research."
C13|Inference with Arbitrary Clustering|Analyses of spatial or network data are now very common. Nevertheless, statistical inference is challenging since unobserved heterogeneity can be correlated across neighboring observational units. We develop an estimator for the variance-covariance matrix (VCV) of OLS and 2SLS that allows for arbitrary dependence of the errors across observations in space or network structure and across time periods. As a proof of concept, we conduct Monte Carlo simulations in a geospatial setting based on U.S. metropolitan areas. Tests based on our estimator of the VCV asymptotically correctly reject the null hypothesis, whereas conventional inference methods, e.g., those without clusters or with clusters based on administrative units, reject the null hypothesis too often. We also provide simulations in a network setting based on the IDEAS structure of coauthorship and real-life data on scientific performance. The Monte Carlo results again show that our estimator yields inference at the correct significance level even in moderately sized samples and that it dominates other commonly used approaches to inference in networks. We provide guidance to the applied researcher with respect to (i) whether or not to include potentially correlated regressors and (ii) the choice of cluster bandwidth. Finally, we provide a companion statistical package (acreg) enabling users to adjust the OLS and 2SLS coefficient's standard errors to account for arbitrary dependence.
C13|Statistical Analysis and Evaluation of Macroeconomic Policies: A Selective Review|In this paper, we highlight some recent developments of a new route to evaluate macroeconomic policy effects, which are investigated under the framework with potential out- comes. First, this paper begins with a brief introduction of the basic model setup in modern econometric analysis of program evaluation. Secondly, primary attention goes to the focus on causal effect estimation of macroeconomic policy with single time series data together with some extensions to multiple time series data. Furthermore, we examine the connection of this new approach to traditional macroeconomic models for policy analysis and evaluation. Finally, we conclude by addressing some possible future research directions in statistics and econometrics.
C13|Testing Unconfoundedness Assumption Using Auxiliary Variables|In this paper, we propose an alternative test procedure for testing the conditional independence assumption which is an important identication condition commonly imposed in the literature of program analysis and policy evaluation. We transform the conditional independence test to a nonparametric conditional moment test using an auxiliary variable which is independent of the treatment assignment variable conditional on potential outcomes and observable covariates. The proposed test statistic is shown to have a limiting normal distribution under null hypotheses of conditional independence. Furthermore, the suggested method is shown to be valid under time series framework and thus the corresponding test statistic and its limiting distribution are also established. Monte Carlo simulations are conducted to examine the finite sample performances of the proposed test statistics. Finally, the proposed test method is applied to test the conditional independence in real examples: the 401(k) participation program and return to college education.
C13|Influence functions for linear regression (with an application to regression adjustment)|Influence functions are useful, for example, because they provide an easy and flexible way to estimate standard errors. This paper contains a brief overview of influence functions in the context of linear regression and illustrates how their empirical counterparts can be computed in Stata, both for unweighted data and for weighted data. Influence functions for regression-adjustment estimators of average treatment effects are also covered.
C13|Efficient Estimation of Nonparametric Regression in The Presence of Dynamic Heteroskedasticity|We study the efficient estimation of nonparametric regression in the presence of heteroskedasticity. We focus our analysis on local polynomial estimation of nonparametric regressions with conditional heteroskedasticity in a time series setting. We introduce a weighted local polynomial regression smoother that takes account of the dynamic heteroskedasticity. We show that, although traditionally it is adviced that one should not weight for heteroskedasticity in nonparametric regressions, in many popular nonparametric regression models our method has lower asymptotic variance than the usual unweighted procedures. We conduct a Monte Carlo investigation that confirms the efficiency gain over conventional nonparametric regression estimators infinite samples.
C13|Nonparametric Recovery of the Yield Curve Evolution from Cross-Section and Time Series Information|We develop estimation methodology for an additive nonparametric panel model that is suitable for capturing the pricing of coupon-paying government bonds followed over many time periods. We use our model to estimate the discount function and yield curve of nominally riskless government bonds. The novelty of our approach is the combination of two different techniques: cross-sectional nonparametric methods and kernel estimation for time varying dynamics in the time series context. The resulting estimator is able to capture the yield curve shapes and dynamics commonly observed in the fixed income markets. We establish the consistency, the rate of convergence, and the asymptotic normality of the proposed estimator. A Monte Carlo exercise illustrates the good performance of the method under different scenarios. We apply our methodology to the daily CRSP bond dataset, and compare with the popular Diebold and Li (2006) method.
C13|The Impact of Unilateral Carbon Taxes on Cross-Border Electricity Trading|Market coupling makes efficient use of interconnectors by ensuring higher-price markets import until prices are equated or interconnectors constrained. A carbon tax in one of the market can distort trade and reduce price convergence. We investigate econometrically the impact of the British Carbon Price Support (CPS, an extra carbon tax) on GBs cross-border electricity trading with France (through IFA) and the Netherlands (through BritNed). Over 2015-2018 the CPS would have raised the GB day-ahead price by an average of about €10/MWh in the absence of compensating adjustments through increased imports. The actual price differential with our neighbours fell to about €8/MWh allowing for replacement by cheaper imports. The CPS increased GB imports by 13 TWh/yr, thereby reducing carbon tax revenue by €103 million/yr. Congestion income increased by €133 m/yr, half transferred to foreign interconnector owners. The unilateral CPS created €28 m/yr deadweight loss. About 18% of the increase in the GB price caused by the CPS was passed through to higher French prices and 29% in higher Dutch prices.
C13|Dependent Microstructure Noise and Integrated Volatility: Estimation from High-Frequency Data|In this paper, we develop econometric tools to analyze the integrated volatility (IV) of the efficient price and the dynamic properties of microstructure noise in high-frequency data under general dependent noise. We first develop consistent estimators of the variance and autocovariances of noise using a variant of realized volatility. Next, we employ these estimators to adapt the pre-averaging method and derive consistent estimators of the IV, which converge stably to a mixed Gaussian distribution at the optimal rate n1/4. To improve the finite sample performance, we propose a multi-step approach that corrects the finite sample bias, which turns out to be crucial in applications. Our extensive simulation studies demonstrate the excellent performance of our multi-step estimators. In an empirical study, we analyze the dependence structures of microstructure noise and provide intuitive economic interpretations; we also illustrate the importance of accounting for both the serial dependence in noise and the finite sample bias when estimating IV.
C13|State-dependent Monetary Policy Regimes|Are monetary policy regimes state-dependent? To answer the question this paper estimates New Keynesian general equilibrium models that allow the state of the economy to influence the monetary authority’s stance on inflation. I take advantage of recent developments in solving rational expectations models with state-dependent parameter drift to estimate three models on U.S. data between 1965-2009. In these models, the probability of remaining in a monetary policy regime that is relatively accommodative towards inflation, varies over time and depends on endogenous model variables; in particular, either deviations of inflation or output from their respective targets or a monetary policy shock. The main contribution of this paper is that it finds evidence of state-dependent monetary policy regimes. The model that allows inflation to influence the monetary policy regime in place, fits the data better than an alternative model with regime changes that are not state-dependent. This finding points towards reconsidering how changes in monetary policy are modeled.
C13|Estimation and inference in spatial models with dominant units|Estimation and inference in the spatial econometrics literature are carried out assuming that the matrix of spatial or network connections has uniformly bounded absolute column sums in the number of cross-section units, n. In this paper, we consider spatial models where this restriction is relaxed. The linear-quadratic central limit theorem of Kelejian and Prucha (2001) is generalized and then used to establish the asymptotic properties of the GMM estimator due to Lee (2007) in the presence of dominant units. A new Bias-Corrected Method of Moments estimator is also proposed that avoids the problem of weak instruments by self-instrumenting the spatially lagged dependent variable. Both estimators are shown to be consistent and asymptotically normal, depending on the rate at which the maximum column sum of the weights matrix rises with n. The small sample properties of the estimators are investigated by Monte Carlo experiments and shown to be satisfactory. An empirical application to sectoral price changes in the US over the pre- and post-2008 financial crisis is also provided. It is shown that the share of capital can be estimated reasonably well from the degree of sectoral interdependence using the input-output tables, despite the evidence of dominant sectors being present in the US economy.
C13|Estimation of Large Dimensional Conditional Factor Models in Finance|This chapter provides an econometric methodology for inference in large-dimensional conditional factor models in finance. Changes in the business cycle and asset characteristics induce time variation in factor loadings and risk premia to be accounted for. The growing trend in the use of disaggregated data for individual securities motivates our focus on methodologies for a large number of assets. The beginning of the chapter outlines the concept of approximate factor structure in the presence of conditional information, and develops an arbitrage pricing theory for large-dimensional factor models in this framework. Then we distinguish between two different cases for inference depending on whether factors are observable or not. We focus on diagnosing model specification, estimating conditional risk premia, and testing asset pricing restrictions under increasing cross-sectional and time series dimensions. At the end of the chapter, we review some of the empirical findings and contrast analysis based on individual stocks and standard sets of portfolios. We also discuss the impact on computing time-varying cost of equity for a firm, and summarize differences between results for developed and emerging markets in an international setting.
C13|Does Obamacare Care? A Fuzzy Difference-in-discontinuities Approach|This paper explores the use of fuzzy regression-discontinuity design in the context where multiple treatments are applied at the threshold. The identification result shows that, under a very strong assumption of equality of treatment probability changes at the cutoff point, a difference in fuzzy discontinuity identify a treatment effect of interest. Using the data from the National Health Interview Survey (NHIS), we apply this identification strategy to evaluate the causal effect of the Affordable Care Act (ACA) on health care access and utilization of old Americans. We find results suggesting that the implementation of the Affordable Care Act has led to an increase in the hospitalization rate of elderly American--5% more hospitalization. It has caused a minor increase of cost-related direct barrier to access to care--3.6% increase in the probability of delaying care for cost reasons. The ACA has also exacerbated cost-related barriers to follow-up and continuity care -- 7% more elderly couldn't afford prescriptions, 7% more couldn't see a specialist and, 5.5% more couldn't afford a follow-up visit -- as result of ACA
C13|Identification versus misspecification in New Keynesian monetary policy models|In this paper, we study identification and misspecification problems in standard closed and open-economy empirical New-Keynesian DSGE models used in monetary policy analysis. We find that problems with model misspecification still appear to be a first-order issue in monetary DSGE models, and argue that it is problems with model misspecification that may benefit the most from moving from a classical to a Bayesian framework. We also argue that lack of identification should neither be ignored nor be assumed to affect all DSGE models. Fortunately, identification problems can be readily assessed on a case-by-case basis, by applying recently developed pre-tests of identification.
C13|Dealing with misspecification in structural macroeconometric models|We consider a set of potentially misspecified structural models, geometrically combine their likelihood functions, and estimate the parameters using composite methods. Composite estimators may be preferable to likelihood-based estimators in the mean squared error. Composite models may be superior to individual models in the Kullback-Leibler sense. We describe Bayesian quasi-posterior computations and compare the approach to Bayesian model averaging, finite mixture methods, and robustness procedures. We robustify inference using the composite posterior distribution of the parameters and the pool of models. We provide estimates of the marginal propensity to consume and evaluate the role of technology shocks for output fluctuations.
C13|Treatment Effects with Heterogeneous Externalities|This paper proposes a new method for estimating heterogeneous externalities in policy analysis when social interactions take the linear-in-means form. We establish that the parameters of interest can be identified and consistently estimated using specific functions of the share of the eligible population. We also study the finite sample performance of the proposed estimators using Monte Carlo simulations. The method is illustrated using data on the PROGRESA program. We find that more than 50 percent of the effects of the program on schooling attendance are due to externalities, which are heterogeneous within and between poor and nonpoor households.
C13|Imposing Equilibrium Restrictions in the Estimation of Dynamic Discrete Games|Imposing equilibrium restrictions provides substantial gains in the estimation of dynamic discrete games. Estimation algorithms imposing these restrictions -- MPEC, NFXP, NPL, and variations -- have different merits and limitations. MPEC guarantees local convergence, but requires the computation of high-dimensional Jacobians. The NPL algorithm avoids the computation of these matrices, but -- in games -- may fail to converge to the consistent NPL estimator. We study the asymptotic properties of the NPL algorithm treating the iterative procedure as performed in finite samples. We find that there are always samples for which the algorithm fails to converge, and this introduces a selection bias. We also propose a spectral algorithm to compute the NPL estimator. This algorithm satisfies local convergence and avoids the computation of Jacobian matrices. We present simulation evidence illustrating our theoretical results and the good properties of the spectral algorithm.
C13|Bias assessment and reduction for the 2SLS estimator in general dynamic simultaneous equations models|We consider the bias of the 2SLS estimator in general dynamic simultaneousequation models with g endogenous regressors. By using asymptotic expansion techniques we approximate 2SLS coefficient estimation bias under innovation errors, p lagged-dependent variables and strongly-exogenous explanatory variables. Large-T approximations bias of the structural form is then used to construct corrected estimators for the parameters of interest in the general DSEM (C2SLS). Simulations show that the C2SLS gives almost unbiased estimators and low mean squared errors. Alternatively, the numerical bootstrap method results suggest that the non-parametric bootstrap could be used in 2SLS for improving estimation in general DSEM.
C13|The analysis of marked and weighted empirical processes of estimated residuals|An extended and improved theory is presented for marked and weighted empirical processes of residuals of time series regressions. The theory is motivated by 1-step Huber-skip estimators, where a set of good observations are selected using an initial estimator and an updated estimator is found by applying least squares to the selected observations. In this case, the weights and marks represent powers of the regressors and the regression errors, respectively. The inclusion of marks is a non-trivial extention to previous theory and requires refined martingale arguments.
C13|Keynesian Models, Detrending, and the Method of Moments|One important question in the Keynesian literature is whether we should detrend data when estimating the parameters of a Keynesian model using the moment method. It has been common in the literature to detrend data in the same way the model is detrended. Doing so works relatively well with linear models, in part because in such a case the information that disappears from the data after the detrending process is usually related to the parameters that also disappear from the detrended model. Unfortunately, in heavy non-linear Keynesian models, parameters rarely disappear from detrended models, but information does disappear from the detrended data. Using a simple real business cycle model, we show that both the moment method estimators of parameters and the estimated responses of endogenous variables to a technological shock can be seriously inaccurate when the data used in the estimation process are detrended. Using a dynamic stochastic general equilibrium model and U.S. data, we show that detrending the data before estimating the parameters may result in a seriously misleading response of endogeneous variables to monetary shocks. We suggest building the moment conditions using raw data, irrespective of the trend observed in the data.
C13|Integer-valued stochastic volatility|We propose a novel class of count time series models, the mixed Poisson integer-valued stochastic volatility models. The proposed specification, which can be considered as an integer-valued analogue of the discrete-time stochastic volatility model, encompasses a wide range of conditional distributions of counts. We study its probabilistic structure and develop an easily adaptable Markov chain Monte Carlo algorithm, based on the Griddy-Gibbs approach that can accommodate any conditional distribution that belongs to that class. We demonstrate that by considering the cases of Poisson and negative binomial distributions. The methodology is applied to simulated and real data.
C13|Economic complexity and jobs: an empirical analysis|This paper analyses the impact of economic complexity on the labour market using annual data on OECD countries for the period 1985-2008 and averaged data over the period 1990-2010 for 74 developed and developing countries with a large number of controls. We show that moving to higher levels of economic sophistication leads to less unemployment and more employment, showing that economic complexity does not induce job loss. Our findings remain robust across alternative econometric specifications. Furthermore, we place the spotlight on the link between products’ embodied knowledge (sophistication) and labour market outcomes at the micro-level. We build a product-level index that attaches a product to the average level of unemployment (or employment) in the countries that export it. With this index, we illustrate how the development of sophisticated products is associated with changes in the labour market and show that the economic sophistication of an economy captures information about the economy’s job creation and destruction.
C13|Exchange Rate Pass-through to Prices : Bayesian VAR Evidence for Ghana|Using quarterly data from 2006q3 to 2017q4, this paper employed sign restrictions with rejection method in a Vector Autoregression to estimate the pass-through of exchange rate dynamics to domestic prices in Ghana. The priors of the model belongs to the flat Normal inverted-Wishart family. Markov Chain Monte Carlo (MCMC) is used to collect 1000 draws from the posterior distribution of the SVAR parameters that satisfy the sign restrictions. The model specification included some idiosyncratic features of the Ghanaian economy such as the dependence on primary export commodities for foreign exchange revenue and the dependence on foreign aid. Impulse response functions was used to analyze exchange rate pass-through whilst variance decomposition was used to explain the most dominant source of inflation in the study sample. The impulse response showed a fairly large but not unitary pass-through of exchange rate dynamics to domestic prices. The implication herein is that exchange rate depreciation led to upsurge in prices in Ghana albeit, the impact is incomplete. Results from the variance decomposition indicated a monetary expansion was most dominant in explaining inflationary pressures in Ghana. For inflation to be lowered, policy directives should be geared towards exchange rate stability as well as ensuring a stable interest rate environment.
C13|A sectoral approach to the electricity-growth nexus in the Eastern Cape province of South Africa|This paper takes a sectoral, panel approach to investigating the electricity-growth nexus for the Eastern Cape province of South Africa between the period of 2003 and 2017. The empirical investigation was carried out using the Pooled Mean Group (PMG) panel estimators applied to an augmented dynamic growth model whilst the caulisty tests between electricity consumption and growth where performed using the Dumitrescu-Hurlin (2012) panel non-causality tests. The findings confirm the absence of significant long-run relationship between electricity and growth whilst finding a significant and positive effect over the short-run. Moreover, our causality tests provide strong evidence of causality running from electricity consumption to economic growth hence supporting the “growth hypothesis”. In a nutshell, our results not only demonstrate the importance of performing the electricity-growth analysis at provincial level as opposed to relying on national aggregated estimates but also provides important provincial-specific policy implications and recommendations.
C13|Resolutions to flip-over credit risk and beyond|Abstract Given a risk outcome y over a rating system {R_i }_(i=1)^k for a portfolio, we show in this paper that the maximum likelihood estimates with monotonic constraints, when y is binary (the Bernoulli likelihood) or takes values in the interval 0≤y≤1 (the quasi-Bernoulli likelihood), are each given by the average of the observed outcomes for some consecutive rating indexes. These estimates are in average equal to the sample average risk over the portfolio and coincide with the estimates by least squares with the same monotonic constraints. These results are the exact solution of the corresponding constrained optimization. A non-parametric algorithm for the exact solution is proposed. For the least squares estimates, this algorithm is compared with “pool adjacent violators” algorithm for isotonic regression. The proposed approaches provide a resolution to flip-over credit risk and a tool to determine the fair risk scales over a rating system.
C13|Monotonic Estimation for the Survival Probability over a Risk-Rated Portfolio by Discrete-Time Hazard Rate Models|Monotonic estimation for the survival probability of a loan in a risk-rated portfolio is based on the observation arising, for example, from loan pricing that a loan with a lower credit risk rating is more likely to survive than a loan with a higher credit risk rating, given the same additional risk covariates. Two probit-type discrete-time hazard rate models that generate monotonic survival probabilities are proposed in this paper. The first model calculates the discrete-time hazard rate conditional on systematic risk factors. As for the Cox proportion hazard rate model, the model formulates the discrete-time hazard rate by including a baseline component. This baseline component can be estimated outside the model in the absence of model covariates using the long-run average discrete-time hazard rate. This results in a significant reduction in the number of parameters to be otherwise estimated inside the model. The second model is a general form model where loan level factors can be included. Parameter estimation algorithms are also proposed. The models and algorithms proposed in this paper can be used for loan pricing, stress testing, expected credit loss estimation, and modeling of the probability of default term structure.
C13|Monotonic Estimation for Probability Distribution and Multivariate Risk Scales by Constrained Minimum Generalized Cross-Entropy|Minimum cross-entropy estimation is an extension to the maximum likelihood estimation for multinomial probabilities. Given a probability distribution {r_i }_(i=1)^k, we show in this paper that the monotonic estimates {p_i }_(i=1)^k for the probability distribution by minimum cross-entropy are each given by the simple average of the given distribution values over some consecutive indexes. Results extend to the monotonic estimation for multivariate outcomes by generalized cross-entropy. These estimates are the exact solution for the corresponding constrained optimization and coincide with the monotonic estimates by least squares. A non-parametric algorithm for the exact solution is proposed. The algorithm is compared to the “pool adjacent violators” algorithm in least squares case for the isotonic regression problem. Applications to monotonic estimation of migration matrices and risk scales for multivariate outcomes are discussed.
C13|New Approach to Estimating Gravity Models with Heteroscedasticity and Zero Trade Values|This paper proposes new estimation techniques for gravity models with zero trade values and heteroscedasticity. We revisit the standard PPML estimator and we propose an improved version. We also propose various Heckman estimators with different distributions of the residuals, nonlinear forms of both selection and measure equations, and various process of the variance. We add to the existent literature alternative estimation methods taking into account the non-linearity of both the variance and the selection equation. Moreover, because of the unavailability of pre-set package in the econometrics software (Stata, Eviews, Matlab, etc.) to perform the estimation of the above-mentioned Heckman versions, we had to code it in Matlab using a combination of fminsearch and fminunc functions. Using numerical gradient matrix G, we report standard errors based on the BHHH technique. The proposed new Heckman version could be used in other applications. Our results suggest that previous empirical studies might be overestimating the contribution of the GDP of both import and export countries in determining the bilateral trade.
C13|Dynamics between Financial development, Energy consumption and Economic growth in Sub-Saharan African countries: Evidence from an asymmetrical and nonlinear analysis|This paper analyzes the asymmetrical relationship between financial development, energy consumption and economic growth in twenty-one (21) sub-Saharan African (SSA) countries from 1990Q1 to 2014Q4. We used the nonlinear autoregressive distributed lag (NARDL) framework and asymmetrical causality tests to examine the relationship between the variables. First, the country-level analysis reveals that there is asymmetrical cointegration between the variables in some countries and mixed results of the causal effects of financial development and energy consumption on economic growth across countries. Second, the results of the panel data analysis confirm the asymmetrical cointegration in the SSA region, especially in lower-middle-income countries than in upper-middle-income countries. We find that positive changes in energy consumption significantly reduce economic growth, contrary to the negative changes in the long-term. Besides, positive shocks to financial development favor more economic growth than the adverse shocks in the long-term in the SSA region. However, financial development hurts economic growth, contrary to energy consumption in the short-term. Finally, the results show bidirectional causality between positive changes in energy consumption and economic growth, but unidirectional causality running from negative changes in energy consumption to economic growth in the SSA region. There is also bidirectional causality between positive and negative shocks to financial development and economic growth in SSA region, but mixed results across lower-income countries and upper-middle-income countries. Therefore, our study suggests that energy-saving policies such as renewable energies can be implemented in the SSA region to promote sustainable development. In addition, policy-makers should adopt an efficient allocation of the credits to the private sector supporting productive investments. They should also pay attention to the asymmetrical relationship between financial development, energy consumption and economic growth in most SSA countries in the conduct of economic policies.
C13|IFRS9 Expected Credit Loss Estimation: Advanced Models for Estimating Portfolio Loss and Weighting Scenario Losses|Estimation of portfolio expected credit loss is required for IFRS9 regulatory purposes. It starts with the estimation of scenario loss at loan level, and then aggregated and summed up by scenario probability weights to obtain portfolio expected loss. This estimated loss can vary significantly, depending on the levels of loss severity generated by the IFSR9 models, and the probability weights chosen. There is a need for a quantitative approach for determining the weights for scenario losses. In this paper, we propose a model to estimate the expected portfolio losses brought by recession risk, and a quantitative approach for determining the scenario weights. The model and approach are validated by an empirical example, where we stress portfolio expected loss by recession risk, and calculate the scenario weights accordingly.
C13|South African unemployment in the post-financial crisis era: What are the determinants?|High unemployment rates is one of the greatest economic challenges facing post-apartheid South African government over the past two decades and this problem has become more worrisome in the post-global financial crisis period. Our study examines the determinants of unemployment for the South African economy in the post-crisis period over a quarterly frequency period of 2009:Q1 to 2018:Q4. The determinants are examined for 4 classes of unemployment rates (total, male, female and youth) and we further partition possible unemployment determinants into fiscal, monetary and macroeconomic variables. The estimation results from the employed autoregressive distributive lag (ARDL) models find income tax, repo rates, economic growth, trade, investment, household debt and savings to be significant determinants of unemployment in the post-crisis South African economy and yet we note discrepancies of the significance of these determinants amongst different unemployment categories. Relevant policy implications are matched against our obtained empirical findings.
C13|The calculation of Solvency Capital Requirement using Copulas|Our aim is to present an alternative methodology to the standard formula imposed to the insurance regulation (the European directive knows as Solvency II) for the calculus of the capital requirements. We want to demonstrate how this formula is now obsolete and how is possible to obtain lower capital requirement through the theory of the copulas, function that are gaining increasing importance in various economic areas. A lower capital requirement involves the advantage for the various insurance companies not to have unproductive capital that can therefore be used for the production of further profits. Indeed the standard formula is adequate only with some particular assumptions, otherwise it can overestimate the capital requirements that are actually needed as the standard formula underestimates the effect of diversification.
C13|The Impacts of Domestic and Foreign Direct Investments on Economic Growth: Fresh Evidence from Tunisia|This paper aims to analyze the impact of domestic investment and Foreign Direct Investment on economic growth in Tunisia during the period 1976–2017. This study is based on the Auto-Regressive Distributive Lags (ARDL) approach that is proposed by Pesaran et al (2001). Bound testing approaches to the analysis of level relationships. According to the results of the analysis, domestic investment and foreign direct investment have a negative effect on economic growth in the long run. However, in the short run, only domestic investment causes economic growth. The findings are important for Tunisian economic policy makers to undertake the effective policies that can promote and lead domestic and foreign investments to boost economic growth.
C13|Economic growth, environmental degradation and business cycles in Eswatini|This study investigates the impact of the business cycle on the Environmental Kuznets Curve (EKC) for the Eswatini Kingdom over the period 1970 – 2014. To this end, we employ the nonlinear autoregressive distributive lag (NARDL) model to capture the long-run and short-run cointegration effects between economic activity and greenhouse gas (GHG) emissions over different phases of the business cycle. Our findings reveal that economic activity only degrades the environment during upswing of the economic cycle whilst this relationship is insignificant during downswing of the cycle. We specifically compute a value of $3.57 worth of output been gained at the cost of a metric unit of emissions during economic expansionary phases. Altogether, these results insinuate much needed government intervention in the market for emissions via environmental tax reforms (ETR) which should be designed with countercyclical bias towards upswing the business cycle.
C13|The human capital-economic growth nexus in SSA countries: What can strengthen the relationship?|The World Bank has recently placed increasing emphasis on the role of human capital development in facilitating economic development in the Sub-Saharan African (SSA) region. Our study examines the impact of human capital on economic growth for a selected sample of 9 SSA countries between 1980 and 2016 using a panel econometric approach. Interestingly enough, our empirical analysis shows an insignificant effect of human capital on economic growth for our selected sample. These findings remain unchanged even after adding interactive terms to human capital which are representative of government spending as well as foreign direct investment. Nevertheless, we establish a positive and significant effect of the interactive term between urbanization and human capital on economic growth, a result which emphasizes the importance of developing urbanized, ‘smart’, technologically-driven cities within the SSA region as a platform towards strengthening the impact of human capital- economic growth relationship.
C13|Do presidential elections affect stock market returns in Nigeria?|Evidences thrive globally on the effects of political regimes, presidential elections, on stock market returns. In the same vein, this paper analyses the effects of general elections on stock returns and volatility around the election periods at the Nigerian Stock Exchange (NSE) market. The paper applies an event study approach to delineate event windows, a 5-month event window for each election was adopted comprising of an election month, and 2 months before and after each election. Returns were calculated using daily closing prices of NSE’s All Share Index (ASI) for a total of 6 elections held between 1999 and 2019. Asymmetric GARCH – EGARCH and TARCH and the Markov Switching autoregressive methodologies were applied. ASI exhibits nonlinearity and structural breaks across all the presidential elections which makes single regime model ill appropriate for modelling stock runs volatility. Evidence of an unstable and explosive conditional variance is noticeable in the 2015 presidential election market returns while leverage effect was found in the 1999 and 2007 elections, that is, bad news produces more volatility on stock returns than good news. The MS-AR (3) model neatly characterizes the NSE’s daily stock returns into bearish and bullish regime, i.e., high (low) volatility low (high) returns as regime 1 and 2, respectively. The time varying transition volatility and regime durations corroborate, in different magnitude, the regime characterization across the 6 time horizons. The paper pioneer’s an analysis of effects of elections on stock returns in Nigeria and a useful information to investors.
C13|Virtual Historical Simulation for estimating the conditional VaR of large portfolios|"In order to estimate the conditional risk of a portfolio's return, two strategies can be advocated. A multivariate strategy requires estimating a dynamic model for the vector of risk factors, which is often challenging, when at all possible, for large portfolios. A univariate approach based on a dynamic model for the portfolio's return seems more attractive. However, when the combination of the individual returns is time varying, the portfolio's return series is typically non stationary which may invalidate statistical inference. An alternative approach consists in reconstituting a ""virtual portfolio"", whose returns are built using the current composition of the portfolio and for which a stationary dynamic model can be estimated. This paper establishes the asymptotic properties of this method, that we call Virtual Historical Simulation. Numerical illustrations on simulated and real data are provided."
C13|Forecasting with many predictors using message passing algorithms|Machine learning methods are becoming increasingly popular in economics, due to the increased availability of large datasets. In this paper I evaluate a recently proposed algorithm called Generalized Approximate Message Passing (GAMP) , which has been very popular in signal processing and compressive sensing. I show how this algorithm can be combined with Bayesian hierarchical shrinkage priors typically used in economic forecasting, resulting in computationally efficient schemes for estimating high-dimensional regression models. Using Monte Carlo simulations I establish that in certain scenarios GAMP can achieve estimation accuracy comparable to traditional Markov chain Monte Carlo methods, at a tiny fraction of the computing time. In a forecasting exercise involving a large set of orthogonal macroeconomic predictors, I show that Bayesian shrinkage estimators based on GAMP perform very well compared to a large set of alternatives.
C13|State-dependent Monetary Policy Regimes| Are monetary policy regimes state-dependent? To answer the question this paper estimates New Keynesian general equilibrium models that allow the state of the economy to influence the monetary authority's stance on inflation. I take advantage of recent developments in solving rational expectations models with state-dependent parameter drift to estimate three models on U.S. data between 1965-2009. In these models, the probability of remaining in a monetary policy regime that is relatively accommodative towards inflation, varies over time and depends on endogenous model variables; in particular, either deviations of inflation or output from their respective targets or a monetary policy shock. The main contribution of this paper is that it finds evidence of state-dependent monetary policy regimes. The model that allows inflation to influence the monetary policy regime in place, fits the data better than an alternative model with regime changes that are not state-dependent. This finding points towards reconsidering how changes in monetary policy are modelled.
C13|A meta-analysis of the price and income elasticities of food demand| Food demand elasticities are crucial parameters in the calibration of simulation models used to assess the impacts of political reforms or to analyse long-term projections, notably in agricultural sectors. Numerous estimates of these parameters are now available in the economic literature. The main objectives of this work are twofold: we seek first to identify general patterns characterizing the demand elasticities of food products and second to identify the main sources of heterogeneity between the elasticity estimates available in the literature. To achieve these objectives, we conduct a broad literature review of food demand elasticity estimates and perform a meta-regression analysis. Our results reveal the important impacts of income levels on income and price elasticities both at the country (gross domestic product-GDP) and household levels: the higher the income is, the lower the level of elasticities. Food demand responses to changes in income and prices appear to follow different patterns depending on the global regions involved apart from any income level consideration. From a methodological viewpoint, the functional forms used to represent food demand are found to significantly affect elasticity estimates. This result sheds light on the importance of the specification of demand functions, and particularly of their flexibility, in simulation models.
C13|Reducing Informality Using Two-Sided Incentives: Theory and Experiment|We study the impact of two-sided incentives on the reduction of informality. We model those incentives using the notion of network externalities, which link the (formal or informal) merchant’s profits to the type of customers they serve (formal or informal). Our theoretical framework yields two straightforward testable implications: the merchant will find more profitable to become formal (or informal), as long as more of their customers are formal (or informal); and, formal and informal commercial sectors may coexist in equilibrium. We test these hypotheses using data from a field experiment, conducted with micro and small enterprises in Lima, Peru. Our subjects had to choose, in a repeated fashion, among three ‘platforms’, which proxy for being formal, informal, or performing a reservation activity. We then changed the relative size of the network of formal vis-á-vis informal customers, in order to calculate the consumer’s network externality. We find that the network externality is relatively large, a result that opens up the possibility to reduce commercial informality using two-sided incentives. Moreover, the platform choice between the formal and informal sectors is sensitive to risk preferences.
C13|Estimación del Consumo a partir de sus Componentes Principales en la Tabla Insumo-Producto|Una forma de entender el consumo privado es subdividir esta variable macroeconómica agregada en sus componentes y estudiar las partes. En este documento se estima el comportamiento de los componentes más importantes del consumo privado al cual se denomina componentes principales. Tomando como punto de inicio la información de la Tabla Insumo Producto para distintos años, se utiliza un conjunto de variables proxi para cada componente a partir de los cuales se obtiene una distribución del consumo por componente para cada año. Los componentes restantes forman parte de una serie denominada Otros, cuyo rol es de disciplinar los resultados mediante el registro de ciertas regularidades en su conducta. Esta metodología permite proyectar el consumo privado con un bajo error de proyección.
C13|Specification Tests for Temporal Heterogeneity in Spatial Panel Models with Fixed Effects|We propose score type tests for testing the existence of temporal heterogeneity in slope and spatial parameters in spatial panel data (SPD) models, allowing for the presence of individual-specific and/or time-specific fixed effects (or in general intercept heterogeneity). The SPD model with spatial lag effect is treated in detail by first considering the model with individual-specific effects only, and then extending it to the model with both individual and time specific effects. Two types of tests (naive and robust) are proposed, and their asymptotic properties are presented. These tests are then fully extended to an SPD model with both spatial lag and spatial error effects. Monte Carlo results show that the robust tests have much superior finite and large sample properties than the naive tests. Thus, the proposed robust tests provide reliable tools for identifying possible existence of temporal heterogeneity in regression and spatial coefficients. Empirical illustrations of the proposed tests are given.
C13|Estimation of Stochastic Frontier Panel Data Models with Spatial Inefficiency|"This paper proposes a stochastic frontier panel data model in which unit-specific inefficiencies are spatially correlated. In particular, this model has simultaneously three important features: i) the total inefficiency of a productive unit depends on its own inefficiency and on the inefficiency of its neighbors; ii) the spatially correlated and time varying inefficiency is disentangled from time invariant unobserved heterogeneity in a panel data model à la Greene (2005); iii) systematic differences in inefficiency can be explained using exogenous determinants. We propose to estimate both the ""true"" fixed- and random-effects variants of the model using a feasible simulated composite maximum likelihood approach. The finite sample behavior of the proposed estimators are investigated through a set of Monte Carlo experiments. Our simulation results suggest that the estimation approach is consistent, showing good finite sample properties especially in small samples."
C13|Nearest Comoment Estimation With Unobserved Factors|We propose a minimum distance estimator for the higher-order comoments of a multivariate distribution exhibiting a lower dimensional latent factor structure. We derive the in uence function of the proposed estimator and prove its consistency and asymptotic normality. The simulation study confirms the large gains in accuracy compared to the traditional sample comoments. The empirical usefulness of the novel framework is shown in applications to portfolio allocation under non-Gaussian objective functions and to the extraction of factor loadings in a dataset with mental ability scores.
C13|Temporal Disaggregation of Business Dynamics: New Evidence for U.S. Economy|We provide new disaggregated data and stylized facts on firm dynamics of the U.S economy by using a state-space method to transform Census yearly data of entry and exit from 1977 to 2013 into quarterly frequency. Entry is lagging and symmetric, while exit is leading and asymmetric along the business cycle. We select the most significant determinants of these variables by matching Census data with a new database by Federal Reserve. These determinants differ considerably among entry and exit. Finally, standard macroeconometric models estimated on our disaggregated series support the recent theoretical literature, according to which the cleansing effect of recession is mainly due to exit asymmetry.
C13|Identification-Robust Nonparametric Inference in a Linear IV Model|For a linear IV regression, we propose two new inference procedures on parameters of endogenous variables that are robust to any identification pattern, do not rely on a linear first-stage equation, and account for heteroskedasticity of unknown form. Building on Bierens (1982), we first propose an Integrated Conditional Moment (ICM) type statistic constructed by setting the parameters to the value under the null hypothesis. The ICM procedure tests at the same time the value of the coefficient and the specification of the model. We then adopt the conditionality principle used by Moreira (2003) to condition on a set of ICM statistics that informs on identification strength. Our two procedures uniformly control size irrespective of identification strength. They are powerful irrespective of the nonlinear form of the link between instruments and endogenous variables and are competitive with existing procedures in simulations and applications.
C13|Technological Innovation, Diffusion and Business Cycle Dynamics|In the U.S. economy, real output growth is forecastable with its own lag and the lagged consumption-output ratio. With technological progress modeled as a random walk, standard real-business-cycle (RBC) models fails to replicate this fact: these models generate equilibrium output dynamics, leading some researchers to question the empirical relevance of technology shocks for generating business cycle dynamics. In this paper, we develop a model economy that exhibits endogenous stochastic growth with aggregate fluctuations driven by technology shocks that are absorbed with some lag, reflecting the costly and time-consuming nature of innovation and diffusion. Our purpose is to compare the resulting business cycle dynamics with those predicted by standard RBC models, which implicitly assume the instantaneous diffusion of new technology. Unlike the standard RBC model, the environment studied here generates business cycle dynamics that better resemble observed patterns.<br><small>(This abstract was borrowed from another version of this item.)</small>
C13|Drawbacks in the 3-Factor Approach of Fama and French (2018)|"This paper features a statistical analysis of the monthly three factor Fama/French return series. We apply rolling OLS regressions to explore the relationship between the 3 factors, using monthly and weekly data from July 1926 to June 2018, that are freely available on French's website. The results suggest there are significant and time-varying relationships between the factors. This is conirmed by non-parametric tests. We then switch to a sub-sample from July 1990 to July 2018, also taken from French's website. The three series and their interrelationships are analysed using two stage least squares and the Hausman test to check for issues related to endogeneity, the Sargan over-identification test and the Cragg-Donald weak instrument test. The relationship between factors is also examined using OLS, incorporating Ramsey's RESET tests of functional form misspecification, plus Naradaya-Watson kernel regression techniques. The empirical results suggest that the factors, when combined in OLS regression analysis, as suggested by Fama and French (2018), are likely to suffer from endogeneity. OLS regression analysis and the application of Ramsey's RESET tests suggest a non-linear relationship exists between the three series, in which cubed terms are significant. This non-linearity is also confirmed by the kernel regression analysis. We use two instruments to estimate the market betas, and then use the factor estimates in a second set of panel data tests using a small sample of monthly returns for US firms that are drawn from the online data source ""tingo"". These issues are analysed using methods suggested by Petersen (2009) to permit clustering in the panels by date and firm. The empirical results suggest that using an instrument to capture endogeneity reduces the standard error of market beta in subsequent cross-sectional tests, but that clustering effects, as suggested by Petersen (2009), will also impact on the estimated standard errors. The empirical results suggest that using these factors in linear regression analysis, such as suggested by Fama and French (2018), as a method of screening factor relevance, is problematic in that the estimated standard errors are highly sensitive to the correct model specification."
C13|The Role of Asymmetry and Uncertainties in the Capital Flows- Economic Growth Nexus|This study examines the asymmetry between capital flows and economic growth in 42 countries for the period 1990-2017. It further argues that uncertainty is an important channel through which asymmetry operates. As such, the three measures of uncertainty are macroeconomic, fiscal and institutional. The Generalised Method of Moments is used as an empirical strategy. The existence of an asymmetry is confirmed by the findings as capital flows are more reactive to economic drag when compared to economic growth. Furthermore, the channels through which asymmetry operate are heterogeneous to measures of capital flows and proxies for uncertainty.
C13|Estimating impulse response functions when the shock series is observed|We compare the finite sample performance of a variety of consistent approaches to estimating impulse response functions (IRFs) in a linear setup when the shock of interest is observed. Although there is no uniformly superior approach, iterated approaches turn out to perform well in terms of root mean-squared error (RMSE) in diverse environments and sample sizes. For smaller sample sizes, the inclusion of all ‘relevant’ variables is not always desirable.
C13|Ties That Bind: Estimating the Natural Rate of Interest for Small Open Economies|This paper estimates the natural interest rate for six small open economies (Australia, Canada, South Korea, Sweden, Switzerland and the U.K.) with a structural New Keynesian model using Bayesian techniques. Our empirical analysis establishes the following four novel findings: First, we show that the open-economy framework provides a better fit of the data than its closed-economy counterpart for the six countries we investigate. Second, we also show that, in all six countries, a monetary policy rule in which the domestic real policy rate tracks the Wicksellian domestic short-term natural rate fits the data better than an otherwise standard Taylor (1993) rule. Third, we show that over the past 35 years, the natural interest rates in all six countries have shifted downwards and strongly comoved with each other. Fourth, our findings illustrate that foreign output shocks (spillovers from the rest of the world) are a major contributor to the dynamics of the natural rate in these six small open economies, and that natural rates comove strongly with estimated U.S. natural rates.
C13|When Simplicity Offers a Benefit, Not a Cost: Closed-Form Estimation of the GARCH(1,1) Model that Enhances the Efficiency of Quasi-Maximum Likelihood|Simple, multi-step estimators are developed for the popular GARCH(1,1) model, where these estimators are either available entirely in closed form or dependent upon a preliminary estimate from, for example, quasi-maximum likelihood. Identification sources to asymmetry in the model's innovations, casting skewness as an instrument in a linear, two-stage least squares estimator. Properties of regular variation coupled with point process theory establish the distributional limits of these estimators as stable, though highly non-Gaussian, with slow convergence rates relative to the âˆšn-case. Moment existence criteria necessary for these results are consistent with the heavy-tailed features of many financial returns. In light-tailed cases that support asymptotic normality for these simple estimators, conditions are discovered where the simple estimators can enhance the asymptotic efficiency of quasi-maximum likelihood estimation. In small samples, extensive Monte Carlo experime nts reveal these efficiency enhancements to be available for (very) heavy tailed cases. Consequently, the proposed simple estimators are members of the class of multi-step estimators aimed at improving the efficiency of the quasi-maximum likelihood estimator.
C13|Off-Balance Sheet Activities, Inefficiency and Market Power of U.S. Banks|The Lerner index is a well-established measure of firms’ market power, but estimation and interpretation present several challenges, especially for banks. We estimate Lerner indices for U.S. banks for 2001-2016 while (i) accounting for banks’ off-balancesheet activities, (ii) estimating cost and profit functions nonparametrically to avoid mis-specification inherent in parametric estimation of translog functions on banking data, and (iii) allowing for cost and profit inefficiency that can otherwise bias index estimates. We find that banks have more market power than previous studies found, and that failure to account for off-balance-sheet activities or inefficiency can seriously bias estimates of market power.
C13|Dynamic specification tests for dynamic factor models|We derive computationally simple expressions for score tests of misspecification in parametric dynamic factor models using frequency domain techniques. We interpret those diagnostics as time domain moment tests which assess whether certain autocovariances of the smoothed latent variables match their theoretical values under the null of correct model specification. We also reinterpret reduced‐form residual tests as checking specific restrictions on structural parameters. Our Gaussian tests are robust to nonnormal, independent innovations. Monte Carlo exercises confirm the finite‐sample reliability and power of our proposals. Finally, we illustrate their empirical usefulness in an application that constructs a US coincident indicator.
C13|Estimating Stochastic Ray Production Frontiers|In this paper, we consider the stochastic ray production function that has been revived recently by Henningsen et al. (2017). We use a profit-maximizing framework to resolve endogeneity problems that are likely to arise, as in all distance functions, and we derive the system of equations after incorporating technical inefficiency. As technical inefficiency enters non-trivially into the system of equations and the Jacobian is highly complicated, we propose Monte Carlo methods of inference. We illustrate the new approach using US banking data and we also address the problems of missing prices and selection of ordering for outputs.
C13|Full Information Estimation of Household Income Risk and Consumption Insurance|Blundell, Pistaferri, and Preston (2008) report an estimate of household consumption insurance with respect to permanent income shocks of 36%. Their estimate is distorted by an error in their code and is not robust to weighting scheme for GMM. We propose instead to use quasi maximum likelihood estimation (QMLE), which produces a more precise and signiﬁcantly higher estimate of consumption insurance at 55%. For sub-groups by age and education, diﬀerences between estimates are even more pronounced. Monte Carlo experiments with non-Normal shocks demonstrate that QMLE is more accurate than GMM, especially given a smaller sample size.
C13|A Doubly Corrected Robust Variance Estimator for Linear GMM|We propose a new finite sample corrected variance estimator for the linear generalized method of moments (GMM) including the one-step, two-step, and iterated estimators. Our formula additionally corrects for the over-identification bias in variance estimation on top of the commonly used finite sample correction of Windmeijer (2005) which corrects for the bias from estimating the efficient weight matrix, so is doubly corrected. Formal stochastic expansions are derived to show the proposed double correction estimates the variance of some higher-order terms in the expansion. In addition, the proposed double correction provides robustness to misspecification of the moment condition. In contrast, the conventional variance estimator and the Windmeijer correction are inconsistent under misspecification. That is, the proposed double correction formula provides a convenient way to obtain improved inference under correct specification and robustness against misspecification at the same time.
C13|Asymptotic Theory for Rotated Multivariate GARCH Models|In this paper, we derive the statistical properties of a two step approach to estimating multivariate GARCH rotated BEKK (RBEKK) models. By the denition of rotated BEKK, we estimate the unconditional covariance matrix in the rst step in order to rotate observed variables to have the identity matrix for its sample covariance matrix. In the second step, we estimate the remaining parameters via maximizing the quasi-likelihood function. For this two step quasi-maximum likelihood (2sQML) estimator, we show consistency and asymptotic normality under weak conditions. While second-order moments are needed for consistency of the estimated unconditional covariance matrix, the existence of nite sixth-order moments are required for convergence of the second-order derivatives of the quasi-log-likelihood function. We also show the relationship of the asymptotic distributions of the 2sQML estimator for the RBEKK model and the variance targeting (VT) QML estimator for the VT-BEKK model. Monte Carlo experiments show that the bias of the 2sQML estimator is negligible, and that the appropriateness of the diagonal specication depends on the closeness to either of the Diagonal BEKK and the Diagonal RBEKK models.
C13|Imposing equilibrium restrictions in the estimation of dynamic discrete games|Imposing equilibrium restrictions provides substantial gains in the estimation of dynamic discrete games. Estimation algorithms imposing these restrictions -- MPEC, NFXP, NPL, and variations -- have different merits and limitations. MPEC guarantees local convergence, but requires the computation of high-dimensional Jacobians. The NPL algorithm avoids the computation of these matrices, but -- in games -- may fail to converge to the consistent NPL estimator. We study the asymptotic properties of the NPL algorithm treating the iterative procedure as performed in finite samples. We find that there are always samples for which the algorithm fails to converge, and this introduces a selection bias. We also propose a spectral algorithm to compute the NPL estimator. This algorithm satisfies local convergence and avoids the computation of Jacobian matrices. We present simulation evidence illustrating our theoretical results and the good properties of the spectral algorithm.
C13|Access to Banking and the Role of Inequality and the Financial Crisis|No abstract is available for this item.
C13|Identification-Robust Nonparametric Inference in a Linear IV Model|For a linear IV regression, we propose two new inference procedures on parameters of endogenous variables that are robust to any identification pattern, do not rely on a linear first-stage equation, and account for heteroskedasticity of unknown form. Building on Bierens (1982), we first propose an Integrated Conditional Moment (ICM) type statistic constructed by setting the parameters to the value under the null hypothesis. The ICM procedure tests at the same time the value of the coefficient and the specification of the model. We then adopt the conditionality principle used by Moreira (2003) to condition on a set of ICM statistics that informs on identification strength. Our two procedures uniformly control size irrespective of identification strength. They are powerful irrespective of the nonlinear form of the link between instruments and endogenous variables and are competitive with existing procedures in simulations and applications.
C13|What Time Use Surveys Can (And Cannot) Tell Us about Labor Supply|It has been widely acknowledged that the measurement of labor supply in the Current Population Survey (CPS) and other conventional microeconomic surveys has nonclassical measurement error, which will bias the estimates of crucial parameters in labor economics, such as labor supply elasticity. Time diary studies, such as the American Time Use Survey (ATUS), only have accurate measurement of hours worked on a single day, hence the weekly hours worked are unobserved. Despite the missing data problem, we provide several consistent estimators of the parameters in weekly labor supply equation using the information in the time use surveys. The consistency of our estimators does not require more conditions beyond those for a usual two stage least square (2SLS) estimator when the true weekly hours worked are observed. We also show that it is impossible to recover the weekly number of hours worked or its distribution function from time use surveys like the ATUS. In our empirical application we find considerable evidence of nonclassical measurement error in the hours worked in the CPS, and illustrate the consequences of using mismeasured weekly hours worked in empirical studies.
C13|Price discovery in a continuous-time setting|We formulate a continuous-time price discovery model and investigate how the standard price discovery measures vary with respect to the sampling interval. We ï¬ nd that the component share measure is invariant to the sampling interval, and hence, discrete-sampled prices suï¬ƒce to identify the continuous-time component share. In contrast, information share estimates are not comparable across diï¬€erent sampling intervals because the contemporaneous correlation between markets increases in magnitude as the sampling interval grows. We show how to back out the continuous-time information share from discrete-sampled prices under cer-tain assumptions on the contemporaneous correlation. We assess our continuous-time model by comparing the estimates of the (continuous-time) component and information shares at diï¬€erent sampling intervals for 30 stocks in the US. We ï¬ nd that both price discovery measures are typ-ically stable across the diï¬€erent sampling intervals, suggesting that our continuous-time price discovery model ï¬ ts the data very well.
C13|Accounting for structural patterns in construction of value functions: a convex optimization approach|A common approach in decision analysis is to infer a preference model in form of a value function from the holistic decision examples. This paper introduces an analytical framework for joint estimation of preferences of a group of decision makers through uncovering structural patterns that regulate general shapes of individual value functions. We investigated the impact of incorporating information on such structural patterns governing the general shape of value functions on the preference estimation process through an extensive simulation study and analysis of real decision makers’ preferences. We found that accounting for structural patterns at the group level vastly improves predictive performance of the constructed value functions at the individual level. This finding is confirmed across a wide range of decision scenarios. Moreover, improvement in the predictive performance is larger when considering the entire ranking of alternatives rather than the top choice, but it is not affected by the level of heterogeneity among the decision makers. We also found that improvement in the predictive performance in ranking problems is independent of individual characteristics of decision makers, and is larger when smaller amount of preference information is available, while for choice problems this improvement is individual-specific and invariant to the amount of input preference information.
C13|Bootstrapping the Gini Index of the Network Degree: An Application for Italian Corporate Governance|We propose a new approach based on bootstrapping to compare complex networks. This is an important task when we wish to compare the effect of a (policy) shock on the structure of a network. The bootstrap test compares two values of the Gini index, and the test is performed on the difference between them. The application is based on the interlocking directorship network. At the director level, Italian corporate governance is characterized by the widespread occurrence of interlocking directorates. Article 36 of Law 214/2011 prohibited interlocking directorates in the financial sector. We compare the interlocking directorship networks in 2009 (before the reform) with 2012 (after the reform) and find evidence of an asymmetric effect of the reform on the network centrality of the different companies but no significant effects on Gini indices.
C13|Nonparametric Homogeneity Pursuit in Functional-Coefficient Models|This paper explores the homogeneity of coefficient functions in nonlinear models with functional coefficients and identifies the underlying semiparametric modelling structure. With initial kernel estimates of coefficient functions, we combine the classic hierarchical clustering method with a generalised version of the information criterion to estimate the number of clusters, each of which has a common functional coefficient, and determine the membership of each cluster. To identify a possible semi-varying coefficient modelling framework, we further introduce a penalised local least squares method to determine zero coefficients, non-zero constant coefficients and functional coefficients which vary with an index variable. Through the nonparametric kernel-based cluster analysis and the penalised approach, we can substantially reduce the number of unknown parametric and nonparametric components in the models, thereby achieving the aim of dimension reduction. Under some regularity conditions, we establish the asymptotic properties for the proposed methods including the consistency of the homogeneity pursuit. Numerical studies, including Monte-Carlo experiments and an empirical application, are given to demonstrate the finite-sample performance of our methods.
C13|Estimating the term structure with linear regressions: Getting to the roots of the problem|Linear estimators of the affine term structure model are inconsistent since they cannot reproduce the factors used in estimation. This is a serious handicap empirically,giving a worse fit than the conventional ML estimator that ensures consistency. We show that a simple self-consistent estimator can be constructed using the eigenvalue decomposition of a regression estimator. The remaining parameters of the model follow analytically. The fit of this model is virtually indistinguishable from that of the ML estimator. We apply the method to estimate various models of U.S. Treasury yields and a joint model of the U.S. and German yield curves.
C13|Estimating regional wealth in Germany: How different are East and West really?|More than 25 years after German reunification, key economic indicators for households living in eastern German regions are still below the western German levels. This particularly holds for private net wealth, which reaches only about 40% of the western German level. However, a more granular regional perspective may reveal a more diverse picture. Therefore, this study is designed to develop regional wealth indicators for the 16 federal states and for the 96 regional planning regions (Raumordnungsregionen) in Germany based on the second wave of the Panel on Household Finances (PHF) conducted by the Deutsche Bundesbank in 2014. These estimates are derived by means of a modified Fay-Herriot approach (Fay and Herriot, 1979) dealing with a) the skewness of the wealth distribution using a transformation, b) unit and item non-response, especially the multiple imputation used, and c) inconsistencies of the regional estimates with the national direct estimate. The results show that private wealth in all eastern German regions still remains far below the national average. However, the wealthiest planning regions in the east report higher private wealth figures than the western German regions with the lowest private wealth estimates. Although the paper is particularly focused on Germany, the approach proposed is applicable to surveys with a similar data structure.
C13|Screening instruments for monitoring market power: The return on withholding capacity index (RWC)|While markets have been liberalized all over the world, incumbents often still hold a dominant position, e.g. on energy markets. Thus, wholesale electricity markets are subject to market surveillance. Nevertheless, consolidated findings on abusive practices of market power and their cause and effect in these markets are scarce and non-controversial market monitoring practices fail to exist. Right now, the Residual Supply Index (RSI) is the most important instrument for market monitoring. However, a major drawback of this index is its focus on just one specific aspect of market power in wholesale electricity markets whereas different consequences of market power are possible. Hence, markets could be distorted in several ways and we propose the 'Return on Withholding Capacity Index' (RWC) as a complementary index to the RSI. The index is a measure of the firms' incentive to withhold capacity. The benefits and practicability of the RWC is shown by an application on data for the German-Austrian electricity wholesale market in 2016.
C13|Publication Bias under Aggregation Frictions: Theory, Evidence, and a New Correction Method|This paper questions the conventional wisdom that publication bias must result from the biased preferences of researchers. When readers only compare the number of positive and negative results of papers to make their decisions, even unbiased researchers will omit noisy null results and inflate some marginally insignificant estimates. Nonetheless, the equilibrium with such publication bias is socially optimal. The model predicts that published non-positive results are either precise null results or noisy but extreme negative results. This paper shows this prediction holds with some data, and proposes a new stem-based bias correction method that is robust to this and other publication selection processes.
C13|Can a cusp catastrophe model describe the effect of sanctions on exchange rates?|Fluctuations of exchange rates, like any other economic variables, are very common in financial markets. However, sometimes because of political and economic tensions, exchange rates exhibit abrupt crashes that lead to structural break. In this paper, the author answers the question whether a catastrophe model can be used for modeling the collapse of exchange rates caused by economic sanctions. For this goal, he uses a cusp catastrophe model for fitting the dynamics of fluctuations of the Iranian Rial against the US Dollar. Using two sentiment variables, i.e. trading volume and ratio of institutional to individual trades of gold futures contracts, the author has shown that the collapse of Iranian currency can be best explained by cusp catastrophe theory.
C13|Bootstrap Methods for Inference in the Parks Model|The Parks (1967) estimator is a workhorse for panel data and seemingly unrelated regression equation systems because it allows the incorporation of serial correlation together with heteroskedasticity and cross-sectional correlation. It is efficient both asymptotically and in small samples. Kmenta and Gilbert (1970) and more recently Beck and Katz (1995) note that estimated standard errors are biased downward, often severely. Instead of fixing the Parks standard errors, Beck and Katz abandon the efficient estimator in favor of a Prais-Winston estimator together with “panel corrected standard errors” (PCSE), a procedure that only partially reduces the standard error bias. In this paper we develop both parametric and nonparametric bootstrap approaches to inference that avoid the need to use biased standard errors. We then illustrate the effectiveness of our procedures using Monte Carlo experiments that show that the bootstrap gives rejection probabilities close to the nominal level chosen by the researcher.
C13|Persistent zeros: The extensive margin of trade|The extensive margin of bilateral trade exhibits a high level of persistence that cannot be explained by geography or trade policy. We combine a heterogeneous firms model of international trade with bounded productivity with features from the firm dynamics literature to derive expressions for an exporting country's participation in a specific destination market in a given period. The model framework asks for a dynamic binary choice estimator with two or three sets of high-dimensional fixed effects. To mitigate the incidental parameter problem associated with nonlinear fixed effects models, we characterize and implement suitable bias corrections. Extensive simulation experiments confirm the desirable statistical properties of the bias-corrected estimators. Empirically, taking two sources of persistence - true state dependence and unobserved heterogeneity - into account using a dynamic specification, along with appropriate fixed effects and bias corrections, changes the estimated effects considerably: out of the most commonly studied potential determinants (joint WTO membership, common regional trade agreement, and shared currency), only sharing a common currency retains a significant effect on whether two countries trade with each other at all in our preferred estimation.
C13|The power of (non-)linear shrinking: a review and guide to covariance matrix estimation|Many econometric and data-science applications require a reliable estimate of the covariance matrix, such as Markowitz portfolio selection. When the number of variables is of the same magnitude as the number of observations, this constitutes a difficult estimation problem; the sample covariance matrix certainly will not do. In this paper, we review our work in this area going back 15+ years. We have promoted various shrinkage estimators, which can be classified into linear and nonlinear. Linear shrinkage is simpler to understand, to derive, and to implement. But nonlinear shrinkage can deliver another level of performance improvement, especially if overlaid with stylized facts such as time-varying co-volatility or factor models.
C13|Shrinkage estimation of large covariance matrices: keep it simple, statistician?|Under rotation-equivariant decision theory, sample covariance matrix eigenvalues can be optimally shrunk by recombining sample eigenvectors with a (potentially nonlinear) function of the unobservable population covariance matrix. The optimal shape of this function reflects the loss/risk that is to be minimized. We introduce a broad family of covariance matrix estimators that can handle all regular functional transformations of the population covariance matrix under large-dimensional asymptotics. We solve the problem of optimal covariance matrix estimation under a variety of loss functions motivated by statistical precedent, probability theory, and differential geometry. The key statistical ingredient of our nonlinear shrinkage methodology is a new estimator of the angle between sample and population eigenvectors, without making strong assumptions on the population eigenvalues. We also compare our methodology to two simpler ones from the literature, linear shrinkage and shrinkage based on the spiked covariance model, via both Monte Carlo simulations and an empirical application.
C13|Risk reduction and efficiency increase in large portfolios: leverage and shrinkage|Two basic solutions have been proposed to fix the well-documented incompatibility of the sample covariance matrix with Markowitz mean-variance portfolio optimization: first, restrict leverage so much that no short sales are allowed; or, second, linearly shrink the sample covariance matrix towards a parsimonious target. Mathematically, there is a deep connection between the two approaches, and empirically they display similar performances. Recent developments have turned the choice between no-short-sales and linear shrinkage into a false ’either-or’ dichotomy. What if, instead of 0% leverage we considered fully-invested, long-short 130/30 portfolios, or even 150/50, given that prime brokers, fund regulators and investors have started to allow it? And instead of linearly shrinking the unconditional covariance matrix, what if we allowed for each of the eigenvalues of the sample covariance matrix to have its own shrinkage intensity, optimally determined under large-dimensional asymptotics, while also incorporating Multivariate GARCH effects? Our empirical evidence finds that, indeed, these new developments enable us to have ‘the best of both worlds’ by combining some appropriate leverage constraint with a judiciously chosen shrinkage method. The overall winner is a 150/50 investment strategy where the covariance matrix estimator is a combination of DCC (Dynamic Conditional Correlation — a well-known Multivariate GARCH model) — with NL (Non-Linear shrinkage, a substantial upgrade upon linear shrinkage technology); although 130/30 DCC-NL comes a close second. This is true both in the ‘pure’ case of estimating the Global Minimum Variance portfolio, and also for textbook-style construction of Markowitz mean-variance efficient portfolio.
C13|The analysis of marked and weighted empirical processes of estimated residuals|An extended and improved theory is presented for marked and weighted empirical processes of residuals of time series regressions. The theory is motivated by 1-step Huber-skip estimators, where a set of good observations are selected using an initial estimator and an updated estimator is found by applying least squares to the selected observations. In this case, the weights and marks represent powers of the regressors and the regression errors, respectively. The inclusion of marks is a non-trivial extention to previous theory and requires refined martingale arguments.
C13|Models where the Least Trimmed Squares and Least Median of Squares estimators are maximum likelihood|The Least Trimmed Squares (LTS) and Least Median of Squares (LMS) estimators are popular robust regression estimators. The idea behind the estimators is to find, for a given h, a sub-sample of h good observations among n observations and estimate the regression on that sub-sample. We find models, based on the normal or the uniform distribution respectively, in which these estimators are maximum likelihood. We provide an asymptotic theory for the location-scale case in those models. The LTS estimator is found to be sqrt(h) consistent and asymptotically standard normal. The LMS estimator is found to be h consistent and asymptotically Laplace.
C13|The Role of Asymmetry and Uncertainties in the Capital Flows- Economic Growth Nexus|This study examines the asymmetry between capital flows and economic growth in 42 countries for the period 1990-2017. It further argues that uncertainty is an important channel through which asymmetry operates. As such, the three measures of uncertainty are macroeconomic, fiscal and institutional. The Generalised Method of Moments is used as an empirical strategy. The existence of an asymmetry is confirmed by the findings as capital flows are more reactive to economic drag when compared to economic growth. Furthermore, the channels through which asymmetry operate are heterogeneous to measures of capital flows and proxies for uncertainty.
C13|A meta-analysis of the price and income elasticities of food demand|Food demand elasticities are crucial parameters in the calibration of simulation models used to assess the impacts of political reforms or to analyse long-term projections, notably in agricultural sectors. Numerous estimates of these parameters are now available in the economic literature. The main objectives of this work are twofold: we seek first to identify general patterns characterizing the demand elasticities of food products and second to identify the main sources of heterogeneity between the elasticity estimates available in the literature. To achieve these objectives, we conduct a broad literature review of food demand elasticity estimates and perform a meta-regression analysis. Our results reveal the important impacts of income levels on income and price elasticities both at the country (gross domestic product-GDP) and household levels: the higher the income is, the lower the level of elasticities. Food demand responses to changes in income and prices appear to follow different patterns depending on the global regions involved apart from any income level consideration. From a methodological viewpoint, the functional forms used to represent food demand are found to significantly affect elasticity estimates. This result sheds light on the importance of the specification of demand functions, and particularly of their flexibility, in simulation models.
C13|Weather Shocks|How much do weather shocks matter? The literature addresses this question in two isolated ways: either by looking at long-term effects through the prism of theoretical models, or by focusing on short-term effects using empirical analysis. We propose a framework to bring together both the short and long-term effects through the lens of an estimated DSGE model with a weather-dependent agricultural sector. The model is estimated using Bayesian methods and quarterly data for New Zealand using the weather as an observable variable. In the short-run, our analysis underlines the key role of weather as a driver of business cycles over the sample period. An adverse weather shock generates a recession, boosts the non-agricultural sector and entails a domestic currency depreciation. Taking a long-term perspective, a welfare analysis reveals that weather shocks are not a free lunch: the welfare cost of weather is currently estimated at 0.19% of permanent consumption. Climate change critically increases the variability of key macroeconomic variables (such as GDP, agricultural output or the real exchange rate) resulting in a higher welfare cost peaking to 0.29% in the worst case scenario.
C13|Asymptotic theory for clustered samples|We provide a complete asymptotic distribution theory for clustered data with a large number of independent groups, generalizing the classic laws of large numbers, uniform laws, central limit theory, and clustered covariance matrix estimation. Our theory allows for clustered observations with heterogeneous and unbounded cluster sizes. Our conditions cleanly nest the classical results for i.n.i.d. observations, in the sense that our conditions specialize to the classical conditions under independent sampling. We use this theory to develop a full asymptotic distribution theory for estimation based on linear least-squares, 2SLS, nonlinear MLE, and nonlinear GMM.
C13|Deciding with judgment|Non sample information is hidden in frequentist statistics in the choice of the hypothesis to be tested and of the confidence level. Explicit treatment of these elements provides the connection between Bayesian and frequentist statistics. A frequentist decision maker starts from a judgmental decision and moves to the closest boundary of the confidence interval of the first order conditions, for a given loss function. This statistical decision rule does not perform worse than the judgmental decision with a probability equal to the confidence level. For any given prior, there is a mapping from the sample realization to the confidence level which makes Bayesian and frequentist decision rules equivalent. Frequentist decision rules can be interpreted as decisions under ambiguity. JEL Classification: C1, C11, C12, C13, D81
C13|Virtual Historical Simulation for estimating the conditional VaR of large portfolios|"In order to estimate the conditional risk of a portfolio's return, two strategies can be advocated. A multivariate strategy requires estimating a dynamic model for the vector of risk factors, which is often challenging, when at all possible, for large portfolios. A univariate approach based on a dynamic model for the portfolio's return seems more attractive. However, when the combination of the individual returns is time varying, the portfolio's return series is typically non stationary which may invalidate statistical inference. An alternative approach consists in reconstituting a ""virtual portfolio"", whose returns are built using the current composition of the portfolio and for which a stationary dynamic model can be estimated. This paper establishes the asymptotic properties of this method, that we call Virtual Historical Simulation. Numerical illustrations on simulated and real data are provided."
C13|Matching Estimators with Few Treated and Many Control Observations|We analyze the properties of matching estimators when the number of treated observations is fixed while the number of treated observations is large. We show that, under standard assumptions, the nearest neighbor matching estimator for the average treatment effect on the treated is asymptotically unbiased, even though this estimator is not consistent. We also provide a test based on the theory of randomization tests under approximate symmetry developed in Canay et al. (2014) that is asymptotically valid when the number of control observations goes to infinity. This is important because large sample inferential techniques developed in Abadie and Imbens (2006) would not be valid in this setting.
C13|The Effect of Interest Rate Caps on Bankruptcy: Synthetic Control Evidence from Recent Payday Lending Bans|Citing consumer protection concerns, New Hampshire – along with three other states – recently banned payday lending by implementing an APR cap on small loans. New Hampshire presents a compelling quasi-experiment: its neighbors already had a payday loan ban inplace. Hence, New Hampshire consumers were completely shut out of the storefront payday loan market. We perform a synthetic control analysis for all four of the recently-banning states. Our results show that, on the aggregate, bankruptcies are largely unaffected by the bans. Our New Hampshire results are characterized by an initial rise in bankruptcies after the ban, followed by a fall. This is consistent with the notion that payday bans hurt credit-constrained consumers in the short-run, but could help them in the long-run. We also analyze survey data of payday borrowers and find that while bankruptcies are unaffected, consumers substitute toward paying their credit card bills late and using pawnshops.
C13|Testing for Correlated Factor Loadings in Cross Sectionally Dependent Panels|A large strand of the literature on panel data models has focused on explicitly modelling the cross-section dependence between panel units. Factor augmented approaches have been proposed to deal with this issue. Under a mild restriction on the correlation of the factor loadings, we show that factor augmented panel data models can be encompassed by a standard two-way fixed effect model. This highlights the importance of verifying whether the factor loadings are correlated, which, we argue, is an important hypothesis to be tested, in practice. As a main contribution, we propose a Hausman-type test that determines the presence of correlated factor loadings in panels with interactive effects. Furthermore, we develop two nonparametric variance estimators that are robust to the presence of heteroscedasticity, autocorrelation as well as slope heterogeneity. Via Monte Carlo simulations, we demonstrate desirable size and power performance of the proposed test, even in small samples. Finally, we provide extensive empirical evidence in favour of uncorrelated factor loadings in panels with interactive effects.
C13|Estimation and Inference for Multi-dimensional Heterogeneous Panel Datasets with Hierarchical Multi-factor Error Structure|Given the growing availability of large datasets and following recent research trends on multi-dimensional modelling, we develop three dimensional (3D) panel data models with hierarchical error components that allow for strong cross-sectional dependence through unobserved heterogeneous global and local factors. We propose consistent estimation procedures by extending the common correlated effects (CCE) estimation approach proposed by Pesaran (2006). The standard CCE approach needs to be modified in order to account for the hierarchical factor structure in 3D panels. Further, we provide the associated asymptotic theory, including new nonparametric variance estimators. The validity of the proposed approach is confirmed by Monte Carlo simulation studies. We also demonstrate the empirical usefulness of the proposed approach through an application to a 3D panel gravity model of bilateral export flows.
C13|Mitigating misleading implications for policy: Treatment of outliers in a difference-indifferences framework|Applications of the difference-in-differences estimator in economics, banking and finance, and management commonly treat outliers using the winsorize method. However, failure to winsorize outliers in both the treatment and controls groups introduces volatility in estimated coefficients, significance levels, and standard errors. A faulty process can lead to an exogenous event realising a significant effect that proper process would fail to detect. In demonstration, we randomly generate placebo interventions in bank-level data and discuss how to detect and limit the problem.
C13|Lower bank capital requirements as a policy tool to support credit to SMEs: evidence from a policy experiment|"Starting in 2014 with the implementation of the European Commission Capital Requirement Directive, banks operating in the Euro area were benefiting from a 25% reduction (the Supporting Factor or ""SF"" hereafter) in their own funds requirements against Small and Medium-sized enterprises (""SMEs"" hereafter) loans. We investigate empirically whether this reduction has supported SME financing and to which extent it is consistent with SME credit risk. Economic capital computations based on multifactor models do confirm that capital requirements should be lower for SMEs. Taking into account the uncertainty surrounding their estimates and adopting a conservative approach, we show that the SF is consistent with the difference in economic capital between SMEs and large corporates. As for the impact on credit distribution, our differences-in-differences specification enables us to find a positive and significant impact of the SF on the credit supply."
C13|Time-Varying Risk Premia in Large International Equity Markets|We use an estimation methodology tailored for large unbalanced panels of individual stock returns to address key economic questions about the factor structure, pricing performance of factor models, and time-variations in factor risk premia in international equity markets. We estimate factor models with time-varying factor exposures and risk premia at the individual stock level using 62,320 stocks in 46 countries over the 1985-2018 period. We consider market, size, value, momentum, profitability, and investment factors aggregated at the country, regional, and world level. We find that adding an excess country market factor to world or regional factors is sufficient to capture the factor structure for both developed and emerging markets. We do not reject asset pricing restriction tests for multifactor models in 74% to 91% of countries. Value and momentum premia show more variability over time and across countries than profitability and investment premia. The excess country market premium is statistically significant in many developed and emerging markets but economically larger in emerging markets.
C13|Alternative Estimators For The Fdi Gravity Model: An Application To German Outward Fdi|Despite the sound theoretical foundations of FDI gravity models and its popularity in empirical studies, there is a lack of consensus regarding the econometric specification and the estimation of the gravity equation. This paper provides a comprehensive empirical evidence of the determinants of German outward FDI comparing several estimation methods in their multiplicative form. We use four versions of the Generalized Linear Model (GLM), namely, Poisson Pseudo Maximum Likelihood(PPML), Gamma Pseudo Maximum Likelihood (GPML), Negative Binomal Pseudo Maximum Likelihood (NBPML) and Gaussian-GLM. The results of the empirical application indicate that NBPML is the best performing estimator followed by GPML.
C13|Doubly Robust-type Estimation of Population Moments and Parameters in Biased Sampling|"We propose an estimation method of population moments or population parameters in ""biased sampling data"" in which for some units of data, not only the variable of interest but also the covariates, have missing observations and the proportion of ""missingness"" is unknown. We use auxiliary information such as the distribution of covariates or their moments in random sampling data in order to correct the bias. Moreover, with additional assumptions, we can correct the bias even if we have only the moment information of covariates. The main contribution of this paper is the development of a doubly robust-type estimator for biased sampling data. This method provides a consistent estimator if either the regression function or the assignment mechanism is correctly specified. We prove the consistency and semi-parametric efficiency of the doubly robust estimator. Both the simulation and empirical application results demonstrate that the proposed estimation method is more robust than existing methods."
C13|The analysis of marked and weighted empirical processes of estimated residuals|An extended and improved theory is presented for marked and weighted empirical processes of residuals of time series regressions. The theory is motivated by 1-step Huber-skip estimators, where a set of good observations are selected using an initial estimator and an updated estimator is found by applying least squares to the selected observations. In this case, the weights and marks represent powers of the regressors and the regression errors, respectively. The inclusion of marks is a non-trivial extention to previous theory and requires refined martingale arguments.
C13|Models where the Least Trimmed Squares and Least Median of Squares estimators are maximum likelihood|The Least Trimmed Squares (LTS) and Least Median of Squares (LMS) estimators are popular robust regression estimators. The idea behind the estimators is to fi?nd, for a given h; a sub-sample of h '?good' ?observations among n observations and estimate the regression on that sub-sample. We fi?nd models, based on the normal or the uniform distribution respectively, in which these estimators are maximum likelihood. We provide an asymptotic theory for the location-scale case in those models. The LTS estimator is found to be h1/2 consistent and asymptotically standard normal. The LMS estimator is found to be h consistent and asymptotically Laplace.
C13|Recentered Influence Functions in Stata: Methods for Analyzing the Determinants of Poverty and Inequality|Recentered influence functions (RIFs) are statistical tools popularized by Firpo, Fortin, and Lemieux (2009) for analyzing unconditional partial effects on quantiles in a regression analysis framework (unconditional quantile regressions). The flexibility and simplicity of these tools has opened the possibility of extending the analysis to other distributional statistics using linear regressions or decomposition approaches. In this paper, I introduce three Stata commands to facilitate the use of RIFs in the analysis of outcome distributions: rifvar() is an egen extension used to create RIFs for a large set of distributional statistics; rifhdreg facilitates the estimation of RIF regressions, enabling the use of high-dimensional fixed effects; and oaxaca_rif to implement Oaxaca-Blinder type decomposition analysis (RIF decompositions).
C13|Partially heterogeneous tests for Granger non-causality in panel data|The power of Granger non-causality tests in panel data depends on the type of the alternative hypothesis: feedback from other variables might be homogeneous, homogeneous within groups or heterogeneous across different panel units. Existing tests have power against only one of these alternatives and may fail to reject the null hypothesis if the specified type of alternative is incorrect. This paper proposes a new Union-Intersections (UI) test which has correct size and good power against any type of alternative. The UI test is based on an existing test which is powerful against heterogeneous alternatives and a new Wald-type test which is powerful against homogeneous alternatives. The Wald test is designed to have good size and power properties for moderate to large time series dimensions and is based on a bias-corrected split panel jackknife-type estimator. Evidence from simulations confirm the new UI tests provide power against any direction of the alternative.
C13|Fertility response to climate shocks|In communities highly dependent on rainfed agriculture for their livelihoods, the common oc-currence of climatic shocks such as droughts can lower the opportunity cost of having children, and raise fertility. Using longitudinal household data from Madagascar, we estimate the causal effect of drought occurrences on fertility, and explore the nature of potential mechanisms driving this effect. We exploit exogenous within-district year-to-year variation in rainfall deficits, and find that droughts occurring during the agricultural season significantly increase the number of children born to women living in agrarian communities. This effect is long lasting, as it is not reversed within four years following the drought occurrence. Analyzing the mechanism, we find that droughts have no effect on common underlying factors of high fertility such as marriage timing and child mortality. Furthermore, droughts have no significant effect on fertility if they occur during the non-agricultural season or in non-agrarian communities, and their positive effect in agrarian communities is mitigated by irrigation. These findings provide evidence that a low opportunity cost of having children is the main channel driving the fertility effect of drought in agrarian communities.
C13|Does social context affect poverty? The role of religious congregations|This paper contributes to the literature that aims at identifying and measuring the impact of social context on individual-level outcomes. We focus on religious congregations (social groups with which Christian worshipers feel associated) and investigate congregation effects on individual poverty using U.S. data and a multilevel approach. In order to correct for selection effects, we model congregation choice using a multinomial logit model and subsequently incorporate correction components into the multilevel model of congregation effects. Our empirical results support the existence of congregation effects and, therefore, the importance of social context on individual poverty. We find that congregation size, recreational services, initiatives to integrate new members and behavior standards play important roles in shaping the probability that churchgoers experience poverty. Individual behavior (in terms of participation in the religious life of congregations) also matters. These finding are in line with the idea that congregations’ activities can foster social interactions and cooperation reducing individual probability of experiencing poverty.
C13|Economic growth, Environmental degradation and business cycles in Eswatini|This study investigates the impact of the business cycle on the Environmental Kuznets Curve (EKC) for the Eswatini Kingdom over the period 1970 – 2014. To this end, we employ the nonlinear autoregressive distributive lag (NARDL) model to capture the long-run and short-run cointegration effects between economic activity and greenhouse gas (GHG) emissions over different phases of the business cycle. Our findings reveal that economic activity only degrades the environment during upswing of the economic cycle whilst this relationship is insignificant during downswing of the cycle. We specifically compute a value of $3.57 worth of output been gained at the cost of a metric unit of emissions during economic expansionary phases. Altogether, these results insinuate much needed government intervention in the market for emissions via environmental tax reforms (ETR) which should be designed with countercyclical bias towards upswing the business cycle.
C13|A sectoral approach to the electricity-growth nexus in the Eastern Cape province of South Africa|This paper takes a sectoral, panel approach to investigating the electricity-growth nexus for the Eastern Cape province of South Africa between the period of 2003 and 2017. The empirical investigation was carried out using the Pooled Mean Group (PMG) panel estimators applied to an augmented dynamic growth model whilst the caulisty tests between electricity consumption and growth where performed using the Dumitrescu-Hurlin (2012) panel non-causality tests. The findings confirm the absence of significant long-run relationship between electricity and growth whilst finding a significant and positive effect over the short-run. Moreover, our causality tests provide strong evidence of causality running from electricity consumption to economic growth hence supporting the “growth hypothesis”. In a nutshell, our results not only demonstrate the importance of performing the electricity-growth analysis at provincial level as opposed to relying on national aggregated estimates but also provides important provincial-specific policy implications and recommendations.
C13|South African unemployment in the post-financial crisis era: What are the determinants?|High unemployment rates is one of the greatest economic challenges facing post-apartheid South African government over the past two decades and this problem has become more worrisome in the post-global financial crisis period. Our study examines the determinants of unemployment for the South African economy in the post-crisis period over a quarterly frequency period of 2009:Q1 to 2018:Q4. The determinants are examined for 4 classes of unemployment rates (total, male, female and youth) and we further partition possible unemployment determinants into fiscal, monetary and macroeconomic variables. The estimation results from the employed autoregressive distributive lag (ARDL) models find income tax, repo rates, economic growth, trade, investment, household debt and savings to be significant determinants of unemployment in the post-crisis South African economy and yet we note discrepancies of the significance of these determinants amongst different unemployment categories. Relevant policy implications are matched against our obtained empirical findings.
C13|Can the South African Reserve Bank (SARB) protect the purchasing power of citizens? A new look at Fisher’s hypothesis|In this paper, we evaluate whether the South African Reserve Bank (SARB) has been successful at fulfilling it’s mandate of protecting the purchasing power of the country’s citizens. To this end, we use monthly data covering the post-inflation targeting era of 2002:01 to 2018:04 to re-examine Fisher’s hypothesis for the South African economy by testing for stationarity in real interest rates. Our study makes three noteworthy empirical contributions. Firstly, we use three measures of inflation in computing the real interest rate variable. Secondly, our inflation expectations variables are constructed in alignment with the inflation forecast horizons of 12 to 24 months as practiced by the SARB. Thirdly, we rely on the more powerful flexible Fourier unit root test in testing for integration properties of the real exchange rate. All-in-all, our findings highlight the Reserve Bank’s struggles in protecting the purchasing power of citizen’s for periods subsequent to the global financial crisis but not for periods before the crisis. Policy recommendations are also provided.
C13|Towards resolving the Purchasing Power Parity (PPP) ‘puzzle’ in Newly Industrialized Countries (NIC’s)|The Purchasing Power Parity (PPP) hypothesis represents one of the oldest existing economic doctrines and is plagued with empirical inconsistencies collectively labelled as ‘puzzles’. Our study resolves these ‘puzzles’ for 14 Newly Industrialized Countries (NIC) whose developmental strategies are impinged on the stability of real exchange rates which, in turn, validates the PPP hypothesis. We test for the stationarity of real exchange rates (RER’s) by applying an exponential smooth transition autoregressive unit root test augmented with a fractional frequency flexible Fourier form component (ESTAR-FFFFF) to capture heterogeneous smooth transition asymmetries and approximate unknown structural breaks in the time series. We find the RER’s in all 14 NIC’s are mean-reverting over monthly period of 1970:1-2018:12 which confirms the PPP hypothesis for these economies in the presence of exchange-rate regime shifts, oil and food shocks, financial crisis and other forms of asymmetries and structural breaks. Length: 29 pages
C13|Imposing equilibrium restrictions in the estimation of dynamic discrete games|Imposing equilibrium restrictions provides substantial gains in the estimation of dynamic discrete games. Estimation algorithms imposing these restrictions – MPEC, NFXP, NPL, and variations – have different merits and limitations. MPEC guarantees local convergence, but requires the computation of high-dimensional Jacobians. The NPL algorithm avoids the computation of these matrices, but – in games – may fail to converge to the consistent NPL estimator. We study the asymptotic properties of the NPL algorithm treating the iterative procedure as performed in finite samples. We find that there are always samples for which the algorithm fails to converge, and this introduces a selection bias. We also propose a spectral algorithm to compute the NPL estimator. This algorithm satisfies local convergence and avoids the computation of Jacobian matrices. We present simulation evidence illustrating our theoretical results and the good properties of the spectral algorithm.
C13|The economic importance of the Belgian ports : Flemish maritime ports, Liège port complex and the port of Brussels – Report 2017|This Working Paper analyses the economic importance of the Belgian ports largely based on annual accounts data for the year 2017. As the years prior to 2017 have been described in earlier papers in the same series, the emphasis lies on the ﬁgures for 2017 and the developments between 2016 and 2017 . After the stagnation in 2016, direct value added at the Belgian ports rose by 7.3% from € 18 052 million to € 19 368 million (current prices) or roughly 4.4% of Belgium’s GDP. All ports, with the exception of the Liège port complex, contributed to value added growth at the Belgian ports. The ports of Antwerp and Ghent were the most important players. The biggest contributing sectors to value added growth were the chemical industry and, to a lesser extent, cargo handling and the metalworking industry. In 2017, indirect value added was around 82% of direct value added. Direct value added increased significantly at the ports of Ghent, Brussels and Antwerp, by 13.4%, 16.0% and 6.1% respectively. The increase by more than 3% of direct value added at the ports of Zeebrugge and Ostend was also substantial. Direct value added fell by 2.4% at the Liège port complex. After the decline between 2012 and 2015, direct employment at the Belgian ports was up for the second year in a row. Between 2016 and 2017, the number of direct full-time equivalent jobs rose by 0.8%, from 115 401 to 116 311 or approximately 2.8% of Belgium’s total domestic employment. All ports, with the exception of Ostend and Brussels, contributed to employment growth at the Belgian ports. The ports of Antwerp and Ghent were the most important players. The biggest contributing sectors to employment growth were cargo handling and, to a lesser extent, the chemical industry. In 2017, indirect employment was around 120% of direct employment. Direct employment increased by around 1% at the ports of Antwerp, Ghent and Zeebrugge. Growth at the Liège port complex was more modest at 0.4%. The number of direct full-time equivalent jobs fell at the ports of Ostend and Brussels, by 1.2% and 4.2% respectively. The pattern of investment is closely linked to projects and is therefore highly volatile. After the decline between 2012 and 2014, direct investment at the Belgian ports was up for the third year in a row. Between 2016 and 2017, investment was up by 2.4%, from € 4 711 million to € 4 825 million. The port of Ghent and, to a lesser extent, the Liège port complex contributed to investment growth at the Belgian ports. The biggest contributing sectors to investment growth were the ‘port construction and dredging’ sector and, to a lesser extent, cargo handling, and the energy and chemical industries. Based on the figures of the traffic, the Flemish ports can be considered as real bridgeheads for trade with the UK. Developments regarding the modalities and consequences of the Brexit therefor should be followed with the greatest attention. Given the existing import and export volumes in terms of tonnage, it seems it will mostly be a challenge in Zeebrugge and to some extent for Antwerp. As a supplier to both China and the United States, Belgium is indirectly involved in trade between the two countries. If protectionism would close the United States off to exports from abroad, Belgian economy might get impacted one of the most in Europe.
C13|Inference on winners| Many questions in econometrics can be cast as inference on a parameter selected through optimization. For example, researchers may be interested in the effectiveness of the best policy found in a randomized trial, or the best-performing investment strategy based on historical data. Such settings give rise to a winner's curse, where conventional estimates are biased and conventional confi dence intervals are unreliable. This paper develops optimal con fidence sets and median-unbiased estimators that are valid conditional on the parameter selected and so overcome this winner's curse. If one requires validity only on average over target parameters that might have been selected, we develop hybrid procedures that combine conditional and projection con fidence sets and offer further performance gains that are attractive relative to existing alternatives.
C13|Average Crossing Time: An Alternative Characterization of Mean Aversion and Reversion|We evaluate the properties of mean reversion and mean aversion in asset prices and returns as commonly characterized in the finance literature. The study is undertaken within a class of well-known dynamic stochastic general equilibrium models and shows that the mean reversion/aversion distinction is largely artificial. We then propose an alternative measure, the ‘Average Crossing Time’ that both unifies these concepts and provides an alternative characterization. Ceteris paribus, mean reverting processes have a relatively shorter average crossing time as compared to mean averting processes.
C13|Best Linear Approximations to Set Identified Functions: With an Application to the Gender Wage Gap|This paper provides inference methods for best linear approximations to functions which are known to lie within a band. It extends the partial identification literature by allowing the upper and lower functions defining the band to carry an index, and to be unknown but parametrically or non-parametrically estimable functions. The identification region of the parameters of the best linear approximation is characterized via its support function, and limit theory is developed for the latter. We prove that the support function can be approximated by a Gaussian process and establish validity of the Bayesian bootstrap for inference. Because the bounds may carry an index, the approach covers many canonical examples in the partial identification literature arising in the presence of interval valued outcome and/or regressor data: not only mean regression, but also quantile and distribution regression, including sample selection problems, as well as mean, quantile, and distribution treatment effects. In addition, the framework can account for the availability of instruments. An application is carried out, studying female labor force participation using data from Mulligan and Rubinstein (2008) and insights from Blundell, Gosling, Ichimura, and Meghir (2007). Our results yield robust evidence of a gender wage gap, both in the 1970s and 1990s, at quantiles of the wage distribution up to the 0.4, while allowing for completely unrestricted selection into the labor force. Under the assumption that the median wage offer of the employed is larger than that of individuals that do not work, the evidence of a gender wage gap extends to quantiles up to the 0.7. When the assumption is further strengthened to require stochastic dominance, the evidence of a gender wage gap extends to all quantiles, and there is some evidence at the 0.8 and higher quantiles that the gender wage gap decreased between the 1970s and 1990s.
C13|In Search of Systematic Risk and the Idiosyncratic Volatility Puzzle in the Corporate Bond Market|We propose a comprehensive measure of systematic risk for corporate bonds as a nonlinear function of robust risk factors and find a significantly positive link between systematic risk and the time-series and cross-section of future bond returns. We also find a positive but insignificant relation between idiosyncratic risk and future bond returns, suggesting that institutional investors dominating the bond market hold well-diversified portfolios with a negligible exposure to bond-specific risk. The composite measure of systematic risk also predicts the distribution of future market returns, and the systematic risk factor earns a positive price of risk, consistent with Merton's (1973) ICAPM.
C13|Identification of heterogeneous treatment effects as a function of potential untreated outcome under the nonignorable assignment condition|We provide sufficient conditions for the identification of hetero- geneous treatment effects (HTE), in which the missing mechanism is nonignorable, when the information on the marginal distribution of untreated outcome is available. It is also shown that, under such a situ- ation, the same result holds for the identification of average treatment effects (ATE). Exposing certain additivity on the regression function of the assignment probability, we reduce the identication of HTE to the uniqueness of a solution of some integral equation, and discuss it borrowing the idea from the literature on statistical inverse prob- lems. Our result contributes to theoretical understandings in causal inference with heterogeneity and also the relaxation of the conditional independence assumption in statistical data fusion or statistical data combination.
C13|The top tail of South Africa's earnings distribution 1993-2014: Evidence from the Pareto distribution|We estimate the parameters of a Pareto distribution for South African earnings as measured through the October Household Surveys, Labour Force Surveys and Quarterly Labour Force Surveys, as assembled in the Post-Apartheid Labour Market Series (PALMS). We develop an outlier detection algorithm consistent with this distribution and then adjust the Gini coefficient for inequality in the top tail, using the robust estimation technique of Cowell and Flachaire. That procedure suggests that wage inequality is a bit higher than conventionally estimated. We also show that the top tail of the South African earnings distribution is 'thick tailed' and explore what that means. Our analyses show big shifts in the distribution in some of the surveys in ways that suggest measurement changes rather than changes in the underlying distribution.
C13|Network-based macro fluctuations: Evidence from Lithuania|Do inter-sectoral linkages of intermediate products affect the spread of sectoral shocks at the aggregate level in Lithuania, a small and open economy? We answer this question by: i) constructing the domestic sector-by-sector direct requirements table using the Lithuanian interindustry transactions tables, and ii) applying Acemoglu et al. (2012)'s network-based methodology and Gabaix and Ibragimov (2011)'s modified log rank-log size regression to analyse the nature of inter-sectoral linkages. Our results indicate that the direct and indirect inter-sectoral linkages cause aggregate volatility to decay at a rate lower than square root of n - the rate predicted by the standard diversification argument. Furthermore, indirect linkages play an important role in the above-mentioned process, supporting the findings of Acemoglu et al. (2012). These results suggest that the inter-sectoral network of linkages represent a potential propagation mechanism for idiosyncratic shocks throughout the Lithuanian economy.
C13|What drives the rise of antidepressant consumption? Evidence from Switzerland|Antidepressant (AD) consumption has been steadily increasing in the last decade in most countries. The explanations suggested by researchers for this increment are still under scrutiny. This study attempts to identify the determinants of AD consumption by exploiting small area variations from Switzerland between 2003 and 2014. We observe that two specific drugs - Citalopram and Escitalopram - within the Selective Serotonin Re-uptake Inhibitors (SSRI) category are mainly responsible for the increasing trend in AD consumption. Socio-economic, demographic, cultural, and geographical characteristics of the area are included in multiple regression models with random and fixed effects of AD consumption per capita. While most of these factors contribute to explain cross-area variations in AD use, they provide little explanation for the temporal trend in overall AD consumption. Conversely, we find that the time trend in AD consumption is explained at least partially by the density of prescribing physicians. More precisely, generic AD turn out to be positively associated with adverse local economic conditions, while branded AD are negatively associated with adverse economic conditions and positively related to the presence of neurologists and psychiatrists in the area. This may suggest that generic AD drugs are more likely prescribed in accordance to need, whereas branded AD are more likely to respond to preferences and financial incentives affecting suppliers.
C13|Comparing different methods for the estimation of interbank intraday yield curves|In this paper, we compare three different models, namely the Nelson- Siegel model, the Svensson model and the Diebold- Li model, for the estimation of an intraday yield curve on the Italian interbank credit market e-MID. Using a sample which spans from October 2005 until March 2010, the first important finding is that all three models are highly suitable for the estimation of an intraday yield curve providing superior empirical results when compared with similar works on e-MID. The second important finding is that, based on different in sample statistics, the Svensson model dominates the other two models before, during and after the financial crisis from 2007. Moreover, the Nelson- Siegel model seems to dominate the Diebold- Li model although these differences in goodness-of-fit between these two models may not be statistically significant.
C13|Factor-Driven Two-Regime Regression|We propose a novel two-regime regression model where the switching between the regimes is driven by a vector of possibly unobservable factors. When the factors are latent, we estimate them by the principal component analysis of a panel data set. We show that the optimization problem can be reformulated as mixed integer optimization and present two alternative computational algorithms. We derive the asymptotic distributions of the resulting estimators under the scheme that the threshold effect shrinks to zero. In particular, we establish a phase transition that describes the effect of first stage factor estimation as the cross-sectional dimension of panel data increases relative to the time-series dimension. Moreover, we develop a consistent factor selection procedure with a penalty term on the number of factors and present bootstrap methods for carrying out inference and testing linearity with the aid of efficient computational algorithms. Finally, we illustrate our methods via numerical studies.
C13|Optimal Estimation with Complete Subsets of Instruments|In this paper we propose a two-stage least squares (2SLS) estimator whose first stage is based on the equal-weight average over a complete subset. We derive the approximate mean squared error (MSE) that depends on the size of the complete subset and characterize the proposed estimator based on the approximate MSE. The size of the complete subset is chosen by minimizing the sample counterpart of the approximate MSE. We show that this method achieves the asymptotic optimality. To deal with weak or irrelevant instruments, we generalize the approximate MSE under the presence of a possibly growing set of irrelevant instruments, which provides useful guidance under weak IV environments. The Monte Carlo simulation results show that the proposed estimator outperforms alternative methods when instruments are correlated with each other and there exists high endogeneity. As an empirical illustration, we estimate the logistic demand function in Berry, Levinsohn, and Pakes (1995).
C13|Exchange Rate Regimes As Thresholds: The Main Determinants Of Capital Inflows In Emerging Market Economies|This study investigates whether the impacts of the main common push (global financial conditions, GFC) and country-specific pull (growth) factors on capital inflows are invariant to the prevailing exchange rate regimes (ERRs) in emerging market economies. Our results suggest that endogenously estimated ERR thresholds do matter especially for the impact of GFC. The impact of GFC is substantially high under more flexible ERRs for all capital inflow types except FDI. FDI inflows are basically determined by the pull factor across all ERRs. Portfolio inflows are mainly determined by GFC. The sensitivity of aggregate and other investment inflows to the pull factor seems to be much higher under more rigid ERRs. Our results are broadly in line with the literature suggesting that credible managed ERRs encourage capital inflows by allowing countries to import monetary policy credibility of the center country and to provide exchange rate guarantee.
C13|Limited Asset Market Participation And The Euro Area Crisis: An Empirical Dsge Model|We estimate a medium‐scale dynamic stochastic general equilibrium model for the Euro area with limited asset market participation (LAMP). Our results suggest that in the recent European Monetary Union years LAMP is particularly sizable (39% during 1993–2012) and important to understand business cycle features. The Bayes factor and the forecasting performance show that the LAMP model is preferred to its representative household counterpart. In the representative agent model the risk premium shock is the main driver of output volatility in order to match consumption correlation with output. In the LAMP model this role is played by the investment‐specific shock, because non‐Ricardian households introduce a Keynesian multiplier effect and raise the correlation between consumption and investments. We also detect the contractionary role of monetary policy shocks during the post‐2007 years. In this period consumption of non‐Ricardian households fell dramatically, but this outcome might have been avoided by a more aggressive policy stance. (JEL C11, C13, C32, E21, E32, E37)
C13|Using the GB2 Income Distribution: A Review|To use the GB2 distribution for the analysis of income and other positively-skewed distributions, knowledge of estimation methods and the ability to compute quantities of interest from the estimated parameters are required. We review estimation methodology that has appeared in the literature, and summarise expressions for inequality, poverty, and propoor growth that can be used to compute these measures from GB2 parameter estimates. An application to data from China and Indonesia is provided.
C13|Determinants of FDI in South Africa: Do macroeconomic variables matter?|In this study we examine the macroeconomic determinants of FDI for the South African economy using data collected between 1994 and 2016 using the ARDL model for cointegration. The specific macroeconomic determinants which are used in the study are per capita GDP, the inflation rate, government size, real interest rate variable, and terms of trade. With the exception of inflation the remaining macroeconomic determinants employed in the study are positively and significantly related with FDI. However, in the short-run all variables are positively and significantly correlated with FDI. Collectively, these results have important implications for policymakers.
C13|Are fiscal budgets sustainable in South Africa? Evidence from provincial level data| This study uses the nonlinear autoregressive distributive lag (N-ARDL) model to investigate the expenditure-revenue relationship for all nine South African provinces using annual data spanning from 2000 to 2016. Whereas other cointegration models can only depict whether budgets are sustainable or not, the N-ARDL model presents features which further enable us to predict a course of action which individual provincial governments can take towards attaining higher levels of budgetary sustainability in both the short and the long-run. Ultimately, our empirical study demonstrates that the ‘one rule fit all’ strategy as suggested by previous studies may not be an appropriate approach seeing that provincial governments have differing requirements for attaining improved levels of budget sustainability
C13|Are fiscal budgets sustainable in South Africa? Evidence from provincial level data|This study uses the nonlinear autoregressive distributive lag (N-ARDL) model to investigate the expenditure-revenue relationship for all nine South African provinces using annual data spanning from 2000 to 2016. Whereas other cointegration models can only depict whether budgets are sustainable or not, the N-ARDL model presents features which further enable us to predict a course of action which individual provincial governments can take towards attaining higher levels of budgetary sustainability in both the short and the long-run. Ultimately, our empirical study demonstrates that the ‘one rule fit all’ strategy as suggested by previous studies may not be an appropriate approach seeing that provincial governments have differing requirements for attaining improved levels of budget sustainability.
C13|How sustainable are fiscal budgets in the Kingdom of Swaziland?|The recently experienced Swazi fiscal crisis of 2011 has facilitated the need for an academic probe into the sustainability of fiscal budgets in the Kingdom. Against the absence of empirical evidence evaluating the sustainability of Swazi fiscal budget, our study fills the hiatus by econometrically evaluating the sustainability of the fiscal budget of the Swazi economy between 1999 and 2016. Our empirical study depends on a combination of linear and asymmetric unit root and cointegration empirical procedures to attain this objective. In reviewing the obtained results, the evidence obtained from the linear econometric frameworks is inconclusive whereas the results from the more vigorous asymmetric models point to the unsustainability of Swazi fiscal budget over both the short and long-run. Important policy implications for Swazi fiscal policymakers are drawn from the analysis.
C13|Financial development and economic growth in Brazil: A Non-linear ARDL approach|Financial intermediation through the banking system plays an important role in economic development through the allocation of savings, thus improving productivity, and ultimately increasing the rate of economic growth. This paper examines the interrelationships between financial development and economic growth using the Nonlinear Autoregressive Distributed Lag (NARDL) model for Brazil. The time component of the study’s database is 1985 – 2015 inclusive. The study focused on the banking sector and stock market indicators of financial developments. The empirical results suggest that the banking sector measures of financial development have a negative relationship with economic growth while the financial development indicators representing stock market development are positively related to economic growth. The study also established an evidence of a long run and short run asymmetric relationship between financial development and growth. The empirical results open new insights for policy makers for long run and sustainable economic development.
C13|African stock markets integration: an analysis of the relationship between major stock markets in Africa|This paper examines the dynamic relationships between thirteen major stock markets in Africa in normal times and in times of financial crises using the Johansen cointegration and Granger causality methodologies. The empirical results revealed evidence of time-varying relationships among African stock markets. While the long-run relationships among the markets were strong prior to the 2007 global financial crisis (GFC) and during the Eurozone sovereign debt crisis (ESDC) periods, the relationships were severely weakened during the period of the GFC. The result also revealed a high degree of short-run dynamic causal relationships among African stock markets during both crises periods compared to the pre-crisis period.
C13|Structural changes in exchange rate-stock returns dynamics in South Africa: Examining the role of crisis and new trading platform|The 2007 sub-prime crisis and the adoption of Millennium trading platform represent two of the most important recent structural developments for the Johannesburg Stock Exchange (JSE). Under an environment of flexible and volatile exchange rates, this study seeks to examine the effects of these two structural events on the exchange rate-equity returns nexus for 4 JSE indices using the nonlinear autoregressive distributive lag (N-ARDL) cointegration. We use monthly data collected from 2000:M01 to 2017:M12, and conduct our empirical analysis over sub-periods corresponding to breaks caused by the crisis and the use of a new trading platform. We find prior the crisis exchange rates appreciations generally cause stock returns whereas depreciations are unlikely to cause stock returns to decrease. However, during crisis period this relationship entire disappears whilst resurfacing subsequent to the adoption of a new trading platform although the dynamics of the time series differs between sectors. Our overall empirical results caution regulatory authorities to closely monitor stock market developments as the new trading platform offers market participants opportunities of using the exchange rate to beat the market.
C13|A provincial perspective of nonlinear Okun's law for emerging markets: The case of South Africa|A provincial analysis of Okun’s law in South Africa is provided in this article over a period of 1996 to 2016. Empirically, we rely on the nonlinear autoregressive distributive lag (N-ARDL) model whilst the Corbae-Ouliaris filter is used to extract the ‘gap’ variables required for our regression estimates. Okun’s law is found to be significant hold in the long-run exclusively for the Western Cape and Kwa-Zulu Natal provinces whereas the remaining provinces partially display significant short-run effects. Our sensitivity analysis in which panel N-ARDL estimations for all provinces finds insignificant long-run Okun effects for the country as a whole, whilst validating the relationship only in the short-run. Our study hence implies that/advices that the epicentre of policy efforts in addressing the country’s high unemployment and low economic growth dilemma should be concentrated at a provincial level.
C13|Trade Openness and Economic Growth in SADC Countries|In spite of the wave of liberalisation studied during the past decades, the debate still remains open on the issue of the trade openness and economic growth nexus. The paper reviews the relationship between trade openness and economic growth for 11 SADC countries for the period between 1990 and 2016. Investments, labour and inflation are incorporated in the model to form a multivariate framework. The study employed the ARDL-bounds test approach and the Pooled Mean Group (PMG) model to estimate the long-run relationship among the variables. The evidence suggests that co-integration is detected at the 1% level in all countries with the exception of Malawi, Mauritius, Swaziland and Tanzania. Co-integration is only detected at the 10% level in Tanzania while Malawi, Mauritius and Swaziland the null of no co-integration is not rejected. Furthermore, the PMG results revealed that trade openness has a negative impact on economic growth in the long-run. A positive relationship between the variables was found only in the short-run. Apertura commerciale e crescita economica nei paesi SADC Nonostante l’onda liberista che ha caratterizzato gli studi degli ultimi decenni, il dibattito sulla relazione tra apertura commerciale e crescita economica è ancora aperto. Questo articolo rivede detta relazione su un campione di 11 paesi SADC (Southern African Development Cooperation) nel periodo 1990-2016. Investimenti, lavoro e inflazione sono incorporati nel modello. Lo studio impiega i test ARDL-bound (Autoregressive Distributed Lag) e il modello PMG (Pooled Mean Group) per valutare la relazione di lungo periodo tra le variabili. Vi sono evidenze di cointegrazione all’1% in tutti i paesi ad eccezione di Malawi, Muritius, Swaziland e Tanzania. La cointegrazione è significativa solo al 10% in Tanzania mentre per Malawi, Mauritius e Swaziland non si rifiuta il valore zero di non cointegrazione. Inoltre, i risultati PGM rivelano che l’apertura commerciale ha un impatto negativo sulla crescita economica nel lungo periodo. Una relazione positiva tra le due variabili è stata riscontrata solo nel breve periodo.
C13|Robust analysis of convergence in per capita GDP in BRICS economies|Whilst the issue of whether or not per capita GDP adheres to the convergence theory continues to draw increasing attention within the academic paradigm, with very little consensus having been reached in the literature thus far. Our study contributes to the literature by examining the stationarity of per capita GDP for BRICS countries using annual data collected between 1971 and 2015. Considering that our sample covers a period underlying a number of crisis and structural breaks within and amongst the BRICS countries, we rely on a robust nonlinear unit root testing procedure which captures a series of unobserved structural breaks. Our results confirm on Brazil and China being the only two BRICS economies who present the most convincing evidence of per capita GDP converging back to it’s natural equilibrium after an economic shock, whilst Russia and South Africa provide less convincing evidence of convergence dynamics in the time series and India having the weakest convergence properties.
C13|FDI as a contributing factor to economic growth in Burkina Faso: How true is this?|Much emphasis has been placed on attracting FDI into Burkina Faso as a catalyst for improved economic growth within the economy. Against the lack of empirical evidence evaluating this claim, we use data collected from 1970 to 2017 to investigate the FDI-growth nexus for the country using the ARDL bounds cointegration analysis. Our empirical model is derived from endogenous growth theoretical framework in which FDI may have direct or spillover effects on economic growth via improved human capital development as well technological developments reflected in urbanization and improved export growth. Our findings fail to establish any direct or indirect effects of FDI on economic growth except for FDI’s positive interaction with export-oriented growth, albeit being constrained to the short-run. Therefore, in summing up our recommendations, political reforms and the building of stronger economic ties with the international community in order to raise investor confidence, which has been historically problematic, should be at the top of the agenda for policymakers in Burkina Faso.
C13|Inflation-Growth Nexus in Botswana: Can Lower Inflation Really Spur Growth in the Country?|Does inflation affect economic growth in Botswana over the short-run and long-run? In applying bounds procedure for modelling ARDL cointegation effects applied to empirical data collected between 1975 and 2016 we find that this hypothesis does not hold true for Botswana as inflation is found to be insignificantly related with economic growth over both the short and long-run. Our growth equation estimates point to exports (positive), government size (negative) and an Pula/Dollar exchange rate (negative) as being significantly correlated with steady-state GDP growth. Further empirical exercises show that an appreciated Pula/dollar exchange rate increases inflation whilst bearing no effect on economic growth. Conversely, a depreciated Pula/Dollar exchange simultaneously decreases inflation and economic growth for the Botswana economy. Policymakers should be this aware that attainment of lower inflation rates which occurs through a depreciated Pula/Dollar currency will only retard economic growth.
C13|Renewable energy-economic growth nexus in South Africa: Linear, nonlinear or non-existent?|With escalating fears of climate change reaching irreversible levels, much emphasis has been recently placed on shifting to renewable sources of energy in supporting future economic livelihood. Focusing on South Africa, as Africa’s largest energy consumer and producer, our study investigates the short-run and long-run effects of renewable energy on economic growth using linear and nonlinear autoregressive distributive lag (ARDL) models. Working with data availability, our empirical analysis is carried out over the period of 1991 to 2016, and our results unanimously fail to confirm any linear or nonlinear cointegration effects of the consumption and production of renewable energy on South African economic growth. We view the absence of cointergation relations as an indication of inefficient usage of renewable energy in supporting sustainable growth in South Africa and hence advise policymakers to accelerate the establishment of necessary renewable infrastructure in supporting future energy requirements.
C13|Financial development and economic growth in SADC countries: A panel study|The impact of financial development on economic growth has received considerable attention since the 2008/2009 global financial crisis. High levels of credit to the private sector were partly to blame for the crisis and this has re-ignited the debate on whether the growth enhancing effects of financial development outweigh the retarding effects associated with financial crises. This paper therefore, examines the financial development-growth nexus in SADC countries during the period 1990-2015. Financial development indices are created due to the strong correlations between the individual financial development indicators using principal component analysis. The empirical analysis is conducted using the Pooled Mean Group estimator and the results show that financial development has a negative impact on economic growth. Due to financial vulnerabilities emanating from an inadequate monitoring and supervisory framework, further enhancement of financial development should be undertaken with caution in SADC countries.
C13|Credit Risk Analysis using Machine and Deep learning models|Due to the hyper technology associated to Big Data, data availability and computing power, most banks or lending financial institutions are renewing their business models. Credit risk predictions, monitoring, model reliability and effective loan processing are key to decision making and transparency. In this work, we build binary classifiers based on machine and deep learning models on real data in predicting loan default probability. The top 10 important features from these models are selected and then used in the modelling process to test the stability of binary classifiers by comparing performance on separate data. We observe that tree-based models are more stable than models based on multilayer artificial neural networks. This opens several questions relative to the intensive used of deep learning systems in the enterprises
C13|Series estimation for single-index models under constraints|This paper discusses a semiparametric single-index model. The link function is allowed to be unbounded and has unbounded support that fill the gap in the literature. The link function is treated as a point in an infinitely many dimensional function space which enables us to derive the estimates for the index parameter and the link function simultaneously. This approach is different from the profile method commonly used in the literature. The estimator is derived from an optimization with the constraint of an identification condition for the index parameter, which solves an important problem in the literature of single-index models. In addition, making use of a property of Hermite orthogonal polynomials, an explicit estimator for the index parameter is obtained. Asymptotic properties of the two estimators of the index parameter are established. Their efficiency is discussed in some special cases as well. The finite sample properties of the two estimators are demonstrated through an extensive Monte Carlo study and an empirical example.
C13|Panel Data Binary Response Model In A Triangular System|We propose a new control function (CF) method for binary response outcomes in a triangular system with unobserved heterogeneity of multiple dimensions. The identified CFs are the expected values of the heterogeneity terms in the reduced form equations conditional on the endogenous, Xi ≡ (xi1, . . . ,xiT ), and the exogenous, Zi ≡ (zi1, . . . , ziT ), variables. The method requires weaker restrictions compared to traditional CF methods for triangular systems with imposed structures similar to ours, and point-identifies average partial effects with discrete instruments. We discuss semiparametric identification of structural measures using the proposed CFs. An application and Monte Carlo experiments compare several alternative methods with ours.
C13|Mapping The Stocks In Micex: Who Is Central To The Moscow Stock Exchange?|In this article we use partial correlations to derive bidirectional connections between major firms listed in the Moscow Stock Exchange. We obtain coefficients of partial correlation from the correlation estimates of the Constant Conditional Correlation GARCH (CCC-GARCH) and the consistent Dynamic Conditional Correlation GARCH (cDCC-GARCH) models. We map the graph of partial correlations using the Gaussian Graphical Model and apply network analysis to identify the most central firms in terms of both shock propagation and connectedness with others. Moreover, we analyze some network characteristics over time and based on these we construct a measure of system vulnerability to external shocks. Our findings suggest that during the crisis interconnectedness between firms strengthens and becomes polarized and the system becomes more vulnerable to systemic shocks. In addition, we found that the most connected firms are the state-owned firms Sberbank and Gazprom and the private oil company Lukoil, while in the top most central in terms of systemic risk contributors Sberbank gave its place to NLMK Group.
C13|Complementarities In Performance Between Product Innovation, Marketing Innovation And Cooperation With Clients|This paper examines the complementary relationship between product innovation, marketing innovation and cooperation with clients, based on data from Estonian firms. The author evaluated complementary relationship in terms of its effect on the firm’s total factor productivity. This study uses the Community Innovation Survey (CIS) and Estonian Business Register data from the years 2002–2012 and the Heckman selection model to research the complementarity effect between studied innovation activities using the supermodularity approach. The results show that product innovation and marketing innovation are complementary in the service industry, but in manufacturing industry there is lack of evidence for the effect of complementarity. Cooperation with clients showed inconclusive complementarity test results involving both innovation types in both industries. Using panel data as a robustness test showed more insights into the complementary effects between cooperation with clients and the studied forms of innovation. However, the results show a weak complementarity effect between cooperation and innovation and suggest that there is still no clear complementarity effect.
C13|Shift-Share Designs: Theory and Inference|We study inference in shift-share regression designs, such as when a regional outcome is regressed on a weighted average of observed sectoral shocks, using regional sector shares as weights. We conduct a placebo exercise in which we estimate the effect of a shift-share regressor constructed with randomly generated sectoral shocks on actual labor market outcomes across U.S. Commuting Zones. Tests based on commonly used standard errors with 5% nominal significance level reject the null of no effect in up to 55% of the placebo samples. We use a stylized economic model to show that this overrejection problem arises because regression residuals are correlated across regions with similar sectoral shares, independently of their geographic location. We derive novel inference methods that are valid under arbitrary cross-regional correlation in the regression residuals. We show that our methods yield substantially wider confidence intervals in popular applications of shift-share regression designs.
C13|Linear IV Regression Estimators for Structural Dynamic Discrete Choice Models|In structural dynamic discrete choice models, the presence of serially correlated unobserved states and state variables that are measured with error may lead to biased parameter estimates and misleading inference. In this paper, we show that instrumental variables can address these issues, as long as measurement problems involve state variables that evolve exogenously from the perspective of individual agents (i.e., market-level states). We define a class of linear instrumental variables estimators that rely on Euler equations expressed in terms of conditional choice probabilities (ECCP estimators). These estimators do not require observing or modeling the agent’s entire information set, nor solving or simulating a dynamic program. As such, they are simple to implement and computationally light. We provide constructive identification arguments to identify the model primitives, and establish the consistency and asymptotic normality of the estimator. A Monte Carlo study demonstrates the good finite-sample performance of the ECCP estimator in the context of a dynamic demand model for durable goods.
C13|A structural model of firm collaborations with unobserved heterogeneity| We develop and estimate a structural model of strategic network formation to study the determinants of firms collaborations for patenting new technology in the medical device industry. Our aim is to bridge the strategy literature on interorganizational networks and the economic literature on structural estimation of network models. In our model, firms have payoffs that depend on linking costs and benefits, as well as externalities from common partners and popular partners. Firms are characterized by observed and unobserved characteristics, that affect both their opportunity and their willingness to form links. The equilibrium networks are sparse and match the aggregate clustering levels observed in the data. We use the network of patent collaborations among medical device firms, to estimate the structural parameters using a Bayesian approach. Our results show that firms tend to partner domestically and collaborate with companies in similar markets, perhaps due to technological complementarities or regulation effects. Unobserved heterogeneity matters: we find that firms' payoffs vary by type. Finally we show that the estimated model including unobserved heterogeneity provides a better fit of crucial features of the data.
C13|Methods Matter: P-Hacking and Causal Inference in Economics|The economics 'credibility revolution' has promoted the identification of causal relationships using difference-in-differences (DID), instrumental variables (IV), randomized control trials (RCT) and regression discontinuity design (RDD) methods. The extent to which a reader should trust claims about the statistical significance of results proves very sensitive to method. Applying multiple methods to 13,440 hypothesis tests reported in 25 top economics journals in 2015, we show that selective publication and p-hacking is a substantial problem in research employing DID and (in particular) IV. RCT and RDD are much less problematic. Almost 25% of claims of marginally significant results in IV papers are misleading.
C13|Predictability Hidden by Anomalous Observations|Testing procedures for predictive regressions with lagged autoregressive variables imply a suboptimal inference in presence of small violations of ideal assumptions. We propose a novel testing framework resistant to such violations, which is consistent with nearly integrated regressors and applicable to multi-predictor settings, when the data may only approximately follow a predictive regression model. The Monte Carlo evidence demonstrates large improvements of our approach, while the empirical analysis produces a strong robust evidence of market return predictability hidden by anomalous observations, both in- and out-of-sample, using predictive variables such as the dividend yield or the volatility risk premium.
C13|Bootstrap Inference for Penalized GMM Estimators with Oracle Properties|We study the validity of bootstrap methods in approximating the sampling distribution of penalized GMM estimators with oracle properties. More precisely, we focus on bridge estimators with L_q penalty for 0
C13|Multidimensional Parameter Heterogeneity in Panel Data Models|This article introduces an approach to estimation for static or dynamic panel data models that feature intercept and slope heterogeneity across individuals and over time. It is able to estimate each individual observation coefficient as well as the average coefficient over the sample, and allows for correlation between the heterogeneity and the regressors. Asymptotic theory establishes the consistency and asymptotic normality of the estimates as N and T jointly go to infinity. Finally, Monte Carlo simulations demonstrate that the estimator performs well in environments where fixed effects and mean group estimators are inconsistent and severely biased.
C13|Inference for Iterated GMM Under Misspecification and Clustering|This paper develops a new distribution theory and inference methods for over-identified Generalized Method of Moments (GMM) estimation focusing on the iterated GMM estimator, allowing for moment misspecification, and for clustered dependence with heterogeneous and growing cluster sizes. This paper is the first to provide a rigorous theory for the iterated GMM estimator. We provide conditions for its existence by demonstrating that the iteration sequence is a contraction mapping. Our asymptotic theory allows the moments to be possibly misspecified, which is a general feature of approximate over-identified models. This form of moment misspecification causes bias in conventional standard error estimation. Our results show how to correct for this standard error bias. Our paper is also the first to provide a rigorous distribution theory for the GMM estimator under cluster dependence. Our distribution theory is asymptotic, and allows for heterogeneous and growing cluster sizes. Our results cover standard smooth moment condition models, including dynamic panels, which is a common application for GMM with cluster dependence. Our simulation results show that conventional heteroskedasticity-robust standard errors are highly biased under moment misspecification, severely understating estimation uncertainty, and resulting in severely over-sized hypothesis tests. In contrast, our misspecification-robust standard errors are approximately unbiased and properly sized under both correct specification and misspecification. We illustrate the method by extending the empirical work reported in Acemoglu, Johnson, Robinson, and Yared (2008, American Economic Review) and Cervellati, Jung, Sunde, and Vischer (2014, American Economic Review). Our results reveal an enormous effect of iterating the GMM estimator, demonstrating the arbitrari- ness of using one-step and two-step estimators. Our results also show a large effect of using misspecification robust standard errors instead of the Arellano-Bond standard errors. Our results support Acemoglu, Johnson, Robinson, and Yared’s conclusion of an insignificant effect of income on democracy, but reveal that the heterogeneous effects documented by Cervellati, Jung, Sunde, and Vischer are less statistically significant than previously claimed.
C13|Import demand function for Turkey|This study revisits the import demand function for Turkey using the newly defined national income data and examines the evolution of the income and price elasticities over time. In this respect, demand functions are estimated for the total imports and its subcomponents separately, and the corresponding time varying elasticities are obtained by applying the method of Kalman filter between 2003 and 2018. The findings suggest that the growth of total imports is mainly explained by income and relative price changes. The income and expenditure elasticities decrease over time in total imports and in sub-components except for intermediate goods. The relative price elasticity remains almost unchanged for investment and consumption goods imports but increases considerably for the intermediate goods imports and total imports.
C13|Likelihood based inference for an Identifiable Fractional Vector Error Correction Model|We consider the Fractional Vector Error Correction model proposed in Avarucci (2007), which is characterized by a richer lag structure than the models proposed in Granger (1986) and Johansen (2008, 2009). In particular, we discuss the properties of the model of Avarucci (2007) (FECM) in comparison to the model of Johansen (2008, 2009) (FCVAR). Both models generate the same class of processes, but the properties of the two models are different. First, opposed to the model of Johansen (2008, 2009), the model of Avarucci has a convenient nesting structure, which allows for testing the number of lags and the cointegration rank exactly in the same way as in the standard I(1) cointegration framework of Johansen (1995) and hence might be attractive for econometric practice. Second, we find that the model of Avarucci (2007) is almost free from identification problems, contrary to the model of Johansen (2008, 2009) and Johansen and Nielsen (2012), which identification problems are discussed in Carlini and Santucci de Magistris (2017). However, due to a larger number of parameters, the estimation of the FECM model of Avarucci (2007) turns out to be more complicated. Therefore, we propose a 4-step estimation procedure for this model that is based on the switching algorithm employed in Carlini and Mosconi (2014), together with the GLS procedure of Mosconi and Paruolo (2014). We check the performance of the proposed estimation procedure in finite samples by means of a Monte Carlo experiment and we prove the asymptotic distribution of the estimators of all the parameters. The solution of the model has been previously derived in Avarucci (2007), while testing for the rank has been discussed in Lasak and Velasco (for cointegration strength >0.5) and Avarucci and Velasco (for cointegration strength
C13|Improved Estimation of the Extreme Value Index Using Related Variables|Heavy tailed phenomena are naturally analyzed by extreme value statistics. A crucial step in such an analysis is the estimation of the extreme value index, which describes the tail heaviness of the underlying probability distribution. We consider the situation where we have next to the n observations of interest another n+m observations of one or more related variables, like, e.g., financial losses due to earthquakes and the related amounts of energy released, for a longer period than that of the losses. Based on such a data set, we present an adapted version of the Hill estimator that shows greatly improved behavior and we establish the asymptotic normality of this estimator. For this adaptation the tail dependence between the variable of interest and the related variable(s) plays an important role. A simulation study confirms the substantially improved performance of our adapted estimator relative to the Hill estimator. We also present an application to the aforementioned earthquake losses.
C13|ExpectHill estimation, extreme risk and heavy tails|Risk measures of a financial position are traditionally based on quantiles. Replacing quantiles with their least squares analogues, called expectiles, has recently received increasing attention. The novel expectile-based risk measures satisfy all coherence requirements. We revisit their extreme value estimation for heavy-tailed distributions. First, we estimate the underlying tail index via weighted combinations of top order statistics and asymmetric least squares estimates. The resulting expectHill estimators are then used as the basis for estimating tail expectiles and Expected Shortfall. The asymptotic theory of the proposed estimators is provided, along with numerical simulations and applications to actuarial and financial data.
C13|"""Generalized Measures of Correlation for Asymmetry, Nonlinearity, and Beyond"": Comment"|This note comments on the Generalised Measure of Correlation (GMC) suggested by Zheng et al. (2012). The GMC concept was largely anticipated in a publication 115 years earlier, undertaken by Yule (1897), in the proceedings of the Royal Society. The note is directed at giving Yule (1897) credit for covering the foundations of the topic comprehensively.
C13|Stein-like Shrinkage Estimation of Panel Data Models with Common Correlated Effects|This paper examines the asymptotic properties of the Stein-type shrinkage combined (averaging) estimation of panel data models. We introduce a combined estimation when the fixed effects (FE) estimator is inconsistent due to endogeneity arising from the correlated common effects in the regression error and regressors. In this case the FE estimator and the CCEP estimator of Pesaran (2006) are combined. This can be viewed as the panel data model version of the shrinkage to combine the OLS and 2SLS estimators as the CCEP estimator is a 2SLS or control function estimator that controls for the endogeneity arising from the correlated common effects. The asymptotic theory, Monte Carlo simulation, and empirical applications are presented. According to our calculation of the asymptotic risk, the Stein-like shrinkage estimator is more efficient estimation than the CCEP estimator.
C13|A Combined Random Effect and Fixed Effect Forecast for Panel Data Models|When some of the regressors in a panel data model are correlated with the random individual effects, the random effect (RE) estimator becomes inconsistent while the fixed effect (FE) estimator is consistent. Depending on the various degree of such correlation, we can combine the RE estimator and FE estimator to form a combined estimator which can be better than each of the FE and RE estimators. In this paper, we are interested in whether the combined estimator may be used to form a combined forecast to improve upon the RE forecast (forecast made using the RE estimator) and the FE forecast (forecast using the FE estimator) in out-of-sample forecasting. Our simulation experiment shows that the combined forecast does dominate the FE forecast for all degrees of endogeneity in terms of mean squared forecast errors (MSFE), demonstrating that the theoretical results of the risk dominance for the in-sample estimation carry over to the out-of-sample forecasting. It also shows that the combined forecast can reduce MSFE relative to the RE forecast for moderate to large degrees of endogeneity and for large degrees of heterogeneity in individual effects.
C13|The Second-order Asymptotic Properties of Asymmetric Least Squares Estimation|"The higher-order asymptotic properties provide better approximation of the bias for a class of estimators. The first-order asymptotic properties of the asymmetric least squares (ALS) estimator have been investigated by Newey and Powell (1987). This paper develops the second-order asymptotic properties (bias and mean squared error) of the ALS estimator, extending the second-order asymptotic results for the symmetric least squares (LS) estimators of Rilstone, Srivastava and Ullah (1996). The LS gives the mean regression function while the ALS gives the ""expectile"" regression function, a generalization of the usual regression function. The second-order bias result enables an improved bias correction and thus an improved ALS estimation in finite sample. In particular, we show that the second-order bias is much larger as the asymmetry is stronger, and therefore the benefit of the second-order bias correction is greater when we are interested in extreme expectiles which are used as a risk measure in financial economics. The higher-order MSE result for the ALS estimation also enables us to better understand the sources of estimation uncertainty. The Monte Carlo simulation confirms the benefits of the second-order asymptotic theory and indicates that the second-order bias is larger at the extreme low and high expectiles."
C13|Combined Estimation of Semiparametric Panel Data Models|The combined estimation for the semiparametric panel data models is proposed. The properties of estimators for the semiparametric panel data models with random effects (RE) and fixed effects (FE) are examined. When the RE estimator suffers from endogeneity due to the individual effects correlated with the regressors, the semiparametric RE and FE estimators may be adaptively combined, with the combining weights depending on the degree of endogeneity. The asymptotic distributions of these three estimators (RE, FE, and combined estimators) for the semiparametric panel data models are derived using a local asymptotic framework. These three estimators are then compared in asymptotic risk. The semiparametric combined estimator has strictly smaller asymptotic risk than the semiparametric fixed effect estimator. The Monte Carlo study shows that the semiparametric combined estimator outperforms semiparametric FE and RE estimators except when the degrees of endogeneity and heterogeneity of the individual effects are very small. Also presented is an empirical application where the effect of public sector capital in the private economy production function is examined using the US state level panel data.
C13|Impact of Trade Openness on Economic Growth: Empirical Evidence from South Africa|This paper examines the impact of trade openness on economic growth in South Africa. The study employs the autoregressive distributed lag (ARDL) bounds testing approach to investigate the dynamic impact of trade openness on economic growth. Unlike some previous studies, the current study uses four proxies of trade openness, with each proxy addressing a different aspect of trade openness. The first proxy of trade openness is derived from the ratio of trade to gross domestic product (GDP). The second proxy is the ratio of exports to GDP, while the third proxy is the ratio of imports to GDP. The last proxy is an index of trade openness, which captures the effects of residual openness, resulting from taking the country’s size and geography into account. Based on the long run empirical results, this study finds that trade openness has a positive and significant impact on economic growth when the ratio of total trade to GDP is used as a proxy, but not when the three other proxies are employed. However, in the short-run, when the first three proxies of openness are used, the study finds trade openness to have a positive impact on economic growth, but not so when the trade openness index is employed. These results, therefore, suggest that the promotion of policies that support international trade is relevant in the South African economy. L’impatto dell’apertura commerciale sulla crescita: evidenze empiriche dal Sud Africa Questa ricerca esamina l’impatto dell’apertura commerciale sulla crescita in Sud Africa utilizzando il test ARDL (Autoregressive Distributed Lag). A differenza di studi precedenti, il presente lavoro utilizza quattro indici di apertura commerciale, ciascuno dei quali ne rappresenta un aspetto diverso. Il primo si ottiene dal rapporto commercio/PIL. Il secondo è rappresentato dal rapporto esportazioni/PIL, il terzo dal rapporto importazioni/PIL. L’ultimo è un indice di apertura commerciale che esprime gli effetti dell’apertura residuale ottenuta considerando la dimensione e le caratteristiche geografiche del paese. Sulla base di risultati empirici di lungo periodo questo studio rileva che l’apertura commerciale ha un effetto positivo significativo sulla crescita quando si utilizza l’indice ottenuto dal rapporto commercio totale/PIL, mentre tale effetto non emerge quando sono utilizzati gli altri tre indici. I risultati dello studio evidenziano che, nel breve periodo, se si utilizzano i primi tre indici risulta un impatto positivo sulla crescita, mentre ciò non si verifica quando viene impiegato il quarto indice.
C13|Bayesian Dynamic Tensor Regression|Multidimensional arrays (i.e. tensors) of data are becoming increasingly available and call for suitable econometric tools. We propose a new dynamic linear regression model for tensor-valued response variables and covariates that encompasses some well-known multivariate models such as SUR, VAR, VECM, panel VAR and matrix regression models as special cases. For dealing with the over-parametrization and over-fitting issues due to the curse of dimensionality, we exploit a suitable parametrization based on the parallel factor (PARAFAC) decomposition which enables to achieve both parameter parsimony and to incorporate sparsity effects. Our contribution is twofold: first, we provide an extension of multivariate econometric models to account for both tensor-variate response and covariates; second, we show the effectiveness of proposed methodology in defining an autoregressive process for time-varying real economic networks. Inference is carried out in the Bayesian framework combined with Monte Carlo Markov Chain (MCMC). We show the efficiency of the MCMC procedure on simulated datasets, with different size of the response and independent variables, proving computational efficiency even with high-dimensions of the parameter space. Finally, we apply the model for studying the temporal evolution of real economic networks.
C13|Bayesian Markov Switching Tensor Regression for Time-varying Networks|We propose a new Bayesian Markov switching regression model for multi-dimensional arrays (tensors) of binary time series. We assume a zero-inflated logit dynamics with time-varying parameters and apply it to multi-layer temporal networks. The original contribution is threefold. First, in order to avoid over-fitting we propose a parsimonious parametrization of the model, based on a low-rank decomposition of the tensor of regression coefficients. Second, the parameters of the tensor model are driven by a hidden Markov chain, thus allowing for structural changes. The regimes are identified through prior constraints on the mixing probability of the zero-inflated model. Finally, we model the jointly dynamics of the network and of a set of variables of interest. We follow a Bayesian approach to inference, exploiting the Pólya-Gamma data augmentation scheme for logit models in order to provide an efficient Gibbs sampler for posterior approximation. We show the effectiveness of the sampler on simulated datasets of medium-big sizes, finally we apply the methodology to a real dataset of financial networks.
C13|Nonparametric Forecasting of Multivariate Probability Density Functions|The study of dependence between random variables is the core of theoretical and applied statistics. Static and dynamic copula models are useful for describing the dependence structure, which is fully encrypted in the copula probability density function. However, these models are not always able to describe the temporal change of the dependence patterns, which is a key characteristic of financial data. We propose a novel nonparametric framework for modelling a time series of copula probability density functions, which allows to forecast the entire function without the need of post-processing procedures to grant positiveness and unit integral. We exploit a suitable isometry that allows to transfer the analysis in a subset of the space of square integrable functions, where we build on nonparametric functional data analysis techniques to perform the analysis. The framework does not assume the densities to belong to any parametric family and it can be successfully applied also to general multivariate probability density functions with bounded or unbounded support. Finally, a noteworthy field of application pertains the study of time varying networks represented through vine copula models. We apply the proposed methodology for estimating and forecasting the time varying dependence structure between the S&P500 and NASDAQ indices.
C13|Estimating the Trade and Welfare Effects of Brexit: A Panel Data Structural Gravity Model|This paper proposes a new panel data structural gravity approach for estimating the trade and welfare effects of Brexit. The suggested Constrained Poisson Pseudo Maximum Likelihood Estimator exhibits some useful properties for trade policy analysis and allows to obtain estimates and confidence intervals which are consistent with structural trade theory. Assuming different counterfactual post-Brexit scenarios, our main findings suggest that UKs (EUs) exports of goods to the EU (UK) are likely to decline within a range between 7.2% and 45.7% (5.9% and 38.2%) six years after the Brexit has taken place. For the UK, the negative trade effects are only partially offset by an increase in domestic goods trade and trade with third countries, inducing a decline in UKs real income between 1.4% and 5.7% under the hard Brexit scenario. The estimated welfare effects for the EU are negligible in magnitude and statistically not different from zero.
C13|Have a son, gain a voice: Son preference and female participation in household decision making|Son preference is common in many Asian countries. Though a growing body of literature examines the drivers and socioeconomic impacts of phenomenon in case of China and India, work on other Asian countries is scarce. This study uses nationally representative survey of over 13 thousand households from Pakistan (PDHS 2012-13) to analyze the effects of observed preference for sons on women’s participation in intra-household decision-making. Four key intra-household decisions are considered: decisions regarding healthcare, family visits, large household purchases and spending husband's income. These correspond to four categories of household decisions, namely healthcare, social, consumption and financial. Probit and Ordered Probit are employed as the main estimation techniques and other determinants of household decision-making are controlled for. Besides, a number of matching routines are employed to account for the possibility of potential selection bias. We find that women with at least one son have more say in household decisions. Bearing at least one son is associated with 5%, 7% and 5% higher say in decisions involving healthcare, social and consumption matters respectively. Women's role in financial affairs, however, does not differ significantly from women with no sons. Female participation in decision-making grows significantly with the number of sons but only up to the third parity. These results are particularly visible among younger, wealthier and educated women, and those who got married earlier. The findings suggest a limited improvement in women's bargaining power at home resulting from the birth of one or more sons. This in part explains higher desire for sons expressed by women compared to men in household surveys.
C13|Long-Run Determinants of Japanese Import Flows from USA and China : A Sectoral Approach|We analyze the determinants of the sectoral Japanese imports from her two main partners, China and the USA over the period 1971-2007. We estimate cointegration relationships with breaks, using the Saikkonen-Lütkepohl method. For six sectors: foods, raw materials, textile, mineral fuel, chemicals and machinery and equipment, we show that if the domestic demand affects positively the imports, the impact of prices changes can be different whether we retain the relative prices (homogeneity hypothesis) or we consider both domestic and import, while when we decompose the relative prices between imports prices and domestic (corporate) prices, except in one case (textile imports from the USA), we can reject the homogeneity hypothesis. A possible explanation is the greater volatility of import prices compared to domestic prices which leads importers to wait when import prices change, insofar as they don't if these changes are temporary or permanent.
C13|Long‐Run Determinants Of Japanese Exports To China And The United States: A Sectoral Analysis|We show that during the period 1971–2007, Japanese sectoral exports to China and the United States have depended on real exchange rate fluctuations and external demand (gross domestic product of the country of destination). This result holds for six sectors: foods, textile, metal products, chemicals, non-metal products, and machinery and equipment, as well as for both geographical destinations. Generally, the real exchange rate fluctuations and GDP have had the expected effects. In particular, a real appreciation of the yen and a bigger uncertainty has reduced the Japanese exports. But there is an important exception, as we find a price inelasticity of the principal Japanese exports to USA, i.e. Machinery and Equipment, which represent 80 percent of total exports to USA. So, a real depreciation of the yen may constitute an inappropriate policy to favor a process of growth export-led.<br><small>(This abstract was borrowed from another version of this item.)</small><br><small>(This abstract was borrowed from another version of this item.)</small><br><small>(This abstract was borrowed from another version of this item.)</small>
C13|Time varying integration amongst the South Asian equity markets: An empirical study| In this paper, we examine the dynamic nature of equity market integration for the South Asian countries. The daily data for local equity indices are used from 6 January 2004 to 31 March 2015. Copula GARCH models and Diebold and Yilmaz methodology have been employed to study the inter-temporal process of equity market integration. Empirical results show that the sample countries of the region exhibit very little or no levels of integration between them. Equity portfolio flows within the South Asian region reconfirms this trend for low integration in the region. Further, trend analysis of the fundamental determinants of financial integration for the SAARC countries was performed and the same was compared with its neighbouring regional economic bloc in Asia i.e. ASEAN + 6. It indicated that SAARC countries have to show sincere political commitment and require collaboration in efforts of policy realignment to work on their governance parameters, improve on their trade linkages and trade tariffs and develop their equity market infrastructure to achieve higher levels of financial integration. The paper contributes to the International Finance literature, especially dealing with regional economic blocs and has important implications for policy-makers, portfolio managers and academia.
C13|Asymptotics of Cholesky GARCH models and time-varying conditional betas|This paper proposes a new model with time-varying slope coefficients. Our model, called CHAR, is a Cholesky-GARCH model, based on the Cholesky decomposition of the conditional variance matrix introduced by Pourahmadi (1999) in the context of longitudinal data. We derive stationarity and invertibility conditions and prove consistency and asymptotic normality of the Full and equation-by-equation QML estimators of this model. We then show that this class of models is useful to estimate conditional betas and compare it to the approach proposed by Engle (2016). Finally, we use real data in a portfolio and risk management exercise. We find that the CHAR model outperforms a model with constant betas as well as the dynamic conditional beta model of Engle (2016).
C13|An Overview of Modified Semiparametric Memory Estimation Methods|Several modified estimation methods of the memory parameter have been introduced in the past years. They aim to decrease the upward bias of the memory parameter in cases of low frequency contaminations or an additive noise component, especially in situations with a short-memory process being contaminated. In this paper, we provide an overview and compare the performance of nine semiparametric estimation methods. Among them are two standard methods, four modified approaches to account for low frequency contaminations and three procedures developed for perturbed fractional processes. We conduct an extensive Monte Carlo study for a variety of parameter constellations and several DGPs. Furthermore, an empirical application of the log-absolute return series of the S&P 500 shows that the estimation results combined with a long-memory test indicate a spurious long-memory process.
C13|Poverty and Inequality in Francophone Africa, 1960s-2010s|The paper provides first generation estimates of poverty and inequality rates for three countries in francophone Africa – Cameroon, Côte d’Ivoire and Gabon – in the aftermath of independence. Sources – a large collection of historical household budgets – are new, as is the method that allows to connect historical sources to modern household budget surveys, and to deliver nationally representative estimates. The second part of the paper identifies the trend of poverty and inequality in Côte d’Ivoire for the years 1965-2015; we find that mean income growth failed to reduce poverty during the fifteen years of economic boom post-independence (1965-1979) because of increasing inequality. Conversely, in the following period (1979-2015) poverty changes are mostly guided by the evolution of growth.
C13|Norwegian export of farmed salmon − trade costs and market concentration|While variation in unit value most commonly has been associated with quality in the trade literature, observed differences in prices between markets might also be explained by variation in market concentration and the degree of competition. Using transaction data on Norwegian exports of salmon, we introduce a Herfindahl index as a measure of competition in a standard gravity model. We find that competition typically is weaker in small and distant markets that due to high trade costs are served by relatively few firms. We argue that the anti-competitive impact of trade costs may explain price differentiation between markets even for homogeneous products.
C13|Euler Equations, Subjective Expectations and Income Shocks|In this paper, we make three substantive contributions: first, we use elicited subjective income expectations to identify the levels of permanent and transitory income shocks in a life-cycle framework; second, we use these shocks to assess whether households' consumption is insulated from them; third, we use the shock data to estimate an Euler equation for consumption. We find that households are able to smooth transitory shocks, but adjust their consumption in response to permanent shocks, albeit not fully. The estimates of the Euler equation parameters with and without expectational errors are similar, which is consistent with rational expectations. We break new ground by combining data on subjective expectations about future income from the Michigan Survey with micro data on actual Income from the Consumer Expenditure Survey.
C13|Sample statistics as convincing evidence: A tax fraud case|This report deals with the analysis of data used by tax officers to support their claim of tax fraud at a pizzeria. The possibilities of embezzlement under study are overreporting of take-away sales and underreporting of cash payments. Several modelling approaches are explored, ranging from simple well-known methods to presumably more precise tools. More specifically, we contrast common methods based on normal assumptions and models based on Gamma-assumptions. For the latter, both maximum likelihood and Bayesian approaches are covered. Several criteria for the choice of method in practice are discussed, among them, how easy the method is to understand, justify and communicate to the parties. Some dilemmas present itself: the choice of statistical method, its role in building the evidence, the choice of risk factor, the application of legal principles like “clear and convincing evidence” and “beyond reasonable doubt”. The insights gained may be useful for both tax officers and defenders of the taxpayer, as well as for expert witnesses.
C13|The Eﬀect of Income Shocks on the Oil Price|This paper identiﬁes the eﬀect of income shocks on the real price of oil. We ﬁnd that for the period 1973-2016 shocks to world GDP created a response of a permanent rise in the oil price. In contrast, oil production does not correct the disequilibrium from a stable long-run equilibrium. Whereas shocks to GDP are persistent, shocks to the oil price are mostly transitory once we control for changes in world GDP and oil production. We ﬁnd evidence of a structural change in the response of the oil price after 1973. We conjecture that the response of oil production is key to the diﬀerences.
C13|Estimation of the linear fractional stable motion|In this paper we investigate the parametric inference for the linear fractional stable motion in high and low frequency setting. The symmetric linear fractional stable motion is a three-parameter family, which constitutes a natural non-Gaussian analogue of the scaled fractional Brownian motion. It is fully characterised by the scaling parameter $\sigma>0$, the self-similarity parameter $H \in (0,1)$ and the stability index $\alpha \in (0,2)$ of the driving stable motion. The parametric estimation of the model is inspired by the limit theory for stationary increments L\'evy moving average processes that has been recently studied in \cite{BLP}. More specifically, we combine (negative) power variation statistics and empirical characteristic functions to obtain consistent estimates of $(\sigma, \alpha, H)$. We present the law of large numbers and some fully feasible weak limit theorems.
C13|The Kink and Notch Bunching Estimators Cannot Identify the Taxable Income Elasticity|Bunching estimators were developed and extended by Saez (2010), Chetty et al. (2011) and Kleven and Waseem (2013). Using this method one can get an estimate of the taxable income elasticity from the bunching pattern around a kink point. The bunching estimator has become popular, with a large number of papers applying the method. In this paper, we show that the bunching estimator cannot identify the taxable income elasticity when the functional form of the distribution of preference heterogeneity is unknown. We find that an observed distribution of taxable income around a kink point or over the whole budget set can be consistent with any positive taxable income elasticity if the distribution of heterogeneity is unrestricted. If one is willing to assume restrictions on the heterogeneity density some information about the taxable income elasticity can be obtained. We give bounds on the taxable income elasticity based on monotonicity of the heterogeneity density and apply these bounds in an example. We also consider identification from budget set variation. We find that kinks alone may not be informative even when budget sets vary. However, if the taxable income specification is restricted to be of the parametric isoelastic form assumed in Saez (2010) the taxable income elasticity can be well identified from variation among budget sets. The key condition is that the tax rates at chosen taxable income differ across budget sets for some individuals.
C13|Forecasting Inflation in Argentina|In 2016 the Central Bank of Argentina began to announce inflation targets. In this context, providing authorities with good estimates of relevant macroeconomic variables is crucial for making pertinent corrections in order to reach the desired policy goals. This paper develops a group of models to forecast inflation for Argentina, which includes autoregressive models and different scale Bayesian VARs (BVAR), and compares their relative accuracy. The results show that the BVAR model can improve the forecast ability of the univariate autoregressive benchmark’s model of inflation. The Giacomini-White test indicates that a BVAR performs better than the benchmark in all forecast horizons. Statistical differences between the two BVAR model specifications (small and large-scale) are not found. However, looking at the RMSEs, one can see that the larger model seems to perform better for longer forecast horizons.
C13|Why does the peso-dollar exchange rate show a depreciation trend? The role of productivity differentials|Over the last three decades, Mexico's macroeconomic policy has been driven by a sound orthodox strategy: an open economy via many trade agreements signed since the mid-1980s, a nominal exchange rate under a flexible regime since 1994, central bank autonomy, and responsible fiscal policy, among other benchmarks. Nevertheless, the exchange rate has continued on a path of depreciation against the US dollar. In this paper, we show that although an equilibrium relationship exists between the exchange rate and prices in Mexico and the US (its main commercial partner), there are other forces affecting the former. The main factor in this relentless long-term depreciation is the loss of productivity in Mexico relative to the US. In addition, we show that the extraordinary liquidity supplied by the US during the 2008 crisis caused the Mexican peso to appreciate against the dollar.
C13|The influence of renewables on electricity price forecasting: a robust approach|In this paper a robust approach to modelling electricity spot prices is introduced. Differently from what has been recently done in the literature on electricity price forecasting, where the attention has been mainly drawn by the prediction of spikes, the focus of this contribution is on the robust estimation of nonlinear SETARX models (Self-Exciting Threshold Auto Regressive models with eXogenous regressors). In this way, parameters estimates are not, or very lightly, influenced by the presence of extreme observations and the large majority of prices, which are not spikes, could be better forecasted. A Monte Carlo study is carried out in order to select the best weighting function for Generalized M-estimators of SETAR processes. A robust procedure to select and estimate nonlinear processes for electricity prices is introduced, including robust tests for stationarity and nonlinearity and robust information criteria. The application of the procedure to the Italian electricity market reveals the forecasting superiority of the robust GM-estimator based on the polynomial weighting function respect to the non-robust Least Squares estimator. Finally, the introduction of external regressors in the robust estimation of SETARX processes contributes to the improvement of the forecasting ability of the model.
C13|On the iterated estimation of dynamic discrete choice games|" We study the asymptotic properties of a class of estimators of the structural parameters in dynamic discrete choice games. We consider K-stage policy iteration (PI) estimators, where K denotes the number of policy iterations employed in the estimation. This class nests several estimators proposed in the literature. By considering a ""maximum likelihood"" criterion function, our estimator becomes the K- ML estimator in Aguirregabiria and Mira (2002, 2007). By considering a ""minimum distance"" criterion function, it de nes a new K-MD estimator, which is an iterative version of the estimators in Pesendorfer and Schmidt-Dengler (2008) and Pakes et al. (2007). First, we establish that the K-ML estimator is consistent and asymptotically normal for any K. This complements ndings in Aguirregabiria and Mira (2007), who focus on K = 1 and K large enough to induce convergence of the estimator. Furthermore, we show that the asymptotic variance of the K-ML estimator can exhibit arbitrary patterns as a function K. Second, we establish that the K-MD estimator is consistent and asymptotically normal for any K. For a specifi c weight matrix, the K-MD estimator has the same asymptotic distribution as the K-ML estimator. Our main result provides an optimal sequence of weight matrices for the K-MD estimator and shows that the optimally weighted K-MD estimator has an asymptotic distribution that is invariant to K. This new result is especially unexpected given the findings in Aguirregabiria and Mira (2007) for K-ML estimators. Our main result implies two new and important corollaries about the optimal 1-MD estimator (derived by Pesendorfer and Schmidt-Dengler (2008)). First, the optimal 1-MD estimator is optimal in the class of K-MD estimators for all K. In other words, additional policy iterations do not provide asymptotic efficiency gains relative to the optimal 1-MD estimator. Second, the optimal 1-MD estimator is more or equally asymptotically efficient than any K-ML estimator for all K."
C13|GEL-based inference with unconditional moment inequality restrictions| This paper studies the properties of generalised empirical likelihood (GEL) methods for the estimation of and inference on partially identifi ed parameters in models specifi ed by unconditional moment inequality constraints. The central result is, as in moment equality condition models, a large sample equivalence between the scaled optimised GEL objective function and that for generalised method of moments (GMM) with weight matrix equal to the inverse of the efficient GMM metric for moment equality restrictions. Consequently, the paper provides a generalisation of results in the extant literature for GMM for the non-diagonal GMM weight matrix setting. The paper demonstrates that GMM in such circumstances delivers a consistent estimator of the identi fied set, i.e., those parameter values that satisfy the moment inequalities, and derives the corresponding rate of convergence. Based on these results the consistency of and rate of convergence for the GEL estimator of the identifi ed set are obtained. A number of alternative equivalent GEL criteria are also considered and discussed. The paper proposes simple conservative consistent confi dence regions for the identi fied set and the true parameter vector based on both GMM with a non-diagonal weight matrix and GEL. A simulation study examines the efficacy of the non-diagonal GMM and GEL procedures proposed in the paper and compares them with the standard diagonal GMM method.
C13|Locally robust semiparametric estimation| This paper shows how to construct locally robust semiparametric GMM estimators, meaning equivalently moment conditions have zero derivative with respect to the first step and the first step does not affect the asymptotic variance. They are constructed by adding to the moment functions the adjustment term for first step estimation. Locally robust estimators have several advantages. They are vital for valid inference with machine learning in the first step, see Belloni et. al. (2012, 2014), and are less sensitive to the specification of the first step. They are doubly robust for affine moment functions, where moment conditions continue to hold when one first step component is incorrect. Locally robust moment conditions also have smaller bias that is flatter as a function of first step smoothing leading to improved small sample properties. Series first step estimators confer local robustness on any moment conditions and are doubly robust for affine moments, in the direction of the series approximation. Many new locally and doubly robust estimators are given here, including for economic structural models. We give simple asymptotic theory for estimators that use cross-fitting in the first step, including machine learning.
C13|Inference on winners| Many empirical questions can be cast as inference on a parameter selected through optimization. For example, researchers may be interested in the effectiveness of the best policy found in a randomized trial, or the best-performing investment strategy based on historical data. Such settings give rise to a winner’s curse, where conventional estimates are biased and conventional confidence intervals are unreliable. This paper develops optimal confidence sets and median-unbiased estimators that are valid conditional on the parameter selected and so overcome this winner’s curse. If one requires validity only on average over target parameters that might have been selected, we develop hybrid procedures that combine conditional and projection confidence sets to offer further performance gains relative to existing alternatives.
C13|Nonlinear Factor Models for Network and Panel Data|Factor structures or interactive effects are convenient devices to incorporate latent variables in panel data models. We consider fixed effect estimation of nonlinear panel single-index models with factor structures in the unobservables, which include logit, probit, ordered probit and Poisson specifications. We establish that fixed effect estimators of model parameters and average partial effects have normal distributions when the two dimensions of the panel grow large, but might suffer of incidental parameter bias. We show how models with factor structures can also be applied to capture important features of network data such as reciprocity, degree heterogeneity, homophily in latent variables and clustering. We illustrate this applicability with an empirical example to the estimation of a gravity equation of international trade between countries using a Poisson model with multiple factors.
C13|Skewed logistic distribution for statistical temperature post-processing in mountainous areas|Non-homogeneous post-processing is often used to improve the predictive performance of probabilistic ensemble forecasts. A common quantity to develop, test, and demonstrate new methods is the near-surface air temperature frequently assumed to follow a Gaussian response distribution. However, Gaussian regression models with only few covariates are often not able to account for site-specific local features leading to strongly skewed residuals. This residual skewness remains even if many covariates are incorporated. Therefore, a simple refinement of the classical non-homogeneous Gaussian regression model is proposed to overcome this problem by assuming a skewed response distribution to account for possible skewness. This study shows a comprehensive analysis of the performance of non-homogeneous post-processing for 2m temperature for three different site types comparing Gaussian, logistic, and skewed logistic response distributions. Satisfying results for the skewed logistic distribution are found, especially for sites located in mountainous areas. Moreover, both alternative model assumptions but in particular the skewed response distribution, can improve on the classical Gaussian assumption with respect to overall performance, sharpness, and calibration of the probabilistic predictions.
C13|Trade creation and trade diversion of regional trade agreements revisited: A constrained panel pseudo-maximum likelihood approach|For the estimation of structural gravity models using PPML with countrypair, exporter-time and importer-time effects it proves useful to exploit the equilibrium restrictions imposed by the system of multilateral resistances. This yields an iterative projection based PPML estimator that is unaffected by the incidental parameters problem. Further, in this setting it is straight forward to establish the asymptotic distribution of the structural parameters and that of counterfactual predictions. The present contribution applies the constrained panel PPML estimator to reconsider the trade creation and trade diversion effects of regional trade agreements. Results show significant trade creation effects of RTAs ranging in between 8.7 and 21.7 percent in 2012, but also point to substantial trade diversion in the range of -14.4 and -5.8 percent. These counterfactual predictions account for adjustment in multilateral trade resistances. The quite large confidence intervals of counterfactual predictions seem to be an overlooked issue in the literature.
C13|LASSO-Type Penalization in the Framework of Generalized Additive Models for Location, Scale and Shape|For numerous applications it is of interest to provide full probabilistic forecasts, which are able to assign probabilities to each predicted outcome. Therefore, attention is shifting constantly from conditional mean models to probabilistic distributional models capturing location, scale, shape (and other aspects) of the response distribution. One of the most established models for distributional regression is the generalized additive model for location, scale and shape (GAMLSS). In high dimensional data set-ups classical fitting procedures for the GAMLSS often become rather unstable and methods for variable selection are desirable. Therefore, we propose a regularization approach for high dimensional data set-ups in the framework for GAMLSS. It is designed for linear covariate effects and is based on L1 -type penalties. The following three penalization options are provided: the conventional least absolute shrinkage and selection operator (LASSO) for metric covariates, and both group and fused LASSO for categorical predictors. The methods are investigated both for simulated data and for two real data examples, namely Munich rent data and data on extreme operational losses from the Italian bank UniCredit.
C13|State Space Models with Endogenous Regime Switching|This article studies the estimation of state space models whose parameters are switching endogenously between two regimes, depending on whether an autoregressive latent factor crosses some threshold level. Endogeneity stems from the sustained impacts of transition innovations on the latent factor, absent from which our model reduces to one with exogenous Markov switching. Due to the flexible form of state space representation, this class of models is vastly broad, including classical regression models and the popular dynamic stochastic general equilibrium (DSGE) models as special cases. We develop a computationally efficient filtering algorithm to estimate the nonlinear model. Calculations are greatly simplified by appropriate augmentation of the transition equation and exploiting the conditionally linear and Gaussian structure. The algorithm is shown to be accurate in approximating both the likelihood function and filtered state variables. We also apply the filter to estimate a small-scale DSGE model with threshold-type switching in monetary policy rule, and find apparent empirical evidence of endogeneity in the U.S. monetary policy shifts. Overall, our approach provides a greater scope for understanding the complex interaction between regime switching and measured economic behavior.
C13|Understanding Regressions with Observations Collected at High Frequency over Long Span|In this paper, we analyze regressions with observations collected at small time intervals over a long period of time. For the formal asymptotic analysis, we assume that samples are obtained from continuous time stochastic processes, and let the sampling interval δ shrink down to zero and the sample span T increase up to infinity. In this setup, we show that the standard Wald statistic diverges to infinity and the regression becomes spurious as long as δ → 0 sufficiently fast relative to T → ∞. Such a phenomenon is indeed what is frequently observed in practice for the type of regressions considered in the paper. In contrast, our asymptotic theory predicts that the spuriousness disappears if we use the robust version of the Wald test with an appropriate longrun variance estimate. This is supported, strongly and unambiguously, by our empirical illustration.
