C18|On binscatter|Binscatter is very popular in applied microeconomics. It provides a flexible, yet parsimonious way of visualizing and summarizing “big data” in regression settings, and it is often used for informal testing of substantive hypotheses such as linearity or monotonicity of the regression function. This paper presents a foundational, thorough analysis of binscatter: We give an array of theoretical and practical results that aid both in understanding current practices (that is, their validity or lack thereof) and in offering theory-based guidance for future applications. Our main results include principled number of bins selection, confidence intervals and bands, hypothesis tests for parametric and shape restrictions of the regression function, and several other new methods, applicable to canonical binscatter as well as higher-order polynomial, covariate-adjusted, and smoothness-restricted extensions thereof. In particular, we highlight important methodological problems related to covariate adjustment methods used in current practice. We also discuss extensions to clustered data. Our results are illustrated with simulated and real data throughout. Companion general-purpose software packages for Stata and R are provided. Finally, from a technical perspective, new theoretical results for partitioning-based series estimation are obtained that may be of independent interest.
C18|From Local to Global: External Validity in a Fertility Natural Experiment|We study issues related to external validity for treatment effects using over 100 replications of the Angrist and Evans (1998) natural experiment on the effects of sibling sex composition on fertility and labor supply. The replications are based on census data from around the world going back to 1960. We decompose sources of error in predicting treatment effects in external contexts in terms of macro and micro sources of variation. In our empirical setting, we find that macro covariates dominate over micro covariates for reducing errors in predicting treatments, an issue that past studies of external validity have been unable to evaluate. We develop methods for two applications to evidence-based decision-making, including determining where to locate an experiment and whether policy-makers should commission new experiments or rely on an existing evidence base for making a policy decision.
C18|Measuring credit-to-gdp gaps. The hodrick-prescott filter revisited|The credit-to-GDP gap computed under the methodology recommended by Basel Committee for Banking Supervision (BCBS) suffers of important limitations mainly regarding the great inertia of the estimated long-run trend, which does not allow capturing properly structural changes or sudden changes in the trend. As a result, the estimated gap currently yields large negative values which do not reflect properly the position in the financial cycle and the cyclical risk environment in many countries. Certainly, most countries that have activated the Countercyclical Capital Buffer (CCyB) in recent years appear not to be following the signals provided by this indicator. The main underlying reason for this might not be only related to the properties of statistical filtering methods, but to the particular adaptation made by the BCBS for the computation of the gap. In particular, the proposed one-sided Hodrick-Prescott filter (HP) only accounts for past observations and the value of the smoothing parameter assumes a much longer length of the credit cycle that those empirically evidenced in most countries, leading the trend to have very long memory. This study assesses whether relaxing this assumption improves the performance of the filter and would still allow this statistical method to be useful in providing accurate signals of cyclical systemic risk and thereby inform macroprudential policy decisions. Findings suggest that adaptations of the filter that assume a lower length of the credit cycle, more consistent with empirical evidence, help improve the early warning performance and correct the downward bias compared to the original gap proposed by the BCBS. This is not only evidenced in the case of Spain but also in several other EU countries. Finally, the results of the proposed adaptations of the HP filter are also found to perform fairly well when compared to other statistical filters and model-based indicators.
C18|Predicción de precios de vivienda: Aprendizaje estadístico con datos de oferta y transacciones para la ciudad de Montevideo|En este trabajo se presentan modelos predictivos para el precio de un activo de difícil valuación como la vivienda. Se utilizan dos fuentes de datos para la ciudad de Montevideo: una proveniente de sitios web (a través de web scraping) y otra de registros administrativos de transacciones. Se implementan tres modelos fácilmente replicables: modelo lineal, árbol de regresión y bosques aleatorios. Los resultados arrojan una mejor performance del modelo de bosques aleatorios respecto al modelo lineal hedónico, ampliamente difundido en la literatura. Se busca incorporar al análisis de predicción de precios una metodología aún escasamente difundida a nivel nacional, implementada en el software R y poner a disposición una nueva base de datos.
C18|Mitigating misleading implications for policy: Treatment of outliers in a difference-indifferences framework|Applications of the difference-in-differences estimator in economics, banking and finance, and management commonly treat outliers using the winsorize method. However, failure to winsorize outliers in both the treatment and controls groups introduces volatility in estimated coefficients, significance levels, and standard errors. A faulty process can lead to an exogenous event realising a significant effect that proper process would fail to detect. In demonstration, we randomly generate placebo interventions in bank-level data and discuss how to detect and limit the problem.
C18|A Performance Analysis of Some New Meta-Analysis Estimators Designed to Correct Publication Bias|Publication selection bias is widely recognized as a serious challenge to the validity of meta-analyses. This study analyses the performance of three new estimators designed to correct publication bias: the weighted average of the adequately powered (WAAP) estimator of Stanley et al. (2017), and two estimators proposed by Andrews & Kasy (2019), which we call AK1 and AK2. With respect to bias, we find that none of these is consistently superior to the commonly used PET-PEESE estimator. With respect to mean squared error, we find that Andrews & Kasey’s AK1 estimator does consistently better than other estimators except when publication bias is focused solely on the sign, as opposed to the significance, of an effect. With respect to coverage rates, we find that all the estimators perform consistently poorly, so that hypothesis tests about the mean true effect are unreliable. We also find that effect heterogeneity generally worsens estimator performance, and that its adverse impact compounds with greater heterogeneity. This is particularly of concern for meta-analyses in business and economics, where I2 values, a measure of heterogeneity, are often 90 percent or higher. Finally, we find that the type of simulation environment used in the Monte Carlo experiments significantly impacts estimator performance. A better understanding of what makes an “appropriate” simulation environment for analysing meta-analysis estimators would be a potentially productive subject for future research.
C18|Total, asymmetric and frequency connectedness between oil and forex markets|We analyze total, asymmetric and frequency connectedness between oil and forex markets using high-frequency, intra-day data over the period 2007 -- 2017. By employing variance decompositions and their spectral representation in combination with realized semivariances to account for asymmetric and frequency connectedness, we obtain interesting results. We show that divergence in monetary policy regimes affects forex volatility spillovers but that adding oil to a forex portfolio decreases the total connectedness of the mixed portfolio. Asymmetries in connectedness are relatively small. While negative shocks dominate forex volatility connectedness, positive shocks prevail when oil and forex markets are assessed jointly. Frequency connectedness is largely driven by uncertainty shocks and to a lesser extent by liquidity shocks, which impact long-term connectedness the most and lead to its dramatic increase during periods of distress.
C18|The Fair Reward Problem: The Illusion of Success and How to Solve It|Humanity has been fascinated by the pursuit of fortune since time immemorial, and many successful outcomes benefit from strokes of luck. But success is subject to complexity, uncertainty, and change – and at times becoming increasingly unequally distributed. This leads to tension and confusion over to what extent people actually get what they deserve (i.e., fairness/meritocracy). Moreover, in many fields, humans are over-confident and pervasively confuse luck for skill (I win, it’s skill; I lose, it’s bad luck). In some fields, there is too much risk-taking; in others, not enough. Where success derives in large part from luck – and especially where bailouts skew the incentives (heads, I win; tails, you lose) – it follows that luck is rewarded too much. This incentivizes a culture of gambling, while downplaying the importance of productive effort. And, short term success is often rewarded, irrespective, and potentially at the detriment, of the long-term system fitness. However, much success is truly meritocratic, and the problem is to discern and reward based on merit. We call this the fair reward problem. To address this, we propose three different measures to assess merit: (i) raw outcome; (ii) risk-adjusted outcome, and (iii) prospective. We emphasize the need, in many cases, for the deductive prospective approach, which considers the potential of a system to adapt and mutate in novel futures. This is formalized within an evolutionary system, comprised of five processes, inter alia handling the exploration-exploitation trade-off. Several human endeavors – including finance, politics, and science – are analyzed through these lenses, and concrete solutions are proposed to support a prosperous and meritocratic society.
C18|Dynamic Social Interactions and Health Risk Behavior|We study risky behavior of adolescents. Concentrating on smoking and alcohol use, we structurally estimate a dynamic social interaction model in the context of students' school networks included in the National Longitudinal Study of Adolescent Health (Add Health). The model allows for forward-looking behavior of agents, addiction effects, and social interactions in the form of preferences for conformity in the social network. We find strong evidence for forward looking dynamics and addiction effects. We also find that social interactions in the estimated dynamic model are quantitatively large. A misspecified static model would fit data substantially worse, while producing a much smaller estimate of the social interaction effect. With the estimated dynamic model, a temporary shock to students' preferences in the 10th grade has effects on their behavior in grades 10, 11, 12, with estimated social multipliers 1:53, 1:03, and 0:76, respectively. The multiplier effect of a permanent shock is much larger, up to 3:7 in grade 12. Moreover (semi-) elasticities of a permanent change in the availability of alcohol or cigarettes at home on child risky behavior implied by the dynamic equilibrium are 25%, 63%, and 79%, in grades 10, 11, 12, respectively.
C18|Measurement Errors in Index Trader Positions Data: Is the Price Pressure Hypothesis Still Invalid?|In this paper, we examine whether the repeated rejection of Masters' price pressure hypothesis is robust with respect to measurement errors in index trader position data. We allow for autocorrelated errors and a potential impact of index trader positions on the level and volatility of commodity returns. The resulting state-space model is estimated via particle MCMC. The empirical investigation relies on weekly data for eleven commodities contained in the SCoT reports. Our empirical findings show that the rejection of the price pressure hypothesis is robust concerning the inclusion of measurement errors in index trader positions data.
C18|The effect of observables, functional specifications, model features and shocks on identification in linearized DSGE models|Both the investment adjustment costs parameters in Kim (2003) and the monetary policy rule parameters in An & Schorfheide (2007) are locally not identifiable. We show means to dissolve this theoretical lack of identification by looking at (1) the set of observed variables, (2) functional specifications (level vs. growth costs, output-gap definition), (3) model features (capital utilization, partial inflation indexation), and (4) additional shocks (investment-specific technology, preference). Moreover, we discuss the effect of these changes on the strength of parameter identification from a Bayesian point of view. Our results indicate that researchers should treat parameter identification as a model property, i.e. from a model building perspective.
C18|On the construction of confidence intervals for ratios of expectations|In econometrics, many parameters of interest can be written as ratios of expectations. The main approach to construct confidence intervals for such parameters is the delta method. However, this asymptotic procedure yields intervals that may not be relevant for small sample sizes or, more generally, in a sequence-of-model framework that allows the expectation in the denominator to decrease to 0 with the sample size. In this setting, we prove a generalization of the delta method for ratios of expectations and the consistency of the nonparametric percentile bootstrap. We also investigate finite-sample inference and show a partial impossibility result: nonasymptotic uniform confidence intervals can be built for ratios of expectations but not at every level. Based on this, we propose an easy-to-compute index to appraise the reliability of the intervals based on the delta method. Simulations and an application illustrate our results and the practical usefulness of our rule of thumb.
C18|Investment Climate Effects on Alternative Firm-Level Productivity Measures|Developing countries are increasingly concerned about improving country competitiveness and productivity. Investment Climate surveys (ICs) at the firm level, are becoming the standard way for the World Bank to identify key obstacles to country competitiveness. This paper develops a general to specific econometric methodology, based on firm level observable fixed effects that generate robust investment climate effects (elasticities) on total factor productivity (TFP). By robust IC elasticities on TFP we mean elasticity estimates with equal signs and of similar magnitudes for several competing TFP measures. We apply this econometric methodology to the IC survey of Costa Rica showing how robust the investment climate effects are for several measures of TFP when conditioning on relevant plant-level information that is usually unobserved. For the economic evaluation we estimate the marginal effects of each IC variable on TFP as well as their IC impacts on average TFP obtaining important economic differences. These IC estimates are obtained from five blocks of IC variables, (i) infrastructure, (ii) red tape, corruption and crime, (iii) finance and corporate governance, (iv) quality, innovation and labor skills and (v) other control variables, could be used as benchmarks to assess cross-country IC assessments of TFP.
C18|Breaking Ties: Regression Discontinuity Design Meets Market Design|Centralized school assignment algorithms must distinguish between applicants with the same preferences and priorities. This is done with randomly assigned lottery numbers, non-lottery tie-breakers like test scores, or both. The New York City public high school match illustrates the latter, using test scores, grades, and interviews to rank applicants to screened schools, combined with lottery tie-breaking at unscreened schools. We show how to identify causal effects of school attendance in such settings. Our approach generalizes regression discontinuity designs to allow for multiple treatments and multiple running variables, some of which are randomly assigned. Lotteries generate assignment risk at screened as well as unscreened schools. Centralized assignment also identifies screened school effects away from screened school cutoffs. These features of centralized assignment are used to assess the predictive value of New York City’s school report cards. Grade A schools improve SAT math scores and increase the likelihood of graduating, though by less than OLS estimates suggest. Selection bias in OLS estimates is egregious for Grade A screened schools.
C18|Lies, damned lies, and RCT : une expérience de J-PAL sur le microcrédit rural au Maroc|"Comment expliquer le succès académique d’une étude randomisée dont la validité, tant interne qu’externe, est pourtant très problématique ? Prenant l’exemple d’une étude menée par le laboratoire J-PAL sur le microcrédit rural marocain, cet article mobilise les outils analytiques de la statistique, de l’économie politique et de la sociologie des sciences pour répondre à cette question. Il décrit l’ensemble de la chaîne de production de l’étude, depuis l’échantillonnage jusqu’à la publication et la dissémination des résultats, en passant par la collecte de données, la saisie et le recodage, les estimations et les interprétations. Il met en évidence une stratégie particulièrement offensive qui permet aux chercheurs de J-PAL de faire table rase du passé, y compris en s’affranchissant d’une « culture de la donnée », de refuser la critique et de contourner les règles de base de l’exercice scientifique tout au long du processus de recherche. Bien au-delà de J-PAL, nos analyses questionnent la supposée supériorité des méthodes randomisées tout en reflétant un malaise grandissant au sein du champ académique, qui parvient de moins en moins à faire respecter les règles de base de l’éthique et de la déontologie scientifique._______english_______How can we explain the academic success of a randomized study whose validity, both internal and external, is very problematic? Drawing on a study conducted on Moroccan rural microcredit by J-PAL, this article uses analytical tools from statistics, political economy and sociology of science to answer this question. It describes the entire study production chain, from sampling, data collection, data entry and recoding, estimates and interpretations to publication and dissemination of results. It highlights a particularly aggressive strategy carried out throughout the study process and in the field of research. This allows J-PAL researchers to put the past behind them, including by freeing themselves from a ""data culture"", rejecting criticism and bypassing the basic rules of scientific exercise throughout the research process. Well beyond J-PAL, our analyses question the supposed superiority of randomized methods while reflecting a growing unease within the academic field, which is less and less successful in enforcing the basic rules of ethics and scientific deontology."
C18|Makroökonomie: Blind Spot Gender: Erweiterung makroökonomischer Indikatoren durch eine Gender-Komponente am Beispiel der empirischen Phillips-Kurve|Dieser Beitrag möchte einen Impuls zur stärkeren Berücksichtigung von Genderaspekten in makroökonomischen Modellen geben. Am Beispiel der Philipps-Kurve geht es um die Frage, ob sich das Erwerbsverhalten von Frauen und Männern so stark voneinander unterscheidet, dass sich dies im Verlauf des Zusammenhangs von Inflation und Arbeitslosigkeit niederschlägt. Erste Hinweise dafür werden in deskriptiven Analysen für die Beobachtungszeiträume 1971 bis 1990 und 1991 bis 2017 gefunden. Die Studie bezieht sich auf die klassische Phillips-Kurve, die den empirischen Zusammenhang zwischen Inflation und Arbeitslosigkeit untersucht. Von einer Modellierung nach neukeynesianschem Vorbild wird zunächst abgesehen. Die Phillips-Kurve büßte in dieser Zeit erheblich an Erklärungskraft ein. Aus dem teilweise gegensätzlichen Verlauf der Philipps-Kurve unter Verwendung geschlechterspezifischer Erwerbslosenquoten wird abgeleitet, dass sich diese Entwicklung im Zuge der stark gestiegenen Erwerbsbeteiligung von Frauen noch beschleunigt hat. Die geschlechterspezifischen Unterschiede im Verlauf der Philipps-Kurve werden besonders deutlich unter Verwendung der von konjunkturellen Schwankungen weitgehend befreiten Erwerbslosenquote. Dies wird als Indiz für strukturelle Unterschiede im Erwerbsverhalten von Frauen und Männern gewertet. Das Ergebnis stärkt damit die Argumentation nach einer stärkeren Berücksichtigung von Genderaspekten in makroökonomischen Modellen. Weitere Forschungsarbeiten sind notwendig, um Aussagen über kausale Zusammenhänge treffen zu können.
C18|Diversity and its decomposition into variety, balance and disparity|Diversity is a central concept in many fields. Despite its importance, there is no unified methodological framework to measure diversity and its three components of variety, balance and disparity. Current approaches take into account disparity of the types by considering their pairwise similarities. Pairwise similarities between types do not adequately capture total disparity, since they fail to take into account in which way pairs are similar. Hence, pairwise similarities do not discriminate between similarity of types in terms of the same feature and similarity of types in terms of different features. This paper presents an alternative approach which is based similarities of features between types over the whole set. The proposed measure of diversity properly takes into account the aspects of variety, balance and disparity, and without having to set an arbitrary weight for each aspect of diversity. Based on this measure, the 'ABC decomposition' is introduced, which pro- vides separate measures for the variety, balance and disparity, allowing them to enter analysis separately. The method is illustrated by analyzing the industrial diversity from 1850 to present while taking into account the overlap in occupations they employ. Finally, the framework is extended to take into account disparity considering multiple features, providing a helpful tool in analysis of high-dimensional data.
C18|Size, Internationalization, and University Rankings: Evaluating and Predicting Times Higher Education (THE) Data for Japan|International and domestic rankings of academics, academic departments, faculties, schools and colleges, institutions of higher learning, states, regions, and countries are of academic and practical interest and importance to students, parents, academics, and private and public institutions. International and domestic rankings are typically based on arbitrary methodologies and criteria. Evaluating how the rankings might be sensitive to different factors, as well as forecasting how they might change over time, requires a statistical analysis of the factors that affect the rankings. Accurate data on rankings and the associated factors are essential for a valid statistical analysis. In this respect, the Times Higher Education (THE) World University Rankings represent one of the three leading and most influential annual sources of international university rankings. Using recently released data for a single country, namely Japan, the paper evaluates the effects of size (specifically, the number of full-time-equivalent (FTE) students, or FTE (Size)) and internationalization (specifically, the percentage of international students, or IntStud) on academic rankings using THE data for 2017 and 2018 on 258 national, public (that is, prefectural or city), and private universities. The results show that both size and internationalization are statistically significant in explaining rankings for all universities, as well as separately for private and non-private (that is, national and public) universities, in Japan for 2017 and 2018.
C18|On Binscatter|Binscatter is very popular in applied microeconomics. It provides a flexible, yet parsimonious way of visualizing and summarizing large data sets in regression settings, and it is often used for informal evaluation of substantive hypotheses such as linearity or monotonicity of the regression function. This paper presents a foundational, thorough analysis of binscatter: we give an array of theoretical and practical results that aid both in understanding current practices (i.e., their validity or lack thereof) and in offering theory-based guidance for future applications. Our main results include principled number of bins selection, confidence intervals and bands, hypothesis tests for parametric and shape restrictions of the regression function, and several other new methods, applicable to canonical binscatter as well as higher-order polynomial, covariate-adjusted and smoothness-restricted extensions thereof. In particular, we highlight important methodological problems related to covariate adjustment methods used in current practice. We also discuss extensions to clustered data. Our results are illustrated with simulated and real data throughout. Companion general-purpose software packages for \texttt{Stata} and \texttt{R} are provided. Finally, from a technical perspective, new theoretical results for partitioning-based series estimation are obtained that may be of independent interest.
C18|All that Glitters is not Gold. The Political Economy of Randomized Evaluations in Development|Randomized control trials (RCTs) have a narrow scope, restricted to basic intervention schemes. Experimental designs also display specific biases and political uses when implemented in the real world. Despite these limitations, the method has been advertised as the gold standard to evaluate development policies. This article adopts a political economy approach to explore this paradox. It argues that the success of RCTs is driven mainly by a new scientific business model based on a mix of simplicity and mathematical rigour, media and donor appeal, and academic and financial returns. This in turn meets current interests and preferences in the academic world and the donor community.
C18|Editorial: Enhancing Quantitative Exploratory Entrepreneurship Research|The purpose of this editorial is to discuss ways to enhance exploratory quantitative studies in entrepreneurship. We use examples from entrepreneurship research and other scientific fields to illustrate the advantages of graphical data display for both exploratory purposes and post hoc tests. We provide suggestions for authors, reviewers, and editors on ways to enhance the transparency, accuracy, and pedagogical presentation of quantitative data in papers with the explicit purpose of illuminating emerging and important entrepreneurship phenomena. Our hope is that we spark a conversation among entrepreneurship scholars about the state of our empirical work and the possibilities that lie ahead to enhance exploratory entrepreneurship research.
C18|2 Editorial: Enhancing Quantitative Theory-Testing Entrepreneurship Research|The purpose of this editorial is to discuss methodological advancements to enhance quantitative theory-testing entrepreneurship research. As the impact of entrepreneurship scholarship accelerates and deepens, our methods must keep pace to continue shaping theory, policy, and practice. Like our sister fields in business, entrepreneurship is coming to terms with the replication and credibility crisis in the social sciences, forcing the field to revisit commonly-held assumptions that limit the promise and prospect of our scholarship. Thus, we provide suggestions for reviewers and editors to identify concerns in empirical work, and to guide authors in improving their analyses and research designs. We hope that our editorial provides useful and actionable guidance for entrepreneurship researchers submitting theory-testing papers to Journal of Business Venturing.
C18|Breaking Ties: Regression Discontinuity Design Meets Market Design|Centralized school assignment algorithms must distinguish between applicants with the same preferences and priorities. This is done with randomly assigned lottery numbers, nonlottery tie-breakers like test scores, or both. The New York City public high school match illustrates the latter, using test scores, grades, and interviews to rank applicants to screened schools, combined with lottery tie-breaking at unscreened schools. We show how to identify causal effects of school attendance in such settings. Our approach generalizes regression discontinuity designs to allow for multiple treatments and multiple running variables, some of which are randomly assigned. Lotteries generate assignment risk at screened as well as unscreened schools. Centralized assignment also identifies screened school effects away from screened school cutoffs. These features of centralized assignment are used to assess the predictive value of New York City’s school report cards. Grade A schools improve SAT math scores and increase the likelihood of graduating, though by less than OLS estimates suggest. Selection bias in OLS estimates is egregious for Grade A screened schools.
C18|Population, light, and the size distribution of cities|We provide new insights on the city size distribution of countries around the world. Using geo-spatial data and a globally consistent city identification scheme, our data set contains 13,844 cities in 194 countries. City size is measured both in terms of population and night time lights proxying for local economic activity. We find that Zipf's law holds for many, but not all, countries in terms of population, while city size in terms of light is distributed more unequally. These deviations from Zipf's law are to a large extent driven by an undue concentration in the largest cities. They benefit from agglomeration effects which seem to work through scale rather than through density. Examining the cross-country heterogeneity in the city size distribution, our model selection approach suggests that historical factors play an important role, in line with the time of development hypothesis.
C18|Partial Identification of Economic Mobility: With an Application to the United States|The economic mobility of individuals and households is of fundamental interest. While many measures of economic mobility exist, reliance on transition matrices remains pervasive due to simplicity and ease of interpretation. However, estimation of transition matrices is complicated by the well-acknowledged problem of measurement error in self-reported and even administrative data. Existing methods of addressing measurement error are complex, rely on numerous strong assumptions, and often require data from more than two periods. In this paper, we investigate what can be learned about economic mobility as measured via transition matrices while formally accounting for measurement error in a reasonably trans- parent manner. To do so, we develop a nonparametric partial identification approach to bound transition probabilities under various assumptions on the measurement error and mobility processes. This approach is applied to panel data from the United States to explore short-run mobility before and after the Great Recession.
C18|Improving Estimation of Labor Market Disequilibrium Using Shortage Indicators, with an Application to the Market for Anesthesiologists|While economic studies often assume that labor markets are in equilibrium, there may be specialized labor markets that are likely in disequilibrium. We develop a new methodology to improve the estimation of a reduced form disequilibrium model from the existing models by incorporating survey-based shortage indicators into the model and estimation. Our shortage-indicator informed disequilibrium model includes as a special case the foundational model of Maddala and Nelson (1974). We demonstrate the gains in information provided by our methodology. We show how the model can be implemented by applying it to the market for anesthesiologists, a profession susceptible to disequilibrium. In this application, we find that our new disequilibrium model informed by a shortage indicator fits the data better than the Maddala-Nelson model, and has better out-of-sample predictive power.
C18|An Empirical Total Survey Error Decomposition Using Data Combination|Survey error is known to be pervasive and to bias even simple, but important estimates of means, rates, and totals, such as poverty statistics and the unemployment rate. To summarize and analyze the extent, sources, and consequences of survey error, we define empirical counterparts of key components of the Total Survey Error Framework that can be estimated using data combination. Specifically, we estimate total survey error and decompose it into three high level sources of error: representation error, item non-response error and measurement error. We further decompose these sources into lower level sources such as a failure to report a positive amount and errors in amounts conditional on reporting a positive value. For error in dollars paid by two large government transfer programs, we use administrative records on the universe of program payments in New York State linked to three major household surveys to estimate the error components we define. We find that total survey error is large and varies in its size and composition, but measurement error is always by far the largest source of error. Our application shows that data combination makes it possible to routinely measure total survey error and its components. The results allow survey producers to assess error reduction strategies and survey users to mitigate the consequences of survey errors or gauge the reliability of their conclusions.
C18|Combining Administrative and Survey Data to Improve Income Measurement|We describe methods of combining administrative and survey data to improve the measurement of income. We begin by decomposing the total survey error in the mean of survey reports of dollars received from a government transfer program. We decompose this error into three parts, generalized coverage error (which combines coverage and unit non-response error and any error from weighting), item non-response or imputation error, and measurement error. We then discuss these three sources of error in turn and how linked administrative and survey data can assess and reduce each of these sources. We then illustrate the potential of linked data by showing how using linked administrative variables improves the measurement of income and poverty in the Current Population Survey, focusing on the substitution of administrative for survey data for three government transfer programs. Finally, we discuss how one can examine the accuracy of the underlying links used in the combined data.
C18|What Do We Really Know about the Employment Effects of the UK's National Minimum Wage?|A substantial body of research on the UK's National Minimum Wage (NMW) has concluded that the the NMW has not had a detrimental effect on employment. This research has directly influenced, through the Low Pay Commission, the conduct of policy, including the subsequent introduction of the National Living Wage (NLW). We revisit this literature and offer a reassessment, motivated by two concerns. First, much of this literature employs difference-in-difference designs, even though there are significant challenges in conducting appropriate inference in such designs, and they can have very low power when inference is conducted appropriately. Second, the literature has focused on the binary outcome of statistical rejection of the null hypothesis, without attention to the range of (positive or negative) impacts on employment that are consistent with the data. In our re-analysis of the data, we conduct inference using recent suggestions for best practice and consider what magnitude of employment effects the data can and cannot rule out. We find that the data are consistent with both large negative and small positive impacts of the UK National Minimum Wage on employment. We conclude that the existing data, combined with difference-in-difference designs, in fact offered very little guidance to policy makers.
C18|Measuring Economic Mobility in India Using Noisy Data: A Partial Identification Approach|We examine economic mobility in India while rigorously accounting for measurement error. Such an analysis is imperative to fully understand the welfare effects of the rise in inequality that has occurred in India over the past few decades. To proceed, we extend recently developed methods on the partial identification of transition matrices and apply this methodology to newly available panel data on household consumption. We find overall mobility has been markedly low: at least 7 out of 10 poor households remain poor or at-risk of being poor between 2005 and 2012. We also find Muslims, lower caste groups, and rural households are in a more disadvantageous position in terms of escaping poverty or transitioning into poverty compared to Hindus, upper caste groups, and urban households. These findings suggest inequality in India is likely to be chronic and also challenges the conventional wisdom that marginalized households are catching up on average.
C18|Missing Data in Imputed Highest Grade Completed in the 2015 -2018 NBER CPS Extracts|"In 2015, the Current Population Survey (CPS) eliminated three questions related to educational attainment. These questions are used by the NBER to calculate the variable, ""Imputed Highest Grade Completed"" (ihigrdc) in their Monthly Outgoing Rotation Group (MORG) extracts. Imputed Highest Grade Completed provides a convenient measure of years of education and is based on the credential oriented CPS variable that is coded from 31 to 46. Because the NBER continues to use coding that relies on these eliminated questions to calculate ihigrdc, this variable has a missing value for 27.5% of the observations in the 2015Ã¢â‚¬â€œ2018 extracts. These missing values, in turn, lead to an average Imputed Highest Grade Completed of about 1.2 fewer years after 2014 than would result from using the whole CPS. We informed the NBER of this issue in January 2019; however, because this variable with missing values remains on their website for download as of this writing (July 15, 2019), we are posting this working paper to inform users of these data so that they may address the issue appropriately in their own work."
C18|Accounting for Skewed or One-Sided Measurement Error in the Dependent Variable|"While classical measurement error in the dependent variable in a linear regression framework results only in a loss of precision, non-classical measurement error can lead to estimates which are biased and inference which lacks power. Here, we consider a particular type of non-classical measurement error: skewed errors. Unfortunately, skewed measurement error is likely to be a relatively common feature of many outcomes of interest in political science research. This study highlights the bias that can result even from relatively ""small"" amounts of skewed measurement error, particularly if the measurement error is heteroskedastic. We also assess potential solutions to this problem, focusing on the stochastic frontier model and nonlinear least squares. Simulations and three replications highlight the importance of thinking carefully about skewed measurement error, as well as appropriate solutions."
C18|Doubly Robust-type Estimation of Population Moments and Parameters in Biased Sampling|"We propose an estimation method of population moments or population parameters in ""biased sampling data"" in which for some units of data, not only the variable of interest but also the covariates, have missing observations and the proportion of ""missingness"" is unknown. We use auxiliary information such as the distribution of covariates or their moments in random sampling data in order to correct the bias. Moreover, with additional assumptions, we can correct the bias even if we have only the moment information of covariates. The main contribution of this paper is the development of a doubly robust-type estimator for biased sampling data. This method provides a consistent estimator if either the regression function or the assignment mechanism is correctly specified. We prove the consistency and semi-parametric efficiency of the doubly robust estimator. Both the simulation and empirical application results demonstrate that the proposed estimation method is more robust than existing methods."
C18|Emergence of a urban traffic macroscopic fundamental diagram|This paper examines mild conditions under which a macroscopic fundamental diagram (MFD) emerges, relating space-averaged speed to occupancy in some area. These conditions are validated against empirical data. We allow local speed-occupancy relationships and , in particular, require no equilibrating process to be in operation. This means that merely observing the stable relationship between the space-averages of speed, flow and occupancy are not sufficient to infer a robust relationship and the emerging MFD cannot be guaranteed to be stable if traffic interventions are implemented.
C18|Recentered Influence Functions in Stata: Methods for Analyzing the Determinants of Poverty and Inequality|Recentered influence functions (RIFs) are statistical tools popularized by Firpo, Fortin, and Lemieux (2009) for analyzing unconditional partial effects on quantiles in a regression analysis framework (unconditional quantile regressions). The flexibility and simplicity of these tools has opened the possibility of extending the analysis to other distributional statistics using linear regressions or decomposition approaches. In this paper, I introduce three Stata commands to facilitate the use of RIFs in the analysis of outcome distributions: rifvar() is an egen extension used to create RIFs for a large set of distributional statistics; rifhdreg facilitates the estimation of RIF regressions, enabling the use of high-dimensional fixed effects; and oaxaca_rif to implement Oaxaca-Blinder type decomposition analysis (RIF decompositions).
C18|A Simulation Study for Monotonic Dependence in the Presence of Outliers|This paper aims at examining the performance of a recently proposed measure of dependence – the Monotonic Dependence Coefficient – with respect to classical correlation measures like the Pearson’s product-moment and the Spearman’s rank-order correlation coefficients, using simulated outlier contaminated and non-contaminated datasets as well as a real dataset. The comparison aims at checking how and when these coefficients detect dependence relationships between two variables when outliers are present. Several scenarios are created, contemplating in particular multiple values for the coefficients, multiple outlier contamination percentages, various simulation data patterns, or a combination of these. The basic simulation dataset is generated from a bivariate standard normal distribution. Then, the contaminated data are generated from exponential, power-transformed and lognormal distributions. The main findings tend to favour the Spearman’s rank-order correlation coefficient for most of the scenarios, especially when the contamination is taken into account, whereas MDC performs better than the Spearman’s rank-order correlation coefficient in non-contaminated data.
C18|Optimal Bias Correction of the Log-periodogram Estimator of the Fractional Parameter: A Jackknife Approach|We use the jackknife to bias correct the log-periodogram regression (LPR) estimator of the fractional parameter in a stationary fractionally integrated model. The weights for the jackknife estimator are chosen in such a way that bias reduction is achieved without the usual increase in asymptotic variance, with the estimator viewed as `optimal' in this sense. The theoretical results are valid under both the non-overlapping and moving-block sub-sampling schemes that can be used in the jackknife technique, and do not require the assumption of Gaussianity for the data generating process. A Monte Carlo study explores the Ã–nite sample performance of diÂ§erent versions of the optimal jackknife estimator under a variety of fractional data generating processes. The simulations reveal that when the weights are constructed using the true parameter values, a version of the optimal jackknife estimator almost always out-performs alternative bias-corrected estimators. A feasible version of the jackknife estimator, in which the weights are constructed using consistent estimators of the unknown parameters, whilst not dominant overall, is still the least biased estimator in some cases.
C18|Meta-Analysis for Medical Decisions|Statisticians have proposed meta-analysis to combine the findings of multiple studies of health risks or treatment response. The standard practice is to compute a weighted-average of the estimates. Yet it is not clear how to interpret a weighted average of estimates reported in disparate studies. Meta-analyses often answer this question through the lens of a random-effects model, which interprets a weighted average of estimates as an estimate of a mean parameter across a hypothetical population of studies. The relevance to medical decision making is obscure. Decision-centered research should aim to inform risk assessment and treatment for populations of patients, not populations of studies. This paper lays out principles for decision-centered meta-analysis. One first specifies a prediction of interest and next examines what each available study credibly reveals. Such analysis typically yields a set-valued prediction rather than a point prediction. Thus, one uses each study to conclude that a probability of disease, or mean treatment response, lies within a range of possibilities. Finally, one combines the available studies by computing the intersection of the set-valued predictions that they yield. To demonstrate decision-centered meta-analysis, the paper considers assessment of the effect of anti-hypertensive drugs on blood pressure.
C18|Should We Trust Clustered Standard Errors? A Comparison with Randomization-Based Methods|We compare the precision of critical values obtained under conventional sampling-based methods with those obtained using sample order statics computed through draws from a randomized counterfactual based on the null hypothesis. When based on a small number of draws (200), critical values in the extreme left and right tail (0.005 and 0.995) contain a small bias toward failing to reject the null hypothesis which quickly dissipates with additional draws. The precision of randomization-based critical values compares favorably with conventional sampling-based critical values when the number of draws is approximately 7 times the sample size for a basic OLS model using homoskedastic data, but considerably less in models based on clustered standard errors, or the classic Differences-in-Differences. Randomization-based methods dramatically outperform conventional methods for treatment effects in Differences-in-Differences specifications with unbalanced panels and a small number of treated groups.
C18|Measurement Error Mechanisms Matter: Agricultural Intensification with Farmer Misperceptions and Misreporting|The mechanism(s) that generate measurement error matter to inference. Survey measurement error is typically thought to represent simple misreporting correctable through improved measurement. But errors might also or alternatively reflect respondent misperceptions that materially affect the respondent decisions under study. We show analytically that these alternate data generating processes imply different appropriate regression specifications and have distinct effects on the bias in parameter estimates. We introduce a simple empirical technique to generate unbiased estimates under more general conditions and to apportion measurement error between misreporting and misperceptions in measurement error when one has both self-reported and objectively-measured observations of the same explanatory variable. We then apply these techniques to the longstanding question of agricultural intensification: do farmers increase input application rates per unit area as the size of the plots they cultivate decreases? Using nationally representative data from four sub-Saharan African countries, we find strong evidence that measurement error in plot size reflects a mixture of farmer misreporting and misperceptions. The results matter to inference around the intensification hypothesis and call into question whether more objective, precise measures are always preferable when estimating behavioral parameters.
C18|Dynamic Social Interactions and Health Risk Behavior|We study risky behavior of adolescents. Concentrating on smoking and alcohol use, we structurally estimate a dynamic social interaction model in the context of students' school networks included in the National Longitudinal Study of Adolescent Health (Add Health). The model allows for forward-looking behavior of agents, addiction effects, and social interactions in the form of preferences for conformity in the social network. We find strong evidence for forward looking dynamics and addiction effects. We also find that social interactions in the estimated dynamic model are quantitatively large. A misspecified static model would fit data substantially worse, while producing a much smaller estimate of the social interaction effect. With the estimated dynamic model, a temporary shock to students' preferences in the 10th grade has effects on their behavior in grades 10, 11, 12, with estimated social multipliers 1.53, 1.03, and 0.76, respectively. The multiplier effect of a permanent shock is much larger, up to 3.7 in grade 12. Moreover, (semi-) elasticities of a permanent change in the availability of alcohol or cigarettes at home on child risky behavior implied by the dynamic equilibrium are 25%, 63%, and 79%, in grades 10, 11, 12, respectively.
C18|A Bayesian approach for correcting bias of data envelopment analysis estimators|The validity of data envelopment analysis (DEA) efficiency estimators depends on the robustness of the production frontier to measurement errors, specification errors and the dimension of the input-output space. It has been proven that DEA estimators, within the interval (0, 1], are overestimated when finite samples are used while asymptotically this bias reduces to zero. The non-parametric literature dealing with bias correction of efficiencies solely refers to estimators that do not exceed one. We prove that efficiency estimators, both lower and higher than one, are biased. A Bayesian DEA method is developed to correct bias of efficiency estimators. This is a two-stage procedure of super-efficiency DEA followed by a Bayesian approach relying on consistent efficiency estimators. This method is applicable to ‘small’ and ‘medium’ samples. The new Bayesian DEA method is applied to two data sets of 50 and 100 E.U. banks. The mean square error, root mean square error and mean absolute error of the new method reduce as the sample size increases.
C18|Microeconometric Dynamic Panel Data Methods: Model Specification and Selection Issues|A motivated strategy is presented to find step by step an adequate model specification and a matching set of instrumental variables by applying the programming tools provided by the Stata package Xtabond2. The aim is to implement generalized method of moment techniques such that useful and reasonably accurate inferences are extracted from an observational panel data set on a single microeconometric structural presumably dynamic behavioral relationship. In the suggested specification search three comprehensive heavily interconnected goals are pursued, namely: (i) to include all the relevant appropriately transformed possibly lagged regressors, as well as any interactions between these if it is required to relax the otherwise very strict homogeneity restrictions on the dynamic impacts of the explanatories in standard linear panel data models; (ii) to correctly classify all regressors as either endogenous, predetermined or exogenous, as well as being either effect-stationary or effect-nonstationary, implying which internal variables could represent valid and relatively strong instruments; (iii) to enhance the accuracy of inference in finite samples by omitting irrelevant regressors and by profitably reducing the space spanned by the full set of available internal instruments. For the various tests which trigger the decisions to be made in the sequential selection process the relevant considerations are spelled out to interpret the magnitude of p-values. Also the complexities to establish and interpret the ultimately established dynamic impacts are explained. Finally the developed strategy is applied to a classic data set and is shown to yield new insights.
C18|Die Dimensionsanalyse und ihre Anwendung in der Ökonomik<BR>[Dimensional analysis and its application in economics]|The task of dimensional analysis is to check the formal correctness of formulas in a branch of empirical science and to explore the effects of a change in units of measurement on law-like formulas. Scale invariance is only a special case to ensure the neutrality of changes of units of measurement of independent variables on the variable to be explained. In general, mathematical functions and their applications in a discipline like economics are not scale-invariant. This alone should be a sufficient reason to deal more intensively with the question of the dependence of law-like formulations on the units of measurement of the variables integrated into them. There can be no dimensionless variables in an empirical scientific discipline. Even if there are numerous examples of variables that do not carry units of measurement, i.e. that are seemingly pure numbers, these variables do mean something special in observable reality. They capture a relationship which is important for the discipline and which is more than just a number. The dimension of an empirical quantity is anchored in a context that defines it. As a rule, these are the definitions of the system of national accounts, which show up to eight different dimensions even for simple aggregates such as GDP, investment or exports. Most of these dimensions are measured and reported in prices; they have the same unit of measurement, although they are different and are in need of a different treatment in an economic theory.
C18|TF-MIDAS: a new mixed-frequency model to forecast macroeconomic variables|This paper tackles the mixed-frequency modeling problem from a new perspective. Instead of drawing upon the common distributed lag polynomial model, we use a transfer function representation to develop a new type of models, named TF-MIDAS. We derive the theoretical TF-MIDAS implied by the high-frequency VARMA family models and as a function of the aggregation scheme (flow and stock). This exact correspondence leads to potential gains in terms of nowcasting and forecasting performance against the current alternatives. A Monte Carlo simulation exercise confirms that TF-MIDAS beats UMIDAS models in terms of out-of-sample nowcasting performance for several data generating high-frequency processes.
C18|Growth and Inflation Regimes in Greater Tumen Initiative Area|This paper tests for multiple structural breaks in the mean, seasonality, dynamics and conditional volatility of Greater Tumen Initiative Countries’ (GTI) growth and inflation, while also accounting for outliers. It finds a drop in the level of Chinese growth rate in the third quarter of 2011 and of inflation rate in 1998. There are more volatility regimes than the growth regimes and most GTI countries are currently enjoying historically low volatility of their growth and inflation. Two exceptions are the increased growth volatility for Japan since 2006 and inflation volatility for Russia since 2012. There is an increased importance of seasonality in GTI and especially in Chinese inflation volatility, constituting at least a half of the total volatility.
C18|Resolutions to flip-over credit risk and beyond|Abstract Given a risk outcome y over a rating system {R_i }_(i=1)^k for a portfolio, we show in this paper that the maximum likelihood estimates with monotonic constraints, when y is binary (the Bernoulli likelihood) or takes values in the interval 0≤y≤1 (the quasi-Bernoulli likelihood), are each given by the average of the observed outcomes for some consecutive rating indexes. These estimates are in average equal to the sample average risk over the portfolio and coincide with the estimates by least squares with the same monotonic constraints. These results are the exact solution of the corresponding constrained optimization. A non-parametric algorithm for the exact solution is proposed. For the least squares estimates, this algorithm is compared with “pool adjacent violators” algorithm for isotonic regression. The proposed approaches provide a resolution to flip-over credit risk and a tool to determine the fair risk scales over a rating system.
C18|Monotonic Estimation for the Survival Probability over a Risk-Rated Portfolio by Discrete-Time Hazard Rate Models|Monotonic estimation for the survival probability of a loan in a risk-rated portfolio is based on the observation arising, for example, from loan pricing that a loan with a lower credit risk rating is more likely to survive than a loan with a higher credit risk rating, given the same additional risk covariates. Two probit-type discrete-time hazard rate models that generate monotonic survival probabilities are proposed in this paper. The first model calculates the discrete-time hazard rate conditional on systematic risk factors. As for the Cox proportion hazard rate model, the model formulates the discrete-time hazard rate by including a baseline component. This baseline component can be estimated outside the model in the absence of model covariates using the long-run average discrete-time hazard rate. This results in a significant reduction in the number of parameters to be otherwise estimated inside the model. The second model is a general form model where loan level factors can be included. Parameter estimation algorithms are also proposed. The models and algorithms proposed in this paper can be used for loan pricing, stress testing, expected credit loss estimation, and modeling of the probability of default term structure.
C18|Monotonic Estimation for Probability Distribution and Multivariate Risk Scales by Constrained Minimum Generalized Cross-Entropy|Minimum cross-entropy estimation is an extension to the maximum likelihood estimation for multinomial probabilities. Given a probability distribution {r_i }_(i=1)^k, we show in this paper that the monotonic estimates {p_i }_(i=1)^k for the probability distribution by minimum cross-entropy are each given by the simple average of the given distribution values over some consecutive indexes. Results extend to the monotonic estimation for multivariate outcomes by generalized cross-entropy. These estimates are the exact solution for the corresponding constrained optimization and coincide with the monotonic estimates by least squares. A non-parametric algorithm for the exact solution is proposed. The algorithm is compared to the “pool adjacent violators” algorithm in least squares case for the isotonic regression problem. Applications to monotonic estimation of migration matrices and risk scales for multivariate outcomes are discussed.
C18|IFRS9 Expected Credit Loss Estimation: Advanced Models for Estimating Portfolio Loss and Weighting Scenario Losses|Estimation of portfolio expected credit loss is required for IFRS9 regulatory purposes. It starts with the estimation of scenario loss at loan level, and then aggregated and summed up by scenario probability weights to obtain portfolio expected loss. This estimated loss can vary significantly, depending on the levels of loss severity generated by the IFSR9 models, and the probability weights chosen. There is a need for a quantitative approach for determining the weights for scenario losses. In this paper, we propose a model to estimate the expected portfolio losses brought by recession risk, and a quantitative approach for determining the scenario weights. The model and approach are validated by an empirical example, where we stress portfolio expected loss by recession risk, and calculate the scenario weights accordingly.
C18|The calculation of Solvency Capital Requirement using Copulas|Our aim is to present an alternative methodology to the standard formula imposed to the insurance regulation (the European directive knows as Solvency II) for the calculus of the capital requirements. We want to demonstrate how this formula is now obsolete and how is possible to obtain lower capital requirement through the theory of the copulas, function that are gaining increasing importance in various economic areas. A lower capital requirement involves the advantage for the various insurance companies not to have unproductive capital that can therefore be used for the production of further profits. Indeed the standard formula is adequate only with some particular assumptions, otherwise it can overestimate the capital requirements that are actually needed as the standard formula underestimates the effect of diversification.
C18|Does Agricultural investment still promote economic growth in China? Empirical evidence from ARDL bounds testing model|One of the major unresolved research issues in agriculture is the question as to whether agricultural investments still a promoter of economic growth at the regional and local levels. The concern is not with the agricultural benefits, principally measured as food security, but whether there are additional development benefits from these investments. In this paper, we have developed a new approach to study the impact of agricultural investment on economic growth. By taking the case of China, this study is based on the Auto-Regressive Distributive Lags (ARDL) approach that is proposed by Pesaran et al (2002). The empirical estimate yields interesting results. In the short and long terms, agricultural investment has a positive effect on economic growth. The findings of this research have important implications for further policy designs that seek to maintain the agricultural sector in China in the future.
C18|New Essentials of Economic Theory|This paper develops economic theory tools and framework free from general equilibrium assumptions. We describe macroeconomics as system of economic agents under action risks. Economic and financial variables of agents, their expectations and transactions between agents define macroeconomic variables. Agents variables depend on transactions between agents and transactions are performed under agents expectations. Agents expectations are formed by economic variables, transactions, expectations of other agents, other factors that impact macroeconomic evolution. We use risk ratings of agents as their coordinates on economic space and approximate description of economic and financial variables, transactions and expectations of numerous separate agents by description of variables, transactions and expectations of aggregated agents as density functions on economic space. Motion of separate agents on economic space due to change of agents risk rating induce economic flows of variables, transactions and expectations and we describe their impact on economic evolution. We apply our model equations to description of business cycles, model wave propagation for disturbances of economic variables and transactions, model asset price fluctuations and argue hidden complexities of classical Black-Scholes-Merton option pricing.
C18|The State of DSGE Modelling|This survey and assessment of the state of DSGE modelling is structured around six key criticisms levelled at the approach. The first is fundamental and common to macroeconomics and microeconomics alike - namely, problems with rationality and expected utility maximization. The second is that DSGE models examine fluctuations about an exogenous balanced growth path and there is no role for endogenous growth, either medium or long-term. The third consists of a number of concerns associated with systems estimation. The fourth is another fundamental problem with any microfounded macro-model - that of heterogeneity and aggregation. The fifth and sixth concerns focus on the rudimentary nature of earlier models that lacked unemployment and a banking sector.
C18|A New Index Score for the Assessment of Firm Financial Risks|There are several indicators and univariate ratios that measure the soundness of firms' balance sheets (Leverage, profitability, liquidity ratio, etc.). However, each indicator alone cannot measure the overall financial risk or the financial distress level of firms. In this study, we measure the financial strength of the real sector firms quoted in Borsa Istanbul (BIST) by producing a composite index score which is a combination of several different corporate finance ratios. In the first part, we will apply multiple discriminant analysis to the variables used in Altman Z-score (1968), which is the most prevalent composite index score measuring the firms’ financial risks in the literature. In the second part, a new index, named as MFA-score (Multivariate Firm Assessment Score) will be introduced by using the ratios that best explain the characteristics of the BIST companies. Both the tailored version of Altman Z-score and our new index score have a predictive power around 90 percent. Furthermore, MFA-score is capable of detecting the impact of macro-economic developments on firm balance sheets, which enables us to use MFA-score as an early warning indicator of financial distress for Turkish firms. Our analyses with MFA-score suggest that non-exporter firms and firms with FX open position have relatively weaker balance sheets.
C18|Un héritage des Annales, la cliométrie à Strasbourg|C’est par sa volonté de combiner la rigueur des modèles théoriques et mathématiques avec la prise en compte, de la façon la plus exhaustive possible, de la complexité de toutes les données (qualitatives et quantitatives) que l’Ecole cliométrique strasbourgeoise reste fidèle à l’esprit des Annales et prolonge le mouvement initié en 1929 par Marc Bloch et Lucien Febvre.
C18|Implications of Quantal Response Statistical Equilibrium|This paper explores the foundations and properties of the quantal response statistical equilibrium (QRSE) model developed by Scharfenaker and Foley (2017). The QRSE model provides a behavioral foundation for the formation of aggregate economic outcomes in social systems characterized by negative feedbacks. It can approximate a wide range of commonly encountered theoretical distributions that have been identified as economic statistical equilibrium and displays qualitatively similar behavior to the Subbotin and Asymmetric Subbotin distributions that range from the Laplace to the Normal distribution in the limit. Asymmetry in the frequency distributions of economic outcomes arises from the unfulfilled expectations of entropy- constrained decision makers. This paper demonstrates the logic of the QRSE model in an application to US stock market data dating back to 1926. The model provides a parsimonious explanation for the distribution of rates of return on private equities as well as clear behavioral foundation for asset price fluctuations.
C18|Publication Bias under Aggregation Frictions: Theory, Evidence, and a New Correction Method|This paper questions the conventional wisdom that publication bias must result from the biased preferences of researchers. When readers only compare the number of positive and negative results of papers to make their decisions, even unbiased researchers will omit noisy null results and inflate some marginally insignificant estimates. Nonetheless, the equilibrium with such publication bias is socially optimal. The model predicts that published non-positive results are either precise null results or noisy but extreme negative results. This paper shows this prediction holds with some data, and proposes a new stem-based bias correction method that is robust to this and other publication selection processes.
C18|Real GDP: The Flawed Metric at the Heart of Macroeconomics|The study of economic growth is central to macroeconomics. More than anything else, macroecon-omists are concerned with finding policies that encourage growth. And by ‘growth’, they mean the growth of real GDP. This measure has become so central to macroeconomics that few economists question its validity. Our intention here is to do just that. We argue that real GDP is a deeply flawed metric. It is presented as an objective measure of economic scale. But when we look under the surface, we find crippling subjectivity. Moreover, few economists seem to realize that real GDP is based on a non-existent quantum – utility. In light of these problems, it seems to us that much of macroeconomics needs to be rethought.
C18|A quantum framework for economic science: New directions|The current paper explores the cutting-edge applications of quantum field theory and quantum information theory modelling in different areas of economic science, namely, in the behavioural modelling of agents under market uncertainty, and mathematical modelling of asset or option prices and firm theory. The paper then provides a brief discussion into a possible extension of the extant literature of quantum-like modelling based on scattering theory and statistical field theory. A statistical theory of firm based on Feynman path integral technique is also proposed very recently. The collage of new initiatives as described in the current paper will hopefully ignite still newer ideas.
C18|Takeaways from the special issue on The Practice of Replication|In July 2017, Economics: The Open Access, Open Assessment E-Journal issued a call for papers for a special issue on 'The Practice of Replication.' In that call, the journal explained that there was no generally accepted procedure for how to do a replication. Likewise, there was no generally accepted standard for determining whether a replication 'confirms or disconfirms' an original study. Accordingly, the journal called for papers to identify principles for how to do a replication and how to interpret its results; and to apply those principles in crafting a replication plan for a study of the author's choosing. The hope was that this exercise would produce some progress on 'the practice of replication.' The special issue is now complete with a total of eight journal articles. This commentary places the respective articles within a common framework and identifies observations and lessons learned from the respective studies.
C18|Reliable real-time output gap estimates based on a modified Hamilton filter|The authors contribute to the debate regarding the reliability of output gap estimates. As an alternative to the Hodrick-Prescott (HP) filter, they propose a simple modification of the filter proposed by Hamilton in 2018 that shares its favorable real-time properties, but leads to a more even coverage of typical business cycle frequencies. Based on output growth and inflation forecasts and a comparison to revised output gap estimates from policy institutions, they find that real-time output gaps based on the modified Hamilton filter are economically much more meaningful measures of the business cycle than those based on other simple statistical trend-cycle decomposition techniques such as the HP or the Bandpass filter.
C18|Instrument Validity Tests with Causal Trees: With an Application to the Same-sex Instrument|The use of instrumental variables (IVs) to identify causal effects is widespread in empirical economics, but it is fundamentally impossible to proof their validity. However, assumptions sufficient for the identification of local average treatment effects (LATEs) jointly generate necessary conditions in the observed data that allow to refute an IV's validity. Suitable tests exist, but they may not be able to detect even severe violations of IV validity in practice. In this paper, we employ recently developed machine learning tools as data-driven improvements for these tests. Specifically, we use the causal tree (CT) algorithm from Athey and Imbens (2016) to directly search the covariate space for violations of the LATE assumptions. The new approach is applied to the sibling sex composition instrument in census data from China and the United States. We expect that, because of son preferences, the siblings sex instrument is invalid in the Chinese context. However, existing IV validity tests are unable to detect violations, while our CT based procedure does.
C18|Minimum Wage and the Labor Market: What Can We Learn from the French Experience?|Since it was introduced in 1950, and even more since it was reformed in 1970, the statutory minimum wage has been playing a key role in the French labor market. It has very specific fixing mechanisms, and, from the eighties, it has been one of the highest among the OECD countries – both in relative and absolute terms. After presenting the specific features of the minimum wage setting regime in France as well as the minimum wage policies implemented since the 1950s, we provide a comprehensive survey of existing empirical evidence on the impacts of the minimum wage on the French labor market. We use a meta-analysis to draw the lessons from the empirical studies on its effects on employment. We also survey the other potential effects, such as the impact on wage bargaining and other wages, on inequalities, on profit and prices, on working conditions.
C18|Issues in the Estimation of Mis-Specified Models of Fractionally Integrated Processes|In this paper we quantify the impact of model mis-specification on the properties of parameter estimators applied to fractionally integrated processes. We demonstrate the asymptotic equivalence of four alternative parametric methods: frequency domain maximum likelihood, Whittle estimation, time domain maximum likelihood and conditional sum of squares. We show that all four estimators converge to the same pseudo-true value and provide an analytical representation of their (common) asymptotic distribution. As well as providing theoretical insights, we explore the finite sample properties of the alternative estimators when used to fit mis-specified models. In particular we demonstrate that when the difference between the true and pseudo-true values of the long memory parameter is sufficiently large, a clear distinction between the frequency domain and time domain estimators can be observed - in terms of the accuracy with which the finite sample distributions replicate the common asymptotic distribution - with the time domain estimators exhibiting a closer match overall. Simulation experiments also demonstrate that the two time-domain estimators have the smallest bias and mean squared error as estimators of the pseudo-true value of the long memory parameter, with conditional sum of squares being the most accurate estimator overall and having a relative efficiency that is approximately double that of frequency domain maximum likelihood, across a range of mis-specification designs.
C18|On endogeneity and shape invariance in extended partially linear single index models|In this paper, the important (but so far unrevealed) usefulness of the extended generalized partially linear single-index (EGPLSI) model introduced by Xia et al. (1999) in its ability to model a flexible shape-invariant specification is elaborated. More importantly, a control function approach is proposed to address the potential endogeneity problems in the EGPLSI model in order to enhance its applicability to empirical studies. In the process, it is shown that the attractive asymptotic features of the single-index type of a semiparametric model are still valid in our proposed estimation procedure given intrinsic generated covariates. Our newly developed method is then applied to address the endogeneity of expenditure in the semiparametric analysis of a system of empirical Engel curves by using the British data, highlights the convenient applicability of our proposed method.
C18|Panel Data Binary Response Model In A Triangular System|We propose a new control function (CF) method for binary response outcomes in a triangular system with unobserved heterogeneity of multiple dimensions. The identified CFs are the expected values of the heterogeneity terms in the reduced form equations conditional on the endogenous, Xi ≡ (xi1, . . . ,xiT ), and the exogenous, Zi ≡ (zi1, . . . , ziT ), variables. The method requires weaker restrictions compared to traditional CF methods for triangular systems with imposed structures similar to ours, and point-identifies average partial effects with discrete instruments. We discuss semiparametric identification of structural measures using the proposed CFs. An application and Monte Carlo experiments compare several alternative methods with ours.
C18|Bartik Instruments: What, When, Why, and How|The Bartik instrument is formed by interacting local industry shares and national industry growth rates. We show that the typical use of a Bartik instrument assumes a pooled exposure research design, where the shares measure differential exposure to common shocks, and identification is based on exogeneity of the shares. Next, we show how the Bartik instrument weights each of the exposure designs. Finally, we discuss how to assess the plausibility of the research design. We illustrate our results through three applications: estimating the elasticity of labor supply, estimating local labor market effects of Chinese imports, and estimating the elasticity of substitution between immigrants and natives.
C18|Generic machine learning inference on heterogenous treatment effects in randomized experiments| We propose strategies to estimate and make inference on key features of heterogeneous effects in randomized experiments. These key features include best linear predictors of the effects using machine learning proxies, average effects sorted by impact groups, and average characteristics of most and least impacted units. The approach is valid in high dimensional settings, where the effects are proxied by machine learning methods. We post-process these proxies into the estimates of the key features. Our approach is generic, it can be used in conjunction with penalized methods, deep and shallow neural networks, canonical and new random forests, boosted trees, and ensemble methods. Our approach is agnostic and does not make unrealistic or hard-to-check assumptions; we don’t require conditions for consistency of the ML methods. Estimation and inference relies on repeated data splitting to avoid overfitting and achieve validity. For inference, we take medians of p-values and medians of confidence intervals, resulting from many different data splits, and then adjust their nominal level to guarantee uniform validity. This variational inference method is shown to be uniformly valid and quantifies the uncertainty coming from both parameter estimation and data splitting. The inference method could be of substantial independent interest in many machine learning applications. An empirical application to the impact of micro-credit on economic development illustrates the use of the approach in randomized experiments. An additional application to the impact of the gender discrimination on wages illustrates the potential use of the approach in observational studies, where machine learning methods can be used to condition flexibly on very high-dimensional controls.
C18|Revealed Growth: A Method for Bounding the Elasticity of Demand with an Application to Assessing the Antitrust Remedy in the Du Pont Decision|We propose a method for bounding the demand elasticity in growing, homogeneous-product markets that requires only minimal data—market price and quantity over a time span as short as two periods. Reminiscent of revealed-preference arguments using choices over time to bound the shape of indifference curves, we use shifts in the equilibrium over time to bound the shape of the demand curve under the assumption that growing demand curves do not cross. We apply the method to assess the effectiveness of the antitrust remedy in the 1952 Du Pont decision, ordering the incumbent manufacturers to license their patents for commercial plastics. Commentators have suggested that the incumbents may have preserved the monopoly outcome by gaming the licensing contracts. The upper bounds on demand elasticities that we compute are significantly less than 1 in many post-remedy years. Such inelastic demand is inconsistent with monopoly, suggesting the remedy may have been effective.
C18|The Promise and Pitfalls of Differences-in-Differences: Reflections on ‘16 and Pregnant’ and Other Applications|We use the exchange between Kearney/Levine and Jaeger/Joyce/Kaestner on “16 and Pregnant” to reexamine the use of DiD as a response to the failure of nature to properly design an experiment for us. We argue that 1) any DiD paper should address why the original levels of the experimental and control groups differed, and why this would not impact trends, 2) the parallel trends argument requires a justification of the chosen functional form and that the use of the interaction coefficients in probit and logit may be justified in some cases, and 3) parallel trends in the period prior to treatment is suggestive of counterfactual parallel trends, but parallel pre-trends is neither necessary nor sufficient for the parallel counterfactual trends condition to hold. Importantly, the purely statistical approach uses pretesting and thus generates the wrong standard errors. Moreover, we underline the dangers of implicitly or explicitly accepting the null hypothesis when failing to reject the absence of a differential pre-trend.
C18|Quasi-Experimental Shift-Share Research Designs|"Many empirical studies leverage shift-share (or ""Bartik"") instruments that average a set of observed shocks with shock exposure weights. We derive a necessary and sufficient shock-level orthogonality condition for these instruments to identify causal effects. We then show that orthogonality holds when observed shocks are as-good-as-randomly assigned and growing in number, with the average shock exposure sufficiently dispersed. Quasi-experimental shift-share designs may be implemented with new shock-level procedures, which help visualize the identifying variation, correct standard errors, choose appropriate specifications, test identifying assumptions, and optimally combine multiple sets of quasi-random shocks. We illustrate these ideas by revisiting Autor et al. (2013)'s analysis of the labor market effects of Chinese import competition"
C18|Behavior within a Clinical Trial and Implications for Mammography Guidelines|I unite the economics and medical literatures by examining behavior within a clinical trial to inform treatment guidelines. I use data from the Canadian National Breast Screening Study, an influential clinical trial on mammography. During the active study period of the trial, a substantial fraction of women in the control group received mammograms, and some women in the intervention group did not. Using this mammography behavior, random assignment within the trial, and a standard model from the economics literature, I divide participants into three groups that differ in how likely they are to receive mammograms. Making comparisons across these groups, I find two important relationships. First, I find heterogeneous selection into mammography: women more likely to receive mammograms are healthier. I find this relationship using a marginal treatment effect model that assumes no more than the local average treatment effect assumptions. Second, I find treatment effect heterogeneity along the margin of selection into mammography: women more likely to receive mammograms are more likely to experience harm from them. I find this relationship using an ancillary assumption that builds on the first empirical relationship. I find additional empirical support for the ancillary assumption using baseline covariates. My findings contribute to the literature concerned about harms from mammography by demonstrating variation across the margin of selection into mammography. This variation is problematic for current mammography guidelines for women in their 40s because it implies that they unintentionally encourage mammography for healthier women who are more likely to experience harm from them.
C18|Sniff Tests in Economics: Aggregate Distribution of Their Probability Values and Implications for Publication Bias|"The increasing demand for rigor in empirical economics has led to the growing use of auxiliary tests (balance, specification, over-identification, placebo, etc.) supporting the credibility of a paper's main results. We dub these ""sniff tests"" because standards for passing are subjective and rejection is bad news for the author. Sniff tests offer a new window into publication bias since authors prefer them to be insignificant, the reverse of standard statistical tests. Collecting a sample of nearly 30,000 sniff tests across 60 economics journals, we provide the first estimate of their aggregate probability-value (p-value) distribution. For the subsample of balance tests in randomized controlled trials (for which the distribution of p-values is known to be uniform absent publication bias, allowing reduced-form methods to be employed) estimates suggest that 45% of failed tests remain in the ""file drawer"" rather than being published. For the remaining sample with an unknown distribution of p-values, structural estimates suggest an even larger file-drawer problem, as high as 91%. Fewer significant sniff tests show up in top-tier journals, smaller tables, and more recent articles. We find no evidence of author manipulation other than a tendency to overly attribute significant sniff tests to bad luck."
C18|On the Informativeness of Descriptive Statistics for Structural Estimates|Researchers often present treatment-control differences or other descriptive statistics alongside structural estimates that answer policy or counterfactual questions of interest. We ask to what extent confidence in the researcher's interpretation of the former should increase a reader's confidence in the latter. We consider a structural estimate ĉ that may depend on a vector of descriptive statistics ̂γ. We define a class of misspecified models in a neighborhood of the assumed model. We then compare the bounds on the bias of ĉ due to misspecification across all models in this class with the bounds across the subset of these models in which misspecification does not affect ̂γ. Our main result shows that the ratio of the lengths of these tight bounds depends only on a quantity we call the informativeness of ̂γ for ĉ, which can be easily estimated even for complex models. We recommend that researchers report the estimated informativeness of descriptive statistics. We illustrate with applications to three recent papers.
C18|Behavioral Feedback: Do Individual Choices Influence Scientific Results?|In many health domains, we are concerned that observed links - for example, between “healthy” behaviors and good outcomes - are driven by selection into behavior. This paper considers the additional factor that these selection patterns may vary over time. When a particular health behavior becomes more recommended, the take-up of the behavior may be larger among people with other positive health behaviors. Such changes in selection would make it even more difficult to learn about causal effects. I formalize this change in selection in a simple model. I test for evidence of these patterns in the context of diet and vitamin supplementation. Using both microdata and evidence from published results I show that selection varies over time with recommendations about behavior and that estimates of the relationship between health outcomes and health behaviors vary over time in the same way. I show that adjustment for selection on observables is insufficient to address the bias. I suggest a possible robustness approach relying on assumptions about proportional selection of observed and unobserved variables.
C18|Arbitrage Or Narrow Bracketing? On Using Money to Measure Intertemporal Preferences|If experimental subjects arbitrage against market interest rates when making intertemporal allocations of cash, the data will reveal nothing about subjects' discount rates, only uncovering subjects' market interest rates. If they frame choices narrowly, market rates will not be salient and the experiment will uncover subjects' utility discount rates. We test arbitrage directly by forcing all transactions with subjects to be instant electronic bank transfers, thus making arbitrage easy and salient. We also employ four decision frames to test alternative hypotheses. Our evidence contradicts arbitrage, supports money as a valid reward, and suggests framing as a correlate with present bias.
C18|Treatment Effects with Multiple Outcomes|This paper proposes strategies for defining, identifying, and estimating features of treatment-effect distributions in contexts where multiple outcomes are of interest. After describing existing empirical approaches used in such settings, the paper develops a notion of treatment preference that is shown to be a feature of standard treatment-effect analysis in the single-outcome case. Focusing largely on binary outcomes, treatment-preference probability treatment effects (PTEs) are defined and are seen to correspond to familiar average treatment effects in the single-outcome case. The paper suggests seven possible characterizations of treatment preference appropriate to multiple-outcome contexts. Under standard assumptions about unconfoundedness of treatment assignment, the PTEs are shown to be point identified for three of the seven characterizations and set identified for the other four. Probability bounds are derived and empirical approaches to estimating the bounds—or the PTEs themselves in the point-identified cases—are suggested. These empirical approaches are straightforward, involving in most instances little more than estimation of binary-outcome probability models of what are commonly known as composite outcomes. The results are illustrated with simulated data and in analyses of two microdata samples. Finally, the main results are extended to situations where the component outcomes are ordered or categorical.
C18|Equilibrium-Disequilibrium Dynamics of the US Housing Market, 2000-2015: A Quantal Response Statistical Equilibrium Approach|In this article, we demonstrate that a quantal response statistical equilibrium approach to the US housing market with the help of maximum entropy method of modeling is a powerful way of revealing di erent characteristics of the housing market behavior before, during and after the recent housing market crash in the US. In this line, a maximum entropy approach to quantal response statistical equilibrium model (QRSE), introduced by Scharfenaker and Foley (2017), is employed in order to model housing market dynamics in di erent phases of the most recent housing market cycle using the S&P Case Shiller housing price index for 20 largest- Metropolitan Regions, and Freddie Mac housing price index (FMHPI) for 367 Metropolitan Cities for the US between 2000 and 2015. Estimated model parameters provide an alternative way to understand and explain the behaviors of economic agents, and market dynamics by questioning the traditional economic theory, which takes assumption for the behavior of rational utility maximizing representative agent with self-fulfilled expectations as given.
C18|Classifying Occupations According to Their Skill Requirements in Job Advertisements|In this work, we propose a methodology for classifying occupations based on skill requirements provided in online job adverts. To develop the classification methodology, we apply semi-supervised machine learning techniques to a dataset of 37 million UK online job adverts collected by Burning Glass Technologies. The resulting occupational classification comprises four hierarchical layers: the first three layers relate to skill specialisation and group jobs that require similar types of skills. The fourth layer of the hierarchy is based on the offered salary and indicates skill level. The proposed classification will have the potential to enable measurement of an individual's career progression within the same skill domain, to recommend jobs to individuals based on their skills and to mitigate occupational misclassification issues. While we provide initial results and descriptions of occupational groups in the Burning Glass data, we believe that the main contribution of this work is the methodology for grouping jobs into occupations based on skills.
C18|Classifying occupations using web-based job advertisements: an application to STEM and creative occupations|Rapid technological, social and economic change is having significant impacts on the nature of jobs. In fast-changing environments it is crucial that policymakers have a clear and timely picture of the labour market. Policymakers use standardised occupational classifications, such as the Office for National Statisticsâ€™ Standard Occupational Classification (SOC) in the UK to analyse the labour market. These permit the occupational composition of the workforce to be tracked on a consistent and transparent basis over time and across industrial sectors. However, such systems are by their nature costly to maintain, slow to adapt and not very flexible. For that reason, additional tools are needed. At the same time, policymakers over the world are revisiting how active skills development policies can be used to equip workers with the capabilities needed to meet the new labour market realities. There is in parallel a desire for more granular understandings of what skills combinations are required of occupations, in part so that policymakers are better sighted on how individuals can redeploy these skills as and when employer demands change further. In this paper, we investigate the possibility of complementing traditional occupational classifications with more flexible methods centred around employersâ€™ characterisations of the skills and knowledge requirements of occupations as presented in job advertisements. We use data science methods to classify job advertisements as STEM or non-STEM (Science, Technology, Engineering and Mathematics) and creative or non-creative, based on the content of ads in a database of UK job ads posted online belonging to Boston-based job market analytics company, Burning Glass Technologies. In doing so, we first characterise each SOC code in terms of its skill make-up; this step allows us to describe each SOC skillset as a mathematical object that can be compared with other skillsets. Then we develop a classifier that predicts the SOC code of a job based on its required skills. Finally, we develop two classifiers that decide whether a job vacancy is STEM/non-STEM and creative/non-creative, based again on its skill requirements.
C18|An Open and Data-driven Taxonomy of Skills Extracted from Online Job Adverts|In this work we offer an open and data-driven skills taxonomy, which is independent of ESCO and O*NET, two popular available taxonomies that are expert-derived. Since the taxonomy is created in an algorithmic way without expert elicitation, it can be quickly updated to reflect changes in labour demand and provide timely insights to support labour market decision-making. Our proposed taxonomy also captures links between skills, aggregated job titles, and the salaries mentioned in the millions of UK job adverts used in this analysis. To generate the taxonomy, we employ machine learning methods, such as word embeddings, network community detection algorithms and consensus clustering. We model skills as a graph with individual skills as vertices and their co-occurrences in job adverts as edges. The strength of the relationships between the skills is measured using both the frequency of actual co-occurrences of skills in the same advert as well as their shared context, based on a trained word embeddings model. Once skills are represented as a network, we hierarchically group them into clusters. To ensure the stability of the resulting clusters, we introduce bootstrapping and consensus clustering stages into the methodology. While we share initial results and describe the skill clusters, the main purpose of this paper is to outline the methodology for building the taxonomy.
C18|Systematic Systemic Stress Tests|For a given set of banks, which economic and financial scenarios will lead to big losses? How big can losses in such scenarios possibly get? These are the two central questions of macro stress tests. We believe that most current macro stress testing models have deficits in answering these questions. They select stress scenarios in a way which might leave aside many dangerous scenarios and thus create an illusion of safety; and which might consider highly implausible scenarios and thus trigger a false alarm. With respect to loss evaluation most stress tests do not include tools to analyse systemic risk arising from the interactions of banks with each other and with the markets. We make a conceptual proposal how these shortcomings may be addressed and how stress tests could be made both systematic and systemic. We demonstrate the application of our concepts using publicly available data on European banks and capital markets, in particular the EBA 2016 stress test results.
C18|Social Investment and youth labour market participation: a EU regional analysis|"In this paper, we first rely on small area techniques to derive from EU-SILC survey new indicators of compensatory and investment policies at regional level. While compensatory policies have mainly the goal of protecting individuals from “old” risks (e.g. old-age), investment-related social policies tend to focus more on ""new social risks"" (i.e. skill deficits). We rely on these new indicators to perform a data-driven SVAR analysis to investigate the casual relationships between youth labour market outcomes and these two types of spending. Our results support the view that investment policies are more effective for tackling new social challenges."
C18|Assessment of national waste generation in EU Member States’ efficiency|Waste generation and management may be considered as either a by-product of economic actions or even used as input to economic activity like energy recovery. Every country produces different amounts of municipal solid waste (MSW) and with different composition. This paper deals with the efficiency of 28 EU Member States for the years 2008, 2010 and 2012 by employing Data Envelopment Analysis (DEA) and by using eight parameters, namely waste generation, employment rate, capital formation, GDP, population density and for the first time SOx, NOx and GHG emissions for the relevant countries. With these parameters six environmental production frameworks have been designed each with different inputs and outputs. The empirical analysis shows that overall the more efficient countries according to all frameworks include Belgium, Germany, Austria, the Netherlands, Sweden and Norway. These results were then reviewed against the recycling rate of each country for the examined time periods. The recycling rate actually depicts the DEA results, namely more efficient countries seem to have a higher recycling rate too. Moreover the DEA efficiency results were contrasted to the overall treatment options used in the countries under consideration. Overall it is noticed that countries employing all four treatment options with high use of more sustainable ones and decrease in the use of landfill are the ones that also proved to be efficient according to DEA.
C18|Theoretical and Methodological Context of (Post)-Modern Econometrics and Competing Philosophical Discourses for Policy Prescription|"This research article was championed as a way of providing discourses pertaining to the concept of ""Critical Realism (CR)"" approach, which is amongst many othe forms of competing postmodern philosophical concepts for the engagement of dialogical discourses in the area of established econonetric methodologies for effective policy prescription in the economic science discipline. On the the whole, there is no doubt surrounding the value of empirical endeavours in econometrics to address real world economic problems, but equally so, the heavy weighted use and reliance on mathematical contents as a way of justifying its scientific base seemed to be loosing traction of the intended focus of economics when it comes to confronting real world problems in the domain of social interaction. In this vein, the construction of mixed methods discourse(s), which favour that of CR philosophy is hereby suggested in this article as a way forward in confronting with issues raised by critics of mainstream economics and other professionals in the postmodern era."
C18|Firm performance after high growth: A comparison of absolute and relative growth measures|Do high-growth firms continue to create jobs after the high-growth period or is high-growth a one-time event? Does the answer to this question depend on the definition of high growth? This paper analyzes data from Amadeus on Bulgarian firms for three consecutive 3-year periods (2001-2004, 2004-2007, and 2007-2010). Previously, high growth has been defined in terms of relative growth or composite measures such as recommended by Eurostat-OECD. We additionally apply an absolute measure of growth, i. e. the actual change in headcount. Using a two-part model with separate equations for sur-vival and growth, we moreover specifically account for the impact of firm exits on aggregate effects. We find that definitions are central for outcomes. In terms of relative and Eurostat-OECD high growth our results for Bulgarian firms largely confirm what has been found for high-income countries: surviv-ing relative high-growth firms are characterized by negative future growth rates. High growth firms defined according to Eurostat-OECD continue to grow positively after high growth. If growth is meas-ured in absolute terms, then high growth firms only continue to create more jobs than non-high growth firms as far as surviving firms are concerned. Taking firm exits into account, absolute high-growth firms are outperformed by average firms due to the job losses of large exiting high-growth firms – with one notable exception: absolute high-growth firms of initially small size (10-49 employees) continue to grow faster than other firms even if exits are accounted for and indeed seem a worthwhile target for policies promoting high-growth entrepreneurship.
C18|Health and economic growth in Vista countries: An ARDL bounds test approach|The present study examined the relationship between health and economic growth in the VISTA countries (Vietnam, Indonesia, South Africa, Turkey and Argentina). The study employed time series data covering the period between 1990 and 2016. Labor and capital were incorporated in the model to form a multivariate framework. The ARDL bounds test approach was used to determine the presence of the long run relationship among the variables. The findings posited that there is long run relationship between economic growth, health, capital and labour in all the countries except for Argentina. There were mix results in terms of the long run and short estimates. It was established that in Vietnam, Indonesia and South Africa, there is evidence of a long run positive and significant relationship between economic growth and health while in Turkey a negative relationship was established. Therefore, the findings of the study have different implications for the different countries.
C18|A review of more than one hundred Pareto-tail index estimators|This paper reviews more than one hundred Pareto (and equivalent) tail index estimators. It focuses on univariate estimators for nontruncated data. We discuss basic ideas of these estimators and provide their analytical expressions. As samples from heavy-tailed distributions are analysed by researchers from various fields of science, the paper provides nontechnical explanations of the methods, which could be understood by researchers with intermediate skills in statistics. We also discuss strengths and weaknesses of the estimators, if they are known. The paper can be viewed as a catalog or a reference book on Pareto-tail index estimators.
C18|Count and duration time series with equal conditional stochastic and mean orders|We consider a positive-valued time series whose conditional distribution has a time-varying mean, which may depend on exogenous variables. The main applications concern count or duration data. Under a contraction condition on the mean function, it is shown that stationarity and ergodicity hold when the mean and stochastic orders of the conditional distribution are the same. The latter condition holds for the exponential family parametrized by the mean, but also for many other distributions. We also provide conditions for the existence of marginal moments and for the geometric decay of the beta-mixing coefficients. Simulation experiments and illustrations on series of stock market volumes and of greenhouse gas concentrations show that the multiplicative-error form of usual duration models deserves to be relaxed, as allowed in the present paper.
C18|Future developments in cyber risk assessment for the internet of things|This article is focused on the economic impact assessment of Internet of Things (IoT) and its associated cyber risks vectors and vertices – a reinterpretation of IoT verticals. We adapt to IoT both the Cyber Value at Risk model, a well-established model for measuring the maximum possible loss over a given time period, and the MicroMort model, a widely used model for predicting uncertainty through units of mortality risk. The resulting new IoT MicroMort for calculating IoT risk is tested and validated with real data from the BullGuard's IoT Scanner (over 310,000 scans) and the Garner report on IoT connected devices. Two calculations are developed, the current state of IoT cyber risk and the future forecasts of IoT cyber risk. Our work therefore advances the efforts of integrating cyber risk impact assessments and offer a better understanding of economic impact assessment for IoT cyber risk.
C18|Inference with difference-in-differences with a small number of groups: a review, simulation study and empirical application using SHARE data|Background - Difference-in-differences (DID) estimation has become increasingly popular as an approach to evaluate the effect of a group-level policy on individual-level outcomes. Several statistical methodologies have been proposed to correct for the within-group correlation of model errors resulting from the clustering of data. Little is known about how well these corrections perform with the often small number of groups observed in health research using longitudinal data. Methods - First, we review the most commonly used modelling solutions in DID estimation for panel data, including generalized estimating equations (GEE), permutation tests, clustered standard errors (CSE), wild cluster bootstrapping, and aggregation. Second, we compare the empirical coverage rates and power of these methods using a Monte Carlo simulation study in scenarios in which we vary the degree of error correlation, the group size balance, and the proportion of treated groups. Third, we provide an empirical example using the Survey of Health, Ageing and Retirement in Europe (SHARE). Results - When the number of groups is small, CSE are systematically biased downwards in scenarios when data are unbalanced or when there is a low proportion of treated groups. This can result in over-rejection of the null even when data are composed of up to 50 groups. Aggregation, permutation tests, bias-adjusted GEE and wild cluster bootstrap produce coverage rates close to the nominal rate for almost all scenarios, though GEE may suffer from low power. Conclusions - In DID estimation with a small number of groups, analysis using aggregation, permutation tests, wild cluster bootstrap, or bias-adjusted GEE is recommended.
C18|Econometric Perspectives on Economic Measurement|It turns out that price index functions share a basic interpretation; practically all of them measure a change in some average of quality-adjusted prices. The different options are distinguished by their choice of average, their definition of quality, and their stance on what I label 'equal interest'. This new perspective updates the so-called stochastic approach to choosing index functions. It also offers new avenues to understand and tackle measurement problems. I discuss three examples.
C18|Diagnostic Tests for Homoskedasticity in Spatial Cross-Sectional or Panel Models|We propose tests for homoskedasticity in spatial econometric models, based on joint or concentrated score functions and an Outer-Product-of-Martingale-Difference (OPMD) estimate of the variance of the joint or concentrated score functions. Versions of these tests robust against non-normality are also given. Asymptotic properties of the proposed tests are formally examined using a cross-section model and a panel model with fixed effects. Monte Carlo results show that the proposed tests based on the concentrated score function have good finite sample properties. Finally, the generality of the proposed approach in constructing tests for homoskedasticity is further demonstrated using a spatial dynamic panel data model with short panels.
C18|Pc Complex: Pc Algorithm For Complex Survey Data|PC algorithm is one of the most known procedures for Bayesian networks structural learning. The structure is inferred carrying out several independence tests on a database and building a Bayesian network in agreement with the tests results. The PC algorithm is based on the assumption of independent and identically distributed observations. In practice, sample selection in surveys involves more complex sampling designs, then the standard test procedure is not valid even asymptotically. In order to avoid misleading results about the true causal structure the sample selection process must be taken into account in the structural learning process. In this paper, a modi ed version of the PC algorithm is proposed for inferring casual structure from complex survey data. It is based on resampling techniques for nite population. A simulation experiment showing the robustness with respect to departures from the assumptions and the good performance of the proposed algorithm is carried out.
C18|Familiarization Activities ? Achieving a Good Understanding of the CEFR Levels and Descriptors|The Common European Framework of Reference for Languages: Learning, teaching, assessment (CEFR) integrates concepts related to target language learning/teaching and assessment from a number of different theoretical studies and focuses on meaningful target language use in real-life situations. The six-level framework with positively worded ?can do? statements refers to performances validated for particular proficiency levels. The CEFR includes many descriptor scales to encourage language users to develop differentiated profiles. The main function of descriptors is to help align language curricula, teaching and assessment, selecting illustrative descriptors according to educators? relevance to the particular context. To relate local language curricula, teaching and assessment to the CEFR, a group of local experts need to be aware of reference levels and illustrative samples to start validating their claims. A good understanding of the CEFR levels and descriptors can be achieved through familiarization activities. Their effectiveness will be discussed and analysed in the presentation and paper.
C18|The Effects Of Autocorrelation And Number Of Repeated Measures On Glmm Robustness With Ordinal Data|Longitudinal studies involving ordinal responses are widely conducted in many fields of the education, health and social sciences. In these cases, when units are observed over time, the possibility of auto-correlation between observations on the same subject exists. Therefore the assumption of independence which underlines the generalized linear models is violated. Generalized linear mixed models (GLMMs) accommodate repeated measures data for which the usual assumption of independent observations is untenable, and also accommodate a non-normally distributed dependent variable (i.e. multinomial distribution for ordinal data). Thus, GLMMs constitute a good technique for modelling correlated data and ordinal responses. In this study, for a split-plot design with two groups for the between-subjects factor and five response categories, we investigated empirical Type I error rates in GLMMs. To this end, we used a computer program developed by Wicklin to generate longitudinal ordinal data with SAS/IML. We manipulated the total sample size, the coefficient of variation of the group size, the number of repeated measures, and the values of autocorrelation coefficient. For each combination 5,000 replications were performed at a significance level of .05. The GLIMMIX procedure in SAS was used to fit the mixed-effects models for ordinal responses with multinomial distribution and the Kenward-Roger degrees of freedom adjustment for small samples. The results of simulations showed that the test is robust for group effect under all conditions analysed. For time and interaction effects, however, the robustness depends on the number of repeated measures and autocorrelations values. The test tends to be liberal with high autocorrelation, different values of autocorrelation in each group and large number of repeated measures. To sum up, GLMMs are a good analytical option for correlated ordinal outcomes with few repeated measures, low autocorrelation, and the same autocorrelation between groups.This research was supported by grant PSI2016-78737-P (AEI/FEDER, UE) from the Spanish Ministry of Economy, Industry and Competitiveness.
C18|Which Are The Most Common Distributions In Social, Health, And Education Sciences?|Statistical analysis is crucial for research and the choice of analytical technique should take into account the specific distribution of data. Although the data obtained from health, educational and social sciences research are often not normally distributed, there are very few studies detailing which distributions are most likely to represent data in these disciplines. The aim of the present study was to determine the frequency of appearance of the most common non-normal distributions in the health, educational and social sciences by means of a systematic review. The search was carried out in the Web of Science (WOS) database, from which we retrieved 984 abstracts of papers published between 2010 and 2015. In the final review, 148 papers from the area of health, 18 from education and 96 from the social sciences were included. The selection was performed independently by two reviewers. The inter-rater reliability for article selection and agreement regarding the type of distribution was high. The results showed that distributions from the exponential family are the most common non-normal distributions ? and more specifically, gamma as a continuous distribution and the negative binomial as a discrete distribution. In addition to identifying the most common distributions for real data these results will help researchers to decide which distributions should be used in simulation studies examining statistical procedures.This research was supported by grant PSI2016-78737-P (AEI/FEDER, UE) from the Spanish Ministry of Economy, Industry and Competitiveness.
C18|Reframing less conventional speech to disrupt conventions of ?compulsory fluency?: A conversation analysis approach|Our purpose is to illuminate compliances with, and resistances to, what we are calling ?compulsory fluency? which we define as conventions for what constitutes competent speech. We achieve our purpose through a study of day-to-day communication between a woman with less conventional speech and her support providing family members and friends. Drawing from McRuer?s (2006) compulsory ablebodiedness and Kafer?s (2013) compulsory able-mindedness, we use ?compulsory fluency? to refer to a form of articulation that is standardized and idealized and imposed on all speakers including those whose speech is less conventional. We see compulsory fluency as central to North American conceptions of personhood which are tied to individual ability to speak for one?s self (Brueggemann, 2005). In this paper, we trace some North American principles for linguistic competence to outline widely held ideals of receptive and expressive language use, namely, conventions for how language should be understood and expressed. Using critical disability studies (Goodley, 2013; McRuer, 2006) together with a feminist framework of relational autonomy (Nedelsky, 1989), our goal is to focus on experiences of people with less conventional speech and draw attention to power in communication as it flows in idiosyncratic and intersubjective fashion (Mackenzie & Stoljar, 2000; Westlund, 2009). In other words, we use a critical disability and feminist framing to call attention to less conventional forms of communication competence and, in this process, we challenge assumptions about what constitutes competent speech. As part of a larger qualitative study, we conduct a conversation analysis informed by Rapley and Antaki (1996) to examine day-to-day verbal, vocal and non-verbal communications of a young woman who self identifies as ?having autism? - pseudonym Addison - in interaction with her support-providing family members and friends. We illustrate a multitude of Addison?s compliances with, and resistances to, compulsory fluency to bring awareness to competence inherent in less conventional speech and we argue this illumination as a call for listening with greater care and more open expectations in efforts to understand, and participate in the expression of, meanings embedded in less conventional speech.
C18|Inference under a new exponential-exponential loss capturing specified penalties for over- and under-estimation|Asymmetric loss functions have gained enormous importance over the years, with particular relevance to situations where over- and under-estimation of the parameter of interest are considered not of equal consequence. In particular, the linear-exponential (LINEX) loss has been studied and used quite extensively in classical and Bayesian inference. While LINEX loss nicely captures whether over- or under-estimation has a more serious impact, it falls short of incorporating any prior knowledge about the relative penalty for over- vis-à-vis that for under-estimation. Thus, if such prior knowledge is available as happens in many practical situations, notably in finance, medicine and reliability theory, among others, then there is a pressing need for devising a loss function that accounts for this information and hence is more realistic than the LINEX loss. More specifically, suppose the ground realities in a given situation demand that over-estimation needs to be penalized k times the penalty of under-estimation, where k is known. Clearly, over-estimation gets more penalized than under-estimation if k > 1 and it is the other way round if k
C18|Adaptive estimation using records data under asymmetric loss, with applications|We consider a scenario where data are accessible in terms of record values, as can happen in a wide range of practical situations. Examples include the hottest day ever, the lowest stock market figure, auction prices of an item in bidding, etc. Such data can be analyzed as record values from a sequence of observations, an upper or lower record value being one that is larger or smaller, respectively, than all previous observations. The literature on classical theory of records and its several variants is quite rich. A significant literature also exists in reliability theory and associated areas. Not much work has, however, been done so far using records data when over and under estimation of the parameter of interest attract unequal penalties, even though there is a compelling need for considering such an asymmetric loss function whenever the consequences of over and under estimation are not identical. This can happen in such diverse fields of application as real estate management, accounting, reliability analysis, and so on.From the above perspective, we consider the estimation problem based on records data for the scale parameter of an exponential family of distributions under an asymmetric linear-exponential loss function. With a view to controlling the associated risk, we also aim at ensuring a pre-assigned upper bound on it. In the absence of a known and fixed sample size solution to this problem, we consider an adaptive sampling methodology ? for example, a one at a time purely sequential sampling rule. We suggest various estimators of the scale parameter and compare their performances to address the admissibility and other related issues. Monte-Carlo simulations lend strong support to our theory and methodology.
C18|Estimating and accounting for the output gap with large Bayesian vector autoregressions|We demonstrate how Bayesian shrinkage can address problems with utilizing large information sets to calculate trend and cycle via a multivariate Beveridge-Nelson (BN) decomposition. We illustrate our approach by estimating the U.S. output gap with large Bayesian vector autoregressions that include up to 138 variables. Because the BN trend and cycle are linear functions of historical forecast errors, we are also able to account for the estimated output gap in terms of different sources of information, as well as particular underlying structural shocks given identification restrictions. Our empirical analysis suggests that, in addition to output growth, the unemployment rate, CPI inflation, and, to a lesser extent, housing starts, consumption, stock prices, real M1, and the federal funds rate are important conditioning variables for estimating the U.S. output gap, with estimates largely robust to incorporating additional variables. Using standard identification restrictions, we find that the role of monetary policy shocks in driving the output gap is small, while oil price shocks explain about 10% of the variance over different horizons.
C18|Inference with difference-in-differences with a small number of groups: a review, simulation study and empirical application using SHARE data|Difference-in-differences (DID) estimation has become increasingly popular as an approach to evaluate the effect of a group-level policy on individual-level outcomes. Several statistical methodologies have been proposed to correct for the within-group correlation of model errors resulting from the clustering of data. Little is known about how well these corrections perform with the often small number of groups observed in health research using longitudinal data. First, we review the most commonly used modelling solutions in DID estimation for panel data, including generalized estimating equations (GEE), permutation tests, clustered standard errors (CSE), wild cluster bootstrapping, and aggregation. Second, we compare the empirical coverage rates and power of these methods using a Monte Carlo simulation study in scenarios in which we vary the degree of error correlation, the group size balance, and the proportion of treated groups. Third, we provide an empirical example using the Survey of Health, Ageing and Retirement in Europe (SHARE). When the number of groups is small, CSE are systematically biased downwards in scenarios when data are unbalanced or when there is a low proportion of treated groups. This can result in over-rejection of the null even when data are composed of up to 50 groups. Aggregation, permutation tests, bias-adjusted GEE and wild cluster bootstrap produce coverage rates close to the nominal rate for almost all scenarios, though GEE may suffer from low power. In DID estimation with a small number of groups, analysis using aggregation, permutation tests, wild cluster bootstrap, or bias-adjusted GEE is recommended.
C18|Cliometrics|"Cliometrics has been defined and summarized in numerous scholarly articles. They all pretty much start with the obvious, that cliometrics is the application of economic theory and quantitative techniques to study history; and then move on to the origin of the name, the joining of Clio (the muse of history), with metrics (""to measure,"" or ""the art of measurement""), allegedly coined by economist Stanley Reiter while collaborating with economic historians Lance Davis and Jonathan Hughes.<br><small>(This abstract was borrowed from another version of this item.)</small>"
C18|Learning outside the factory: the impact of technological change on the rise of adult education in nineteenth-century France|The paper provides an empirical examination of the effect of the use of steam engine technology on the development of adult education in nineteenth-century France. In particular, we exploit exogenous regional variations in the distribution of steam engines across France to evidence that technological change significantly contributed to the development of lifelong training during the 1850-1881 period. Our research shows that steam technology adoption in France was not deskilling. We argue that this process raised the demand for new skills adapted to the development of French industries.<br><small>(This abstract was borrowed from another version of this item.)</small>
C18|Measuring Labor Market Segmentation from Incomplete Data|This paper proposes a measure of the intensity of competition in labor markets on the basis of limited data. Large-scale socioeconomic surveys often lack detailed information on competitive behavior. It is particularly difficult to determine whether a worker moves between the different segments of the labor market. Here, the Maximum Entropy principle is used to make inferences about the unobserved mobility decisions of workers in US household data. A class of models is proposed that reflects a parsimonious conception of competition in the Smithian tradition, as well as being consistent with a range of detailed behavioral models. The Quantal Response Statistical Equilibrium (QRSE) class of models can be seen to give robust microfoundations to the persistent patterns of wage inequality among equivalent workers. Furthermore, the QRSE effectively endogenizes the definition of labor market segments, allowing us to interpret the estimated competition intensities as partial measures of labor market segmentation. Models of this class generate predictions that capture between 97.5 and 99.5 percent of the informational content of the sample wage distributions. In addition to providing a very good fit to the wage data, the predictions are also consistent with bounded rationality of workers.
C18|Should citations be weighted to assess the influence of an academic article?|Citations are by nature heterogeneous. A citation worth may dramatically vary according to the influence of the citing article or to the journal's reputation from which it is issued. Therefore, while assessing the influence of an academic article, how should we weight citations to take into account their real influence? In order to answer this question, this article suggests various methods of weighting citations in the building of articles quality indicators. These indexes are then used to measure the influence of the articles published in the top five economic journals over the 2000-2010 period and analyses the sensibility of these indicators to the choice of the weighting schemes. Our main result is that whatever the weighting scheme, information carried by the different indicators is not significantly different. From Occam's razor principle, the number of citations provides an efficient and sufficient tool to measure research quality.
C18|A new baseline model for estimating willingness to pay from discrete choice models|We show a substantive problem exists with the widely-used ratio of coefficients approach to calculating willingness to pay (WTP) from discrete choice models. The correctly calculated standard error for WTP using this approach is shown to be undefined. This occurs because the cost parameter's standard error implies some possibility the true parameter value is arbitrarily close to zero. We propose a simple yet elegant way to overcome this problem by reparameterizing the (negative) cost variable's coefficient using an exponential transformation to enforce the theoretically correct positive coefficient. With it the confidence interval for WTP is now finite and well behaved.
C18|Parametric models for biomarkers based on flexible size distributions|Recent advances in social science surveys include collection of biological samples. Although biomarkers offer a large potential for social science and economic research, they impose a number of statistical challenges, often being distributed asymmetrically with heavy tails. Using data from the UK Household Panel Survey, we illustrate the comparative performance of a set of flexible parametric distributions, which allow for a wide range of skewness and kurtosis: the four‐parameter generalized beta of the second kind (GB2), the three‐parameter generalized gamma, and their three‐, two‐, or one‐parameter nested and limiting cases. Commonly used blood‐based biomarkers for inflammation, diabetes, cholesterol, and stress‐related hormones are modelled. Although some of the three‐parameter distributions nested within the GB2 outperform the latter for most of the biomarkers considered, the GB2 can be used as a guide for choosing among competing parametric distributions for biomarkers. Going “beyond the mean” to estimate tail probabilities, we find that GB2 performs fairly well with some disparities at the very high levels of glycated hemoglobin and fibrinogen. Commonly used linear models are shown to perform worse than almost all the flexible distributions.
C18|Theoretical and Methodological Context of (Post)-Modern Econometrics and Competing Philosophical Discourses for Policy Prescription|"This research article was championed as a way of providing discourses pertaining to the concept of ""Critical Realism (CR)"" approach, which is amongst many othe forms of competing postmodern philosophical concepts for the engagement of dialogical discourses in the area of established econometric methodologies for effective policy prescription in the economic science discipline. On the the whole, there is no doubt surrounding the value of empirical endeavours in econometrics to address real world economic problems, but equally so, the heavy weighted use and reliance on mathematical contents as a way of justifying its scientific base seemed to be loosing traction of the intended focus of economics when it comes to confronting real world problems in the domain of social interaction. In this vein, the construction of mixed methods discourse(s), which favour that of CR philosophy is hereby suggested in this article as a way forward in confronting with issues raised by critics of mainstream economics and other professionals in the postmodern era."
C18|Messung der regionalen Versorgung mit Bankdienstleistungen vor dem Hintergrund der Digitalisierung: Erprobung eines neuen Messansatzes an Ländern der Europäischen Union|Der Beitrag setzt sich mit der Bewertung der regionalen Versorgung mit Finanzdienstleistungen auseinander. Vor dem Hintergrund der zunehmenden Digitalisierung, die einerseits zu mehr Onlineangeboten und andererseits zur Reduktion von Filialen führt, wird vorgestellt, wie die regionale Versorgung mit Finanzdienstleistungen gemessen werden kann. Hierbei wird der bisherige Messansatz (Filialen je Quadratkilometer bzw. je Einwohner) kritisch beleuchtet und es wird ein erweiterter Messansatz vorgestellt, der demographische, topographische, siedlungsstrukturelle und - mit Blick auf den Zugang zum Internet - infrastrukturelle Aspekte aufgreift. Dieser Ansatz wurde bereits von Conrad et al. (2018) für öffentlich-rechtliche Sparkassen und Genossenschaftsbanken in Deutschland auf kleinräumiger Ebene angewendet. Dabei konnte zwar ein Vergleich der Versorgung innerhalb Deutschlands durchgeführt werden, eine Gegenüberstellung mit Ergebnissen für unterschiedliche Länder - z.B. der Europäischen Union - fehlte hingegen. Dieser Beitrag setzt hier an und stellt dar, inwieweit ein Übertrag des von Conrad et al. (2018) vorgestellten Messansatzes auf unterschiedliche Länder der Europäischen Union möglich ist und welche Aussagen zu Unterschieden in der Versorgungslage auf der Grundlage des Vergleichs abgeleitet werden können. Es zeigt sich, dass ein adäquater Übertrag des Messansatzes noch nicht gelingt. Grund dafür sind fehlende oder zu wenig spezifizierte Daten, die auf europäischer oder nationaler Ebene bereitgestellt werden. Auf Basis der vorhandenen Daten konnte jedoch die Mechanik des Messansatzes für einzelne Länder der Europäischen Union getestet und Ergebnisse für einen länderübergreifenden Vergleich der Versorgungslage abgeleitet werden. Es zeigt sich, dass die Versorgung in den untersuchten Ländern vergleichsweise homogen ausfällt, wenn der bisherige Messansatz angewendet wird. Unter Anwendung des erweiterten Messansatzes zeigt sich hingegen ein differenzierteres Bild.
C18|Does high growth persist? A focus on growth formulas and the influence of firm exits|This study investigates the development of firms after high-growth. We argue that the formula used for measuring growth determines results. Implications from different formulas are tested with data from Amadeus on Bulgarian firms for the years 2001-2010. We provide first evidence for an absolute growth formula and its systematic comparison to alternative choices. The focus is on growth in employees, but we offer additional evidence for sales and profits. Using a two-part regression model with separate equations for survival and growth, we find that high-growth does not persist when size of exits is accounted for. Losses by exiting high-growth firms outweigh further gains in size by survivors. This result equally holds for the 1 percent fastest growers in absolute terms, the top 1 percent in terms of log growth and high-growth firms defined according to Eurostat-OECD. Implications for the future study of high-growth firms and policies focused at them are discussed.
C18|Frekvensbaserede versus bayesianske metoder i empirisk økonomi|"Indenfor økonomi og samfundsvidenskab har den klassiske frekvens-baserede analysemetode traditionelt været fremherskende, men de senere år er flere samfundsforskere begyndt at anvende bayesianske metoder i empirisk modellering. I denne artikel beskrives og sammenlignes de to metoder. Der argumenteres for, at vi i højere grad bør anvende den bayesianske tilgang. Den klassiske metode giver sandsynligheden for data, givet modellen (nulhypotesen), mens den bayesianske metode giver sandsynligheden for modellen, givet data. Anvendelse af ""p-værdien""i det klassiske hypotesetest fører til for mange ""falsk positive"" resultater. Den bayesianske metode er mere velegnet end den klassiske til analyse af de hypoteser økonomer arbejder med, hvor en model ikke tilstræbes at være ""sand"", men i stedet opfattes som en grov approksimation til virkeligheden."
C18|Cliometrics|"Cliometrics has been defined and summarized in numerous scholarly articles. They all pretty much start with the obvious, that cliometrics is the application of economic theory and quantitative techniques to study history; and then move on to the origin of the name, the joining of Clio (the muse of history), with metrics (""to measure,"" or ""the art of measurement""), allegedly coined by economist Stanley Reiter while collaborating with economic historians Lance Davis and Jonathan Hughes."
C18|Learning outside the factory: the impact of technological change on the rise of adult education in nineteenth-century France|The paper provides an empirical examination of the effect of the use of steam engine technology on the development of adult education in nineteenth-century France. In particular, we exploit exogenous regional variations in the distribution of steam engines across France to evidence that technological change significantly contributed to the development of lifelong training during the 1850-1881 period. Our research shows that steam technology adoption in France was not deskilling. We argue that this process raised the demand for new skills adapted to the development of French industries.
C18|When Does Real Become Consequential in Non-hypothetical Choice Experiments?|The proneness of stated preference methods to hypothetical bias has increased the popularity of incentivized studies, in particular the use of real choice experiments (RCE). Challenges of RCE include the lack of engagement with the choice task by some subjects, and that some of the product alternatives may not be available in order to incentivize all the choices. This issue brings to question whether the proportion of available products influences the results of the RCE. Would the subjects' engagement change? Using an induced value choice experiment with a profit maximization optimal strategy for agents, we varied the number of potentially binding alternatives in four treatments. Our results suggest that incentives matter, as the percentage of optimal choices was lowest in the hypothetical treatment. Interestingly, however, we do not find statistically significant differences in the number of optimal choices between the incentivized treatments, regardless of the number of potentially binding alternatives used in our treatments. This suggests that practitioners could conduct incentivized RCE without the need to have all the product alternatives be made available in the study. Furthermore, we explore the interaction of incentives with subjects' numerical ability and individual reflective state. Both are also shown to influence how incentives impact performance, shedding some light on what individual characteristics to look for when conducting valuation research.
C18|Quasi-Experimental Shift-Share Research Designs|Many empirical studies leverage shift-share (or “Bartik”) instruments that combine a set of aggregate shocks with measures of shock exposure. We derive a necessary and sufficient shock-level orthogonality condition for these instruments to identify causal effects. We then show that orthogonality holds when observed shocks are as-good-as-randomly assigned and growing in number, with the average shock exposure sufficiently dispersed. We recommend that practitioners implement quasi-experimental shift-share designs with new shock-level regressions, which help visualize identifying shock variation, correct standard errors, choose appropriate specifications, test identifying assumptions, and optimally combine multiple sets of quasi-random shocks. We illustrate these points by revisiting Autor et al. (2013)'s analysis of the labor market effects of Chinese import competition.
C18|Monetary Policy Uncertainty: A Tale of Two Tails|We document a strong asymmetry in the evolution of federal funds rate expectations and map this observed asymmetry into measures of monetary policy uncertainty. We show that periods of monetary policy tightening and easing are distinctly related to downside (policy rate is higher than expected) and upside (policy rate is lower than expected) uncertainty. Downside monetary policy uncertainty decreases over time, while upside uncertainty remains rather stable, reflecting the asymmetry in the behavior of the expectational errors—a finding that we attribute to changes in the conduct of monetary policy. We show that this behavior cannot be entirely explained by uncertainty in macroeconomic fundamentals: the asymmetry remains even when we control for macroeconomic uncertainty, emphasizing the importance of monetary policy implementation. Finally, we assess the macroeconomic effects of monetary policy uncertainty. We find that the effects are non-linear and conditional on the economy being in an easing or tightening regime. Though uncertainty is, in general, recessionary, its effects are stronger in a monetary easing regime relative to a tightening one.
C18|Characterizing the Canadian Financial Cycle with Frequency Filtering Approaches|In this note, I use two multivariate frequency filtering approaches to characterize the Canadian financial cycle by capturing fluctuations in the underlying variables with respect to a long-term trend. The first approach is a dynamically weighted composite, and the second is a stochastic cycle model. Applying the two approaches to Canada yields several findings. First, the Canadian financial cycle is more than twice as long as the business cycle, with an amplitude almost four times greater. Second, the overall Canadian financial cycle is most strongly associated with household credit and house prices. Third, while Canadian house prices are mostly associated with the financial cycle, they are also significantly tied to the business cycle. Lastly, house prices are found to lead the overall financial cycle. These results are generally in line with findings for other countries studied in literature. Additionally, I compare each approach’s proneness to revision and find that both are more reliable, when monitored in real time, than the Basel III total credit-to-GDP gap. Nonetheless, further work is encouraged to investigate more variable combinations and undertake a cross-country analysis since data on systemic financial stress in Canada are limited. It should be noted that since the approaches produce a measure of the financial cycle relative to trend, comparison with level indicators (as those monitored in the Bank of Canada’s Financial System Review) is not straightforward.
C18|Review of Methodological Specifics of Consumer Price Index Seasonal Adjustment in the Bank of Russia|Under the inflation targeting regime, the main goal of the Bank of Russia is to maintain price stability. In order to analyse the options that the central bank can use to implement its monetary policy aimed at bringing inflation down to sustainable low levels it is necessary to understand, considering the available short-term statistical data, the dynamics of consumer prices and individual components of the seasonally adjusted consumer price index. At the same time, the seasonal adjustment of the consumer price index requires solving a number of methodological problems, one part of which is common for all economic time series with a seasonal component and the other part is determined by the specific nature of the consumer price index as an aggregate indicator. The paper suggests approaches to solving conceptual problems related to the seasonal adjustment of the consumer price index. It also describes basic principles and methods for their implementation that can lead to a significant increase in the quality of identification and interpretation of short-term meaningful variations in consumer prices that the Bank of Russia takes into account when making its monetary policy decisions.
C18|Minimum Wage and Productivity: Analysis of Manufacturing Industry of Korea (in Korean)|Recent discussions on minimum wage increase (MWI) and its influences on the economy have mainly focused on the quantitative aspects such as labor cost and employment. However, on the qualitative aspects, MWI could have positive effects by enhancing firm productivity and crowding out marginal firms in the market. These positive effects of MWI can offset, to some extent, its potential negative effects ? the increase of labor cost and the decrease of employment among others. In this regard we empirically examine the impact of MWI on the firm productivity (total factor productivity). Using firm level panel data in the manufacturing industry of Korea, we calculate the influence rates of minimum wage by sector and by size (number of workers) and analyze their effect on the firm productivity. In particular the production functions of the firms are estimated by the way of taking into account endogeneity among input factors, in order to resolve the drawbacks of existing studies ? underestimating capital factor coefficient and overestimating labor factor coefficient. This study finds that the influences of MWI on wage, employment, and productivity differ across sectors and firm sizes. While MWI has shown positive influences on the productivity growth in the manufacturing industry as a whole, each sector demonstrates different direction of effect and the degree of productivity change is also varying by sector. The impacts of MWI on the firm productivity are estimated generally to be more negative for smaller firms but for some sectors the effects are found to be positive. In addition, the wage increases resulted from MWI seem to cause productivity enhancement throughout all sectors in the manufacturing industry. The policy implications of this study are as follows. Considering the empirical finding that MWI causes the increases of productivity in many sectors of the manufacturing industry, it will be desirable to evaluate not only the negative side effects but also the positive effects of MWI for designing future minimum wage policy. Moreover, in spite of the uniform minimum wage, this study finds that the diverse influence rates of minimum wage across firms have differential impacts on wage, employment, and productivity across sectors or different sizes of establishment. This finding could be conducive to discussing about the differentiation of minimum wage scheme by sector or by size.
C18|Time-invariant Regressors under Fixed Effects: Identification via a Proxy Variable|Identification of a coefficient associated with a time-invariant regressor (TIR) often relies on the assumption that the TIR is uncorrelated with the unobserved heterogeneity across panel units. We derive an estimator which avoids the random-effects assumption by employing a proxy for the unobserved heterogeneity thus extending the existing results on proxy variables from the cross-sectional literature. In addition, we quantify the sensitivity of the estimates to potential violations of the random-effects assumption when no proxy is available. The utility of this approach is illustrated on the problem of implausibly high distance elasticity produced by gravity models of international trade.
C18|On Heckits, LATE, and Numerical Equivalence|Structural econometric methods are often criticized for being sensitive to functional form assumptions. We study parametric estimators of the local average treatment effect (LATE) derived from a widely used class of latent threshold crossing models and show they yield LATE estimates algebraically equivalent to the instrumental variables (IV) estimator. Our leading example is Heckman's (1979) two‐step (“Heckit”) control function estimator which, with two‐sided non‐compliance, can be used to compute estimates of a variety of causal parameters. Equivalence with IV is established for a semiparametric family of control function estimators and shown to hold at interior solutions for a class of maximum likelihood estimators. Our results suggest differences between structural and IV estimates often stem from disagreements about the target parameter rather than from functional form assumptions per se. In cases where equivalence fails, reporting structural estimates of LATE alongside IV provides a simple means of assessing the credibility of structural extrapolation exercises.
C18|A Residual-based Threshold Method for Detection of Units that are Too Big to Fail in Large Factor Models|The importance of units with pervasive impacts on a large number of other units in a network has become increasingly recognized in the literature. In this paper we propose a new method to detect such influential or dominant units by basing our analysis on unit-specific residual error variances in the context of a standard factor model, subject to suitable adjustments due to multiple testing. Our proposed method allows us to estimate and identify the dominant units without the a priori knowledge of the interconnections amongst the units, or using a short list of potential dominant units. It is applicable even if the cross section dimension exceeds the time dimension, and most importantly it could end up with none of the units selected as dominant when this is in fact the case. The sequential multiple testing procedure proposed exhibits satisfactory small-sample performance in Monte Carlo simulations and compares well relative to existing approaches. We apply the proposed detection method to sectoral indices of US industrial production, US house price changes by states, and the rates of change of real GDP and real equity prices across the world’s largest economies.
C18|Recovering Social Networks from Panel Data: Identification, Simulations and an Application|It is almost self-evident that social interactions can determine economic behavior and outcomes. Yet, information on social ties does not exist in most publicly available and widely used datasets. We present methods to recover information on the entire structure of social networks from observational panel data that contains no information on social ties between individuals. In the context of a canonical social interactions model, we provide sufficient conditions under which the social interactions matrix, endogenous and exogenous social effect parameters are all globally identified. We describe how high-dimensional estimation techniques can be used to estimate the model based on the Adaptive Elastic Net GMM method. We showcase our method in Monte Carlo simulations using two stylized and two real world network structures. Finally, we employ our method to study tax competition across US states. We find the identified network structure of tax competition differs markedly from the common assumption of tax competition between geographically neighboring states. We analyze the identified social interactions matrix to provide novel insights into the long-standing debate on the relative roles of factor mobility and yardstick competition in driving tax setting behavior across states. Most broadly, our method shows how the analysis of social interactions can be usefully extended to economic realms where no network data exists.
C18|The “wrong skewness” problem in stochastic frontier models: A new approach| Stochastic frontier models are widely used to measure, e.g., technical efficiencies of firms. The classical stochastic frontier model often suffers from the empirical artefact that the residuals of the production function may have a positive skewness, whereas a negative one is expected under the model, which leads to estimated full efficiencies of all firms. We propose a new approach to the problem by generalizing the distribution used for the inefficiency variable. This generalized stochastic frontier model allows the sample data to have the wrong skewness while estimating well-defined and nondegenerate efficiency measures. We discuss the statistical properties of the model, and we discuss a test for the symmetry of the error term (no inefficiency). We provide a simulation study to show that our model delivers estimators of efficiency with smaller bias than those of the classical model even if the population skewness has the correct sign. Finally, we apply the model to data of the U.S. textile industry for 1958–2005 and show that for a number of years our model suggests technical efficiencies well below the frontier while the classical one estimates no inefficiency in those years.
C18|Recovering social networks from panel data: identification, simulations and an application| It is almost self-evident that social interactions can determine economic behavior and outcomes. Yet, information on social ties does not exist in most publicly available and widely used datasets. We present results on the identification of social networks from observational panel data that contains no information on social ties between agents. In the context of a canonical social interactions model, we provide sufficient conditions under which the social interactions matrix, endogenous and exogenous social effect parameters are all globally identified. While this result is relevant across different estimation strategies, we then describe how high-dimensional estimation techniques can be used to estimate the model based on the Adaptive Elastic Net GMM method. We showcase the method and its robustness in Monte Carlo simulations using stylized and real world network structures. Finally, we employ the method to study tax competition across US states. We find the identified network structure of tax competition differs markedly from the common assumption of competition between geographically neighboring states. We analyze the identified social interactions matrix to provide novel insights into the long-standing debate on the relative roles of factor mobility and yardstick competition in driving tax setting behavior across states. Most broadly, our results show how the analysis of social interactions can be extended to economic realms where no network data exists.
C18|The rise and fall of the natural interest rate|We document a rise and fall of the natural interest rate (r*) for several advanced economies, which starts increasing in the 1960’s and peaks around the end of the 1980’s. We reach this conclusion after showing that the Laubach and Williams (2003) model cannot estimate r* accurately when either the IS curve or the Phillips curve is fl at. In those empirically relevant situations, a local level specifi cation for the observed interest rate can precisely estimate r*. An estimated Panel ECM suggests that the temporary demographic effect of the young baby-boomers mostly accounts for the rise and fall.
C18|How to decompose the R²?: A comment on Henderson et al. (2018)|Henderson et al. (2018) assessed the economic importance of 24 geographic variables in determining the worldwide spatial distribution of economic activity, as proxied by night lights. In this short piece, I first show that the method they used to measure the economic importance of effects – the Shapley value – is flawed, implying that some their results are misleading. Second, I use an axiomatic approach to build a new method for assessing the economic importance of effects, which corrects for the identified flaws. Finally, I revisit the conclusions of Henderson et al. (2018) in light of the new method.
C18|SDG 16 on Governance and its measurement: Africa in the Lead|(english) This article provides some elements for reflection on an apparent paradox. On the one hand, Africa appears to be the continent most riddled by problems related to governance and conflict; on the other hand, it is at the forefront in both promoting the issue of governance at the international level and in implementing its statistical measurement, an observation that has gone largely unnoticed until now. Will Africa manage to maintain its lead following the adoption by all countries of Sustainable Development Goal 16 on governance, peace and security, to which the continent contributed greatly?_________________________________ (français) Cet article se propose d’apporter des éléments de réflexion sur un apparent paradoxe : alors que l’Afrique apparaît comme le continent où les questions de gouvernance et de conflits sont les plus problématiques, c’est également celui qui se montre le plus en pointe, à la fois dans la promotion de cette thématique au niveau international et dans la mise en oeuvre de sa mesure statistique, un constat passé largement inaperçu jusqu’ici. Cette avance pourra-t-elle se maintenir avec l’adoption par tous les pays de l’Objectif de Développement Durable 16 sur la gouvernance, la paix et la sécurité, auquel le continent a largement contribué ?e la société malgache. La faible organisation, tant du côté d’élites fragmentées que d’une population atomisée géographiquement et socialement, n’est pas propice à l’émergence de violences politiques. Mais c’est surtout la prégnance d’une violence symbolique systémique qui permet le maintien de l’ordre établi et freine l’émergence de mobilisations susceptibles de remettre en cause l’équilibre de la société. Faiblesse organisationnelle, normes sociales et violence symbolique se conjuguent pour une apparente paix sociale. Celle-ci reflète moins une solidité institutionnelle que la domination symbolique de la classe élitaire.
C18|Verifying the internal validity of a flagship RCT: A review of Crépon, Devoto, Duflo and Pariente (American Economic Journal: Applied Economics, 2015)|We replicate a flagship randomised control trial carried out in rural Morocco that showed substantial and significant impacts of microcredit on the assets, the outputs, the expenses and the profits of self-employment activities. The original results rely primarily on trimming, which is the exclusion of observations with the highest values on some variables. However, the applied trimming procedures are inconsistent between the baseline and the endline. Using identical specifications as the original paper reveals large and significant imbalances at the baseline and, at the endline impacts on implausible outcomes, like household head gender, language or education. This calls into question the reliability of the data and the integrity of the experiment protocol. We find a series of coding, measurement and sampling errors. Correcting the identified errors lead to different results. After rectifying identified errors, we still find substantial imbalances at baseline and implausible impacts at the endline. Our re-analysis focused on the lack of internal validity of this experiment, but several of the identified issues also raise concerns about its external validity.
C18|A methodology for automised outlier detection in high-dimensional datasets: an application to euro area banks' supervisory data|Outlier detection in high-dimensional datasets poses new challenges that have not been investigated in the literature. In this paper, we present an integrated methodology for the identification of outliers which is suitable for datasets with higher number of variables than observations. Our method aims to utilise the entire relevant information present in a dataset to detect outliers in an automatized way, a feature that renders the method suitable for application in large dimensional datasets. Our proposed five-step procedure for regression outlier detection entails a robust selection stage of the most explicative variables, the estimation of a robust regression model based on the selected variables, and a criterion to identify outliers based on robust measures of the residuals' dispersion. The proposed procedure deals also with data redundancy and missing observations which may inhibit the statistical processing of the data due to the ill-conditioning of the covariance matrix. The method is validated in a simulation study and an application to actual supervisory data on banks’ total assets. JEL Classification: C18, C81, G21
C18|The Purchasing Power Parity Puzzle: An Update|We show that the Purchasing Power Parity (PPP) puzzle, whereby the half-life of the shock to the real exchange rate is long and unjustifiable by monetary and financial shocks, is a result of specification and estimation issues. We provide an alternative specification for PPP and show that the half-life of the shock could be as short as 6.8 months and as long as 2 years, which is considerably shorter than what have been reported in the literature.
C18|High unknowability of climate damage valuation means the social cost of carbon will always be disputed|"The social cost of carbon (SCC), a carbon price calculated from cost-benefit based integrated assessment models and used to inform some climate policies, will always be highly disputed, partly because a key model assumption, the centennial climate damage valuation function (CDF), will ""always"" be highly unknowable. Current disputes are highlighted here by the huge range of SCCs resulting from alternative values of key parameters like discount rates, climate sensitivity and the CDF; by the implausibility to climate scientists of a leading model's warming projections; and by strong criticisms of mainstream CDFs by many climate economists. The claim that statistical analyses of ""weather"" impacts on local economies can improve centennial CDFs rests on untestable out-of-sample extrapolation. Compared to astronomy, geology and other earth sciences, prediction testing in climate science is generally harder because of Earth's uniqueness, and the unprecedented range and speed of likely centennial climate change, but stable underlying laws make modelling based on past observations meaningful. By contrast, the added complexity of human behaviour means there are no reliable laws for modelling centennial CDFs. For this reason alone, SCCs will always be disputed. I suggest instead more use of carbon prices based on marginal abatement costs, computed on cost-effective paths that achieve socially agreed, physical climate targets. Downplaying the SCC approach to carbon prices poses challenges to many economists, and a cost-effectiveness approach is no panacea, but it avoids the illusion of optimality, and allows more detailed analysis of many current climate policies."
C18|Size, Internationalization and University Rankings: Evaluating Times Higher Education (THE) Data for Japan|International and domestic rankings of academics, academic departments, faculties, schools and colleges, institutions of higher learning, states, regions and countries, are of academic and practical interest and importance to students, parents, academics, and private and public institutions. International and domestic rankings are typically based on arbitrary methodologies and criteria. Evaluating how the rankings might be sensitive to different factors, as well as forecasting how they might change over time, requires a statistical analysis of the factors that affect the rankings. Accurate data on rankings and the associated factors is essential for a valid statistical analysis. In this respect, the Times Higher Education (THE) World University Rankings is one of the three leading and most influential annual sources of international university rankings. Using recently released data for a single country, namely Japan, the paper evaluates the effects of size (specifically, the number of Full-Time Equivalent (FTE) students, or FTE(Size)) and internationalization (specifically, the percentage of international students, or IntStud) on academic rankings using THE data for 2017 and 2018 on 258 national, public (that is, prefectural or city), and private universities. The results show that both size and internationalization are statistically significant in explaining rankings for all universities, as well as separately for private and non-private (that is, national and public) universities, in Japan for each of 2017 and 2018.
C18|Identification of Causal Intensive Margin Effects by Difference-in-Difference Methods|This paper discusses identification of causal intensive margin effects. The causal intensive margin effect is defined as the treatment effect on the outcome of individuals with a positive outcome irrespective of whether they are treated or not (always-takers or participants). A potential selection problem arises when conditioning on positive outcomes, even if treatment is randomly assigned. We propose to use difference-in-difference methods - conditional on positive outcomes - to estimate causal intensive margin effects. We derive sufficient conditions under which the difference-in-difference methods identify the causal intensive margin effect in a setting with random treatment.
C18|Measurement of Volatility Spillovers and Asymmetric Connectedness on Commodity and Equity Markets|We study volatility spillovers among commodity and equity markets by employing a recently developed approach based on realized measures and forecast error variance decomposition invariant to the variable ordering from vector-autoregressions. This enables us to measure total, directional and net volatility spillovers as well as the asymmetry of responses to positive and negative shocks. We exploit high-frequency data on the prices of Crude oil, Corn, Cotton and Gold futures, and the S&P 500 Index and use a sample which spans from January 2002 to December 2015 to cover the entire period around the global financial crisis of 2008. Our empirical analysis reveals that on average, the volatility shocks related to other markets account for around one fifth of the volatility forecast error variance. We find that shocks to the stock markets play the most important role as the S&P 500 Index dominates all commodities in terms of general volatility spillover transmission. Our results further suggest that volatility spillovers across the analyzed assets were rather limited before the global financial crisis, which then boosted the connectedness between commodity and stock markets. Furthermore, the volatility due to positive and negative shocks is transmitted between markets at different magnitudes and the prevailing effect has varied. In the pre-crisis period, the positive spillovers dominated the negative ones, however, in several years following the crisis, the negative shocks have had a significantly higher impact on the volatility spillovers across the markets, pointing to an overall increase in uncertainty in the commodity and equity markets following a major crisis. In recent years, the asymmetric measures seem to have returned to their pre-crises directions and magnitudes.
C18|Easy bootstrap-like estimation of asymptotic variances|The bootstrap is a convenient tool for calculating standard errors of the parameter estimates of complicated econometric models. Unfortunately, the bootstrap can be very time-consuming. In a recent paper, Honoré and Hu (2017), we propose a “Poor (Wo)man’s Bootstrap” based on one-dimensional estimators. In this paper, we propose a modified, simpler method and illustrate its potential for estimating asymptotic variances.
C18|The gold standard for randomized evaluations: from discussion of method to political economy|(english) This last decade has seen the emergence of a new field of research in development economics: randomised control trials. This paper explores the contrast between the (many) limitations and (very narrow) real scope of these methods and their success in sheer number and media coverage. Our analysis suggests that the paradox is due to a particular economic and political mix driven by the innovative strategies used by this new school’s researchers and by specific interests and preferences in the academic world and the donor community. _________________________________ (français) La dernière décennie a vu l'émergence d'un nouveau champ de recherche en économie du développement : les méthodes expérimentales d'évaluation d'impacts par assignation aléatoire. Cet article explore le contraste entre d’une part les limites (nombreuses) et la circonscription (très étroite) du champ réel d'application de ces méthodes et d’autre part leur succès, attesté à la fois par leur nombre et leur forte médiatisation. L’analyse suggère que ce contraste est le fruit d’une conjonction économique et politique particulière, émanant de stratégies novatrices de la part des chercheurs de cette nouvelle école, et d’intérêts et de préférences spécifiques provenant à la fois du monde académique et de la communauté des donateurs.
C18|The nexus between oil price and Russia's real exchange rate: Better paths via unconditional vs conditional analysis|Instead of analyzing the causality between two time series (unconditional analysis), as it is usually done, the present study deals with the nexus between oil price and Russia's real exchange rate conditioning upon potential control variables at well-specified horizons and on a frequency by frequency basis. This research accounts also for the possible transient linkages and signal discontinuities. A major finding of this paper is deeply suggestive of a sharp causality running from oil price to real exchange rate in lower frequencies. This implies that Russia should better tackle with turbulence triggered by oil price and continue to reduce its energy dependency via drastic and proactive measures. The economic and fiscal initiatives of Putin administration may help to cope with sudden shocks, to lessen the great oil dependence and to build confidence needed for economic recovery. While our research does not say much about the routes through which oil price may affect differently real exchange rate, it clearly indicates the presence of short-term relationship conditional to GDP, government expenditures, terms of trade and productivity differential. The conditional analysis and signal detection appear as meaningful exercises to find new insights into the focal issue.
C18|The Periodogram of Spurious Long-Memory Processes|We derive the properties of the periodogram local to the zero frequency for a large class of spurious long-memory processes. The periodogram is of crucial importance in this context, since it forms the basis for most commonly used estimation methods for the memory parameter. The class considered nests a wide range of processes such as deterministic or stochastic structural breaks and smooth trends as special cases. Several previous results on these special cases are generalized and extended. All of the spurious long-memory processes considered share the property that their impact on the periodogram at the Fourier frequencies local to the origin is different than that of true long-memory processes. Both types of processes therefore exhibit clearly distinct empirical features.
C18|Survey Item-Response Behavior as an Imperfect Proxy for Unobserved Ability: Theory and Application|We develop and test an economic model of the cognitive and non-cognitive foundations of survey item-response behavior. We show that a summary measure of response behaviour - the survey item-response rate (SIRR) - varies with cognitive and less so with non-cognitive abilities, has a strong individual fixed component and is predictive of economic outcomes because of its relationship with ability. We demonstrate the usefulness of SIRR, although an imperfect proxy for cognitive ability, to reduce omitted-variable biases in estimated wage returns. We derive both necessary and sufficient conditions under which the use of an imperfect proxy reduces such biases, providing a general guideline for researchers.
C18|The Dynamic Properties of Economic Preferences|The time-stability of preferences is a crucial and ubiquitous assumption in economics, yet to date there is no method to test its validity. Based on a model of the dynamics of individual preferences, I develop a simple method to test this assumption. Time-persistance in preferences is captured via an autoregressive parameter that accounts for observable characteristics and is unattenuated by measurement error, which forms the basis of the test. The method also estimates the variance of persistent shocks to latent preferences, which measures unobserved heterogeneity, and preference measurement error. I illustrate the use of this method by testing the stability of risk aversion and patience using micro-level data, and find that patience is time-stable but risk aversion is not. However, change very slowly over time. This method provides researchers with a simple tool to properly test the assumption on preference stability, and to measure the degree of preference changes due to observable and unobservable factors.
C18|LASSO-Type Penalization in the Framework of Generalized Additive Models for Location, Scale and Shape|For numerous applications it is of interest to provide full probabilistic forecasts, which are able to assign probabilities to each predicted outcome. Therefore, attention is shifting constantly from conditional mean models to probabilistic distributional models capturing location, scale, shape (and other aspects) of the response distribution. One of the most established models for distributional regression is the generalized additive model for location, scale and shape (GAMLSS). In high dimensional data set-ups classical fitting procedures for the GAMLSS often become rather unstable and methods for variable selection are desirable. Therefore, we propose a regularization approach for high dimensional data set-ups in the framework for GAMLSS. It is designed for linear covariate effects and is based on L1 -type penalties. The following three penalization options are provided: the conventional least absolute shrinkage and selection operator (LASSO) for metric covariates, and both group and fused LASSO for categorical predictors. The methods are investigated both for simulated data and for two real data examples, namely Munich rent data and data on extreme operational losses from the Italian bank UniCredit.
C18|The case for NIT+FT in Europe. An empirical optimal taxation exercise|We present an exercise in empirical optimal taxation applied to a Negative Tax with Flat Tax reform for a sample of eight European countries: Austria, Belgium, France, Germany, Ireland, Italy, Luxembourg and the United Kingdom. Two popular approaches to empirical optimal taxation are the structural analytical approach and the non-tructural “sufficient statistics” approach. This paper presents a different approach that combines structural microeconometric modelling, behavioural microsimulation and numerical optimization. For each country, we estimate a microeconometric model of labour supply for both couples and singles. A procedure that simulates the households' choices under given tax-transfer rules is then embedded in a constrained optimization program in order to identify optimal rules under the public budget constraint. The optimality criterion is the class of Kolm's social welfare function. The tax-transfer rules considered as candidates are members of a class that includes as special cases various versions of the Negative Income Tax: Conditional (means-tested) Basic Income, Unconditional Basic Income, In-Work Benefits and General Negative Income Tax, combined with a Flat Tax above the exemption level. The analysis in most cases show that: the General Negative Income Tax strictly dominates the other rules, including the current ones; the Unconditional Basic Income policy is better than the Conditional Basic Income policy; Conditional Basic Income policy may lead to a significant reduction in labour supply and poverty-trap effects; In-Work-Benefit policy is strictly dominated by the General Negative Income Tax and by the Unconditional Basic Income. We also exemplify the possibility of identifying the mapping between the contry-specific “primitives” (social preferences, productivity, public budget constraint, labour supply elasticity and Gini coefficient) and the optimal tax-transfer rules.
C18|A static approach to the Nelson-Siegel-Svensson model: an application for several negative yield cases|The appearance of negative bond yields presents significant challenges for the fixed income markets, which mainly concern related forecasting models. The Nelson-Siegel-Svensson model (NSS) is one of the models that is most frequently used by central banks to estimate the term structure of interest rates. The objective of this study is to evaluate the application of the NSS model to fit the yield curve of a set of 20 countries, the majority from the Eurozone, which registered negative sovereign bond yields. We conclude that the model adjusted well for all countries’ yield curves, although no changes or constraints were introduced. In addition, a comparison was carried out between market instantaneous interest rate and the interest rate for the very distant future, which the model can predict, with good results for the instantaneous interest rate. An evaluation of the possible behaviour of shared debt securities (i.e. Eurobonds) was also analysed. In conclusion, the NSS model seems to remain a valuable, easy to use, and adaptable tool, to fit negative yield curves, for monetary policy institutions and market players alike.
C18|Survey item-response behavior as an imperfect proxy for unobserved ability: Theory and application|We develop and test an economic model of the cognitive and non-cognitive foundations of survey item- response behavior. We show that a summary measure of response behaviour – the survey item-response rate (SIRR) – varies with cognitive and less so with non-cognitive abilities, has a strong individual fixed component and is predictive of economic outcomes because of its relationship with ability. We demonstrate the usefulness of SIRR, although an imperfect proxy for cognitive ability, to reduce omitted-variable biases in estimated wage returns. We derive both necessary and sufficient conditions under which the use of an imperfect proxy reduces such biases, providing a general guideline for researchers.
C18|Distributional impact analysis: toolkit and illustrations of impacts beyond the average treatment effect|Program evaluations often focus on average treatment effects. However, average treatment effects miss important aspects of policy evaluation, such as the impact on inequality and whether treatment harms some individuals. A growing literature develops methods to evaluate such issues by examining the distributional impacts of programs and policies. This toolkit reviews methods to do so, focusing on their application to randomized control trials. The paper emphasizes two strands of the literature: estimation of impacts on outcome distributions and estimation of the distribution of treatment impacts. The article then discusses extensions to conditional treatment effect heterogeneity, that is, to analyses of how treatment impacts vary with observed characteristics. The paper offers advice on inference, testing, and power calculations, which are important when implementing distributional analyses in practice. Finally, the paper illustrates select methods using data from two randomized evaluations.
C18|Convergence of Computed Dynamic Models with Unbounded Shock|The purpose of this paper is to provide the conditions for the convergence of invariant measure obtained from numerical simulations to the exact invariant measure. Santos and Peralta-Alva (2005) have studied the convergence of computed invariant measure of economic models which cannot be solved analytically and must be solved numerically or with some other form of approximation. However, they assume that the state space is compact and therefore, the support of the shock of dynamical system is assumed to be bounded. This paper is to relax the compactness assumption for the convergence of the approximated invariant measure.
C18|Determinants of Foreign Direct Investment in Romania: a Quantitative Approach|This study aims to examine the dynamic relationship between foreign direct investments (FDI) and economic growth, using the Structural Vector Autoregressive model, in the period 2007-2014. The results of the econometric model show that the trajectory of FDI has its own origins, with reduced influences from economic growth. Another important conclusion is that there is a unidirectional causal relationship from the economic growth towards FDI, more precisely the influence of FDI on economic growth does not have a systematic, anticipatory nature. These results were achieved in the condition that, in the analyzed period, the net inflows of FDI were influenced by the lack of certainty on the sustainable re-launching of the economic growth both domestically and internationally, the segmentation of the financial market, the domestic structural reforms.
C18|A cliometric counterfactual: what if there had been neither Fogel nor North?|Abstract 1993 Nobel laureates Robert Fogel and Douglass North were pioneers in the “new” economic history, or cliometrics. Their impact on the economic history discipline is great, though not without its critics. In this essay, we use both the “old” narrative form of economic history, and the “new” cliometric form to analyze the impact each had on the evolution of economic history.
C18|Testing the Consistency of Preferences in Discrete Choice Experiments: An Eye Tracking Study|No abstract is available for this item.
C18|Revisiting Dynamic Complementarity in the Production of Cognitive Skill and its Implications for a Cognitive Achievement Gap Decomposition|The literature shows evidence of dynamic complementarity in the production of cognitive skill. This means that skill attained at earlier stages increases the productivity of inputs occurring later in the life of children. For educational inputs, however, the relation between their productivity and prior cognitive achievement might not always be positive. If the input has a low cognitive demand, more advantaged students will not necessarily benefit from it, but it can be productive among less advantaged children. This is the first study to explore this possibility. I find evidence of heterogeneity in the relation between preschool cognitive achievement and the effect of primary school inputs in Peru. I find dynamic complementarity but only in the upper quintile of the school quality distribution. In the lower 20% of this distribution, a raise in preschool skill reduces the productivity of school inputs. I also propose a decomposition strategy that accounts for complementarity between preschool skill and school inputs. I use it to measure the contribution of school influences to the cognitive skill gap observed between urban and rural children in Peru. I obtain an estimate for this contribution (37%) larger than that found in previous studies that relied on a linear production function. An important implication of this is that one does not need to wait until urban and rural children share similar levels of preschool skill to exploit the equalizing potential of school influences. It is not “too late” for rural children currently at school, despite their preschool skill deficits.
C18|Validity and Reliability of Contingent Valuation and Life Satisfaction Measures of Welfare: An Application to the Value of National Olympic Success|The contingent valuation method (CV) has long been used to estimate nonmarket values of environmental and other public goods and amenities. Recently, life satisfaction (LS) measures have been used to estimate nonmarket values. This paper empirically compares CV and LS measures of welfare. We elicit willingness-to-pay (WTP) estimates for medals won by Canadian athletes and LS measures using Canadian survey data collected before and after the 2010 Winter Olympic Games. These data permit comparative analyses of reliability and validity of CV and LS measures. Both exhibit econometric reliability. CV and LS WTP estimates for medals increases after the Olympics. CV measures of WTP exhibit temporal reliability but LS measures of welfare lack temporal reliability and are significantly greater than CV measures. Key Words: contingent valuation method; life satisfaction method; willingness-to-pay; validity reliability
C18|Inference on breakdown frontiers| A breakdown frontier is the boundary between the set of assumptions which lead to a specifi c conclusion and those which do not. In a potential outcomes model with a binary treatment, we consider two conclusions: First, that ATE is at least a specifi c value (e.g., nonnegative) and second that the proportion of units who bene fit from treatment is at least a speci c value (e.g., at least 50%). For these conclusions, we derive the breakdown frontier for two kinds of assumptions: one which indexes deviations from random assignment of treatment, and one which indexes deviations from rank invariance. These classes of assumptions nest both the point identifying assumptions of random assignment and rank invariance and the opposite end of no constraints on treatment selection or the dependence structure between potential outcomes. This frontier provides a quantitative measure of robustness of conclusions to deviations in the point identifying assumptions. We derive vN-consistent sample analog estimators for these frontiers. We then provide an asymptotically valid bootstrap procedure for constructing lower uniform confi dence bands for the breakdown frontier. As a measure of robustness, this confi dence band can be presented alongside traditional point estimates and con fidence intervals obtained under point identifying assumptions. We illustrate this approach in an empirical application to the e ffect of child soldiering on wages. We fi nd that conclusions are fairly robust to failure of rank invariance, when random assignment holds, but conclusions are much more sensitive to both assumptions for small deviations from random assignment.
C18|Identification of and Correction for Publication Bias|Some empirical results are more likely to be published than others. Selective publication leads to biased estimates and distorted inference. We propose two approaches for identifying the conditional probability of publication as a function of a study's results, the first based on systematic replication studies and the second on meta-studies. For known conditional publication probabilities, we propose bias-corrected estimators and confidence sets. We apply our methods to recent replication studies in experimental economics and psychology, and to a meta-study on the effect of the minimum wage. When replication and meta-study data are available, we find similar results from both.
C18|Economic History Goes Digital: Topic Modeling the Journal of Economic History|Digitization and computer science have established a whole new set of methods to analyze large collections of texts. One of these methods is particularly promising for economic historians: topic models, statistical algorithms that automatically infer themes from large collections of texts. In this article, I present an introduction to topic modeling and give a very first review on the research using topic models. I illustrate their capacity by applying them on 2.675 articles published in the Journal of Economic History between 1941 and 2016. This contributes to traditional research on the JEH and to current research on the cliometric revolution.
C18|Tests of Policy Interventions in DSGE Models|This paper considers tests of the effectiveness of a policy intervention, defined as a change in the parameters of a policy rule, in the context of a macroeconometric dynamic stochastic general equilibrium (DSGE) model. We consider two types of intervention, first the standard case of a parameter change that does not alter the steady state, and second one that does alter the steady state, e.g. the target rate of inflation. We consider two types of test, one a multi‐horizon test, where the postintervention policy horizon, H, is small and fixed, and a mean policy effect test where H is allowed to increase without bounds. The multi‐horizon test requires Gaussian errors, but the mean policy effect test does not. It is shown that neither of these two tests are consistent, in the sense that the power of the tests does not tend to unity as H→∞, unless the intervention alters the steady state. This follows directly from the fact that DSGE variables are measured as deviations from the steady state, and the effects of policy change on target variables decay exponentially fast. We investigate the size and power of the proposed mean effect test by simulating a standard three equation New Keynesian DSGE model. The simulation results are in line with our theoretical findings and show that in all applications the tests have the correct size; but unless the intervention alters the steady state, their power does not go to unity with H.
C18|Posterior Means and Precisions of the Coefficients in Linear Models with Highly Collinear Regressors|When there is exact collinearity between regressors, their individual coefficients are not identified, but given an informative prior their Bayesian posterior means are well defined. The case of high but not exact collinearity is more complicated but similar results follow. Just as exact collinearity causes non-identification of the parameters, high collinearity can be viewed as weak identification of the parameters, which we represent, in line with the weak instrument literature, by the correlation matrix being of full rank for a finite sample size T, but converging to a rank defficient matrix as T goes to infinity. This paper examines the asymptotic behavior of the posterior mean and precision of the parameters of a linear regression model for both the cases of exactly and highly collinear regressors. We show that in both cases the posterior mean remains sensitive to the choice of prior means even if the sample size is sufficiently large, and that the precision rises at a slower rate than the sample size. In the highly collinear case, the posterior means converge to normally distributed random variables whose mean and variance depend on the priors for coefficients and precision. The distribution degenerates to fixed points for either exact collinearity or strong identification. The analysis also suggests a diagnostic statistic for the highly collinear case, which is illustrated with an empirical example.
C18|Meta-Analysis and Publication Bias: How Well Does the FAT-PET-PEESE Procedure Work?|This paper studies the performance of the FAT-PET-PEESE (FPP) procedure, a commonly employed approach for addressing publication bias in the economics and business meta-analysis literature. The FPP procedure is generally used for three purposes: (i) to test whether a sample of estimates suffers from publication bias, (ii) to test whether the estimates indicate that the effect of interest is statistically different from zero, and (iii) to obtain an estimate of the overall mean effect. Our findings indicate that the FPP procedure performs well in the basic but unrealistic environment of “fixed effects”, where all estimates are assumed to derive from a single population value and sampling error is the only reason for why studies produce different estimates. However, when we study its performance in more realistic data environments, where there is heterogeneity in the population effects across and within studies, the FPP procedure becomes unreliable for the first two purposes, and is less efficient than other estimators when estimating overall mean effect. Further, hypothesis tests about the overall, mean effect cannot be trusted.
C18|Considering the Use of Stock and Flow Outcomes in Empirical Analyses: An Examination of Marriage Data|This paper fills an important void assessing how the use of stock outcomes as compared to flow outcomes may yield disparate results in empirical analyses, despite often being used interchangeably. We compare analyses using a stock outcome, marital status, to those using a flow outcome, entry into marriage, from the same dataset, the American Community Survey. This paper considers two different questions and econometric approaches using these alternative measures: the effect of the Affordable Care Act young adult provision on marriage using a difference-indifferences approach and the relationship between aggregate unemployment rates and marriage rates using a simpler ordinary least squares regression approach. Results from both analyses show stock and flow data yield divergent results in terms of sign and significance. Additional analyses suggest prior-period temporary shocks and migration may contribute to this discrepancy. These results suggest using caution when conducting analyses using stock data as they may produce false negative results or spurious false positive results, which could in turn give rise to misleading policy implications.
C18|Posterior Means and Precisions of the Coefficients in Linear Models with Highly Collinear Regressors|When there is exact collinearity between regressors, their individual coefficients are not identified, but given an informative prior their Bayesian posterior means are well defined. The case of high but not exact collinearity is more complicated but similar results follow. Just as exact collinearity causes non-identification of the parameters, high collinearity can be viewed as weak identification of the parameters, which we represent, in line with the weak instrument literature, by the correlation matrix being of full rank for a finite sample size T, but converging to a rank deficient matrix as T goes to infinity. This paper examines the asymptotic behaviour of the posterior mean and precision of the parameters of a linear regression model for both the cases of exactly and highly collinear regressors. We show that in both cases the posterior mean remains sensitive to the choice of prior means even if the sample size is sufficiently large, and that the precision rises at a slower rate than the sample size. In the highly collinear case, the posterior means converge to normally distributed random variables whose mean and variance depend on the priors for coefficients and precision. The distribution degenerates to fixed points for either exact collinearity or strong identification. The analysis also suggests a diagnostic statistic for the highly collinear case, which is illustrated with an empirical example.
C18|Predicting Financial Market Crashes Using Ghost Singularities|We analyse the behaviour of a non-linear model of coupled stock and bond prices exhibiting periodically collapsing bubbles. By using the formalism of dynamical system theory, we explain what drives the bubbles and how foreshocks or aftershocks are generated. A dynamical phase space representation of that system coupled with standard multiplicative noise rationalises the log-periodic power law singularity pattern documented in many historical financial bubbles. The notion of ‘ghosts of finite-time singularities’ is introduced and used to estimate the end of an evolving bubble, using finite-time singularities of an approximate normal form near the bifurcation point. We test the forecasting skill of this method on different stochastic price realisations and compare with Monte Carlo simulations of the full system. Remarkably, the former is significantly more precise and less biased. Moreover, the method of ghosts of singularities is less sensitive to the noise realisation, thus providing more robust forecasts.
C18|Policy relevance of applied economist: Examining sensitivity and inferences|It is assumed that research based on empirical data produces factual insight that can be used to guide evidence based policies. However, researchers may tend to specify models based on prior beliefs and construe results accordingly. In this paper, we argue that greater scrutiny is needed along the research process to acknowledge and communicate the limitations of research findings. To illustrate, we review two empirical papers from applied economists aimed at influencing policy. Each paper is analysed to identify how inferences based on prior beliefs are used to specify models and how this impacts the result. Additionally, consideration is given to the sensitivity of results under alternative assumptions. While we do find that the considered papers provide valuable knowledge to the field of agriculture economics, they fail in disclosing the limitations of their results to decision makers, thus undermining considerably their policy relevance. Finally, approaches to increase objectivity in empirical research are considered. Il est d’avis général que la recherche empirique produit des résultats factuels qui peuvent servir à l’élaboration de politiques publiques. Par contre, le chercheur est sujet à spécifier ses modèles, ainsi qu’à faire des hypothèses susceptibles de refléter ses a priori, influençant potentiellement, du fait, les résultats obtenus. Dans ce papier, nous suggérons qu’une plus grande rigueur tout au long du processus de recherche serait nécessaire afin de communiquer les faiblesses et limitations des résultats. Pour démontrer ce propos, nous considérons deux papiers publiés qui visaient à influencer des politiques agricoles. Pour chacun des papiers, nous identifions des hypothèses associées aux a priori des auteurs et comment ces derniers peuvent influencer les résultats. Par la suite, nous vérifions ces mêmes résultats sous des hypothèses alternatives. Bien que les deux papiers considérés apportent une certaine contribution, la non-divulgation d’hypothèses fortes susceptibles d’affecter les résultats réduit considérablement leur relevance pour les preneurs de décisions. Pour terminer, des suggestions pour améliorer l’objectivité des recherches empiriques sont brièvement discutées.
C18|System Priors for Econometric Time Series|The paper introduces “system priors”, their use in Bayesian analysis of econometric time series, and provides a simple and illustrative application. System priors were devised by Andrle and Benes (2013) as a tool to incorporate prior knowledge into an economic model. Unlike priors about individual parameters, system priors offer a simple and efficient way of formulating well-defined and economically-meaningful priors about high-level model properties. The generality of system priors are illustrated using an AR(2) process with a prior that most of its dynamics comes from business-cycle frequencies.
C18|Microfoundations for Structures and Evolution: Evidence from Experiments|The article discusses whether and to what extent experiments can contribute to a research paradigm based on the study of human behaviour in complex evolving environments and on the problem of asymmetric adjustment among different components of economic system along certain trajectories, focusing on the possibility that experimental evidence may represent an external consistency check on this type of heterodox modelling. It considers methodological issues related with the concept of validity, the evidence on rationality of human agents, and the possibility to identify a microfoundation alternative to homo oeconomicus, discussing the evidence on humans as strong reciprocators, as trusting individuals and as embedded in social norms.
C18|Beyond the Stars|It is frequent to hear in economic seminars or read in academic papers that an effect is economically significant or economically important. Yet, the economic literature is vague on what economic importance means and how it should be measured. In this paper, I show that existing measures of economic importance are flawed and misused. Using an axiomatic approach, I derive a new method to assess the economic importance of each variable in linear regressions. The new measure is interpreted as the percentage contribution of each explanatory variable to deviations in the dependent variable. As an illustration, the method is applied to the study of the causes of long-run economic development.
C18|La correlazione tra PD ed LGD nell’analisi del rischio di credito/The correlation between probability of default and loss given default in the credit risk analysis|The international regulation on banking developed by Basel Committee on Banking Supervision has set a simplified link between default probabilities and loss given default, avoiding to introduce the correlation. The scientific literature ha proposed many models that try to improve the Basel framework. This article examines the most important models proposed in the literature and apply two of them to aggregate data from the Bank of Italy.
C18|Counting What Counts: Africa’s Seminal Effort to Produce Harmonized Official Statistics on Governance, Peace and Security|(english) The paper documents the practical experience of eleven African national statistical offices that tested and eventually institutionalized a methodology for producing official harmonized statistics in the area of governance, peace and security statistics between 2012 and 2017. This took place whilst the rest of the world was still debating the rationale for including this new domain in the next global development agenda. It situates Africa’s successful GPS-SHaSA experiment in the context of the continent’s long-standing commitment to “achieve political sovereignty through data autonomy”. The paper also presents some strategic advantages of the GPS-SHaSA methodology, provides illustrations using selected targets of Africa’s Agenda 2063 and Sustainable Development Goal (SDG) 16 on how the four types of data generated by the methodology can inform policymaking. It finally concludes by identifying a number of methodological, institutional, financial and communicational investments necessary for GPS statistical production by NSOs to be sustainable, in Africa and beyond. _________________________________ (français) Cet article présente l’expérience concrète des instituts nationaux de la statistique (INS) de onze pays africains qui ont testé en pratique et institutionnalisé une méthodologie harmonisée pour produire des statistiques officielles dans le champ de la gouvernance, la paix et la sécurité (GPS) entre 2012 et 2017. Cette expérience s’est déroulée alors que le reste du monde était encore en train de débattre de la pertinence d’inclure ce nouveau champ thématique comme une composante à part entière de l’agenda global du développement post 2015. Le papier montre comment le succès de l’expérience GPS-SHaSA s’inscrit dans un engagement de longue date de l’Afrique pour « atteindre sa souveraineté politique à travers l’autonomie des données ». Il décrit également les avantages stratégiques de la méthodologie GPS-SHaSA, et offre quelques illustrations tirées de cibles particulières de l’Agenda 2063 de l’Afrique et de l’Objectif du Développement Durable (ODD) 16 pour montrer comment les quatre types de données produites par le projet peuvent informer les politiques publiques et le processus de décision. Il conclut en identifiant un certain nombre de défis méthodologiques, institutionnels, financiers et en termes de communication à relever pour que la production de données GPS par les INS puisse être durable, en Afrique et au-delà.
C18|Model economic phenomena with CART and Random Forest algorithms|"The aim of this paper is to highlight the advantages of algorithmic methods for economic research with quantitative orientation. We describe four typical problems involved in econometric modeling, namely the choice of explanatory variables, a functional form, a probability distribution and the inclusion of interactions in a model. We detail how those problems can be solved by using ""CART"" and ""Random Forest"" algorithms in a context of massive increasing data availability. We base our analysis on two examples, the identification of growth drivers and the prediction of growth cycles. More generally, we also discuss the application fields of these methods that come from a machine-learning framework by underlining their potential for economic applications."
C18|A time series paradox: Unit root tests perform poorly when data are cointegrated|Cointegration among times series paradoxically makes it more likely that a unit test will reject the unit root null hypothesis on the individual series. This occurs because at least one series in the system has a negative moving average component.
C18|Who creates jobs? Econometric modeling and evidence for Austrian firm level data|This paper provides evidence on the role of firm size and firm age for firm level net job creation in the Austrian economy between 1993 and 2013 and during the Great Recession. We propose a new estimation strategy based on a two-part model to decompose behavioral differences between exiting and surviving firms. Young firms contribute most to net job creation, despite high relative exit rates, due to high growth rates among young surviving firms. Small firms have similar job creation rates conditional on survival as large firms. Small firms' contribution to job creation is, however, smaller due to higher exit rates. The up-or-out dynamics characterizing less regulated economies such as the US also apply to the more regulated Austrian economy. During the Great Recession both the relative net job creation rate conditional on survival and the relative survival probability of young firms decreased. The relative contribution of small firms to net job creation, by contrast, increased due to increased relative job creation rates of small firms conditional on survival.
C18|Revisiting the evidence for cardinal treatment of ordinal variables|Well-being (life satisfaction or happiness) is a latent variable that is impossible to observe directly. Moreover, it does not have a unit of measurement. Hence, survey questionnaires usually ask people to rate their well-being in different domains. The common practice of comparing well-being by means of averages or linear regressions ignores the fact that well-being is an ordinal variable. Since data is ordinal, monotonic increasing transformations are permissible. We illustrate the sensitivity of empirical studies to monotonic transformations using examples that relate to well-known empirical papers, and provide two theoretical conditions that enable us to rank ordinal variables. In our examples, monotonic increasing transformations can in fact reverse the conclusion reached.
C18|A literature study for DEA applied to energy and environment|This study systematically summarizes previous research efforts on Data Envelopment Analysis (DEA) applied to energy and environment in the past four decades, including concepts and methodologies on DEA environmental assessment. Industrial developments are very important for all nations in terms of their economic prosperities. A problem is that the development produces various pollutions on air, water and other types of contaminations, all of which are usually associated with our health problems and climate changes. Thus, it is necessary for us to consider how to make a balance between economic success and pollution mitigation to maintain a high level of social sustainability in the world. It is widely considered that DEA is one of the methodologies to examine the level of sustainability. This study examines a recent research trend on DEA applications from 1980s to 2010s. Nowadays, many researchers have paid serious attention on how to combat various difficulties in the areas of energy and environment. As a result, the number of articles on DEA applications on energy and environment has dramatically increased, particularly after 2000s. However, it is true that DEA has strengths and drawbacks in the applications. Therefore, it is very important for us to carefully use DEA for guiding large policy issues and business strategies such as the global warming and climate change. An underlying premise of this study is that technology innovation in engineering and natural science may solve various problems by linking it with political and managerial efforts. The use of DEA provides a methodological linkage among them, so enhancing the practicality in mitigating problems due to climate change and environmental pollutions. This literature study, along with a summary on conceptual and methodological developments, provides us with guidelines for our future research works on DEA on energy and environment issues.
C18|A unisex stochastic mortality model to comply with EU Gender Directive|EU Gender Directive ruled out discrimination against gender in charging premium for insurance products. This prohibition prevents the use of the standard actuarial fairness principle to price life insurance products. According to current actuarial practice, unisex premiums are calculated with a simple weighting rule of the gender-specific life tables. This procedure is likely to violate portfolio fairness principles. Up to our knowledge, in the actuarial literature there is no unisex mortality model that respects the unisex fairness principle. This paper is the first attempt to fill this gap. First, we recall the notion of unisex fairness principle and the corresponding unisex fair premium. Then, we provide a unisex stochastic mortality model for the mortality intensity that is underlying the pricing of a life portfolio of females and males belonging to the same cohort. Finally, we calibrate the unisex mortality model using the unisex fairness principle. We find that the weighting coefficient between the males’ and females’ own mortalities depends mainly on the quote of portfolio relative to each gender, on the age, and on the type of insurance products. The knowledge of a proper unisex mortality model could help life insurance companies to better understanding the nature of the risk of a mixed portfolio.
C18|Intuitive and Reliable Estimates of the Output Gap from a Beveridge-Nelson Filter|The Beveridge-Nelson decomposition based on autoregressive models produces estimates of the output gap that are strongly at odds with widely held beliefs about transitory movements in economic activity. This is due to parameter estimates implying a high signal-to-noise ratio in terms of the variance of trend shocks as a fraction of the overall forecast error variance. When we impose a lower signal-to-noise ratio, the resulting Beveridge-Nelson filter produces a more intuitive estimate of the output gap that is large in amplitude and highly persistent, and it typically increases in expansions and decreases in recessions. Notably, our approach is also reliable in the sense of being subject to smaller revisions and predicting future output growth and inflation better than other trend-cycle decompositions that impose a low signal-to-noise ratio.
C18|Realized Stochastic Volatility Models with Generalized Gegenbauer Long Memory|In recent years fractionally differenced processes have received a great deal of attention due to their exibility in nancial applications with long memory. In this paper, we develop a new realized stochastic volatility (RSV) model with general Gegenbauer long memory (GGLM), which encompasses a new RSV model with seasonal long memory (SLM). The RSV model uses the information from returns and realized volatility measures simultaneously. The long memory structure of both models can describe unbounded peaks apart from the origin in the power spectrum. Forestimating the RSV-GGLM model, we suggest estimating the location parameters for the peaks of the power spectrum in the rst step, and the remaining parameters based on the Whittle likelihood in the second step. We conduct Monte Carlo experiments for investigating the nite sample properties of the estimators, with a quasi-likelihood ratio test of RSV-SLM model against theRSV-GGLM model. We apply the RSV-GGLM and RSV-SLM model to three stock market indices. The estimation and forecasting results indicate the adequacy of considering general long memory.
C18|Analysis of Revisions in Indian GDP Data|This paper studies constant price growth estimates of Indiaâ€™s annual GDP data in order to understand the revision policy adopted by the Central Statistics Office. The use of high-frequency indicators to prepare initial estimates overstates the growth of the economy, although at the aggregate level the difference between initial estimates and final revisions is low. At the sectoral level the extent of revision for almost all sectors is large and the magnitude and direction of the revision is unpredictable. The Central Statistical Office must address issues in data quality and revisions by (i) adopting a comprehensive revision policy, (ii) supplying information and data on high frequency indicators and (iii) adopting revision metrics to assess the quality of estimates.
C18|Systemic Risk in Financial Risk Regulation|The paper deals with the systemic risk concept which is important in the framework of modern risk regulatory systems in finance and insurance (the most actual examples are Basel III in finance and Solvency II in insurance). Two numerical applications of possible approaches are presented. The first one shows that marginal expected shortfall MES can be a useful risk measure when the systemic risk is examined using the Czech data represented by the composing index PX of Prague Stock Exchange. The second approach based on the common shock can be suitable for risk regulation in insurance.
C18|Firm Size and Stock Returns: A Meta-Analysis|A prominent factor used in most models predicting stock returns is firm size. Yet no consensus has emerged on the magnitude and stability of the size premium, with some researchers even questioning the usefulness of the factor. To take stock of the voluminous academic literature on the size premium, we collect 1,746 estimates of the effect of size on returns reported in 102 published studies and conduct the first meta-analysis on this topic. We find evidence of strong publication bias: researchers prefer to report estimates that are statistically significant and show a negative relation between size and returns, exaggerating the mean reported coefficient threefold. After correcting for the bias, we find that the literature suggests a size premium (the difference in annual stock returns on the smallest and largest capitalization quintile) of 1.72%. We also find that the premium was much larger prior to the publication of the first study on the topic. Moreover, we show that the intensity of publication bias has been decreasing over time.
C18|Il paradosso di S. Pietroburgo, una rassegna|"In 1738 Daniel Bernoulli presented for the first time a study with a functional relationship between utility and wealth. The goal was to provide a solution to a ""curious"" paradox on probability theory. Almost three centuries after the St. Petersburg paradox is still debated. Two strands of research can be identified: the first, both theoretically and with surveys, examines the reasons for the subjective behavior of a player who is not willing to offer, if not a modest sum, to play a game that has an infinite expected value. The second one is the analysis by computer simulations of a large number of games, where unexpected statistical distributions emerge. From all of the studies it turns out that not only players offer very modest figures, but also that no gambling house will ever offer a St. Petersburg game."
C18|Vulnerability to poverty revisited: flexible modeling and better predictive performance|This paper analyzes several modifications to improve a simple measure of vulnerability as expected poverty. Firstly, in order to model income, we apply distributional regression relating potentially each parameter of the conditional income distribution to the covariates. Secondly, we determine the vulnerability cutoff endogenously instead of defining a household as vulnerable if its probability of being poor in the next period is larger than 0.5. For this purpose, we employ the receiver operating characteristic curve that is able to consider prerequisites according to a particular targeting mechanism. Using long-term panel data from Germany, we build both mean and distributional regression models with the established 0.5 probability cutoff and our vulnerability cutoff. We find that our new cutoff considerably increases predictive performance. Placing the income regression model into the distributional regression framework does not improve predictions further but has the advantage of a coherent model where parameters are estimated simultaneously replacing the original three step estimation approach.
C18|Incidence of value added taxation on inequality: Evidence from Sri Lanka|No abstract is available for this item.
C18|An Oaxaca decomposition for nonlinear models|The widely used Oaxaca decomposition applies to linear models. Extending it to commonly used nonlinear models such as duration models is not straightforward. This paper shows that the original decomposition that uses a linear model can also be obtained by an application of the mean value theorem. By extension, this basis provides a means of obtaining a decomposition formula which applies to nonlinear models which are continuous functions. The detailed decomposition of the explained component is expressed in terms of what are usually referred to as marginal effects. Explicit formulae are provided for the decomposition of some nonlinear models commonly used in applied econometrics including binary choice, duration and Box-Cox models.
C18|Sovereign wealth funds’ cross-border investments: Assessing the role of country-level drivers and spatial competition|The aim of this paper is to identify the driving forces of cross-border investments emanating from Sovereign wealth funds and to test the existence of spatial competition among recipient countries. For this, we develop an original econometric framework that quantifies the role of spatial dependence in the location of investments, and that uses a modified version of the standard estimation procedure of spatial panel model, which accommodates the Inverse Hyperbolic Sine transformation of the dependent variable. This transformation copes with two critical features of net capital flows, namely an highly skewed distribution and the presence of zero and negative values. Using a large-scale database, we provide evidence of negative spatial dependence, investments in one country being on average at the expense of its neighbors.
C18|Origins of Spurious Long Memory|We consider a large class of structural change processes that generate spurious long memory. Among others, this class encompasses structural breaks as well as random level shift processes and smooth trends. The properties of these processes are studied based on a simple representation of their discrete Fourier transform. We find, that under very general conditions all of the models nested in this class generate poles in the periodogram at the zero frequency. These are of order $O(T)$, instead of the usual $O(T^2d)$ for long memory processes and $O(T^2)$ for a random walk. This order arises whenever both the mean changes and sample fractions at which they occur are non-degenerate, asymptotically.
C18|An Investigation Into the Stability of the Big-Five in Germany|This paper investigates the stability of the Big-Five personality traits based on the German Socio-Economic Panel (SOEP) from 2005, 2009, and 2013. The results indicate that the population means only show little variance over the eight year time frame. There is no link between age and mean-levels, and only minor changes of the mean-levels of the Big-Five over time for the working age population (25-64 years of age) in Germany. However, there are intra-individual changes which can partly be explained by adverse life events. They impact the Big-Five traits and thereby contradict the general finding of stability of the traits in the literature. Exploratory fixed effects wage estimations that exploit the intra-individual changes in the Big-Five find no significant effects for men but positive effects of agreeableness and conscientiousness on women's wages.
C18|Using Machine Learning To Model Interaction Effects In Education: A Graphical Approach|Educational systems can be characterized by a complex structure: students, classes and teachers, schools and principals, and providers of education. The added value of schools is likely influenced by all these levels and, especially, by interactions between them. We illustrate the ability of Machine Learning (ML) methods (Regression Trees, Random Forests and Boosting) to model this complex ‘education production function’ using Hungarian data. We find that, in contrast to ML methods, classical regression approaches fail to identify relevant nonlinear interactions such as the role of school principals to accommodate district size policies. We visualize nonlinear interaction effects in a way that can be easily interpreted.
C18|Model misspecification and bias for inverse probability weighting and doubly robust estimators|In the causal inference literature a class of semi-parametric estimators is called robust if the estimator has desirable properties under the assumption that at least one of the working models is correctly specified. A standard example is a doubly robust estimator that specifies parametric models both for the propensity score and the outcome regression. When estimating a causal parameter in an observational study the role of parametric models is often not to be true representations of the data generating process, instead the motivation for their use is to facilitate the adjustment for confounding, for example by reducing the dimension of the covariate vector, making the assumption of at least one true model unlikely to hold. In this paper we propose a crude analytical approach to study the large sample bias of estimators when all models are assumed to be approximations of the true data generating process, i.e., all models are misspecified. We apply our approach to three prototypical estimators, two inverse probability weighting (IPW) estimators, using a misspecified propensity score model, and a doubly robust (DR) estimator, using misspecified models for the outcome regression and the propensity score. To compare the consequences of the model misspecifications for the estimators we show conditions for when using normalized weights leads to a smaller bias compared to a simple IPW estimator. To analyze the question of when the use of two misspecified models are better than one we derive necessary and sucient conditions for when the DR estimator has a smaller bias than the simple IPW estimator and when it has a smaller bias than the IPW estimator with normalized weights. For most conditions in the comparisons, the covariance between the propensity score model error and the conditional outcomes plays an important role. The results are illustrated in a simulation study.
C18|Decompositions of Spatially Varying Quantile Distribution Estimates: The Rise and Fall of Tokyo House Prices|We extend Machado-Mata’s (2005) approach for decomposing the differences in the distribution of a dependent variable across two samples to account for location when the models are estimated using conditional parametric procedures. We find that a substantial portion of the change in the distribution of condominium prices in Tokyo between the rapid rise in prices in 1986 – 1990 and the sharp decline in 1991 – 1995 is due to changes in the values of the explanatory variables. Changes in the locations of sales serve to shift the price distribution to the left because later sales were more likely to be farther from downtown Tokyo, where prices are lower.
C18|Mortality, Life Expectancy, and Daily Air Pollution for the Frail Elderly in Three U.S. Cities|Perhaps the clearest indications of adverse environmental health effects have been responses to short-term excursions in ambient air quality or temperature as deduced from time-series analyses of exposed populations. However, current analyses cannot characterize the prior health status of affected individuals. We used data on daily elderly death counts, ambient air quality indicators, and temperature in Philadelphia, Chicago, and Atlanta to estimate the daily numbers of frail elderly at-risk of premature mortality, their remaining life expectancies, and environmental effects on life expectancy. These unobserved frail populations at-risk were estimated using the Kalman filter. Frail life expectancies range from 13-16 days. Despite substantial differences in demography and environmental conditions in the three cities, frail life expectancies and contributions of ambient conditions are remarkably similar. The loss in frail life expectancy is approximately 12 hours. Conventional time-series analyses of air pollution effects report similar increases in daily mortality associated with air pollution, but our new model shows that such acute environmental risks are limited to a small fraction of the elderly population whose deaths were imminent in any event. This paradigm shift offered by the Kalman filter provides context to previous estimates of acute associations of air pollution with mortality .
C18|Inference on Breakdown Frontiers|Given a set of baseline assumptions, a breakdown frontier is the boundary between the set of assumptions which lead to a specific conclusion and those which do not. In a potential outcomes model with a binary treatment, we consider two conclusions: First, that ATE is at least a specific value (e.g., nonnegative) and second that the proportion of units who benefit from treatment is at least a specific value (e.g., at least 50\%). For these conclusions, we derive the breakdown frontier for two kinds of assumptions: one which indexes relaxations of the baseline random assignment of treatment assumption, and one which indexes relaxations of the baseline rank invariance assumption. These classes of assumptions nest both the point identifying assumptions of random assignment and rank invariance and the opposite end of no constraints on treatment selection or the dependence structure between potential outcomes. This frontier provides a quantitative measure of robustness of conclusions to relaxations of the baseline point identifying assumptions. We derive $\sqrt{N}$-consistent sample analog estimators for these frontiers. We then provide two asymptotically valid bootstrap procedures for constructing lower uniform confidence bands for the breakdown frontier. As a measure of robustness, estimated breakdown frontiers and their corresponding confidence bands can be presented alongside traditional point estimates and confidence intervals obtained under point identifying assumptions. We illustrate this approach in an empirical application to the effect of child soldiering on wages. We find that sufficiently weak conclusions are robust to simultaneous failures of rank invariance and random assignment, while some stronger conclusions are fairly robust to failures of rank invariance but not necessarily to relaxations of random assignment.
C18|Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments|We propose strategies to estimate and make inference on key features of heterogeneous effects in randomized experiments. These key features include best linear predictors of the effects using machine learning proxies, average effects sorted by impact groups, and average characteristics of most and least impacted units. The approach is valid in high dimensional settings, where the effects are proxied by machine learning methods. We post-process these proxies into the estimates of the key features. Our approach is generic, it can be used in conjunction with penalized methods, deep and shallow neural networks, canonical and new random forests, boosted trees, and ensemble methods. It does not rely on strong assumptions. In particular, we don't require conditions for consistency of the machine learning methods. Estimation and inference relies on repeated data splitting to avoid overfitting and achieve validity. For inference, we take medians of p-values and medians of confidence intervals, resulting from many different data splits, and then adjust their nominal level to guarantee uniform validity. This variational inference method is shown to be uniformly valid and quantifies the uncertainty coming from both parameter estimation and data splitting. We illustrate the use of the approach with two randomized experiments in development on the effects of microcredit and nudges to stimulate immunization demand.
C18|What happens when econometrics and psychometrics collide? An example using the PISA data|International large-scale assessments such as PISA are increasingly being used to benchmark the academic performance of young people across the world. Yet many of the technicalities underpinning these datasets are misunderstood by applied researchers, who sometimes fail to take their complex sample and test designs into account. The aim of this paper is to generate a better understanding among economists about how such databases are created, and what this implies for the empirical methodologies one should (or should not) apply. We explain how some of the modeling strategies preferred by economists seem to be at odds with the complex test design, and provide clear advice on the types of robustness tests that are therefore needed when analyzing these datasets. In doing so, we hope to generate a better understanding of international large-scale education databases, and promote better practice in their use.
C18|Comparing access for all: disability-induced accessibility disparity in Lisbon|Abstract It is well known that individual impairments create disparities in the accessibility of individuals to opportunities, lengthening the distances or time needed to reach them or even completely impeding access. However, the accurate calculation and representation of these disparities remain a major challenge for urban and transportation planners. In this paper, we adopt the concept of accessibility disparity, originally applied to measure place accessibility by different modes of transport, to measure and represent the accessibility of individuals with physical disabilities compared to those without disabilities. We use spatial network analysis to calculate spatial connectivity and the accessibility of Lisbon’s city center, revealing what we define as ‘disability-induced accessibility disparity’. Our results reveal not only the locations responsible for reduced accessibility, i.e., barriers and/or deterrents to movement, but also how much any given disparity reduces the accessibility of an individual, allowing the use of this methodology by planners to identify critical areas and to design inclusive public spaces.
C18|The ‘wrong skewness’ problem: a re-specification of stochastic frontiers|Abstract In this paper, we study the ‘wrong skewness phenomenon’ in stochastic frontiers (SF), which consists in the observed difference between the expected and estimated sign of the asymmetry of the composite error, and causes the ‘wrong skewness problem’, for which the estimated inefficiency in the whole industry is zero. We propose a more general and flexible specification of the SF model, introducing dependences between the two error components and asymmetry (positive or negative) of the random error. This re-specification allows us to decompose the third moment of the composite error into three components, namely: (i) the asymmetry of the inefficiency term; (ii) the asymmetry of the random error; and (iii) the structure of dependence between the error components. This decomposition suggests that the wrong skewness anomaly is an ill-posed problem, because we cannot establish ex ante the expected sign of the asymmetry of the composite error. We report a relevant special case that allows us to estimate the three components of the asymmetry of the composite error and, consequently, to interpret the estimated sign. We present two empirical applications. In the first dataset, where the classic SF has the wrong skewness, an estimation of our model rejects the dependence hypothesis, but accepts the asymmetry of the random error, thus justifying the sign of the skewness of the composite error. More importantly, we estimate a non-zero inefficiency, thus solving the wrong skewness problem. In the second dataset, where the classic SF does not yield any anomaly, an estimation of our model provides evidence for the presence of dependence. In such situations, we show that there is a remarkable difference in the efficiency distribution between the classic SF and our class of models.
C18|Asymmetric volatility connectedness on the forex market|We show how bad and good volatility propagate through the forex market, i.e., we provide evidence for asymmetric volatility connectedness on the forex market. Using high-frequency, intra-day data of the most actively traded currencies over 2007–2015 we document the dominating asymmetries in spillovers that are due to bad, rather than good, volatility. We also show that negative spillovers are chiefly tied to the dragging sovereign debt crisis in Europe while positive spillovers are correlated with the subprime crisis, different monetary policies among key world central banks, and developments on commodities markets. It seems that a combination of monetary and real-economy events is behind the positive asymmetries in volatility spillovers, while fiscal factors are linked with negative spillovers.
C18|A solution for multicollinearity in stochastic frontier production function models|This paper considers the problem of collinearity among inputs in a stochastic frontier production model, an issue that has received little attention in the econometric literature. To address this problem, a principal-component-based solution is proposed, which allows carrying out a joint interpretation of technical efficiency and the technology parameters of the model. Applications of the method to simulated and real data show its usability and effective performance
C18|Monotonicity of Probit Weights|We demonstrate that the probit weight function is U-shaped on R, i.e., it is decreasing on (infinity,0), strictly increasing on [0,infinity), and strictly convex on R. Knowledge of the shape of the probit weight function can resolve any confusion that may arise from a result in the classic paper of Sampford (1953).
C18|Alternative Graphical Representations of the Confidence Intervals for the Structural Coefficient from Exactly Identified Two-Stage Least Squares|In the case of the just identified model the exact distribution of the two-stage least squares (2SLS) estimator of the coefficient of the endogenous regressor is a ratio of two normally distributed random variables. Robert Basmann (1960, 1961, 1974) used Fieller’s 1932 result to derive the density function of the estimator. In this paper we employ a novel graphical exposition of Fieller’s subsequent 1954 technique to approximate the confidence interval for the ratio. This approach involves the construction of a constraint shape that provides an insight as to how the characteristics of the reduced form estimates influences the comparison of the Delta and the Fieller confidence intervals. In particular, the degree of endogeneity and the relevance of the instrument can be shown to have a direct influence on these shapes. An example application of this approach is then applied to consider two specifications of an exactly identified model.
