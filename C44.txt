C44|Forecaster’s utility and forecasts coherence|I provide general frequentist framework to elicit the forecaster’s expected utility based on a Lagrange Multiplier-type test for the null of locality of the scoring rules associated to the probabilistic forecast. These are assumed to be observed transition variables in a nonlinear autoregressive model to ease the statistical inference. A simulation study reveals that the test behaves consistently with the requirements of the theoretical literature. The locality of the scoring rule is fundamental to set dating algorithms to measure and forecast probability of recession in US business cycle. An investigation of Bank of Norway’s forecasts on output growth leads us to conclude that forecasts are often suboptimal with respect to some simplistic benchmark if forecaster’s reward is not properly evaluated.
C44|Bayesian Comparative Statics|We study how information affects equilibria and welfare in games. For an agent, more precise information about an unknown state of the world leads to a mean-preserving spread of beliefs. We provide necessary and sufficient conditions to obtain either a non-increasing mean or a non-decreasing-mean spread of actions whenever information precision increases for at least one agent. We apply our Bayesian comparative statics framework to study informational externalities in strategic environments. In persuasion games, we derive sufficient conditions that lead to extremal disclosure of information. In oligopolistic markets, we characterize the incentives of firms to share information. In macroeconomic models, we show that information not only drives the amplitude of business cycles but also affects aggregate output.
C44|Mildly Explosive Dynamics in U.S. Fixed Income Markets|We use a recently developed right-tail variation of the Augmented Dickey-Fuller unit root test to identify and date-stamp periods of mildly explosive behavior in the weekly time series of seven U.S. fixed income yield spreads between September 2002 and January 2015. We find statistically significant evidence of such behavior in six of these spreads. Mild explosivity migrates from short-term funding markets to more volatile medium- and long-term markets during the Great Financial Crisis. For some markets, we statistically validate the conjecture, originally suggested by Gorton (2009a,b), that the financial panic of 2007 initially migrated from segments of the ABX market to other U.S. fixed income markets.
C44|Multi-criteria decision analysis for health technology assessment: addressing methodological challenges to improve the state of the art|Abstract Background Multi-criteria decision analysis (MCDA) concepts, models and tools have been used increasingly in health technology assessment (HTA), with several studies pointing out practical and theoretical issues related to its use. This study provides a critical review of published studies on MCDA in the context of HTA by assessing their methodological quality and summarising methodological challenges. Methods A systematic review was conducted to identify studies discussing, developing or reviewing the use of MCDA in HTA using aggregation approaches. Studies were classified according to publication time and type, country of study, technology type and study type. The PROACTIVE-S approach was constructed and used to analyse methodological quality. Challenges and limitations reported in eligible studies were collected and summarised; this was followed by a critical discussion on research requirements to address the identified challenges. Results 129 journal articles were eligible for review, 56% of which were published in 2015–2017; 42% focused on pharmaceuticals; 36, 26 and 18% reported model applications, issues regarding MCDA implementation analyses, and proposing frameworks, respectively. Poor compliance with good methodological practice (
C44|Exact tests on returns to scale and comparisons of production frontiers in nonparametric models|When benchmarking production units by non-parametric methods like data envelopment analysis (DEA), an assumption has to be made about the returns to scale of the underlying technology. Moreover, it is often also relevant to compare the frontiers across samples of producers. Until now, no exact tests for examining returns to scale assumptions in DEA, or for test of equality of frontiers, have been available. The few existing tests are based on asymptotic theory relying on large sample sizes, whereas situations with relatively small samples are often encountered in practical applications. In this paper we propose three novel tests based on permutations. The tests are easily implementable from the algorithms provided, and give exact significance probabilities as they are not based on asymptotic properties. The first of the proposed tests is a test for the hypothesis of constant returns to scale in DEA. The others are tests for general frontier differences and whether the production possibility sets are, in fact, nested. The theoretical advantages of permutation tests are that they are appropriate for small samples and have the correct size. Simulation studies show that the proposed tests do, indeed, have the correct size and furthermore higher power than the existing alternative tests based on asymptotic theory.
C44|Forecasting economic decisions under risk: The predictive importance of choice-process data|We investigate various statistical methods for forecasting risky choices and identify important decision predictors. Subjects (n=44) are presented a series of 50/50 gambles that each involves a potential gain and a potential loss, and subjects can choose to either accept or reject a displayed lottery. From this data, we use information on 8800 individual lottery gambles and specify four predictor-sets that include different combinations of input categories: lottery design, socioeconomic characteristics, past gambling behavior, eye-movements, and various psychophysiological measures that are recorded during the first three seconds of lottery-information processing. The results of our forecasting experiment show that choice-process data can effectively be used to forecast risky gambling decisions; however, we find large differences among models’ forecasting capabilities with respect to subjects, predictor-sets, and lottery payoff structures.
C44|Optimization of age-structured bioeconomic model: recruitment, weight gain and environmental effects|More and more fishery researchers begin to acknowledge that one-dimensional biomass models may omit key information when generating management guidelines. For the more complicated age-structured models, numerous parameters require a proper estimation or a reasonable assumption. In this paper, the effects of recruitment patterns and environmental impacts on the optimal exploitation of a fish population are investigated. Based on a discrete-time age-structured bioeconomic model of Northeast Atlantic mackerel, we introduce the mechanisms that generate 6 scenarios of the problem. Using the simplest scenario, optimizations are conducted under 8 different parameter combinations. Then, the problem is solved for each scenario and simulations are conducted with constant fishing mortalities. It is found that a higher environmental volatility leads to more net profits but with a lower probability of achieving the mean values. Any parameter combination that favours the older fish tends to lend itself to pulse fishing pattern. The simulations indicate that a constant fishing mortality around 0.06 performs the best. A comparison between the optimal and the historical harvest shows that for more than 70% of the time, the optimal exploitation precedes the historical one, leading to 43% higher net profit and 34% lower fishing cost.
C44|Greed is good: from super-harvest to recovery in a stochastic predator-prey system|This paper demonstrates a predator-prey system of cod and capelin that confronts a possible scenario of prey extinction under the first-best policy in a stochastic world. We discover a novel ‘super-harvest’ phenomenon that the optimal harvest of the predator is even higher than the myopic policy, or the ‘greedy solution’, on part of the state space. This intrinsic attempt to harvest more predator to protect the prey is a critical evidence supporting the idea behind ‘greed is good’. We ban prey harvest and increase predator harvest in a designated state space area based on the optimal policy. Three heuristic recovery plans are generated following this principle. We employ stochastic simulations to analyse the probability of prey recovery and evaluate corresponding costs in terms of value loss percentage. We find that the alternative policies enhance prey recovery rates mostly around the area of 50% recovery probability under the optimal policy. When we scale up the predator harvest by 1.5, the prey recovery rate escalates for as much as 28% at a cost of 5% value loss. We establish two strategies: modest deviation from the optimal on a large area or intense measure on a small area. It seems more cost-effective to target the stock space with accuracy than to simply boost predator harvest when the aim is to achieve remarkable improvement of prey recovery probability.
C44|Autonomous vessels: State of the art and potential opportunities in logistics|The growth in technology on autonomous transportation systems is currently motivating a number of research initiatives. This paper first presents a survey of the literature on autonomous marine vessels in general. By identifying the main research interests in this field, we define nine thematic categories. The collected articles are then classified according to these categories. We show that research on autonomous vessels has increased dramatically in the past decade. However, most of the published articles have focused on navigation control and safety issues. Studies regarding other topics, such as transport and logistics, are very limited. While our main interest is the literature on autonomous vessels, we contrast its development with respect to the literature on autonomous cars so as to have a better understanding about the future potentials in the research on autonomous vessels. The comparison shows that there are great opportunities for research about transportation and logistics with autonomous vessels. Finally, several potential research areas regarding logistics with autonomous vessels are proposed. As the technology behind remote-controlled or autonomous ships is maturing rapidly, we believe that it is already time for researchers in the field to start looking into future water-borne transport and logistics using autonomous vessels.
C44|An Iterative Approach to Ill-Conditioned Optimal Portfolio Selection|Covariance matrix of the asset returns plays an important role in the portfolio selection. A number of papers is focused on the case when the covariance matrix is positive definite. In this paper, we consider portfolio selection with a singular covariance matrix. We describe an iterative method based on a second order damped dynamical systems that solves the linear rank-deficient problem approximately. Since the solution is not unique, we suggest one numerical solution that can be chosen from the iterates that balances the size of portfolio and the risk. The numerical study confirms that the method has good convergence properties and gives a solution as good as or better than the constrained least norm Moore-Penrose solution. Finally, we complement our result with an empirical study where we analyze a portfolio with actual returns listed in S&P 500 index.
C44|Publication Bias and Editorial Statement on Negative Findings|"In February 2015, the editors of eight health economics journals sent out an editorial statement which aims to reduce the extent of specification searching and reminds referees to accept studies that: ""have potential scientific and publication merit regardless of whether such studies' empirical findings do or do not reject null hypotheses"". Guided by a pre-analysis, we test whether the editorial statement decreased the extent of publication bias. Our differences-in-differences estimates suggest that the statement decreased the proportion of tests rejecting the null hypothesis by 18 percentage points. Our findings suggest that incentives may be aligned to promote more transparent research."
C44|A Bayesian approach for correcting bias of data envelopment analysis estimators|The validity of data envelopment analysis (DEA) efficiency estimators depends on the robustness of the production frontier to measurement errors, specification errors and the dimension of the input-output space. It has been proven that DEA estimators, within the interval (0, 1], are overestimated when finite samples are used while asymptotically this bias reduces to zero. The non-parametric literature dealing with bias correction of efficiencies solely refers to estimators that do not exceed one. We prove that efficiency estimators, both lower and higher than one, are biased. A Bayesian DEA method is developed to correct bias of efficiency estimators. This is a two-stage procedure of super-efficiency DEA followed by a Bayesian approach relying on consistent efficiency estimators. This method is applicable to ‘small’ and ‘medium’ samples. The new Bayesian DEA method is applied to two data sets of 50 and 100 E.U. banks. The mean square error, root mean square error and mean absolute error of the new method reduce as the sample size increases.
C44|Enhancing Managerial Decision-Making Through Multicriteria Modeling|The monograph constitutes a crowning of research led in the field of particular methodology of management science, in the field of enhancing managerial decision-making sub-discipline in frames of the practical stream of the management science discipline. The monograph is a development of the research project in which the elaboration of a scientific method for the enhancement of managerial decision-making processes through the Modular Multicriteria Managerial Decision-Making Model (MMUMADEMM) has been proposed.
C44|Optimal Control of the Parameters of the Production Line|The problem of optimal control of the parameters of the production flow line - stocks (work in process) and the rate of processing of objects of labour for a technological operation is considered. The article presents a mathematical formulation of the problem of controlling the parameters of a production line with restrictions on work in progress and the speed of machining parts for each technological operation. The control program is determined by the specified quality criteria. An example of the calculation of the optimal control for the production line parameters is presented.
C44|A note on the UEFA Euro 2020 qualifying play-offs|The 2018-19 UEFA Nations League is the inaugural season of this competition, which provides the basis of the seeding for the 55 men's national football teams participating in the qualification process of the UEFA European Championship 2020. In addition, unlike previous editions, the teams for the play-offs are also selected with the consideration of their performance in the 2018-19 UEFA Nations League. Thus 16 teams, which failed to qualify through their group, are divided into four paths of four teams each according to a complicated rule because the places vacated by the 20 directly qualified teams should be filled. We provide a critical examination of the relevant UEFA regulations and show that the articles may contradict to each other and may lead to an unfair formulation of play-off paths. Straightforward solutions for both problems are suggested.
C44|Распределенная Динамическая Pde-Модель Программного Управления Загрузкой Технологического Оборудования Производственной Линии<BR>[Distributed dynamic PDE-model of a program control by utilization of the technological equipment of production line]|В работе необходимо рассмотреть проектирование системы управления параметрами производственной линии для предприятия с поточным методом организации производства. Методика. Производственная ли-ния предприятия с поточным методом организации производства – это сложная динамическая распреде-ленная система. Технологический маршрут изготовления изделия для многих современных предприятий содержит несколько сотен технологических операций, в межоперационном заделе каждой из которых со-держатся тысячи изделий, ожидающих обработку. Технологические маршруты разных деталей одного вида изделий пересекаются. Это приводит к тому, что распределение предметов труда вдоль технологического маршрута оказывает значительное влияние на пропускную способность производственной линии. Для опи-сания таких систем введен новый класс моделей производственных линий (PDE-model). Модели этого клас-са используют уравнения в частных производных для описания поведения потоковых параметров произ-водственной линии. В данной статье построена PDE-модель производственной линии, потоковые парамет-ры которой зависят от величины коэффициента загрузки технологического оборудования для каждой опе-рации.
C44|Monotonic Estimation for the Survival Probability over a Risk-Rated Portfolio by Discrete-Time Hazard Rate Models|Monotonic estimation for the survival probability of a loan in a risk-rated portfolio is based on the observation arising, for example, from loan pricing that a loan with a lower credit risk rating is more likely to survive than a loan with a higher credit risk rating, given the same additional risk covariates. Two probit-type discrete-time hazard rate models that generate monotonic survival probabilities are proposed in this paper. The first model calculates the discrete-time hazard rate conditional on systematic risk factors. As for the Cox proportion hazard rate model, the model formulates the discrete-time hazard rate by including a baseline component. This baseline component can be estimated outside the model in the absence of model covariates using the long-run average discrete-time hazard rate. This results in a significant reduction in the number of parameters to be otherwise estimated inside the model. The second model is a general form model where loan level factors can be included. Parameter estimation algorithms are also proposed. The models and algorithms proposed in this paper can be used for loan pricing, stress testing, expected credit loss estimation, and modeling of the probability of default term structure.
C44|Monotonic Estimation for Probability Distribution and Multivariate Risk Scales by Constrained Minimum Generalized Cross-Entropy|Minimum cross-entropy estimation is an extension to the maximum likelihood estimation for multinomial probabilities. Given a probability distribution {r_i }_(i=1)^k, we show in this paper that the monotonic estimates {p_i }_(i=1)^k for the probability distribution by minimum cross-entropy are each given by the simple average of the given distribution values over some consecutive indexes. Results extend to the monotonic estimation for multivariate outcomes by generalized cross-entropy. These estimates are the exact solution for the corresponding constrained optimization and coincide with the monotonic estimates by least squares. A non-parametric algorithm for the exact solution is proposed. The algorithm is compared to the “pool adjacent violators” algorithm in least squares case for the isotonic regression problem. Applications to monotonic estimation of migration matrices and risk scales for multivariate outcomes are discussed.
C44|Examining eco-efficiency convergence of European Industries.The existence of technological spillovers within a metafrontier framework|European policies regarding global warming have been outspread the last few decades with many initiatives for industrial production process. In this paper we model eco-efficiency performance under a meta-frontier framework for 14 industries from the manufacturing sector from 27 European countries over the 1995-2011 period. The utilization of NOx, SOx, CO2, CH4, N2O, CO, NMVOC and NH3 as undesirable outputs and GVA as the desirable represent the impact of of economic activities on the environment. In the first stage, we estimate eco-efficiency using the conventional Directional Distance Function (DDF) as well as the non-radial DDF approach. In the second stage of analysis, we investigate the existence of conditional and unconditional convergence according to several methodologies. Our eco-efficiency estimates provide a distinct behavior for energy intensive European industries. Moreover, a decline occurs for the majority of them. In addition, our results using distributional dynamics approach and the recent approach of Philips and Sul (2007) supports the non-convergence hypothesis and the creation of distinct clubs. Finally, the establishment of a catch up index indicate an increase in a speed of convergence.
C44|Дискретно-Событийная Модель Расчета Продолжительного Производственного Цикла Изготовления Партии Деталей<BR>[Discrete-Eventing Model Of Calculation Of The Duration Of The Production Cycle Of Manufacturing A Part Of Products]|The method of calculating the duration of the production cycle for manufacturing a batch of parts is considered. The production cycle of manufacturing the batch of parts is one of the main characteristics of the production system. It is used to calculate the important indicators of planning the production activity of the factory. At present, the task of calculating the duration of the production cycle for unsynchronized production lines remains relevant. The task takes on special relevance in the case when the processing time of a work item in a technological operation is a random quantity. The present work is devoted to the analysis of this case. To derive the equation of motion of labour objects for technological operations, a discrete-event model of the production process is used. The structure of the processing time of an object of labour on a technological operation is considered. The source of change in the value of inter-operational stocks at each technological operation is shown. The interrelation of the trajectories of the previous and after subjects of labour is analyzed. The equation of motion of a subject of labour on technological operations is recorded, taking into account the inter-operational stocks. Methods for its solution are proposed. Conditions for the applicability of the obtained results are considered. The analysis of the machine time spent on calculating the duration of the production cycle of manufacturing the batch of products for the factory of the semiconductor industry was carried out. Prospects for research have been determined.
C44|Smart and Green Buildings Features in the Decision-Making Hierarchy of Office Space Tenants: An Analytic Hierarchy Process Study|In the paper, we investigate the role of smart building or green building innovations on the Polish real estate market using the Analytic Hierarchy Process (AHP) method on the group of experts (consultants, managers, brokers) that are active on the office market in Krakow (study area). The findings point towards the highest relevance of the localisation factor, but also at the relatively low importance of the features of a sustainable building: building automation and information technology systems, as well as energy efficiency or certification. The findings suggest that despite the growing interest in sustainability and technological advancement amongst office market participants in Krakow, the relative importance of smart and green building features in their decision-making processes is relatively low. The study has some interesting practical implications. The knowledge regarding the relative importance of decision criteria can be valuable for developers and investors because the anticipation of tenants’ expectations is directly linked with return on investment and innovation premiums.
C44|The Ordinal Input for Cardinal Output Approach of Non-compensatory Composite Indicators: The PROMETHEE Scoring Method|Despite serious threats as to their soundness, the adoption of composite indicators is constantly growing alongside their popularity, especially when it comes to their adoption in policy-making exercises. This study presents a robust non-compensatory approach to construct composite indicators mainly based, at least with respect to the basic ideas, on the classic Borda scoring procedure. The non- compensatory indicators we are proposing can be seen as aggregation of ordinal non-compensatory preferences between considered units supplying a numerical cardinal comprehensive evaluation. For this reason we define our methodology, the ordinal input for cardinal output non-compensatory approach for composite indicators. To take into account hesitation, imprecision and ill-determination in defining preference relations with respect to the elementary indices, we adopt the PROMETHEE methods, whose net flow score can be seen as an extension to the fuzzy preferences of the Borda score. Moreover, we systematically deal with robustness of the results with respect to weighting and parameters such as indifference and preference thresholds, permitting to define preference relations of elementary indices. In this regard, we couple PROMETHEE methods with the recently proposed σ−μ approach, which permits to explore the whole domain of feasible preference parameters mentioned above, giving a synthetic representation of the distribution of the values assumed by the composite indicators in terms of mean, μ, and standard deviation, σ. μ and σ are also used to define a comprehensive overall composite indicator. Finally, we enrich the results of this analysis with a set of graphical visualizations based on principal component analysis applied to the PROMETHEE methods with the GAIA technique, providing better understanding of the outcomes of our approach. To illustrate its assets, we provide a case study of inclusive development evaluation, based on the data of the homonymous report produced by the World Economic Forum.
C44|The optimal control problem for output material flow on a conveyor belt with input accumulating bunker|The article is devoted to the synthesis of optimal control of the conveyor belt with the accumulating input bunker. Much attention is given to the model of the conveyor belt with a constant speed of the belt. Simulation of the conveyor belt is carried out in the one-moment approximation using partial differential equations. The conveyor belt is represented as a distributed system. The used PDE-model of the conveyor belt allows determining the state of the flow parameters for a given technological position as a function of time. We consider the optimal control problem for flow parameters of the conveyor belt. The problem consists in ensuring the minimum deviation of the output material flow from a given target amount. The control is carried out by the material flow amount, which comes from the accumulating bunker into the conveyor belt input. In the synthesis of optimal control, we take into account the limitations on the size of the accumulating bunker, as well as on both max and min amounts of control. We construct optimal control of the material flow amount coming from the accumulating bunker. Also, we determine the conditions to switch control modes and estimate time period between the moments of the switching.
C44|Statistical Inference for Aggregation of Malmquist Productivity Indices|The Malmquist Productivity Index (MPI) has gained popularity amongst studies on dynamic change of productivity of decision making units (DMUs). In practice, this index is frequently reported at aggregate levels (e.g., public and private rms) in the form of simple equally-weighted arithmetic or geometric means of individual MPIs. A number of studies have emphasized that it is necessary to account for the relative importance of individual DMUs in the aggregations of indices in general and of MPI in particular. While more suitable aggregations of MPIs have been introduced in the literature, their statistical properties have not been revealed yet, preventing applied researchers from making essential statistical inferences such as con dence intervals and hypothesis testing. In this paper, we will ll this gap by developing a full asymptotic theory for an appealing aggregation of MPIs. On the basis of this, some meaningful statistical inferences are proposed and their nite-sample performances are veri ed via extensive Monte Carlo experiments.
C44|Mining Of Classification Trees To Analyze A Multidimensional Phenomenon|During periods of remarkable trade openness, increase income inequality in many countries. This paper analyzes how factors that influence inequality due to commercial globalization interact each other. For which a reliable Classifier Tree -selected through a modeling process of bootstrapping- is built, it has 14 knowledge rules and classifies 84% of the observations correctly. This model indicates that inequality?s changes into a country, due greater economic integration, depend principally on the labor market? structure ?in agricultural countries and urbanization processes (industrialization) it reduces depending in turn on the rule of law; on the other hand, in countries with a strong service sector and good trade terms it increases in periods of stagnation or with low levels of high technology exports.
C44|Statistical Tests for Cross-Validation of Kriging Models|We derive new statistical tests for leave-one-out cross-validation of Kriging models. Graphically, we present these tests as scatterplots augmented with confi…dence intervals. We may wish to avoid extrapolation, which we de…fine as prediction of the output for a point that is a vertex of the convex hull of the given input combinations. Moreover, we may use bootstrapping to estimate the true variance of the Kriging predictor. The resulting tests (with or without extrapolation or bootstrapping) have type-I and type-II error probabilities, which we estimate through Monte Carlo experiments. To illustrate the application of our tests, we use an example with two inputs and the popular borehole example with eight inputs.
C44|Accounting for structural patterns in construction of value functions: a convex optimization approach|A common approach in decision analysis is to infer a preference model in form of a value function from the holistic decision examples. This paper introduces an analytical framework for joint estimation of preferences of a group of decision makers through uncovering structural patterns that regulate general shapes of individual value functions. We investigated the impact of incorporating information on such structural patterns governing the general shape of value functions on the preference estimation process through an extensive simulation study and analysis of real decision makers’ preferences. We found that accounting for structural patterns at the group level vastly improves predictive performance of the constructed value functions at the individual level. This finding is confirmed across a wide range of decision scenarios. Moreover, improvement in the predictive performance is larger when considering the entire ranking of alternatives rather than the top choice, but it is not affected by the level of heterogeneity among the decision makers. We also found that improvement in the predictive performance in ranking problems is independent of individual characteristics of decision makers, and is larger when smaller amount of preference information is available, while for choice problems this improvement is individual-specific and invariant to the amount of input preference information.
C44|Cost-Effective Clinical Trial Design: Application of a Bayesian Sequential Stopping Rule to the ProFHER Pragmatic Trial|We investigate value-based clinical trial design by applying a Bayesian decisiontheoretic model of a sequential experiment to data from the ProFHER pragmatic trial. In the first applied analysis of its kind to use research cost data, we show that the model’s stopping policy would have stopped the trial early, saving about 5% of the research budget (approximately £73,000). A bootstrap analysis based on generating resampled paths from the trial data suggests that the trial’s expected sample size could have been reduced by approximately 40%, saving an expected 15% of the budget, with 93% of resampled paths making a decision consistent with the result of the trial itself. Results show how substantial benefits to trial cost stewardship may be achieved by accounting for research costs in defining the trial’s stopping policy and active monitoring of trial data as it accumulates.
C44|Beyond quantified ignorance: Rebuilding rationality without the bias bias|If we reassess the rationality question under the assumption that the uncertainty of the natural world is largely unquantifiable, where do we end up? In this article the author argues that we arrive at a statistical, normative, and cognitive theory of ecological rationality. The main casualty of this rebuilding process is optimality. Once we view optimality as a formal implication of quantified uncertainty rather than an ecologically meaningful objective, the rationality question shifts from being axiomatic/probabilistic in nature to being algorithmic/ predictive in nature. These distinct views on rationalitymirror fundamental and longstanding divisions in statistics.
C44|Begutachtungsverfahren nach Zahl, Gewichtung und Fehlern der Gutachten|Begutachtungsverfahren werden analysiert anhand der Zahl der Gutachten, der Entscheidungsregel bei abweichenden Gutachtenempfehlungen und den Annahme- und Ablehnungsfehlern der Gutachten. Begutachtungsverfahren können grundsätzlich strenger oder lockerer sein, was entsprechend zu weniger fehlerhaften Annahmen oder Ablehnungen führt.
C44|Coexistence of Symmetry Properties for Bayesian Confirmation Measures|"Many Bayesian Confirmation Measures have been proposed so far. They are used to assess the degree to which an evidence (or premise) E supports or contradicts an hypothesis (or conclusion) H, making use of prior probability P(H), posterior probability P(H|E) and of probability of evidence P(E). Many kinds of comparisons of those measures have already been made. Here we focus on symmetry properties of confirmation measures, which are partly inspired by classical geometric symmetries. We define symmetries relating them to the dihedral group of symmetries of the square, determining the symmetries that can coexist and reconsidering desirable/undesirable symmetry properties for a Bayesian Confirmation Measure."
C44|An EOQ model for multiple products with varying degrees of substitutability|In this paper, the authors present an EOQ model with substitutions between products and a dynamic inventory replenishment policy. The key assumption is that many products in the market are substitutable at different levels, and that, in most cases, a customer who discovers that a desired product is unavailable will choose to consume a product with similar attributes or functionality, rather than not purchase at all. Therefore, given a firm that stocks multiple substitutable products, the authors assume that a stock out of one product has a direct impact on other products' demand. The main purpose of the model is to enable inventory managers to develop ordering policies that ensures that, in the event that a specific product runs out and cannot be replenished due to unforeseen circumstances, the consequent increase in demand for related products will not cause further stock out incidents. To this end, the authors introduce a dependency factor, a variable that indicates the level of dependency, or correlation, between one product and another. The dependencies among the various products offered by the firm are embedded into the EOQ formula and assumptions, enabling managers to update their ordering schedules as needed. This approach has the potential to generate more practical and realistic purchasing and inventory optimization policies.
C44|Spanning Tests for Markowitz Stochastic Dominance|Using properties of the cdf of a random variable defined as a saddle-type point of a real valued continuous stochastic process, we derive first-order asymptotic properties of tests for stochastic spanning w.r.t. a stochastic dominance relation. First, we define the concept of Markowitz stochastic dominance spanning, and develop an analytical representation of the spanning property. Second, we construct a non-parametric test for spanning via the use of an empirical analogy. The method determines whether introducing new securities or relaxing investment constraints improves the investment opportunity set of investors driven by Markowitz stochastic dominance. In an application to standard data sets of historical stock market returns, we reject market portfolio Markowitz efficiency as well as two-fund separation. Hence there exists evidence that equity management through base assets can outperform the market, for investors with Markowitz type preferences.
C44|The potential of big housing data: an application to the Italian real-estate market|We present a new dataset of housing sales advertisements (ads) taken from Immobiliare.it, a popular online portal for real estate services in Italy. This dataset fills a big gap in Italian housing market statistics, namely the absence of detailed physical characteristics for houses sold. The granularity of online data also makes possible timely analyses at a very detailed geographical level. We first address the main problem of the dataset, i.e. the mismatch between ads and actual housing units - agencies have incentives for posting multiple ads for the same unit. We correct this distortion by using machine learning tools and provide evidence about its quantitative relevance. We then show that the information from this dataset is consistent with existing official statistical sources. Finally, we present some unique applications for these data. For example, we provide first evidence at the Italian level that online interest in a particular area is a leading indicator of prices. Our work is a concrete example of the potential of large user-generated online databases for institutional applications.
C44|Technical efficiency of Slovak general hospitals|In this study, technical efficiency of Slovak general hospitals was investigated. The well-known non-parametric Data Envelopment Analysis was used to compare performance of Slovak health care providers. Results are based on four slightly differentiated models. Both CRS and VRS variation with different input approaches were used. Our results suggest low average efficiency in Slovak hospitals in the range 0.45 to 0.62 with great variations in efficiency score between individual Decision Making Units (DMUs). These results are relative without appropriate cross-country comparisons. Furthermore, in type of hospital entity there is no significant difference in efficiency score. However there is not a single efficient DMU in a group of municipality hospitals. Although, these results must be taken with caution due to questionable quality of data, this paper provides some valuable overview on technical efficiency of health care providers
C44|Spanning Tests for Markowitz Stochastic Dominance|We derive properties of the cdf of random variables defined as saddle-type points of real valued continuous stochastic processes. This facilitates the derivation of the first-order asymptotic properties of tests for stochastic spanning given some stochastic dominance relation. We define the concept of Markowitz stochastic dominance spanning, and develop an analytical representation of the spanning property. We construct a non-parametric test for spanning based on subsampling, and derive its asymptotic exactness and consistency. The spanning methodology determines whether introducing new securities or relaxing investment constraints improves the investment opportunity set of investors driven by Markowitz stochastic dominance. In an application to standard data sets of historical stock market returns, we reject market portfolio Markowitz efficiency as well as two-fund separation. Hence, we find evidence that equity management through base assets can outperform the market, for investors with Markowitz type preferences.
C44|The Conjunction Fallacy in Quantum Decision Theory|"The conjunction fallacy is a renowned violation of classical probability laws, which is persistently observed among decision makers. Within Quantum decision theory (QDT), such deviations are the manifestation of interference between decision modes of a given prospect. We propose a novel QDT interpretation of the conjunction fallacy, which cures some inconsistencies of a previous treatment, and incorporates the latest developments of QDT, in particular the representation of a decision-maker's state of mind with a statistical operator. Rather than focusing on the interference between choice options, our new interpretation identifies the origin of uncertainty and interference between decision modes to an entangled state of mind, whose structure determines the representation of prospects. On par with prospects, the state of mind can be a source of uncertainty and lead to interference effects, resulting in characteristic behavioral patterns. We present the first in-depth QDT-based analysis of an empirical study (the touchstone experimental investigations of Shafir et al. (1990)), which enables a data-driven exploration of its underlying theoretical construct. We link typicality judgements to probability amplitudes of the decision modes in the state of mind, and quantify the level of uncertainty and the relative contributions of prospect's interfering modes to its probability judgement. This enables inferences about the key QDT interference ""attraction'' q-factor with respect to different types of prospects - compatible versus incompatible. We propose a novel empirically motivated ""QDT indeterminacy (or uncertainty) principle,'' as a fundamental limit of the precision with which certain sets of prospects can be simultaneously known (or assessed) by a decision maker, or elicited by an experimental procedure. For any type of prospects, we observe a general tendency for the q-factor to converge to the same negative range q > (−0.3,−0.1) in the presence of high uncertainty, which motivates the hypothesis of an universal ""aversion'' q. The ""aversion'' q is independent of the (un-)attractiveness of a prospect under more certain conditions, which is the main difference with the previously considered ""QDT quarter law''. The universal ""aversion'' q substantiates the previously proposed ""QDT uncertainty aversion principle'' and clarifies its domain of application. The universal ""aversion'' q provides a theoretical basis for modelling different risk attitudes, such as aversions to uncertainty, to risk or to losses."
C44|Managerial Spillovers in Project Selection|Selecting investment or research projects is a general managerial decision, which ranges from managing the portfolio of R&D or marketing campaigns within a company, to determining the very boundaries of the firm -- which units or divisions to encompass, what acquisitions or alliances to pursue. Projects are typically assessed individually. However, the different divisions of a firm share managerial resources, so managers can transfer successful practices from one unit to another unit within the firm, or to different firms within their portfolio. This introduces managerial spillovers, so the value of a portfolio is higher than the aggregate value of the isolated projects. In this paper, we analyze the problem of selecting projects in the presence of managerial spillovers, and provide a simple algorithm that implements its solution. We find that, while a project yielding negative marginal profit can be safely discarded, it may be profitable to pass on multiple projects at once even if some of them yield positive marginal profit. Thus, ignoring the spillovers across projects and focusing on marginal profit can lead to excessively diversified firms or economies, as opposed to firms or economies with fewer (albeit possibly larger-scale) projects.
C44|Discriminant Analysis with Spherical Data|Discriminant analysis for spherical data, or directional data in general, has not been extensively studied, and most papers focus on one distribution, the von Mises-Fisher.
C44|Valuation as Promise and Care: The Use of Accounting in the Entrepreneurial Economy|This is a study of analysts’ use of accounting information for valuation purposes in a venture capital setting. This setting is characterized in terms of the distinctive scouting and coaching work of venture capital funders, and the unproven and incomplete nature of the ventures and entrepreneurs, which seek funding to scale operations, pivot into new markets, internationalize, or undertake some other kind of fundamental change. Based on interviews with entrepreneurs (project-makers) and venture funders (analysts), a four phase model of valuation is proposed. The model illuminates that, in contrast with common assumptions in the existing literature, accounting is mobilized neither to reveal truth nor constitute knowledge about the objects of investment, but to promise and to care. This paper articulates these two concepts in the context of accounting. Promising is shown to be a means not to implement a predesigned business plan and a budgeted set of activities but to commit to a new and unclear future and agree high and sometimes unrealistic expectations. Caring is shown not to be a means to predict a final fate for the organization, but to give and take, interact and sometimes discipline in order to determine what is necessary to preserve and possible to change. Understanding that what is valued is not what exists but what is possible to create helps to resolve puzzles about accounting’s uncertain and ambiguous status and significance in the entrepreneurial economy. It also and more generally illustrates how accounting operates in relation to an unknowable object and future: not as a means to know or reveal but to write and rewrite a daring and ambitious narrative in which the protagonists (here the project-maker and venture) become something else (a manager and an organization).
C44|Econometric Analysis of Productivity: Theory and Implementation in R|Our chapter details a wide variety of approaches used in estimating productivity and efficiency based on methods developed to estimate frontier production using Stochastic Frontier Analysis (SFA) and Data Envelopment Analysis (DEA). The estimators utilize panel, single cross section, and time series data sets. The R programs include such approaches to estimate firm efficiency as the time invariant fixed effects, correlated random effects, and uncorrelated random effects panel stochastic frontier estimators, time varying fixed effects, correlated random effects, and uncorrelated random effects estimators, semi-parametric efficient panel frontier estimators, factor models for cross-sectional and time-varying efficiency, bootstrapping methods to develop confidence intervals for index number-based productivity estimates and their decompositions, DEA and Free Disposable Hull estimators. The chapter provides the professional researcher, analyst, statistician, and regulator with the most up to date efficiency modelling methods in the easily accessible open source programming language R.
C44|Evaluating countriesâ€™ innovation potential: an international perspective|The paper proposes a novel two-step approach that evaluates countriesâ€™ innovation efficiency and their responsiveness to expansions in their innovation inputs, while addressing shortcomings associated with composite indicators. Based on our evaluations, we propose innovation policies tailored to take into account the diverse economic environments of the many countries in our study. Applying multidirectional efficiency analysis on data from the Global Innovation Index, we obtain separate efficiency scores for each innovation input and output. We then estimate different sensitivities for each country, by applying partial least squares on explanatory and response matrices which are determined by the nearest neighbours of the country under consideration. The findings reveal substantial asymmetries with respect to innovation efficiencies and sensitivities, which is indicative of the diversity of national innovation systems. Considering these two dimensions in combination, we outline three policy directions that can be followed, offering a platform for better-informed decision-making.
C44|Fake News and Indifference to Truth|State of the Union Addresses (SOUA) by two recent US Presidents, President Obama (2016) and President Trump (2018), and a series of recent of tweets by President Trump, are analysed by means of the data mining technique, sentiment analysis. The intention is to explore the contents and sentiments of the messages contained, the degree to which they dier, and their potential implications for the national mood and state of the economy. President Trump's 2018 SOUA and his sample tweets are identied as being more positive in sentiment than President Obama's 2016 SOUA. This is conrmed by bootstrapped t tests and non-parametric sign tests on components of the respective sentiment scores. The issue of whether overly positive pronouncements amount to self-promotion, rather than intrinsic merit or sentiment, is a topic for future research.
C44|Editorial Statement of Intent for Advances in Decision Sciences (ADS): 22nd Anniversary Special Issue in 2018|This note is concerned with an editorial statement of intent for Advances in Decision Sciences (ADS), which was founded in 1997, so that 2918 marks the 22nd Anniversary of the journal. The note discusses the aims and scope of ADS in Section 1, innovative topics in all fields of optimal decision making in Section 2, invitation to submit papers to ADS in Section 3, editors and members of the editorial board in Section 4, and acknowledgements in Section 5
C44|Research Ideas For Advances In Decision Sciences (Ads): 22nd Anniversary Special Issue In 2018|This note is concerned with an editorial statement of intent for Advances in Decision Sciences (ADS), which was founded in 1997, so that 2918 marks the 22nd Anniversary of the journal. The note discusses the aims and scope of ADS in Section 1, innovative topics in all fields of optimal decision making in Section 2, research areas of interest to ADS in Section 3, invitation to submit papers to ADS in Section 4, editors and members of the editorial board in Section 5, and acknowledgements in Section 6.
C44|Puppets on a String|For more than a century, scholars in psychology have debated whether humans are ‘of two minds,’ that is, whether they have both conscious and unconscious thoughts, and whether both conscious and unconscious thought processes determine their behavior. According to Freud’s iceberg model, conscious thought is just the tip of the iceberg, with most of our thought processes taking place unconsciously. Marketing scholars and practitioners have embraced the iceberg model with great enthusiasm. They have incorporated models where people’s drives and motivations are built in layers, with only the top layer consciously accessible, but the real drivers hidden underneath. According to one of the most influential contemporary theories, human thinking is governed by dual systems. System 1, it is argued, is the evolutionarily oldest system, based in parts of the brain we share with lower animals, operates unconsciously, uncontrollably, with low effort, has huge capacity, is fast, nonverbal, parallel, and associative. System 2, conversely, is evolutionarily more recent, resides in our frontal cortex, operates consciously, controllably, with high effort, has small capacity, is slow, verbal, serial, and based on rules. Despite their intuitive appeal, dual system theories have been challenged in recent years. I discuss some of their more problematic aspects and the research I have conducted testing core propositions of the dual system approach. Especially my research on the way brands become more well-liked through advertising and conditioning procedures is highly relevant for the debate, but so is research on people’s risk perceptions and self-control performance. Overall, I have seen support for some of the key predictions of dual process theory, but no support at all for its strong claim that mental processes should clearly belong to one of two systems with highly separable features. I argue that we need to acknowledge that the human mind cannot be neatly divided into two complementary processing systems. Rather, we should recognize that thought processes can be characterized to a greater or lesser extent by some but not all the features of automaticity. Researchers should start recognizing the full complexity of the human mind and embrace research that is more detailed, more precise – and perhaps a bit less grand in its claims.
C44|Learning with a purpose: the balancing acts of machine learning and individuals in the digital society|No abstract is available for this item.
C44|Testing productivity change, frontier shift, and efficiency change|Inference about productivity change over time based on data envelopment (DEA) has focused primarily on the Malmquist index and is based on asymptotic properties of the index. In this paper we propose a novel set of significance tests for DEA based productivity change measures based on permutations and accounting for the inherent correlations when panel data are observed. The tests are easily implementable and give exact significance probabilities as they are not based on asymptotic properties. Tests are formulated both for the geometric means of the Malmquist index, and also of its components, i.e. the frontier shift index and the eciency change index, which together enable analysis of not only the presence of differences, but also gives an indication of whether the productivity change is due to shifts in the frontiers and/or changes in the efficiency distributions. Simulation results show the power of, and suggest how to interpret the results of, the proposed tests. Finally, the tests are illustrated using a data set from the literature.
C44|Pre- and within-season attendance forecasting in Major League Baseball: A random forest approach|This study explores the forecasting of Major League Baseball game ticket sales and identifies important attendance predictors by means of random forests that are grown from classification and regression trees (CART) and conditional inference trees. Unlike previous studies that predict sport demand, I consider different forecasting horizons and only use information that is publicly accessible in advance of a game or season. Models are trained using data from 2013 to 2014 to make predictions for the 2015 regular season. The static within-season approach is complemented by a dynamic month-ahead forecasting strategy. Out-of-sample performance is evaluated for individual teams and tested against least-squares regression and a naive lagged attendance forecast. My empirical results show high variation in team-specific prediction accuracy with respect to both models and forecasting horizons. Linear and tree-ensemble models, on average, do not vary substantially in predictive accuracy; however, OLS regression fails to account for various team-specific peculiarities.
C44|Placement Optimization in Refugee Resettlement|Every year thousands of refugees are resettled to dozens of host countries. While there is growing evidence that the initial placement of refugee families profoundly affects their lifetime outcomes, there have been few attempts to optimize resettlement destinations. We integrate machine learning and integer optimization technologies into an innovative software tool that assists a resettlement agency in the United States with matching refugees to their initial placements. Our software suggests optimal placements while giving substantial autonomy for the resettlement staff to fine-tune recommended matches. Initial back-testing indicates that Annie can improve short-run employment outcomes by 22%-37%. We discuss several directions for future work such as incorporating multiple objectives from additional integration outcomes, dealing with equity concerns, evaluating potential new locations for resettlement, managing quota in a dynamic fashion, and eliciting refugee preferences.
C44|The value of foresight in the drybulk freight market|"We analyze the value of foresight in the drybulk freight market when repositioning a vessel through space and time. In order to do that, we apply an optimization model on a network with dynamic regional freight rate differences and stochastic travel times. We evaluate the value of the geographical switching option for three cases: the upper bound based on having perfect foresight, the lower bound based on a ""coin flip"", and the case of perfect foresight but only for a limited horizon. By combining a neural network with optimization, we can assess the impact of varying foresight horizon on economic performance. In a simple but realistic two-region case, we show empirically that the upper bound for large vessels can be as high as 25% cumulative outperformance, and that a significant portion of this theoretical value can be captured with limited foresight of several weeks. Our research sheds light on the important issue of spatial efficiency in global ocean freight markets and provides a benchmark for the value of investing in predictive analysis."
C44|Seasonality matters: a multi-season, multi-state dynamic optimization in fishery|Many biological and economic processes in fishery happen seasonally. Most of the extant literature tends to neglect this fact. This work is an initial attempt to treat seasonality in a systematic and proper way. We apply a periodic Bellman approach to obtain the optimal feedback policy of each season. Our approach has rich potentials. It could deal with seasonal patterns of uneven lengths: some may span years and some within the year. We find that, in some cases the equilibrium consists of one harvesting season followed by a moratorium period, indicating an optimal closure of the fishery that would be overlooked by a yearly model. Unlike a typical policy that enforces a moratorium to recover the stock, we find that many states first undergo harvesting all year round and later evolve into the seasonal moratorium. A rising group biomass could be the overshooting effect instead of a clear sign to increase harvest. We sometimes observe declining optimal harvest with increasing states (‘valley’), which may relate to the unit profit difference between seasons. Fishing pressure on the mature elicits even heavier harvest in the next season on the same group. A protective moratorium of the immature seems to hinder the value of the whole stock.
C44|Can an Emission Trading Scheme really reduce CO2 emissions in the short term? Evidence from a maritime fleet composition and deployment model|Global warming has become one of the most popular topics on this planet in the past decades, since it is the challenge that needs the efforts from the whole mankind. Maritime transportation, which carries more than 90% of the global trade, plays a critical role in the contribution of green house gases (GHGs) emission. Unfortunately, the GHGs emitted by the global fleet still falls outside the emission reduction scheme established by the Kyoto Protocol. Alternative solutions are therefore strongly desired. Several market-based measures are proposed and submitted to IMO for discussion and evaluation. In this paper, we choose to focus on one of these measures, namely Maritime Emissions Trading Scheme (METS). An optimization model integrating the classical fleet composition and deployment problem with the application of ETS (global or regional) is proposed. This model is used as a tool to study the actual impact of METS on fleet operation and corresponding CO2 emission. The results of the computational study suggest that in the short term the implementation of METS may not guarantee further emission reduction in certain scenarios. However, in other scenarios with low bunker price, high allowance cost or global METS coverage, a more significant CO2 decrease in the short term can be expected.
C44|Tangency portfolio weights for singular covariance matrix in small and large dimensions: estimation and test theory|In this paper we derive the nite-sample distribution of the esti- mated weights of the tangency portfolio when both the population and the sample covariance matrices are singular. These results are used in the derivation of a statistical test on the weights of the tangency port- folio where the distribution of the test statistic is obtained under both the null and the alternative hypotheses. Moreover, we establish the high-dimensional asymptotic distribution of the estimated weights of the tangency portfolio when both the portfolio dimension and the sam- ple size increase to in nity. The theoretical ndings are implemented in an empirical application dealing with the returns on the stocks included into the S&P 500 index.
C44|Bayesian inference for the tangent portfolio|In this paper we consider the estimation of the weights of tangent portfolios from the Bayesian point of view assuming normal conditional distributions of the logarithmic returns. For di↵use and conjugate priors for the mean vector and the covariance matrix, we derive stochastic representations for the posterior distributions of the weights of tangent portfolio and their linear combinations. Separately we provide the mean and variance of the posterior distributions, which are of key importance for portfolio selection. The analytic results are evaluated within a simulation study, where the precision of coverage intervals is assessed.
C44|Estimation of effects of recent macroprudential policies in a sample of advanced open economies|We analyse a quarterly panel data set consisting of ten advanced open economies that have introduced macroprudential policy measures: caps on loan to value and income (LTV and LTI), and debt service to income (DSTI) requirements in particular, but also risk weights (RW), amortization (Amort) and, less used, countercyclical buffer (CCyB). Estimation of dynamic panel data models, that also include the central bank rate, and controls for common nominal and real trends, gives support to the view that several of the measures may have reduced credit growth when they were introduced.The estimated impact effects are most significant for LTV, LTI and RW. For Amort, the long-run effect on credit growth is significant, and the same is found for RW. The estimation results when house price growth is the dependent variable are in the main consistent with the results for credit growth. The results do not support that CCyB has reduced lending (as a consequence of higher financing costs), and we suggest that the variable is mainly a control in our data set. In that interpretation, it is interesting that the estimated coefficients of the other five instruments are robust with respect to exclusion of CCyB from the empirical models. The results are also robust to controls in the form of impulse indicator saturation (IIS).
C44|Methods Matter: P-Hacking and Causal Inference in Economics|The economics 'credibility revolution' has promoted the identification of causal relationships using difference-in-differences (DID), instrumental variables (IV), randomized control trials (RCT) and regression discontinuity design (RDD) methods. The extent to which a reader should trust claims about the statistical significance of results proves very sensitive to method. Applying multiple methods to 13,440 hypothesis tests reported in 25 top economics journals in 2015, we show that selective publication and p-hacking is a substantial problem in research employing DID and (in particular) IV. RCT and RDD are much less problematic. Almost 25% of claims of marginally significant results in IV papers are misleading.
C44|"Random Consideration and Choice : A Case Study of ""Default"" Options"|A growing number of stochastic choice models include a “default” option for situations where the decision maker selects none of the feasible alternatives. While this is a welcome development, these models also present an empirical challenge—since the situations where the decision-maker chooses nothing may be difficult to observe. Taking Manzini and Mariotti’s (2014) independent random consideration model as a point of departure, I investigate what can be learned about models of choice with default when the no-choice behavior of the decision-maker is unobservable.
C44|"Random consideration and choice: A case study of ""default"" options"|"A growing number of stochastic choice models include a ""default"" option for situations where the decision maker selects none of the feasible alternatives. While this is a welcome development, these models also present an empirical challenge - since the situations where the decision-maker chooses nothing may be difficult to observe. Taking Manzini and Mariotti’s (2014) independent random consideration model as a point of departure, I investigate what can be learned about models of choice with default when the no-choice behavior of the decision-maker is unobservable."
C44|Systematic Systemic Stress Tests|For a given set of banks, which economic and financial scenarios will lead to big losses? How big can losses in such scenarios possibly get? These are the two central questions of macro stress tests. We believe that most current macro stress testing models have deficits in answering these questions. They select stress scenarios in a way which might leave aside many dangerous scenarios and thus create an illusion of safety; and which might consider highly implausible scenarios and thus trigger a false alarm. With respect to loss evaluation most stress tests do not include tools to analyse systemic risk arising from the interactions of banks with each other and with the markets. We make a conceptual proposal how these shortcomings may be addressed and how stress tests could be made both systematic and systemic. We demonstrate the application of our concepts using publicly available data on European banks and capital markets, in particular the EBA 2016 stress test results.
C44|Methods Matter: P-Hacking and Causal Inference in Economics|The economics 'credibility revolution' has promoted the identification of causal relationships using difference-in-differences (DID), instrumental variables (IV), randomized control trials (RCT) and regression discontinuity design (RDD) methods. The extent to which a reader should trust claims about the statistical significance of results proves very sensitive to method. Applying multiple methods to 13,440 hypothesis tests reported in 25 top economics journals in 2015, we show that selective publication and p-hacking is a substantial problem in research employing DID and (in particular) IV. RCT and RDD are much less problematic. Almost 25% of claims of marginally significant results in IV papers are misleading.
C44|Forecasters’ utility and forecast coherence|We introduce a new definition of probabilistic forecasts’ coherence based on the divergence between forecasters’ expected utility and their own models’ likelihood function. When the divergence is zero, this utility is said to be local. A new micro-founded forecasting environment, the “Scoring Structure”, where the forecast users interact with forecasters, allows econometricians to build a formal test for the null hypothesis of locality. The test behaves consistently with the requirements of the theoretical literature. The locality is fundamental to set dating algorithms for the assessment of the probability of recession in U.S. business cycle and central banks’ “fan” charts.
C44|σ-µ efficiency analysis: A new methodology for evaluating units through composite indices|We propose a new methodology to employ composite indicators for performance analysis of units of interest using Stochastic Multiattribute Acceptability Analysis. We start evaluating each unit by means of weighted sums of their elementary indicators in the whole set of admissible weights. For each unit, we compute the mean, µ, and the standard deviation, σ, of its evaluations. Clearly, the former has to be maximized, while the latter has to be minimized as it denotes instability in the evaluations with respect to the variability of weights. We consider a unit to be Pareto-Koopmans efficient with respect to µ and σ if there is no convex combination of µ and σ of the rest of the units with a value of µ that is not smaller, and a value of σ that is not greater, with at least one strict inequality. The set of all Pareto-Koopmans efficient units constitutes the first Pareto-Koopmans frontier. By removing this set and computing the efficiency frontier for the rest of the units, one could obtain the second Pareto-Koopmans frontier. Analogously, the third, fourth and so on Pareto-Koopmans frontiers can be defined. This permits to assign each unit to one of this sequence of Pareto-Koopmans frontiers. We measure the efficiency of each unit not only with respect to the first Pareto-Koopmans frontier, as in the classic Data Envelopment Analysis, but also with respect to the rest of the frontiers, thus enhancing the explicative power of the proposed approach. To illustrate its potential, we apply it to a case study of world happiness based on the data of the homonymous report, annually produced by the United Nations’ Sustainable Development Solutions Network.
C44|Modelowanie AHP wyboru menadżera ds. integracji organizacyjnej w procesie fuzji przedsiębiorstw w kontekście teorii struktury rozwoju przywództwa<BR>[Choosing the merger’s organizational integration manager via AHP modeling of Leadership Development Framework]|Objective: elaboration of a decision-making model for choosing the organizational integration manager in the process of companies’ merger. Research Design & Methods: Analytic Hierarchy Process has been employed for model designing. Decision criteria came from a literature analysis on mergers and acquisitions. Decision alternatives derive from the modified Leadership Development Framework by Torbert & Cook-Greuter. Assessments of relevance of decision-making criteria have been acquired from carefully selected experts in mergers and acquisitions. Findings: research resulted in a decision-making model that proved that the possibly optimal efficiency of managing the organizational integration stage should be attributed to the Achiever leadership level. Implications & Recommendations: the model can be used in praxis for choosing managers, whenever their field of expertise is not the unique selection criterion. Contribution & Value Added: the application of a multicriteria decision-making model allows a wider perspective on managerial personnel selection for precisely defined managerial tasks – incorporating the candidate’s personality as one of the decision-making factors.
C44|A Levy Regime-Switching Temperature Dynamics Model for Weather Derivatives|Weather is a key production factor in agricultural crop production and at the same time the most significant and least controllable source of peril in agriculture. These effects of weather on agricultural crop production have triggered a widespread support for weather derivatives as a means of mitigating the risk associated with climate change on agriculture. However, these products are faced with basis risk as a result of poor design and modelling of the underlying weather variable (temperature). In order to circumvent these problems, a novel time-varying mean-reversion L´evy regime-switching model is used to model the dynamics of the deseasonalized temperature dynamics. Using plots and test statistics, it is observed that the residuals of the deseasonalized temperature data are not normally distributed. To model the nonnormality in the residuals, we propose using the hyperbolic distribution to capture the semiheavy tails and skewness in the empirical distributions of the residuals for the shifted regime. The proposed regime-switching model has a mean-reverting heteroskedastic process in the base regime and a Levy process in the shifted regime. By using the Expectation-Maximization algorithm, the parameters of the proposed model are estimated. The proposed model is flexible as it modelled the deseasonalized temperature data accurately.
C44|Decision-making under environmental uncertainty|Goal of the paper: proposal of a model for decision-making enhancement that includes qualitative and quantitative elements influencing managerial decision-making processes under geopolitical uncertainty. Methods: primary: Analytic Hierarchy Process – for assessment of individual and collective utility of indexes describing the functioning of enterprises; secondary: Delphi questionnaires, Pareto-Lorenz diagram, stratified random sampling; AHP evaluations came from six professional managers Results: a mixed qualitative and quantitative instrument bringing geopolitical occurrences into managerial decision-making under turbulent environmental conditions; Practical implications: increased efficiency of managerial decision-making processes, with managerial decisions closer to the possible optimum, under given environmental conditions. Added value: the application of multicriteria models for enhancement of managerial decision-making provides a larger perspective on environmental threats and lowers the decision-making uncertainty.
C44|Influence model of evasive decision makers|"The aim of this paper is to introduce the notion of truthfulness in an influence based decision making model. An expert may submit his opinions truthfully or he may dismantle the original situation by undermining the actual opinion, such a decision maker is called an evasive decision maker or an almost truthful decision maker in this paper. It is assumed that experts in the panel are dignified members hence even though they are not habitual liars, they are either ""almost truthful"" or evasive. To measure their degree of truthfulness, we use the information provided by them in the form of preference relations. We use this information to state the foundation of influence model of evasive decision makers. Finally, a ranking method is proposed to find best possible solutions."
C44|Role of honesty and confined interpersonal influence in modelling predilections|Classical models of decision-making do not incorporate for the role of influence and honesty that affects the process. This paper develops on the theory of influence in social network analysis. We study the role of influence and honesty of individual experts on collective outcomes. It is assumed that experts have the tendency to improve their initial predilection for an alternative, over the rest, if they interact with one another. It is suggested that this revised predilection may not be proposed with complete honesty by the expert. Degree of honesty is computed from the preference relation provided by the experts. This measure is dependent on average fuzziness in the relation and its disparity from an additive reciprocal relation. Moreover, an algorithm is introduced to cater for incompleteness in the adjacency matrix of interpersonal influences. This is done by analysing the information on how the expert has influenced others and how others have influenced the expert.
C44|Improving Finite Sample Approximation by Central Limit Theorems for DEA and FDH efficiency scores|We propose an improvement of the finite sample approximation of the central limit theorems (CLTs) that were recently derived for statistics involving production efficiency scores estimated via Data Envelopment Analysis (DEA) or Free Disposal Hull (FDH) approaches. The improvement is very easy to implement since it involves a simple correction of the already employed statistics without any additional computational burden and preserves the original asymptotic results such as consistency and asymptotic normality. The proposed approach persistently showed improvement in all the scenarios that we tried in various Monte-Carlo experiments, especially for relatively small samples or relatively large dimensions (measured by total number of inputs and outputs) of the underlying production model. This approach therefore is expected to be valuable (and at almost no additional computational costs) for practitioners wishing to perform statistical inference about production efficiency using DEA or FDH approaches.
C44|Econometric Analysis of Productivity: Theory and Implementation in R|Our chapter details a wide variety of approaches used in estimating productivity and efficiency based on methods developed to estimate frontier production using Stochastic Frontier Analysis (SFA) and Data Envelopment Analysis (DEA). The estimators utilize panel, single cross section, and time series data sets. The R programs include such approaches to estimate firm efficiency as the time invariant fixed effects, correlated random effects, and uncorrelated random effects panel stochastic frontier estimators, time varying fixed effects, correlated random effects, and uncorrelated random effects estimators, semi-parametric efficient panel frontier estimators, factor models for cross-sectional and time-varying efficiency, bootstrapping methods to develop confidence intervals for index number-based productivity estimates and their decompositions, DEA and Free Disposable Hull estimators. The chapter provides the professional researcher, analyst, statistician, and regulator with the most up to date efficiency modeling methods in the easily accessible open source programming language R.
C44|Green Concept Evaluation through Fuzzy AHP-PROMETHEE II|The demand for green products have dramatically increased because the importance and public awareness of the preservation of natural environment was taken into consideration much more last two decades. As a result of this, especially manufacturing companies have been forced to design more green products, resulting in a problem of how they incorporate environmental issues into their design and evaluate concept options. The need for the practical decision making tools to address this problem is rapidly evolving due to the fact that the problem turns into a multiple-criteria decision making (MCDM) problem in the presence of a set of green concept alternatives and criteria. Therefore; in this paper, the four popular MCDM methods in fuzzy environment are utilized to reflect the vagueness and uncertainty on the judgments of DMs, because the crisp pairwise comparison in these conventional MCDM methods seems to be insufficient and imprecise to capture the right judgments of DMs. Of these methods; as Fuzzy AHP is used to calculate criteria weights, the other method; Fuzzy PROMETHEE II is used to rank alternatives. Furthermore, the incorporation of fuzzy set theory into these methods is discussed on a real-life case study.
C44|Difference in Risk Perception of Uncertainties in Supply Chain between Developed and Developing Countries|The objective of this paper is to clarify the relationship between uncertainties in supply chain and its disruption risk under environmental turbulence. Rapid spread of globalization pushes firms to face the higher level of uncertainty, which increases the risk of supply disruption. Firms must formulate necessary and sufficient strategy to supply chain management that they may confront. Such a strategy, however, involves a broad range of factors, including some that are subjective, e.g., risk perception. Formulating strategy for supply chain management and decision making for avoiding supply disruption thus must often rely heavily on past experience, generalities, and intuition. This paper addresses this issue by refining existing structural model of supply disruption. In the analyses, inner dependencies among demand, quality and logistics uncertainties, and outer dependencies between those uncertainties and the magnitude and the probability of disruption risks are evaluated. Pairwise evaluations among the uncertainties and those between representations of supply disruption risk and uncertainties comprise a prioritized analysis. Case studies are conducted in beverage companies that engage in business both in developed and developing countries, which demonstrate the applicability of the prioritized analyses based on the proposed structural model to the real markets. The results clarify the difference in risk perceptions under environmental backgrounds and illustrate the feature of risk perception based on the characteristics of companies. According to the retrospective interview after the case study, the managers in the companies are provided suggestion on how to formulate a strategy to their supply chain management.
C44|Fake News And Indifference To Truth: Dissecting Tweets And State Of The Union Addresses By Presidents Obama And Trump|State of the Union Addresses (SOUA) by two recent US Presidents, President Obama (2016) and President Trump (2018), and a series of recent of tweets by President Trump, are analysed by means of the data mining technique, sentiment analysis. The intention is to explore the contents and sentiments of the messages contained, the degree to which they differ, and their potential implications for the national mood and state of the economy. President Trump's 2018 SOUA and his sample tweets are identified as being more positive in sentiment than President Obama's 2016 SOUA. This is confirmed by bootstrapped t tests and non-parametric sign tests on components of the respective sentiment scores. The issue of whether overly positive pronouncements amount to self-promotion, rather than intrinsic merit or sentiment, is a topic for future research.
C44|Carpooling with heterogeneous users in the bottleneck model|"When drivers opt for carpooling, road capacity will be freed up, and this will reduce congestion. Therefore, carpooling is interesting for policy makers as a possible solution to congestion. We investigate the effects of carpooling in a dynamic equilibrium model of congestion, which captures various dimensions of heterogeneity: heterogeneity in preference for carpooling, ""ratio heterogeneity"" between the values of time and the values of schedule delay, and ""proportional heterogeneity"" that scales all values equally. We investigate three policy scenarios: no-toll, first-best pricing, and subsidization of carpooling. The optimal second-best subsidy equals each type’s heterogeneous marginal external benefit (MEB) of switching to carpooling. If such differentiation is impossible, the third-best subsidy is a weighted average of the MEBs, where the weights depend on the number of each type and their sensitivity to the subsidy. In our numerical example, we find that when increasing the degree of ""ratio heterogeneity"", the relative efficiency of the second-best subsidization first increases and then falls with the degree of heterogeneity and L type carpoolers benefit more than H type carpoolers. However, when increasing the degree of ""proportional heterogeneity"", H type users benefit more than L types for both solo drivers and carpoolers. Moreover, the relative efficiency of the second-best subsidization decreases throughout."
C44|On Interactive Sequencing Situations with Exponential Cost Functions|This paper addresses interactive one-machine sequencing situations in which the costs of processing a job are given by an exponential function of its completion time. The main difference with the standard linear case is that the gain of switching two neighbors in a queue is time-dependent and depends on their exact position. We illustrate that finding an optimal order is complicated in general and we identify specific subclasses, which are tractable from an optimization perspective. More specifically, we show that in these subclasses, all neighbor switches in any path from the initial order to an optimal order lead to a non-negative gain. Moreover, we derive conditions on the time-dependent neighbor switching gains in a general interactive sequencing situation to guarantee convexity of the corresponding cooperative game. These conditions are satisfied within our specific subclasses of exponential interactive sequencing situations.
C44|Prediction for Big Data through Kriging : Small Sequential and One-Shot Designs|"Kriging or Gaussian process (GP) modeling is an interpolation method that assumes the outputs (responses) are more correlated, the closer the inputs (ex- planatory or independent variables) are. A GP has unknown (hyper)parameters that must be estimated; the standard estimation method uses the ""maximum likelihood"" criterion. However, big data make it hard to compute the estimates of these GP parameters, and the resulting Kriging predictor and the variance of this predictor. To solve this problem, some authors select a relatively small subset from the big set of previously observed ""old"" data; their method is se- quential and depends on the variance of the Kriging predictor. The resulting designs turn out to be ""local""; i.e., most design points are concentrated around the point to be predicted. We develop three alternative one-shot methods that do not depend on GP parameters: (i) select a small subset such that this sub- set still covers the original input space–albeit coarser; (ii) select a subset with relatively many— but not all— combinations close to the new combination that is to be predicted, and (iii) select a subset with the nearest neighbors (NNs) of this new combination. To evaluate these designs, we compare their squared prediction errors in several numerical (Monte Carlo) experiments. These experi- ments show that our NN design is a viable alternative for the more sophisticated sequential designs."
C44|Fake news and indifference to scientific fact: President Trump’s confused tweets on global warming, climate change and weather|Abstract A set of 115 tweets on climate change by President Trump, from 2011 to 2015, are analysed by means of the data mining technique, sentiment analysis. The intention is to explore the contents and sentiments of the messages contained, the degree to which they differ, and their implications about his understanding of climate change. The results suggest a predominantly negative emotion in relation to tweets on climate change, but they appear to lack a clear logical framework, and confuse short term variations in localised weather with long term global average climate change.
C44|Variable Selection in Sparse Semiparametric Single Index Models|"In this paper we consider the ""Regularization of Derivative Expectation Operator"" (Rodeo) of Lafferty and Wasserman (2008) and propose a modified Rodeo algorithm for semiparametric single index models in big data environment with many regressors. The method assumes sparsity that many of the regressors are irrelevant. It uses a greedy algorithm, in that, to estimate the semiparametric single index model (SIM) of Ichimura (1993), all coefficients of the regressors are initially set to start from near zero, then we test iteratively if the derivative of the regression function estimator with respect to each coefficient is significantly different from zero. The basic idea of the modified Rodeo algorithm for SIM (to be called SIM-Rodeo) is to view the local bandwidth selection as a variable selection scheme which amplifies the coefficients for relevant variables while keeping the coefficients of irrelevant variables relatively small or at the initial starting values near zero. For sparse semiparametric single index models, the SIM-Rodeo algorithm is shown to attain consistency in variable selection. In addition, the algorithm is fast to finish the greedy steps. We compare SIM-Rodeo with SIM-Lasso method in Zeng et al. (2012). Our simulation results demonstrate that the proposed SIM-Rodeo method is consistent for variable selection and show that it has smaller integrated mean squared errors than SIM-Lasso."
C44|Une nouvelle approche expérimentale pour tester les modèles quantiques de l’erreur de conjonction|In classical probability theory, the probability of the conjunction of two events is smaller than the probability of only one of these events. Yet, agents do not always empirically judge in this way: this is the traditional conjunction fallacy. One of the currently promising accounts of this paradox relies on so-called quantum-like models, which have been developed from mathematical tools used in quantum theory. But are these models empirically adequate? Which versions of these models can be used? In particular, can the simplest versions, the non-degenerate ones, be sufficient? We propose here an original experimental protocol to test the quantum-like models for the conjunction fallacy in the lab. The results we obtain suggest that the non-degenerate models are not empirically adequate, and that future research on quantum-like models should consider degenerate ones. Classification JEL : C60, C91, D03.
C44|Profit-oriented scheduling of resource-constrained projects with flexible capacity constraints|We consider a novel generalization of the resource-constrained project scheduling problem (RCPSP). Unlike many established approaches for the RCPSP that aim to minimize the makespan of the project for given static capacity constraints, we consider the important real-life aspect that capacity constraints can often be systematically modified by temporarily assigning costly additional production resources or using overtime. We furthermore assume that the revenue of the project decreases as its makespan increases and try to find a schedule with a profit-maximizing makespan. Like the RCPSP, the problem is $NP$-hard, but unlike the RCPSP it turns out that an optimal schedule does not have to be among the set of so-called active schedules. Scheduling such a project is a formidable task, both from a practical and a theoretical perspective. We develop, describe, and evaluate alternative solution encodings and schedule decoding mechanisms to solve this problem within a genetic algorithm framework and we compare them to both optimal reference values and the results of a commercial local search solver called LocalSolver.
C44|Stochastic programs with binary distributions: Structural properties of scenario trees and algorithms|Binary random variables often refer to such as customers that are present or not, roads that are open or not, machines that are operable or not. At the same time, stochastic programs often apply to situations where penalties are accumulated when demand is not met, travel times are too long, or profits too low. Typical for these situations is that the penalties imply a partition of the scenarios into two sets: Those that can result in penalties for some decisions, and those that never lead to penalties. We demonstrate how this observation can be used to efficiently calculate out-of-sample values, find good scenario trees and generally simplify calculations. Most of our observations apply to general integer random variables, and not just the 0/1 case.
C44|Scrubber: a potentially overestimated compliance method for the Emission Control Areas - The importance of involving a ship's sailing pattern in the evaluation|Different methods for sulphur emission reductions, available to satisfy the latest Emission Control Areas (ECA) regulations, may lead to different sailing patterns (route and speed choices of a vessel) and thus have significant impact on a shipping company's operating costs. However, the current literature does not include sailing pattern optimization caused by ECA, and its corresponding cost effects, in the evaluation and selection process for sulphur abatement technology. This leads to an inaccurate estimation of the value of certain technologies and hence an incorrect investment decision. In this paper, we integrate the optimization of a ship's sailing pattern into the lifespan cost assessment of the emission control technology, so that such expensive and irreversible decisions can be made more accurately. The results shows that a considerable overestimation of the value of scrubbers, and thus a substantial loss, can occur if the sailing pattern of a ship is not considered in the decision-making process. Furthermore, we also illustrate that it is more important to involve a ship's sailing pattern when the port call density inside ECA is low.
C44|Discriminant analysis in small and large dimensions|In this article we study the distributional properties of the linear discriminant function under the assumption of the normality by comparing two groups with the same covariance matrix but di erent mean vectors. A stochastic representation of the discriminant function coecient is derived which is then used to establish the asymptotic distribution under the high-dimensional asymptotic regime. Moreover, we investigate the classi cation analysis based on the discriminant function in both small and large dimensions. In the numerical study, a good nite-sample perfor- mance of the derived large-dimensional asymptotic distributions is documented.
C44|Higher order moments of the estimated tangency portfolio weights|In this paper we consider the estimated weights of tangency portfolio. The returns are assumed to be independently and multivariate normally distributed. We derive analytical expressions for the higher order non-central and central moments of these weights. Moreover, the expressions for mean, variance, skewness and kurtosis of the estimated weights are obtained in closed-forms. Finally, we complement our result with an empirical study where we analyze a portfolio with actual returns of eight nancial indexes listed in NASDAQ stock exchange.
C44|Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice|One of the main objectives of empirical analysis of experiments and quasiâ€ experiments is to inform policy decisions that determine the allocation of treatments to individuals with different observable covariates. We study the properties and implementation of the Empirical Welfare Maximization (EWM) method, which estimates a treatment assignment policy by maximizing the sample analog of average social welfare over a class of candidate treatment policies. The EWM approach is attractive in terms of both statistical performance and practical implementation in realistic settings of policy design. Common features of these settings include: (i) feasible treatment assignment rules are constrained exogenously for ethical, legislative, or political reasons, (ii) a policy maker wants a simple treatment assignment rule based on one or more eligibility scores in order to reduce the dimensionality of individual observable characteristics, and/or (iii) the proportion of individuals who can receive the treatment is a priori limited due to a budget or a capacity constraint. We show that when the propensity score is known, the average social welfare attained by EWM rules converges at least at nâˆ’1/2 rate to the maximum obtainable welfare uniformly over a minimally constrained class of data distributions, and this uniform convergence rate is minimax optimal. We examine how the uniform convergence rate depends on the richness of the class of candidate decision rules, the distribution of conditional treatment effects, and the lack of knowledge of the propensity score. We offer easily implementable algorithms for computing the EWM rule and an application using experimental data from the National JTPA Study.
C44|What Types are There?|Abstract Preferences differ in the population, and this heterogeneity may not be adequately described by observed characteristics and additive error terms. As a first contribution, this study shows that preference heterogeneity can be represented graphically by means of violations of the Weak Axiom of Revealed Preference (WARP), and that computing the minimum number of partitions necessary to break all WARP violations in the sample is equivalent to computing the chromatic number of this graph. Second, the study builds the bridge between revealed preference theory and cluster analysis to assign individuals to these partitions (i.e. preference types). The practical methods are applied to Dutch labour supply data, to recover reservation wages of individuals who belong to particular preference types.
C44|Using machine learning for financial fraud detection in the accounts of companies investigated for money laundering|Benford’s Law is a well-known system use in accountancy for the analysis and detection of anomalies relating to money laundering and fraud. On that basis, and using real data from transactions undertaken by more than 600 companies from a particular sector, behavioral patterns can be analyzed using the latest machine learning procedures. The dataset is clearly unbalanced, for this reason we will apply cost matrix and SMOTE to different detecting patters methodologies: logistic regression, decision trees, neural networks and random forests. The objective of the cost matrix and SMOTE is to improve the forecasting capabilities of the models to easily identify those companies committing some kind of fraud. The results obtained show that the SMOTE algorithm gets better true positive results, outperforming the cost matrix implementation. However, the general accuracy of the model is very similar, so the amount of a false positive result will increase with SMOTE methodology. The aim is to detect the largest number of fraudulent companies, reducing, as far as possible, the number of false positives on companies operating correctly. The results obtained are quite revealing: Random forest gets better results with SMOTE transformation. It obtains 96.15% of true negative results and 94,98% of true positive results. Without any doubt, the listing ability of this methodology is very high. This study has been developed from the investigation of a real Spanish money laundering case in which this expert team have been collaborating. This study is the first step to use machine learning to detect financial crime in Spanish judicial process cases.
C44|Numerical Modeling of Dependent Credit Rating Transitions with Asynchronously Moving Industries|Abstract Two models of dependent credit rating migrations governed by industry-specific Markovian matrices, are considered. Caused by macroeconomic factors, positive and negative unobserved tendencies, encoded as values “1” or “0” of the corresponding variables, modify the transition probabilities and render the evolutions dependent. They are neither synchronized across industry sectors, nor over credit classes: an upswing in some of them can coexist with a decline of the rest. The models are tested on Standard and Poor’s data. MATLAB optimization software and maximum likelihood estimators are used. Obtained distributions of the hidden variables demonstrate that the considered industries migrate asynchronously trough credit classes. Since downgrading probabilities are less affected by the unobserved tendencies, estimated by Monte-Carlo simulations distributions of defaults, exhibit lighter, than for the known coupling models, tails for schemes with asynchronously moving industries. Moreover, the lightest tails were obtained in the case of industry-specific transition matrices.
C44|Cost-benefit analysis for flood risk management and water governance in the Netherlands; an overview of one century|The Netherlands is a global reference for flood risk management. This reputation is based on a mix of world-class civil engineering projects and innovative concepts of water governance. For more than a century, cost-benefit analysis has been important for flood risk management and water governance in the Netherlands. It has helped to select the most effective and efficient flood risk projects and to coordinate and reconcile the interests of various policy areas, levels of government and private stakeholders. This paper provides for the first time an overview of this well-developed practice. This includes the cost-benefit analysis in the 1901 act for enclosure of the Zuiderzee, van Dantzig’s famous formula for the economically optimal strength of dikes and a whole set of cost-benefit analyses for More room for rivers and the Delta Program for the next century. Dutch practice illustrates how cost-benefit analysis can support and improve flood risk management and water governance; other countries may learn from this. Rough calculations indicate that investing in cost-benefit analysis has been a highly profitable investment for Dutch society.
C44|European qualifiers to the 2018 FIFA World Cup can be manipulated|Tournament organizers supposedly design rules such that a team cannot be better off by exerting a lower effort. It is shown that the European qualifiers to the 2018 FIFA World Cup are not strategy-proof in this sense: a team might be eliminated if it wins in the last matchday of group stage, while it advances to play-offs by playing a draw, provided that all other results do not change. An example reveals that this scenario could have happened in October 2017, after four-fifth of all matches have already been played. We present a model and identify nine incentive incompatible qualifiers to recent UEFA European Championships or FIFA World Cups. A mechanism is suggested in order to seal the way of manipulation in group-based qualification systems.
C44|Is the Grass Always Greener on the Other Side of the fence? Composite Index of Well-Being Taking into Account the Local Relative Appreciations in Better Life Index|The multidimensional measures of well-being, such as the OECD Better Life Index (BLI), are receiving considerable attention. We introduce a composite index that, departing from the current practice, accounts for societal relative appreciation for the considered dimensions. We apply our methodology to the BLI using the data on preferences gathered from the OECD website. Our analysis signals pervasive differences in the country-level performances that cannot be compensated through differences in local preferences. Furthermore, individual preferences exacerbate multidimensional inequality between countries. Hence, we conjecture that better performing countries offer a policy mix better tailored to fit citizens’ preferences.
C44|Was Zidane honest or well-informed? How UEFA barely avoided a serious scandal|UEFA European Championship 1996 qualification is known to violate strategy-proofness. It has been proved recently that a team could be better off by exerting a lower effort: it might be optimal to concede some goals in order to achieve a better position among runners-up, and hence avoid a hazardous play-off. We show that it is not only an irrelevant scenario with a marginal probability since France had an incentive to kick two own goals on its last match against Israel.
C44|Tournaments with subsequent group stages are incentive incompatible|We discuss the strategy-proofness of multistage tournaments. In a tournament with subsequent group stages, players are divided into groups in the preliminary and main rounds, where they play pairwise matches against each other. The higher ranked players qualify to the next stage such that matches are not repeated in the main round if two qualified players have already faced in the preliminary round. Players prefer to carry over better results to the main round, provided that they qualify. It is shown that these tournament systems, widely used in handball, are incentive incompatible. We also present some historical examples where a team was ex ante not interested in winning by a high margin.
C44|Статистическая Оценка Налогового Контроля Как Инструмент Налогового Мониторинга<BR>[Statistical Estimation Of Tax Control As A Tool Of Tax Monitoring]|In the framework of this study, the authors developed and tested a methodology for assessing tax control in Russia as one of the main instruments of tax monitoring. We analyzed the dynamics of the increase in tax revenues to the budget of the Russian Federation due to the intensive work of tax authorities in the regions, which consists in automating the mechanisms of tax administration. We conducted a typification of the regions of Russia in terms of the level and quality of tax control.
C44|Диффузионное Описание Производственного Процесса<BR>[Diffusion description of the production process]|В статье основное внимание уделяется построению стохастического уравнения для расчета производственного цикла изготовления партии изделий на предприятиях с поточным типом организации производства. В качестве базового подхода рассмотрен процесс движения предметов труда по синхронизованной поточной линии. Выполнена оценка размеров межоперационных заделов технологических операций, обеспечивающая бесперебойный режим функционирования поточной линии. Записано в канонической форме стохастическое уравнение и получено определение коэффициента диффузии.
C44|The model of the production process of the party of the subjects of labour|The article discusses the construction of the model of streaming production line with the constraints on the technological trajectory of subjects of labour. The work shows the influence of the subject of labour movement trajectory, which is related to the limited maximum capacity of the operating storage. It analyses constraint that is associated with the serial order of subjects of labour processing. The equation for the trajectory of the regulatory process is built, taking into account the constraints on the trajectory of the subjects of labour, which can be used for closing the balance equations of PDE-models of streaming production lines.
C44|Analytical methods for designing technological trajectories of the object of labour in a phase space of states|Purpose. The development of analytical methods for designing technological motion trajectories of objects of labour in the state space with the purpose of construction of closed PDE-models, used to describe the manufacturing system. Methodology. For derivation of an equation of the labour object movement in the phase space of states, there has been applied a mathematical tool and the variational calculation methods of analytical mechanics. Findings. An equation of labour object movement in the state of space has been derived and motion integrals, related to the uniformity of time and state space have been considered. Originality. PDE-models of manufacturing systems, used for the engineering of the high performance manufacturing control systems have been improved. The offered model of technological resources transfer to the object of labour is based not on the traditional phenomenological description of the static production phenomena, but on conservation laws, which characterize the transfer process of technological resources to the object of labour and space-time structure of the manufacturing process. It allowed deriving the equation of the objects of labour movement along the manufacturing route, followed by the construction of non-steady-state equations of the PDE models on their ground for the description of the parameters status of the manufacturing process. While deriving the equations of the technological path of movement of the object of labour there were taken into consideration differential constraints, being applied by the manufacturing system to the transfer process of technological resources of the objects of labour, resulting from their interaction with production equipment and against each other in the course of transfer from one manufacturing operation to another. Practical value. Methods for driving the equation of technological path of the object of labour allow developing high-quality models of the transfer processes of the manufacturing system, which are the basis for the high-quality enterprise management system engineering with a straight flow method of industrial organization.
C44|Revisiting Forecasting of Recessions via Dynamic Probit for Time Series by Kauppi and Saikkonen (2008)|In this work we first replicate the results of fully parametric dynamic probit model for forecasting US recessions from Kauppi and Saikkonen (2008) (which is in the spirit of Estrella and Mishkin (1995, 1998) and Dueker (1997)) and then contrast them to results from non-parametric local-likelihood dynamic probit model for the same data.
C44|Convexity, Disposability and Returns to Scale in Production Analysis|Adequate modelling of undesirable outputs is a key aspect for any performance analysis of economic systems. A nonparametric approach assuming jointly weak disposability of desirable and undesirable outputs inspired by Shephard (1974) has gained substantial popularity in addressing this issue. Recently, researchers were offered an alternative that is to use multiple scaling factors (rather than a single one as in the Shephardâ€™s (1974) approach) when imposing weak disposability in practice. In this paper we discover new properties and relationships between the two approaches, which in turn sheds some new light on the problem and offers reconciling solutions.
C44|Ordering ambiguous acts|We investigate what it means for one act to be more ambiguous than another. The question is evidently analogous to asking what makes one prospect riskier than another, but beliefs are neither objective nor representable by a unique probability. Our starting point is an abstract class of preferences constructed to be (strictly) partially ordered by a more ambiguity averse relation. First, we define two notions of more ambiguous with respect to such a class. A more ambiguous (I) act makes an ambiguity averse decision maker (DM) worse off but does not affect the welfare of an ambiguity neutral DM. A more ambiguous (II) act adversely affects a more ambiguity averse DM more, as measured by the compensation they require to switch acts. Unlike more ambiguous (I), more ambiguous (II) does not require indifference of ambiguity neutral elements to the acts being compared. Second, we implement the abstract definitions to characterize more ambiguous (I) and (II) for two explicit preference families: α-maxmin expected utility and smooth ambiguity. Thirdly, we give applications to the comparative statics of more ambiguous in a standard portfolio problem and a consumption-saving problem.
C44|Robust Sequential Search|We study sequential search without priors. Our interest lies in decision rules that are close to being optimal under each prior and after each history. We call these rules dynamically robust. The search literature employs optimal rules based on cuto strategies that are not dynamically robust. We derive dynamically robust rules and show that their performance exceeds 1/2 of the optimum against binary environments and 1/4 of the optimum against all environments. This performance improves substantially with the outside option value, for instance, it exceeds 2/3 of the optimum if the outside option exceeds 1/6 of the highest possible alternative.
C44|Analysis Of Inventory Policy For Perishable Goods Having Constant Demand|One of the major problems of retailers is when to order and how much to order their products so that the total inventory cost is minimized i.e. their operating profit is maximized. This study focuses on the inventory management of perishable items having constant demand. Firstly, the mathematical model for perishable items have been framed and then using the principle of maxima- minima the optimal cycle time equation has been obtained. Also using simulation the best possible solution has been obtained for the various cases. By using the above model retailer can easily decide his inventory cycle time and optimal ordering quantity. The above model can be converted into the classical EOQ model if nonperishable items are considered. Otherwise, also it follows the classical EOQ principle which states that the optimal order quantity is obtained when ordering cost is equal to the carrying cost
C44|Exploring farmers? selection of crop protection levels as an adaptation strategy to climate risks|Among the challenges facing the European Union agricultural sector in the coming years, the impacts of climate change could lead to much greater variability in farmers? incomes. In this context, the insurance industry will have to develop new instruments to cover farmers? incomes against losses due to meteorological factors. Some protective technologies that farmers can use for climate risk management have associated costs that vary as a function of the losses involved. These sorts of instruments compete with other less flexible instruments such as crop insurance. We here analyse an issue of decision-making, where the farmer can decide how much to invest in protection, as in situations where the farmer chooses which portion of a loss to protect in the case of adverse weather conditions, and we propose optimal management to mitigate the increasing negative effects of climate uncertainty. By analysing the optimal policy in a continuous choice situation, we consider whether farmers, as part of their crop management duties, should opt to protect some portion of their harvest value with available technologies, or whether they should protect the entire crop. To analyse this decision-making problem, we employ the cost-loss ratio model and take risk aversion into account.
C44|How risky is the optimal portfolio which maximizes the Sharpe ratio?|Abstract In this paper, we investigate the properties of the optimal portfolio in the sense of maximizing the Sharpe ratio (SR) and develop a procedure for the calculation of the risk of this portfolio. This is achieved by constructing an optimal portfolio which minimizes the Value-at-Risk (VaR) and at the same time coincides with the tangent (market) portfolio on the efficient frontier which is related to the SR portfolio. The resulting significance level of the minimum VaR portfolio is then used to determine the risk of both the market portfolio and the corresponding SR portfolio. However, the expression of this significance level depends on the unknown parameters which have to be estimated in practice. It leads to an estimator of the significance level whose distributional properties are investigated in detail. Based on these results, a confidence interval for the suggested risk measure of the SR portfolio is constructed and applied to real data. Both theoretical and empirical findings document that the SR portfolio is very risky since the corresponding significance level is smaller than 90 % in most of the considered cases.
C44|Kriging : Methods and Applications|"In this chapter we present Kriging— also known as a Gaussian process (GP) model— which is a mathematical interpolation method. To select the input combinations to be simulated, we use Latin hypercube sampling (LHS); we allow uniform and non-uniform distributions of the simulation inputs. Besides deterministic simulation we discuss random simulation, which requires adjusting the design and analysis. We discuss sensitivity analysis of simulation models, using ""functional analysis of variance"" (FANOVA)— also known as Sobol sensitivity indexes. Finally, we discuss optimization of the simulated system, including ""robust"" optimization."
C44|Sequential Probability Ration Tests : Conservative and Robust|In practice, most computers generate simulation outputs sequentially, so it is attractive to analyze these outputs through sequential statistical methods such as sequential probability ratio tests (SPRTs). We investigate several SPRTs for choosing between two hypothesized values for the mean output (response). One SPRT is published in Wald (1945), and allows general distribution types. For a normal (Gaussian) distribution this SPRT assumes a known variance, but in our modified SPRT we estimate the variance. Another SPRT is published in Hall (1962), and assumes a normal distribution with an unknown variance estimated from a pilot sample. We also investigate a modification, replacing this pilot-sample estimator by a fully sequential estimator. We present a sequence of Monte Carlo experiments for quantifying the performance of these SPRTs. In experiment #1 the simulation outputs are normal. This experiment suggests that Wald (1945)’s SPRT with estimated variance gives significantly high error rates. Hall (1962)’s original and modified SPRTs are conservative; i.e., the actual error rates are much smaller than the prespecified (nominal) rates. The most efficient SPRT is our modified Hall (1962) SPRT. In experiment #2 we examine the robustness of the various SPRTs in case of nonnormal output. If we know that the output has a specific nonnormal distribution such as the exponential distribution, then we may also apply Wald (1945)’s original SPRT. Throughout our investigation we pay special attention to the design and analysis of these experiments.
C44|Testing the Assumptions of Sequential Bifurcation for Factor Screening (revision of CentER DP 2015-034)|Sequential bifurcation (or SB) is an efficient and effective factor-screening method; i.e., SB quickly identifies the important factors (inputs) in experiments with simulation models that have very many factors—provided the SB assumptions are valid. The specific SB assumptions are: (i) a secondorder polynomial is an adequate approximation (a valid metamodel) of the implicit input/output function of the underlying simulation model; (ii) the directions (signs) of the first-order effects are known (so the first-order polynomial approximation is monotonic); (iii) so-called “heredity” applies; i.e., if an input has no important first-order effect, then this input has no important second-order effects. Moreover—like many other statistical methods—SB assumes Gaussian simulation outputs if the simulation model is stochastic (random). A generalization of SB called “multiresponse SB” (or MSB) uses the same assumptions, but allows for simulation models with multiple types of responses (outputs). To test whether these assumptions hold, we develop new methods. We evaluate these methods through Monte Carlo experiments and a case study.
C44|Design and Analysis of simulation experiments : Tutorial|"This tutorial reviews the design and analysis of simulation experiments. These experiments may have various goals: validation, prediction, sensitivity analysis, optimization (possibly robust), and risk or uncertainty analysis. These goals may be realized through metamodels. Two types of metamodels are the focus of this tutorial: (i) low-order polynomial regression, and (ii) Kriging or Gaussian processes). The type of metamodel guides the design of the experiment; this design .…xes the input combinations of the simulation model. However, before a regression or Kriging metamodel is applied, the many inputs of the underlying realistic simulation model should be screened; the tutorial focuses on sequential bifurcation. Optimization of the simulated system may use either a sequence of low-order polynomials— known as response surface methodology— or Kriging models .…tted through sequential designs. Finally, ""robust"" optimization should account for uncertainty in simulation inputs. The tutorial includes references to earlier WSC papers."
C44|Simulation Optimization through Regression or Kriging Metamodels|"This chapter surveys two methods for the optimization of real-world systems that are modelled through simulation. These methods use either linear regression metamodels, or Kriging (Gaussian processes). The metamodel type guides the design of the experiment; this design …fixes the input combinations of the simulation model. These regression models uses a sequence of local fi…rst-order and second-order polynomials— known as response surface methodology (RSM). Kriging models are global, but are re-estimated through sequential designs. ""Robust"" optimization may use RSM or Kriging, and accounts for uncertainty in simulation inputs."
C44|Robust estimation of cost efficiency in non-parametric frontier models|"The paper proposes a bootstrap methodology for robust estimation of cost efficiency in data envelopment analysis. Our algorithm re-samples ""naive"" input-oriented efficiency scores, rescales original inputs to bring them to the frontier, and then re-estimates cost efficiency scores for the rescaled inputs. We consider the cases with absence and presence of environmental variables. Simulation analyses with multi-input multi-output production function demonstrate consistency of the new algorithm in terms of the coverage of the confidence intervals for true cost efficiency. Finally, we offer real data estimates for Japanese banking industry. Using the nationwide sample of Japanese banks in 2009, we show that the bias of cost efficiency scores may be linked to the bank charter and the presence of the environmental variables in the model. A package `rDEA', developed in the R language, is available from the GitHub and CRAN repository."
C44|A NaÃƒÂ¯ve Approach to Bidding|We propose a novel approach to the modelling of bidding behavior in pay-your-bid auctions that builds on the presumption that bidders are mostly concerned with losing an auction if they happen to have the highest signal. Our models assume risk neutrality, no profit maximization and no belief about competitors' behavior. They may entail overbidding in first-price and all-pay auctions and we discuss conditions for the revenue equivalence of standard pay-your-bid auctions to hold. We fit the models to the data of first-price auction experiments and find that they do at least as well as Vickreys benchmark model for risk neutral bidders. Assuming probability misperception or impulse weighting (when relevant) improves their goodness-of-fit and leads to very similar revenue predictions. An analysis of individuals' heterogeneous behavioral traits suggests that impulse weighting is a more consistent rationale for the observed behavior than a power form of probability misperception.
C44|Testing for Prospect and Markowitz stochastic dominance efficiency|We develop non-parametric tests for prospect stochastic dominance Efficiency (PSDE) and Markowitz stochastic dominance efficiency (MSDE) using block bootstrap resampling. Under the appropriate conditions we show that they are a symptotically conservative and consistent. We employ Monte Carlo experiments to assess the finite sample size and power of the tests. We use the tests to empirically establish whether the value-weighted market portfolio is the best choice of every individual with preferences exhibiting certain patterns of local attitudes to- wards risk. Our results indicate that we cannot reject the hypothesis of prospect stochastic dominance efficiency for the market portfolio. This is supportive of the claim that the par- ticular portfolio can be rationalized as the optimal choice for any S-shaped utility function. Instead,we reject the hypothesis forMarkowitz stochastic dominance,which could imply that there exist reverse S-shaped utility functions that do not rationalize the market portfolio.
C44|Towards Sustainability: Effective Operations Strategies, Quality Management and Operational Excellence in Banking|This paper sets out to extend and deepen the understanding the ways toward economic sustainability through efficient and effective growth operations strategies, quality management and operational excellence in banking. In this study we define new quality management practices based on developed conceptual architecture of digital platform for operations function in banking. Additionally, we employ decision making framework consisted of two parts: introduction of new operations services using Total Unduplicated Reach and Frequency (TURF) statistical analysis and segregation of core from actual and augmented operations services utilizing Analytic Network Process (ANP) method based on BOCR model. Proposed quality management practices were used for the first time in this paper for particular purposes and have the high potential to impact the excellence in banking business. The study can contribute to operations management, quality management, innovation management, IT management, business process management and decision making in service organizations.
C44|Regional effectiveness of innovation – leaders and followers of the EU NUTS 0 and NUTS 2 regions|Innovation constitutes an important factor for growth in all EU countries. Regions of the EU play a principle role in shaping new innovation trajectories and in bringing out the hidden potential for national growth. However, it is not only the level of innovation that diversifies regions, but also the innovative potential and the level of its realization. Therefore, the aim of this paper is to assess the realization of innovative potential, defined as effectiveness, in EU NUTS 0 and, if possible, NUTS 2 regions. To accomplish this goal a relative effectiveness method in used. The DEA (Data Envelopment Analysis) makes it possible to analyse the relative technical effectiveness based on regional inputs and outputs, without incorporating the legal and technological specifications of innovations, thus treating it like a production process. The inputs of the process are employment in technology and knowledge-intensive sectors and R&D expenditure, while the outputs include the number of patents and GDP. All variables are standardized by the size of the economically active population. DEA results divide regions in to two groups – effective, being the leaders; and ineffective, or followers. The DEA approach was combined and extended by ESDA (Exploratory Spatial Data Analysis) in order to pinpoint spatial patterns of innovation efficiency across NUTS 2 regions. Defining the best practices and implementing the learning-from-the-best policy is important in the process of regional development and specialization
C44|Machine learning at central banks|We introduce machine learning in the context of central banking and policy analyses. Our aim is to give an overview broad enough to allow the reader to place machine learning within the wider range of statistical modelling and computational analyses, and provide an idea of its scope and limitations. We review the underlying technical sources and the nascent literature applying machine learning to economic and policy problems. We present popular modelling approaches, such as artificial neural networks, tree-based models, support vector machines, recommender systems and different clustering techniques. Important concepts like the bias-variance trade-off, optimal model complexity, regularisation and cross-validation are discussed to enrich the econometrics toolbox in their own right. We present three case studies relevant to central bank policy, financial regulation and economic modelling more widely. First, we model the detection of alerts on the balance sheets of financial institutions in the context of banking supervision. Second, we perform a projection exercise for UK CPI inflation on a medium-term horizon of two years. Here, we introduce a simple training-testing framework for time series analyses. Third, we investigate the funding patterns of technology start-ups with the aim to detect potentially disruptive innovators in financial technology. Machine learning models generally outperform traditional modelling approaches in prediction tasks, while open research questions remain with regard to their causal inference properties.
C44|Robust estimation of cost efficiency in non-parametric frontier models|"The paper proposes a bootstrap methodology for robust estimation of cost efficiency in data envelopment analysis. Our algorithm re-samples ""naive"" input-oriented efficiency scores, rescales original inputs to bring them to the frontier, and then re-estimates cost efficiency scores for the rescaled inputs. We consider the cases with absence and presence of environmental variables. Simulation analyses with multi-input multi-output production function demonstrate consistency of the new algorithm in terms of the coverage of the confidence intervals for true cost efficiency. Finally, we offer real data estimates for Japanese banking industry. Using the nationwide sample of Japanese banks in 2009, we show that the bias of cost efficiency scores may be linked to the bank charter and the presence of the environmental variables in the model. A package `rDEA', developed in the R language, is available from the GitHub and CRAN repository."
C44|2018 FIFA World Cup qualification can be manipulated|In the European section of the 2018 FIFA World Cup qualification, 13 national teams, which are members of the Union of European Football Associations (UEFA), can qualify for the final competition. The 54 teams are divided into nine groups to play home-and-away round-robin matches in 10 matchdays. The winners of each group qualify, while the eight best second-placed teams advance to play-offs such that the four winners of play-offs also qualify. Ranking of second-placed teams differs from ranking in groups since group matches against the sixth-placed team are discarded. It is shown that this feature opens a way for manipulation: it may happen that a team is eliminated if it wins in the last matchday of group stage, but it advances to play-offs by playing a draw, provided that all other results are fixed. An example reveals that this situation might even occur in October 2017, after eight matchdays are already played in the qualification. Furthermore, by adjusting the result of only two matches played before October 2017 with an addition of one goal each, a team can strictly prefer a draw over a win in its last match as the former may advance it to play-offs, but the latter certainly leads to its elimination.
C44|Performance-Based Contracts for Energy Efficiency Projects|Energy efficiency projects are often executed by specialized entities, namely energy service companies (ESCOs). A typical ESCO's core business is conducted using performance-based contracts, whereby payment terms depend on the energy savings achieved. Despite their success in public, commercial, and industrial sectors, ESCOs in the residential sector are involved in fewer projects and face several challenges. First, an energy efficiency project often leads to changed consumption behavior; hence it is more difficult to evaluate the energy savings that are due to the project itself. The second challenge is that residential clients are more risk averse and, thus, less willing to contract for projects whose outcomes are uncertain. Third, a lack of monitoring protocols leads to ESCO's moral hazard problems. This paper studies ESCO contract design issues, focusing primarily on the residential market for energy efficiency. As opposed to other sectors, coordinating contracts do not exist. We show, however, that simple piecewise linear contracts work reasonably well. To improve their profitability, ESCOs can reduce uncertainty about the technology employed and/or develop ways of verifying post-project energy efficiency. Since policy makers are understandably keen to promote energy efficiency, we show also how regulations and monetary incentives can reduce inefficiencies in ESCOs' relationships and thereby maximize environmental benefits.
C44|Level of Focus of Organizations Operating in Slovakia on Flexible Organizational Structure|Flexible organisational structure is characterised by the creation of temporary units (teams) within a basic departmental structure to solve a particular difficult task, important for the organisation and limited by time. It can be stated on its grounds that team work is the basis of innovative organisation operation. However, fact that organisations most often use individuals and not the creation of a team specialised in the given issue in solving of new, important and demanding projects and tasks sounds negative. If organisations want to create so called innovative environment, it is necessary to focus on organisational structure, while it is necessary to realise that different organisational structures are appropriate for different types of organisations. Presumption which was the basis of our analysis of organisational structure flexibility was the fact that organisational flexibility results from its ability to respond to changing conditions and new situations. Therefore, we monitored the frequency of characteristics like adaptation of organisational structure to changing conditions and delegation of operational decisions to line managers. However, the research implied that only a smaller part of organisational structures of the analysed organisations fulfills these characteristics.
C44|Nonparametric estimation of dynamic discrete choice models for time series data|The non-parametric quasi-likelihood method is generalized to the context of discrete choice models for time series data, where the dynamic aspect is modeled via lags of the discrete dependent variable appearing among regressors. Consistency and asymptotic normality of the estimator for such models in the general case is derived under the assumption of stationarity with strong mixing condition. Monte Carlo examples are used to illustrate performance of the proposed estimator relative to the fully parametric approach. Possible applications for the proposed estimator may include modeling and forecasting of probabilities of whether a subject would get a positive response to a treatment, whether in the next period an economy would enter a recession, or whether a stock market will go down or up, etc.
C44|Regression and Kriging metamodels with their experimental designs in simulation: A review|This article reviews the design and analysis of simulation experiments. It focusses on analysis via two types of metamodel (surrogate. emulator); namely, low-order polynomial regression, and Kriging (or Gaussian process). The metamodel type determines the design of the simulation experiment, which determines the input combinations of the simulation model. For example, a first-order polynomial regression metamodel should use a “resolution-III”design, whereas Kriging may use “Latin hypercube sampling”. More generally, polynomials of first or second order may use resolution III, IV, V, or “central composite” designs. Before applying either regression or Kriging metamodeling, the many inputs of a realistic simulation model can be screened via “sequential bifurcation”. Optimization of the simulated system may use either a sequence of low-order polynomials—known as “response surface methodology” (RSM)—or Kriging models fitted through sequential designs—including “efficient global optimization” (EGO). Finally, “robust”optimization accounts for uncertainty in some simulation inputs.
C44|Technical efficiency, unions and decentralized labor contracts|This paper explores the link between the presence of unions in the workplace, the adoption of decentralized labor agreements and technical efficiency, using a large sample of Italian manufacturing firms. We apply the Data Envelopment Analysis, and its robust version based on bootstrap theory, to get reliable estimates of technical efficiency at the firm level in a standard first stage. We devote particular attention to the specific technology adopted, by distinguishing 20 different sector frontiers, as well as to the presence of outliers. The obtained efficiency scores are analyzed in a second stage applying a truncated regression model estimated via Maximum Likelihood, following the Simar and Wilson (2007, 2011) methodology. Our results highlight that the presence of workplace unionization decreases the level of technical efficiency, while aspects limiting the unions’ power such as a strong exposure to international markets, high debt levels or the prevalence of flexible assets partially reduce the negative effect. However, when firms adopt decentralized labor contracts agreements, the effect on efficiency is positive and partially compensates the negative unions’ effect.
C44|Measuring energy performance with sectoral heterogeneity: A non-parametric frontier approach|Evaluating economy-wide energy performance is an integral part of assessing the effectiveness of a country's energy efficiency policy. Non-parametric frontier approach has been widely used by researchers for such a purpose. This paper proposes an extended non-parametric frontier approach to studying economy-wide energy efficiency and productivity performances by accounting for sectoral heterogeneity. Relevant techniques in index number theory are incorporated to quantify the driving forces behind changes in the economy-wide energy productivity index. The proposed approach facilitates flexible modelling of different sectors' production processes, and helps to examine sectors' impact on the aggregate energy performance. A case study of China's economy-wide energy efficiency and productivity performances in its 11th five-year plan period (2006–2010) is presented. It is found that sectoral heterogeneities in terms of energy performance are significant in China. Meanwhile, China's economy-wide energy productivity increased slightly during the study period, mainly driven by the technical efficiency improvement. A number of other findings have also been reported.
C44|On optimal joint reflective and refractive dividend strategies in spectrally positive Lévy models|The expected present value of dividends is one of the classical stability criteria in actuarial risk theory. In this context, numerous papers considered threshold (refractive) and barrier (reflective) dividend strategies. These were shown to be optimal in a number of different contexts for bounded and unbounded payout rates, respectively.
C44|System stress testing of bank liquidity risk|Using a stress test methodology for bank liquidity risk we estimate the aggregate liquidity shortfall in the U.S. commercial banking system at the height of 2007–09 crisis, identifying key sources of funding vulnerabilities and the dominant composition of liquid asset holdings against liquidity shocks. The largest liquidity shocks to the system are estimated in the first half of the crisis, in line with Acharya and Mora (2015). Large banks experience the largest liquidity shortfall in 2008:Q1 ($154billion or 14% of total assets) and small banks in 2007:Q4 ($117billion or 11% of total assets). The dominant funding vulnerability to the system stems from large time deposits, while government securities largely dominate other classes of liquid assets as liquidity backstop. The analysis draws on detailed bank-level data on balance sheet flows of funds and applies stochastic dominance efficiency methods to capture liquidity risk diversification effects across assets and liabilities.
C44|Do charities get more when they ask more often? Evidence from a unique field experiment|Charitable organizations send out large volumes of direct mailings, soliciting for money in support of many good causes. Without any request, donations are rarely made, and it is well known that each request for money by a charity likely generates at least some revenues. Whether a single request from a charity increases the total amount donated by an individual is however unknown. Indeed, a response to one request can hurt responses to others. The net effect is therefore not easily observable, certainly not when multiple charities address the same individuals.
C44|Computation of the Corrected Cornish-Fisher Expansion using the Response Surface Methodology: Application to V aR and CV aR| The Cornish-Fisher expansion is a simple way to determine quantiles of non- normal distributions. It is frequently used by practitioners and by academics in risk mana- gement, portfolio allocation, and asset liability management. It allows us to consider non- normality and, thus, moments higher than the second moment, using a formula in which terms in higher-order moments appear explicitly. This paper has two primary objectives. First, we resolve the classic confusion between the skewness and kurtosis coefficients of the formula and the actual skewness and kurtosis of the distribution when using the Cornish{ Fisher expansion. Second, we use the response surface approach to estimate a function for these two values. This helps to overcome the difficulties associated with using the Cornish{ Fisher expansion correctly to compute value at risk (V aR). In particular, it allows a direct computation of the quantiles. Our methodology has many practical applications in risk ma- nagement and asset allocation.
C44|On the gains of using high frequency data and higher moments in Portfolio Selection|In this paper we conduct an empirical analysis on the performance gains of using high frequency data in Portfolio Selection. Within a CRRA-utility maximization framework, we suggest the construction of two different portfolios: a low and a high frequency portfolio. For ten different risk aversion levels, we compare the performance of both portfolios in terms of several out-of-sample measures. Using data on fourteen stocks of the CAC 40 stock market index, from January 1999 to December 2003, we conclude that the “fight” is always “won” by the high frequency portfolio for all the considered performance evaluation measures.
C44|Exploring health outcomes by stochastic multi-objective acceptability analysis: an application to Italian regions|This paper introduces the Stochastic Multi-Objective Acceptability Analysis (SMAA) in order to investigate the evolution of mortality rates in the Italian regions over the period 1990-2013. We propose to explore the overall outcome of health care by a Composite Index (CI) of mortality based on the combination of standardized mortality rates for seventeen different diseases. From a methodological standpoint, we propose to overcome the arbitrary of the weighting process, by using the SMAA, which is a methodology that allows to rank regions considering the whole set of possible vectors of weights. Moreover, we explore the spatial segregation in health using the multidimensional generalization of the Gini index, and introducing the multidimensional generalization of ANOGI. The unprecedented use of SMAA in health sector allows to explore regional multidimensional paths beyond the order of importance given to the single dimensions. Our analysis shows that in the 24 years considered there has been no convergence path in terms of health care outcome in Italy, neither between nor within regions.
C44|A critical review of multi-criteria decision making in protected areas| [EN]Multi-criteria analysis in collaborative decision making can provide a useful tool to im-prove the governance in protected areas with strong conflicts between stakeholders. This paper offers an in-depth review about MCDM methods in protected areas. The analysis considers the topics Land Use, Management, Species, and Zoning and it is based in two dimensions: Methods and Participation. Topics and MCDM methods and Topics and Participation were significantly related and contrasted using a Chi-squared test, respectively. We have identified two groups by topics: Zoning and Species use continuous non participative methods and Land Use and Management use discrete methods with increasing participation. [ES]El análisis multi-criterio para toma de decisiones colaborativa ofrece una herramienta útil para mejorar la gobernanza en áreas protegidas, con fuertes conflictos de intereses entre agentes. Este artículo ofrece una revisión en profundidad sobre métodos MCDM en áreas protegidas. El análisis con-sidera los temas Uso de la tierra, Gestión, Especies y Zonificación, y se basa en dos dimensiones: Mé-todos y Participación. Los Temas y las Técnicas MCDM y los Temas y la Participación están significa-tivamente correlacionados respectivamente según un test Chi-cuadrado. Hemos identificado dos grupos: Especies y Zonificación usan métodos continuos no participativos y Uso de la tierra y Gestión, utilizan métodos discretos donde la participación es creciente.
C44|Use of Constraints in the Hierarchical Aggregation Procedure Intramax|Background: Intramax is a hierarchical aggregation procedure for dealing with the multi-level specification problem and with the association issue of data set reduction, but it was used as a functional regionalization procedure many times in the past.
C44|Determination QAD in Audited and Unaudited Companies|Based on QAD (quality of accounting data) information it is possible to determine how the company is able to manage its economic activities. The basic element is the level of QAD perceived by users of accounting information. This article describes using the analytic hierarchy process (AHP) to get an overview to the quality of accounting information. Then there is used statistical methods T-test and Kolmogorov-Smirnov test to set the differences between QAD in audited and unaudited companies. The first step is to define the various criteria which adversely affect the quality of accounting data and then their assignment to groups according to the context. Subsequently, on the basis of Saatys method determine the weights of the criteria in each group and then by using the AHP method determine their importance. Then the quality of accounting data in 71 companies is determined and the differences between accounting data quality of audited companies and accounting data quality of unaudited companies are detected.
C44|Quantum Decision Theory in Simple Risky Choices|Quantum decision theory (QDT) is a novel theory of decision making based on the mathematics of Hilbert spaces, a framework known in physics for its application to quantum mechanics. This framework formalizes the concept of uncertainty and other effects that are particularly manifest in cognitive processes, which makes it well suited for the study of decision making. QDT describes a decision maker's choice as a stochastic event occurring with a probability that is the sum of an objective utility factor and a subjective attraction factor. This article offers a practical guide to researchers who are interested in applying QDT to a data set of binary lotteries in the domain of gains. We find that our results are in good agreement with the quarter law, a quantitative prediction of QDT. We examine gender differences in our sample in order to illustrate how QDT can be used to differentiate between different groups. We find that women in our sample are on average more risk-averse than men, but stress that our sample is too small to generalize this result to the population outside our sample.
C44|Calibration of Quantum Decision Theory, Aversion to Large Losses and Predictability of Probabilistic Choices|"We present the first calibration of quantum decision theory (QDT) to an empirical data set. The data comprise 91 choices between two lotteries (two ""prospects"") presented in 91 random pairs made by 142 subjects offered at two separated times. First, we quantitatively account for the fraction of choice reversals between the two repetitions of the decisions, using a probabilistic choice formulation in the simplest possible form with no model assumption and no adjustable parameter. The prediction of choice reversal is then refined by introducing heterogeneity between decision makers through a differentiation of the population into two similar sized groups in terms of ""over-confident"" and ""contrarian"" decision makers. This supports the first fundamental tenet of QDT, which models the choice of an option as an inherent probabilistic process, such that the probability of a choice can be expressed as the sum of its utility and attraction factors. We propose to model (a) the utility factor with a stochastic version of cumulative prospect theory (logit-CPT), and (b) the attraction factor with a constant absolute risk aversion (CARA) function. This makes logit-CPT nested in our proposed parameterisation of QDT, allowing for a precise quantitative comparison between the two theories. For this data set, the QDT model is found to perform better at both the aggregate and individual levels, and for all considered fit criteria both for the first iteration of the experiment and for predictions (second iteration). The QDT effect associated with the attraction factor is mostly appreciable for prospects with big losses. Our quantitative analysis of the experiment results supports the existence of an intrinsic limit of predictability, which is associated with the inherent probabilistic nature of choice."
C44|The productivity of top researchers: a semi-nonparametric approach|Abstract Research productivity distributions exhibit heavy tails because it is common for a few researchers to accumulate the majority of the top publications and their corresponding citations. Measurements of this productivity are very sensitive to the field being analyzed and the distribution used. In particular, distributions such as the lognormal distribution seem to systematically underestimate the productivity of the top researchers. In this article, we propose the use of a (log)semi-nonparametric distribution (log-SNP) that nests the lognormal and captures the heavy tail of the productivity distribution through the introduction of new parameters linked to high-order moments. The application uses scientific production data on 140,971 researchers who have produced 253,634 publications in 18 fields of knowledge (O’Boyle and Aguinis in Pers Psychol 65(1):79–119, 2012) and publications in the field of finance of 330 academic institutions (Borokhovich et al. in J Finance 50(5):1691–1717, 1995), and shows that the log-SNP distribution outperforms the lognormal and provides more accurate measures for the high quantiles of the productivity distribution.
C44|Quantile forecasting in operational planning and inventory management – an initial empirical verification|In the paper we present our initial results of an empirical verification of different methodologies of quantile forecasting used in operational management to calculate the re-order point or order-up-to level as well as the optimal order quantity according to the newsvendor model. The comparison encompasses 26 procedures including quantile regression, the basic bootstrap method and popular textbook formulas. Our results, obtained on the base of 30 time series concerning such diversified phenomena as supermarket sales, passenger transport and water and gas demand, point to the usefulness of regression medians, regression quantiles, bootstrap methods a 19 19nd the pble in the SAP ERP system.
C44|Group Decision Making Procedure Based On Trapezoidal Intuitionistic Fuzzy Numbers: Multimoora Methodology|This paper proposes a group multi-criteria decision making approach based on MULTIMOORA method and trapezoidal intuitionistic fuzzy numbers (ITFNs). Specifically, the definition of ITFN by Nehi and Maleki (2005) is followed. The proposed approach relies on the trapezoidal intuitionistic fuzzy power aggregation operators, which reduce the impact of biased assessments in the group decision making. An illustrative example is provided to demonstrate the operationality of the proposed methodology.
C44|Development of Network-Ranking Model to Create the Best Production Line Value Chain: A Case Study in Textile Industry|The main reason for creating value chain is fulfilling needs and organizational resources with the least cost and highest quality. Application of most of the current techniques has merely intended to choose the best scenario. But industrial units need to build an ideal scenario as a value chain which focuses on intangible interstitial and hidden factors: good (good nature), bad (bad nature), fixed (obligatory nature) and free (not identifying their nature) and creates value. Therefore, the model presented in this article answers this issue. First of all we present a model based on the network approach of data envelopment analysis, then we assess and rank the stages based on the scenarios for the stages forming the value chain and finally, the ideal decision unit is presented. For this reason, the general efficiency is designed with two natures; 1.input-centered (concentration on the costs) and 2.output-centered (concentration on the incomes).
C44|A New Method Of Assessment Based On Fuzzy Ranking And Aggregated Weights (Afraw) For Mcdm Problems Under Type-2 Fuzzy Environment|Fuzzy multi-criteria decision-making (MCDM) methods and problems have increasingly been considered in the past years. Type-1 fuzzy sets are usually used by decision-makers (DMs) to express their evaluations in the process of decision-making. Interval type-2 fuzzy sets (IT2FSs), which are extensions of type-1 fuzzy sets, have more degrees of flexibility in modeling of uncertainty. In this research, a new ranking method to calculate the ranking values of interval type-2 fuzzy sets is proposed. A comparison is performed to show the efficiency of this ranking method. Using the proposed ranking method and the arithmetic operations of IT2FSs, a new method of Assessment based on Fuzzy Ranking and Aggregated Weights (AFRAW)is developed for multi-criteria group decision-making. To obtain more realistic and practical weights for the criteria, the subjective weights expressed by DMs and objective weights calculated based on a deviation-based method are combined, and the aggregated weights are used in the proposed method. A numerical example related to assessment of suppliers in a supply chain and selecting the best one is used to illustrate the procedure of the proposed method. Moreover, a comparison and a sensitivity analysis are performed in this study. The results of these analyses show the validity and stability of the proposed method.
C44|Multiple Attribute Group Decision Making Methods Based on Intuitionistic Fuzzy Generalized Hamacher Aggregation Operator|With respect to multiple attribute group decision making (MAGDM) problems in which attribute values take the form of the intuitionistic fuzzy values(IFVs), the group decision making method based on some generalized Hamacher aggregation operators which generalized the arithmetic aggregation operators and geometric aggregation operators and extended the Algebraic aggregation operators and Einstein aggregation operators, is developed. Firstly, the generalized intuitionistic fuzzy Hamacher weighted averaging(IFGHWA) operator, intuitionistic fuzzy generalized Hamacher ordered weighted averaging(IFGHOWA) operator, and intuitionistic fuzzy generalized Hamacher hybrid weighted averaging(IFGHHWA) operator, were proposed, and some desirable properties of these operators, such as commutativity, idempotency, monotonicity and boundedness, were studied. At the same time, some special cases in these operators were analyzed. Furthermore, one method to multi-criteria group decision-making based on these operators was developed, and the operational processes were illustrated in detail. Finally, an illustrative example is given to verify the proposed methods and to demonstrate their practicality and effectiveness.
C44|A New Combinative Distance-Based Assessment(Codas) Method For Multi-Criteria Decision-Making|A key factor to attain success in any discipline, especially in a field which requires handling large amounts of information and knowledge, is decision making. Most real-world decision-making problems involve a great variety of factors and aspects that should be considered. Making decisions in such environments can often be a difficult operation to perform. For this reason, we need multi-criteria decision-making (MCDM) methods and techniques, which can assist us for dealing with such complex problems. The aim of this paper is to present a new COmbinative Distance-based ASsessment (CODAS) method to handle MCDM problems. To determine the desirability of an alternative, this method uses the Euclidean distance as the primary and the Taxicab distance as the secondary measure, and these distances are calculated according to the negative- ideal point. The alternative which has greater distances is more desirable in the CODAS method. Some numerical examples are used to illustrate the process of the proposed method. We also perform a comparative sensitivity analysis to examine the results of CODAS and compare it by some existing MCDM methods. These analyses show that the proposed method is efficient, and the results are stable.
C44|Ranking Enterpreneurship Main Risks In Non-Profit Financial Funds By Todim Technique Under Grey Conditions (A Case Study In Iran)|Entrepreneurship risks are the most important topics that should be considered over implementation of any business. Lack of attention to entrepreneurship risk issues in new business can lead to failure in achieving expected result. According to the fact that risk issues always accompany with uncertainty and also success or failure of entrepreneurship activity seems presumable, we should provide a mechanism to consider the importance of it. The purpose of this study is to make a discussion and prioritize entrepreneurship main risks in non-profit financial funds in Iran based on uncertainty circumstance to identify main risks and then providing suitable control activity to reduce their effect. For this purpose in this paper we propose a method from combination of Todim technique and grey numbers theory to prioritize entrepreneurship risks in non-profit financial funds in Iran. The results show that incompetency of the personnel and managers, is the most important entrepreneurship risk in these funds.
C44|Fuzzy Multi Objective Project Scheduling Under Inflationary Conditions|Project scheduling is a major area of project management and planning. Due to the increasing prices of goods in most countries during the project and lack of information related to each parameters and variables, using fuzzy sets under inflationary condition can be efficient. In this paper, fuzzy project scheduling when project’s revenues and costs are increased by different inflationary rate is considered. The duration of each project’s activity is assumed as a fuzzy number. Two objectives of the problem are to minimize the project duration and to maximize the net present value of the project. The nonlinear multi-objective model will be solved by one of the multi- objective decision making methods named LP-Metric method and Frank-Wolf algorithm as one of the nonlinear programming procedure. The solving approach will be explained in a numerical example and the computational results will be reported.
C44|Logistic Regression In Modelling Some Sustainable Development Phenomena|Technological innovations of the last decade have led to a real explosion of data and a practically unlimited capacity to create and to store them, remodelling day to day life. This paper analyses theoretical models for qualitative variables used in sustainable development. More exact and detailed information on natural resources are vital to the state, as well as to environmental agencies and to the private sector. The type of forest vegetation is one of the basic characteristics that are recorded and analysed in order to maintain the ecological balance. Generally, the type of forest vegetation is either recorded directly by the agents, or by tele-detection. Both techniques are costly both in financial and time terms or even impossible to do. Predictive models offer an alternative to obtain this data. Although linear regression models are used on a wide scale by biologists and ecologists, these models are inadequate when the dependent variable is qualitative.. Logit models are a natural complement to regression models, where the endogenous variable is a qualitative variable, a situation that may be obtained or not, or a category of a classification. The popularity of logit models is explained by the multivariate nature of the models and the easiness with which they can be interpreted.
C44|Multicriteria Decision-Making Method Based On Cosine Similarity Measures Between Interval- Valued Fuzzy Sets With Risk Preference|This paper presents the cosine similarity measure between IVFSs with risk preference and gives its decision making method using the cosine similarity measure depending on decision makers’ optimistic, neutral, and pessimistic natures for the subjective judgments that accompany the decision making process. Through the weighted cosine similarity measure between an alternative and the ideal alternative corresponding to one of optimistic, neutral, and pessimistic choices desired by decision makers, we can determine the ranking order of alternatives and the best one. This choosing feature corresponding to decision makers’ preference makes the proposed method not only more flexible, but also more suitable for many practical applications. Finally, an illustrative example is presented to demonstrate the feasibility and applicability of the proposed method.
C44|Finding the Right Yardstick: Regulation under Heterogeneous Environments|Revenue cap regulation is often combined with systematic benchmarking to reveal the managerial inefficiencies when regulating natural monopolies. One example is the European energy sector, where benchmarking methods are based on actual cost data, which are influenced by managerial inefficiency as well as operational heterogeneity. This paper demonstrates how a conditional nonparametric method, which allows the comparison of firms operating under heterogeneous technologies, can be used to estimate managerial inefficiency. A dataset of 123 distribution firms in Norway is used to show aggregate and firm-specific effects of conditioning. By comparing the unconditional model to our proposed conditional model and the model presently used by the Norwegian regulator, we see that the use of conditional benchmarking methods in revenue cap regulation may effectively distinguish between managerial inefficiency and operational heterogeneity. This distinction leads first to a decrease in aggregate efficient costs and second to a reallocation effect that affects the relative profitability of firms and relative customer prices, thus providing a fairer basis for setting revenue caps.
C44|Measuring Efficiency and Total Factor Productivity using Data Envelopment Analysis: An Empirical Study from Banks of Turkey|The main purpose of this study is to assess the performance of deposit banks operating in the Turkish Banking Sector for the years 2014-2015 using Data Envelopment Analysis (DEA) and malmquist productivity index (MPI) methodologies. In the light of this aim, 21 deposit banks’ data obtained from The Banks Association of Turkey between 2014 and 2015 is used. There are 2 inputs and 2 outputs variables for Production Approach and 3 inputs and 3 outputs variables for Intermediation Approaches. To measure productivity changes over time, MPI index is calculated from DEA scores. Thus, the influencing factors of relative efficiency and the efficient (or inefficient) banks are determined. As a result, average # of Staff per Branch, total personal expenses/total assets and total deposits/total assets have important role for efficiency in production approach. In intermediation approach, non-interest expenditure/total assets, total loans/total assets and non-interest income/total assets are associated with the efficiency
C44|Hizmet Personeline Güvenin Belirlenmesi Ýçin Yeni Bir Ölçek Önerisi|Araþtýrmacýlar güven kavramýný farklý açýlardan tanýmlamakta ve ölçmektedirler. Hizmet sektöründe; ürünün soyutluðu ve hizmeti sunandan ayrýlamamasý, güven ölçümlerinde özgün unsurlarýn gözetilmesini gerektirmektedir. Bu çalýþma; hizmet sunana yönelik güven yargýsýný açýklamak için daha bütüncül ve açýklama düzeyi yüksek bir ölçeðin, tüketici davranýþýný anlamak ve tüketim sonrasý eðilimleri yönetebilmek açýsýndan önemi gözetilerek tasarlanmýþtýr. Dört ayrý saha çalýþmasýyla elde edilen verilere uygulanan analizler ve bulgularýn karþýlaþtýrmalý deðerlendirmesi sonucu; beþ maddeden oluþan tek boyutlu ve .70’in üzerinde açýklama gücü olan güven ölçeði geliþtirilmiþtir. Yeni güven ölçeðinin psikometrik nitelikleri, daha önceki güven ölçeklerine kýyasla daha güçlüdür. Yeni güven ölçeðinin tüketici güvenine yönelik çalýþma yapacak olan araþtýrmacý ve uygulayýcýlara katký saðlayacaðý deðerlendirilmektedir
C44|Energy-related CO2 emission in European Union agriculture: Driving forces and possibilities for reduction|Climate change mitigation is a key issue in formulating global environmental policies. Energy production and consumption are the main sources of greenhouse gas (GHG) emissions in Europe. Energy consumption and energy-related GHG emissions from agriculture are an important concern for policymakers, as the agricultural activities should meet food security goals along with proper economic, environmental, and social impacts. Carbon dioxide (CO2) emission is the most significant among energy-related GHG emissions. This paper analyses the main drivers behind energy-related CO2 emission across agricultural sectors of European countries. The analysis is based on aggregate data from the World Input-Output Database. The research explores two main directions. Firstly, Index Decomposition Analysis (IDA), facilitated by the Shapley index, is used to identify the main drivers of CO2 emission. Secondly, the Slack-based Model (SBM) is applied to gauge the environmental efficiency of European agricultural sectors. By applying frontier techniques, we also derive the measures of environmental efficiency and shadow prices, thereby contributing to a discussion on CO2 emission mitigation in agriculture. Therefore, the paper devises an integrated approach towards analysis of CO2 emission based upon advanced decomposition and efficiency analysis models. The research covers eighteen European countries and the applied methodology decomposes contributions to CO2 emission across of regions and factors. Results of IDA suggest that decreasing energy intensity is the main factor behind declines in CO2 emission. According to the SBM, the lowest carbon shadow prices are observed in France, Finland, Sweden, Denmark, the Netherlands, Poland, and Belgium. These countries thus have the highest potential for reduction in CO2 emission. The results imply that measures to increase energy efficiency are a more effective means to reduce CO2 emissions than are changes in the fuel-mix.
C44|Endogenous network production functions with selectivity|We consider a production function that transforms inputs into outputs through peer effect networks. The distinguishing features of this model are that the network is formal and observable through worker scheduling, and selection into the network is done by a manager. We discuss identification and suggest several estimation techniques. We tackle endogeneity arising from selection into groups and exposure to common group factors by employing a polychotomous Heckman-type selection correction. We illustrate our method using data from the Syracuse University Men’s Basketball team, where at any time the coach selects a lineup and players interact strategically to win games.
C44|Capacity market design options: A dynamic capacity investment model and a GB case study|Rising feed-in from renewable energy sources decreases margins, load factors, and thereby profitability of conventional generation in several electricity markets around the world. At the same time, conventional generation is still needed to ensure security of electricity supply. Therefore, capacity markets are currently being widely discussed as a measure to ensure generation adequacy in markets such as France, Germany, and the United States (e.g., Texas), or even implemented for example in Great Britain.
C44|Capacity payment impact on gas-fired generation investments under rising renewable feed-in — A real options analysis|We assess the effect of capacity payments on investments in gas-fired power plants in the presence of different degrees of renewable energy technology (RET) penetration. Low variable cost renewables increasingly make investments in gas-fired generation unprofitable. At the same time, growing feed-in from intermittent RETs amplifies fluctuations in power generation, thus entailing the need for flexible buffer capacity—currently mostly gas-fired power plants. A real options approach is applied to evaluate investment decisions and timing of a single investor in gas-fired power generation. We investigate the necessity and effectiveness of capacity payments. Our model incorporates multiple uncertainties and assesses the effect of capacity payments under different degrees of RET penetration. In a numerical study, we implement stochastic processes for peak-load electricity prices and natural gas prices. We find that capacity payments are an effective measure to promote new gas-fired generation projects. Especially in times of high renewable feed-in, capacity payments are required to incentivize peak-load investments.
C44|Optimal design of feed-in-tariffs to stimulate renewable energy investments under regulatory uncertainty — A real options analysis|Feed-in-tariffs (FITs) are widely used as policy instruments to promote investments in renewable energy sources (RES). While FITs are often regarded as the most effective RES support scheme, regulators around the world continuously review their FIT schemes in the light of budget constraints and evolving policy goals. We assess the impact of adjustments to FIT schemes by quantifying the relationship between FIT levels, i.e., the guaranteed amount paid per quantity of electricity produced and the propensity to invest in RES. Through a regime switching model, we quantify the impact of regulatory uncertainty induced by regulators considering moves from a FIT scheme to a more market-oriented regulatory regime. Our focus is on market-independent, fixed FITs, the dominant scheme in Europe receiving increasing attention globally. We find that RES investment projects under market-independent, fixed FIT schemes become now-or-never decisions and derive FIT thresholds required to induce investment. We show that uncertainty regarding future regulatory regimes delays or even reduces investment activity for FIT levels near electricity market prices and high probabilities of an imminent regime switch.
C44|Value and granularity of ICT and smart meter data in demand response systems|The large-scale integration of intermittent resources of power generation leads to unprecedented fluctuations on the supply side. An electricity retailer can tackle these challenges by pursuing strategies of flexible load shifting — so-called demand response mechanisms. This work addresses the associated trade-off between ICT deployment and economic benefits. The ICT design of a demand response system serves as the basis of a cost-value model, which incorporates all relevant cost components and compares them to the expected savings of an electricity retailer. Our analysis is based on a typical German electricity retailer to determine the optimal read-out frequency of smart meters. For our set of parameters, a positive information value of smart meter read-outs is achieved within an interval of 21 to 57min regarding variable costs. Electricity retailers can achieve a profitable setting by restricting smart meter roll-out to large customers.
C44|Portfolio selection with conservative short-selling|Mean-variance analysis is considered the foundation of portfolio selection. Among various attempts to address the limitations of the original model as formulated by Markowitz more than 60 years ago, one simple solution has been to impose constraints on weights in order to reduce efficient portfolios with extreme weights that may be caused by estimation errors in the inputs. Although no short-selling constraints are often considered, the restriction removes opportunities to gain from short-selling and short positions provide various investment opportunities such as long/short strategies. In this paper we propose a portfolio selection model that allows short positions while examining the worst case only for assets that are assigned negative weights. The proposed model constructs portfolios with conservative short positions and the conservative level can be adjusted by the investor.
C44|Blackwell's informativeness ranking with uncertainty-averse preferences|Blackwell (1951, 1953) proposes an informativeness ranking of experiments: experiment I is more Blackwell-informative than experiment II if and only if the value of experiment I is higher than that of experiment II for all expected-utility maximizers. Under commitment and reduction, our main theorem shows that Blackwell equivalence holds for all convex and strongly monotone preferences—i.e., uncertainty-averse preferences (Cerreia-Vioglio et al., 2011b), which nest most ambiguity-averse preferences commonly used in applications as special cases.
C44|Risk reducers in convex order|Given a risk position X, a random addition Z is called a risk reducer for X if the new position X+Z is less risky than X+E[Z] in convex order. We utilize the concept of convex hull to give a structural description of risk reducers in the case of an atomless probability space. Then we study risk reducers that are fully dependent on X. Applications to multivariate stochastic ordering, index-linked hedging strategies, and optimal reinsurance are proposed.
C44|Endogenous information acquisition in Bayesian games with strategic complementarities|This paper studies covert (or hidden) information acquisition in common value Bayesian games of strategic complementarities. Using the supermodular stochastic order to arrange the structures of information increasingly in terms of preferences, we provide novel, easily interpretable and verifiable, though restrictive conditions under which the value of information is increasing and convex, and study the implications in terms of the equilibrium configuration. Increasing marginal returns to information leads to extreme behavior in that agents opt either for the highest or the lowest quality signal, so that the final analysis of this complex game simplifies greatly into that of a two-action game. This result can rationalize the complete information game as an endogenous outcome. Finally, we also establish that higher-quality information leads players to select more dispersed actions in the Bayesian game.
C44|Testing quantum-like models of judgment for question order effect|Lately, so-called “quantum” models, based on parts of the mathematics of quantum mechanics, have been developed in decision theory and cognitive sciences to account for seemingly irrational or paradoxical human judgments. We consider here some such quantum-like models that address question order effects, i.e. cases in which given answers depend on the order of presentation of the questions. Models of various dimensionalities could be used; can the simplest ones be empirically adequate? From the quantum law of reciprocity, we derive new empirical predictions that we call the Grand Reciprocity equations, that must be satisfied by several existing quantum-like models, in their non-degenerate versions. Using substantial existing data sets, we show that these non-degenerate versions fail the GR test in most cases, which means that, if quantum-like models of the kind considered here are to work, it can only be in their degenerate versions. However, we suggest that the route of degenerate models is not necessarily an easy one, and we argue for more research on the empirical adequacy of degenerate quantum-like models in general.
C44|Size distribution of Portuguese firms between 2006 and 2012|This study aims to describe the size distribution of Portuguese firms, as measured by annual sales and total assets, between 2006 and 2012, giving an economic interpretation for the evolution of the distribution along the time. Three distributions are fitted to data: the lognormal, the Pareto (and as a particular case Zipf) and the Simplified Canonical Law (SCL). We present the main arguments found in literature to justify the use of distributions and emphasize the interpretation of SCL coefficients. Methods of estimation include Maximum Likelihood, modified Ordinary Least Squares in log–log scale and Nonlinear Least Squares considering the Levenberg–Marquardt algorithm. When applying these approaches to Portuguese’s firms data, we analyze if the evolution of estimated parameters in both lognormal power and SCL is in accordance with the known existence of a recession period after 2008. This is confirmed for sales but not for assets, leading to the conclusion that the first variable is a best proxy for firm size.
C44|A new evaluation and decision making framework investigating the elimination-by-aspects model in the context of transportation projects' investment choices|The Transportation Elimination-by-Aspects (TEBA) framework, a new evaluation and decision making framework (and methodology) for large transportation projects, is proposed to elicit, structure and quantify the preferences of stakeholder groups across project alternatives. The decision rule used for group decision making within TEBA is the individual non-compensatory model of choice elimination by aspects (EBA). TEBA is designed to bring out the decision rule employed by decision makers when ranking the options presented, incorporate various criteria types and ease communication of relevant information related to options and criteria for multiple stakeholder groups. It is a platform for democratizing the decision making process. The TEBA framework was tested using a case study investigating alternative land connections between Beirut and Damascus. Key results showed that (1) stakeholders have employed EBA in making decisions, (2) a defined group of decision makers will rank options differently when provided with modified sets of criteria, (3) the public sector and general public groups ranked Impact on Employment among the top criteria, (4) the most important criterion per group from EBA was as expected; (5) the EBA analysis suggested that only 3–4 criteria are significant in reaching a decision; (6) aggregation of user assigned weights masked relative importance of criteria in some cases; and (7) analysis of user assigned weights and Minimum Threshold (MT) values suggest higher risk perception with increased criterion importance. Policy implications include recommendation to reach out to stakeholders for input on decisions, including the “people” but refrain from relying on criteria weights assigned by “experts” and reduce the “experts”’ role in decision making. Also, it is recommended to model the decision making in a probabilistic framework rather than a deterministic “one score” approach, seek to identify a consensus ranking, place particular attention on determining the values of the criteria that emerged as “top” at the evaluation stage and continue to emphasize risk measures.
C44|Performans Degerlemesinde DEMATEL ve Bulanik TOPSIS Uygulamasi|Bu calismada Turkiye Seker Fabrikalari Anonim Sirketi adi altinda toplanmis 23 seker fabrikasinin 2008-2012 yillari arasindaki verileri kullanilarak performans degerlendirmesi yapilmistir. Calismada fiilen islenen pancar, yakit tuketimi, is gucu, seker uretimi, makine kapasitesi, melas miktari ve satis miktari kriter olarak dikkate alinmis ve bu kriterlerin agirliklari DEMATEL yontemi ile hesaplanmistir. Bu agirliklar kullanilarak bulanik TOPSIS yontemi ile performans degerlemesi yapilmistir. Bulanik TOPSIS uygulamasi yapilirken tamamen gercek verilerden yararlanilmistir. Gercek veriler ucgen bulanik sayilar kullanilarak bulaniklastirilmistir. Calismanin sonuclari incelendiginde performans acisindan Eregli, Ilgin ve Eskisehir seker fabrikalari ilk uce girerken, Alpullu, Elazig ve Kars seker fabrikalarinin son siralarda yer aldigi gorulmektedir.
C44|Dynamic benchmark targeting|We study decision making in complex discrete-time dynamic environments where Bayesian optimization is intractable. A decision maker is equipped with a finite set of benchmark strategies. She aims to perform similarly to or better than each of these benchmarks. Furthermore, she cannot commit to any decision rule, hence she must satisfy this goal at all times and after every history. We find such a rule for a sufficiently patient decision maker and show that it necessitates not to rely too much on observations from distant past. In this sense we find that it can be optimal to forget.
C44|Portfolio choice with high frequency data: CRRA preferences and the liquidity effect|Abstract This paper suggests a new approach for portfolio choice. In this framework, the investor, with CRRA preferences, has two objectives: the maximization of the expected utility and the minimization of the portfolio expected illiquidity. The CRRA utility is measured using the portfolio realized volatility, realized skewness and realized kurtosis, while the portfolio illiquidity is measured using the well-known Amihud illiquidity ratio. Therefore, the investor is able to make her choices directly in the expected utility/liquidity (EU/L) bi-dimensional space. We conduct an empirical analysis in a set of fourteen stocks of the CAC 40 stock market index, using high frequency data for the time span from January 1999 to December 2005 (seven years). The robustness of the proposed model is checked according to the out-of-sample performance of different EU/L portfolios relative to the minimum variance and equally weighted portfolios. For different risk aversion levels, the EU/L portfolios are quite competitive and in several cases consistently outperform those benchmarks, in terms of utility, liquidity and certainty equivalent.
C44|A comparison of the GAI model and the Choquet integral with respect to a k-ary capacity|Two utility models are classically used to represent interaction among criteria: the Choquet integral and the Generalized Additive Independence (GAI) model. We propose a comparison of these models. Looking at their mathematical expression, it seems that the second one is much more general than the first one. The GAI model has been mostly studied in the case where attributes are discrete. We propose an extension of the GAI model to continuous attributes, using the multi-linear interpolation. The values that are interpolated can in fact be interpreted as a k-ary capacity, or its extension – called p-ary capacity – where p is a vector and pi is the number of levels attached to criterion i. In order to push the comparison further, the Choquet integral with respect to a p-ary capacity is generalized to preferences that are not necessarily monotonically increasing or decreasing on the attributes. Then the Choquet integral with respect to a p-ary capacity differs from a GAI model only by the type of interpolation model. The Choquet integral is the Lovász extension of a p-ary capacity whereas the GAI model is the multi-linear extension of a p-ary capacity
C44|Star Wars: The Empirics Strike Back|"Using 50,000 tests published in the AER, JPE, and QJE, we identify a residual in the distribution of tests that cannot be explained solely by journals favoring rejection of the null hypothesis. We observe a two-humped camel shape with missing p-values between 0.25 and 0.10 that can be retrieved just after the 0.05 threshold and represent 10-20 percent of marginally rejected tests. Our interpretation is that researchers inflate the value of just-rejected tests by choosing ""significant"" specifications. We propose a method to measure this residual and describe how it varies by article and author characteristics. (JEL A11, C13)"
C44|A Comparison of Different Window Models for Measuring Technical Efficiency|This study analyzes the technical efficiency of Iran's 13 major cotton producing provinces over the period 2000-2012. To assess technical efficiency over time we use several non-parametric versions of the Window methodology. These include Window-DEA, Window-FDH, Window order- and Window order-m models. In a Window’s analysis one uses moving average patterns as a provincial observation in each period treated as if it is a different observation leading to a higher number of observations. The aim is to shed light on temporal patterns and heterogeneity in efficiency, time dependency and sensitivity of the results attributed to different efficiency measurement methodologies that are applied using the same dataset. It then invokes four consistency conditions to examine these scores and rankings. According to our results the models Window-FDH, Window order- and Window order-m are consistent. We were able to identify the most (least) efficient provinces and follow temporal patterns of their performance in the production of cotton. The findings reveal that provinces with larger scales of production are not necessarily more efficient. The results also indicate that although there are differences in the levels of efficiency across the models used, efficiency rankings from various models tend to support similar conclusions about the relative performance of the sample provinces.
C44|Finding the Right Yardstick: Regulation under Heterogeneous Environments|Revenue cap regulation is often combined with systematic benchmarking to reveal the managerial inefficiencies when regulating natural monopolies. One example is the European energy sector, where benchmarking methods are based on actual cost data, which are influenced by managerial inefficiency as well as operational heterogeneity. This paper demonstrates how a conditional nonparametric method, which allows the comparison of firms operating under heterogeneous technologies, can be used to estimate managerial inefficiency. A dataset of 123 distribution firms in Norway is used to show aggregate and firm-specific effects of conditioning. By comparing the unconditional model to our proposed conditional model and the model presently used by the Norwegian regulator, we see that the use of conditional benchmarking methods in revenue cap regulation may effectively distinguish between managerial inefficiency and operational heterogeneity. This distinction leads first to a decrease in aggregate efficient costs and second to a reallocation effect that affects the relative profitability of firms and relative customer prices, thus providing a fairer basis for setting revenue caps.
C44|Integrated maritime bunker management with stochastic fuel prices and new emission regulations|Maritime bunker management (MBM) controls the procurement and consumption of the fuels used on board and therefore manages one of the most important cost drivers in the shipping industry. At the operational level, a shipping company needs to manage its fuel consumption by making optimal routing and speed decisions for each voyage. But since fuel prices are highly volatile, a shipping company sometimes also needs to do tactical fuel hedging in the forward market to control risk and cost volatility. From an operations research perspective, it is customary to think of tactical and operational decisions as tightly linked. However, the existing literature on MBM normally focuses on only one of these two levels, rather than taking an integrated point of view. This is in line with how shipping companies operate; tactical and operational bunker management decisions are made in isolation. We develop a stochastic programming model involving both tactical and operational decisions in MBM in order to minimize the total expected fuel costs, controlled for financial risk, within a planning period. This paper points out that after the latest regulation of the Sulphur Emission Control Areas (SECA) came into force in 2015, an integration of the tactical and operational levels in MBM has become important for shipping companies whose business deals with SECA. The results of the computational study shows isolated decision making on either tactical or operational level in MBM will lead to various problem. Nevertheless, the most server consequence occurs when tactical decisions are made in isolation.
C44|The Impact of Bunker Risk Management on CO2 Emissions in Maritime Transportation Under ECA Regulation|The shipping industry carries over 90 percent of the world’s trade, and is hence a major contributor to CO2 and other airborne emissions. As a global effort to reduce air pollution from ships, the implementation of the ECA (Emission Control Areas) regulations has given rise to the wide usage of cleaner fuels. This has led to an increased emphasis on the management and risk control of maritime bunker costs for many shipping companies. In this paper, we provide a novel view on the relationship between bunker risk management and CO2 emissions. In particular, we investigate how different actions taken in bunker risk management, based on different risk aversions and fuel hedging strategies, impact a shipping company’s CO2 emissions. We use a stochastic programming model and perform various comparison tests in a case study based on a major liner company. Our results show that a shipping company’s risk attitude on bunker costs have impacts on its CO2 emissions. We also demonstrate that, by properly designing its hedging strategies, a shipping company can sometimes achieve noticeable CO2 reduction with little financial sacrifice.
C44|A bridge between continuous and discrete-time bioeconomic models: Seasonality in fisheries|We develop a discretization method to construct a discrete finite-time bioeconomic model, corresponding to bioeconomic models with continuous-time growth function, but allowing the analysis of seasonality in fisheries. The discretization method consists of three steps: first, we estimate a proper growth function for the continuous-time model with the Ensemble Kalman Filter. Second, we use the Runge-Kutta method to discretize the growth function. Third, we use the Bellman approach to analyze the optimal management of seasonal fisheries in a discrete-time setting. We analyze both the case of quarterly harvest and the case of monthly harvest, and we compare these cases with the case of annual harvest. We find that seasonal harvesting is a win–win optimal solution that provides higher harvest, higher optimal steady state equilibrium, and higher economic value than annual harvesting. We also demonstrate that the discretization method overcomes the errors and preserves the strengths of both continuous and discrete-time bioeconomic models.
C44|Smoothing the frontier in the DEA models|Some inadequate results may appear in the DEA models as in any other mathematical model. In the DEA scientific literature several methods were proposed to deal with these difficulties. In our previous paper, we introduced the notion of terminal units. It was also substantiated that only terminal units form necessary and sufficient sets of units for smoothing the frontier. Moreover, some relationships were established between terminal units and other sets of units that were proposed for improving the frontier. In this paper we develop a general algorithm for smoothing the frontier. The construction of algorithm is based on the notion of terminal units. Our theoretical results are verified by computational results using real-life data sets and also confirmed by graphical examples.
C44|Seeding the UEFA Champions League participants: evaluation of the reforms|We evaluate the sporting effects of the seeding system reforms in the Champions League, the major football club tournament organized by the Union of European Football Associations (UEFA). Before the 2015–2016 season, the teams were seeded in the group stage by their ratings. Starting from the 2015–2016 season, national champions of the Top-7 associations are seeded in the first pot, whereas other teams are seeded by their rating as before. Taking effect from the season 2018–2019, the team’s rating no longer includes 20% of the rating of the association that the team represents. Using the prediction model, we simulate the whole UEFA season and obtain numerical estimates for competitiveness changes in the UEFA tournaments caused by these seeding reforms. We report only marginal changes in tournament metrics that characterize ability of the tournament to select the best teams and competitive balance. Probability of changes in the UEFA national association ranking does not exceed several percent for any association.
C44|The three s of public schools: irrelevant inputs, insufficient resources and inefficiency| We examine the educational production function and efficiency of public school districts in Illinois. Using non-parametric kernel methods, we find that most traditional schooling inputs are irrelevant in determining test scores (even in a very general setting). Property tax caps are the only relevant factor that is related to districts’ financial constraints and have predominantly negative associations with test scores. Therefore, insufficient resources may be partially responsible for the lack of growth in test scores. For most other relevant inputs, we find substantial heterogeneity in the returns, which helps reconcile some of the puzzling results in the literature. We further find that there exist inefficiencies in school districts. Moreover, the level of test scores, commonly used as a measure of school effectiveness, (while related) differs substantially from our efficiency scores, and standard parametric approaches drastically underestimate school efficiency. We discuss the policy implications of our results.
C44|Multi-directional program efficiency: the case of Lithuanian family farms|The present paper analyses both managerial and program efficiencies of Lithuanian family farms, in the tradition of Charnes et al. (Manag Sci 27(6):668–697, 1981 ) but with the important difference that multi-directional efficiency analysis rather than the traditional data envelopment analysis approach is used to estimate efficiency. This enables a consideration of input-specific efficiencies. The study shows clear differences between the efficiency scores on the different inputs as well as between the farm types of crop, livestock and mixed farms respectively. We furthermore find that crop farms have the highest program efficiency, but the lowest managerial efficiency and that the mixed farms have the lowest program efficiency (yet not the highest managerial efficiency). Copyright Springer Science+Business Media New York 2016
C44|Comparison of production risks in the state-contingent framework: application to balanced panel data|Abstract In a balanced panel data setting, this article proposes an empirical application of the state-contingent (SC) framework for production uncertainty. The SC approach (e.g., Chambers and Quiggin 2000) casts production decisions under uncertainty as the decision to select a portfolio of Arrow-Debreu SC outputs, scheduled to be delivered in the contingent states of nature. Under some stationarity assumptions on the SC decisions (i.e., no technical change, time-invariant states of nature, time-invariant SC portfolio decisions) and regularity assumptions on the data generating process (i.e., cross-sectionally homogeneous state realizations), SC technology can be estimated from balanced panel data that are framed as cross-sectional data of partially-revealed SC portfolio decisions. This allows one to simulate an optimal SC portfolio, determined by the interaction between the estimated SC technology and given risk preferences. In the application to Maryland dairy production data, the stochastic technologies of confinement and intensive-grazing dairy systems are compared. Of the two time intervals (years 2000–2004 and ye0ars 2006–2009) separately analyzed, the optimal production decision has generally become riskier for the confinement system and less risky for the grazing system. These contrasting trends appear directly related to the volatile milk prices, feed cost hikes, and increasing organic milk production during 2006–2009. The risk associated with the optimal portfolio is substantially lower under the SC analysis compared to a typical residual-as-uncertainty approach, suggesting that the typical approach may overstate the risk due to uncertainty.
C44|Multi-directional productivity change: MEA-Malmquist|Abstract In this paper we introduce an extension of the Malmquist total factor productivity index, which utilizes the Multi-directional Efficiency Analysis approach. This enables variable-specific analysis of productivity change as well as its components (efficiency change and technical change). The new approach is illustrated and compared to the conventional Data Envelopment Analysis Malmquist approach by considering a empirical data set on Lithuanian family farms. The results highlight that important differences in variable-specific performance of the farms can be hidden when using the conventional (radial) Data Envelopment Analysis-based Malmquist index.
C44|Felsőoktatási rangsorok jelentkezői preferenciák alapján<BR>[University rankings on the basis of applicants preferences]|A felsőoktatási intézmények rangsorolása a jelentkezők preferenciái alapján alkalmas lehet számos mérési probléma - például a szempontok önkényes súlyozása - okozta torzítások elkerülésére. Azon feltevés alapján, hogy egy felvételiző pontosan akkor preferál egy objektumot (intézményt, kart, szakot stb.) egy másikkal szemben, ha jelentkezési lapján előrébb szerepelteti, egy súlyozott, irányított gráf generálható, amelynek csúcsai a vizsgált objektumok. A cikk az MTA KRTK adatbankjában elérhető Felvi-adatbázis 2013. évi csaknem teljes körű mintáján - a hálózat részletes elemzése mellett - bemutatja a méret- és összetételhatás kiszűrésének lehetséges eszközeit, valamint három módszer alkalmazásával teljes és részterületekre bontott kari rangsorokat közöl. Journal of Economic Literature (JEL) kód: C44, D71, D85, I23.
C44|A Decision Support Model Suggestion for Logistics Support Unit in Risky Environment|Storage facilities have an important role for uninterrupted product/service flow and ensuring continuity in supply chains. Criteria, which will be taken into account for the location of this facilities, and criteria values, can alter in accordance with private or public sector and risk environment as well. Besides logistics costs, transportation opportunities and proximity to the customers, risk based criteria such as terror, sabotage, air strikes, and natural disasters play an important role in order to select facility location. During production flow, Logistics Support Bases (LSB) are the military facilities, which affect firstly operation process positively or negatively and secondly result of the operations, serve in the risk environment. In a specific environment, selection of LSB becomes a Multi-Criteria Decision Making (MCDM) problem for the decision maker. This study aims to determine qualifications which will be used to select the best suitable location of LSB; define the importance value of selected qualifications via DEMATEL method, and select the best location of LSB between alternative places. DEMATEL has used in the determination of criteria values and then VIKOR method has used to select the most appropriate location for LSB in the risk environment.
C44|Hazardous Materials Warehouse Selection as a Multiple Criteria Decision making Problem|Radioactive, toxic, smother, flammable, and explosive materials in solid, liquid or gas states which can negatively impact goods, organisms, and most importantly humans are called as “Hazardous Materials”. Hazardous material transportation and storage carry risk factors in addition to their other types of transportation operations. Furthermore, selection of a suitable warehouse becomes a problematic issue in which multiple criteria are evaluated as paying attention to risky circumstances. In this context, hazardous material warehouse selection is considered as a multiple criteria decision problem in our study. In particularly, for the explosives storage among other hazardous materials, necessary criteria are determined according to expert’s consultant. The determined criteria are weighted according to decision makers’ consultancy and the alternatives are evaluated by fuzzy MULTIMOORA under uncertainty throughout the decision making process in the study.. The proposed approach is discussed on a case study.
C44|An Analysis of Job Change Decision Using a Hybrid Mcdm Method: A Comparative Analysis|This paper investigates the decision process relating to job change which mostly depends on individualâ€™s expectations about a job. Failing to fully understand the factors shaping these expectations leads to dissatisfaction and poor work performance; which produces unwanted consequences for both individuals and businesses. Since job change decision is defined as a multiple criteria decision making (MCDM) problem. This study uses a hybrid approach as a methodology combining fuzzy Analytic Hierarchy Analysis (AHP) and fuzzy TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) for the job change decision of a faculty working in a university. In this approach, while the use of fuzzy AHP method helps determine the weight of the decision criteria; fuzzy TOPSIS enables the evaluation of the alternatives. In order to investigate the methodsâ€™ applicability in multiple dimensions of decision problem space, a comparison analysis is conducted with the three methodologies; fuzzy AHP, fuzzy TOPSIS and the proposed hybrid approach (named fuzzy AHP-TOPSIS) in the same decision making context. Four factors are considered for the comparison: adequacy to changes of criteria or alternatives; agility in the decision process; computational complexity; and the number of criteria and alternatives. Analysis shows that three methods achieve the same results. This verifies their robustness and indicates that MCDM methods are viable in job change decisions. However; comparison analysis shows that based on the four factors; the proposed hybrid fuzzy AHP-TOPSIS method provide more consistent results than fuzzy AHP and fuzzy TOPSIS methods. Thus the proposed hybrid fuzzy AHP-TOPSIS method is more appropriate to use on a wide range of job change decision problems.
C44|New Tests for Richness and Poorness:A Stochastic Dominance Analysis of Income Distributions in Hong Kong|In this paper, we develop the theory of descending stochastic dominance for application to income distribution analysis. We show that conclusions of dominance obtained using our new tests of richness and poorness offer more accurate and more in-depth characterization of welfare inequality in any population. The empirical application of our proposed approach shows that, for Hong Kong, the distribution of total incomes in 2001 has less proportion of poor units in relatively lower income levels compared to that of 2006 at the same time that the distribution of total incomes in 2006 has a higher proportion of rich units in relatively higher income levels. Our analysis also suggests that there exist lower levels of household welfare in 2011 compared to both 2001 and 2006. In terms of age groups, the application of our new methods showed that the younger age cohorts tended to have lesser proportions of poor units in relatively lower income levels compared to those in the 65+ age group, while at the same time, those in the 65+ age group tended to have a higher proportion of rich units in the relatively higher income levels. These extreme concentrations of income units at the ‘bottom end’ for the younger households and at the ‘top end’ for the older households may help explain the overall high inequality level that has persisted in Hong Kong for several years now.
C44|A comparison of the GAI model and the Choquet integral with respect to a k-ary capacity|Two utility models are classically used to represent interaction among criteria: the Choquet integral and the Generalized Additive Independence (GAI) model. We propose a comparison of these models. Looking at their mathematical expression, it seems that the second one is much more general than the first one. The GAI model has been mostly studied in the case where attributes are discrete. We propose an extension of the GAI model to continuous attributes, using the multi-linear interpolation. The values that are interpolated can in fact be interpreted as a k-ary capacity, or its extension – called p-ary capacity – where p is a vector and pi is the number of levels attached to criterion i. In order to push the comparison further, the Choquet integral with respect to a p-ary capacity is generalized to preferences that are not necessarily monotonically increasing or decreasing on the attributes. Then the Choquet integral with respect to a p-ary capacity differs from a GAI model only by the type of interpolation model. The Choquet integral is the Lovász extension of a p-ary capacity whereas the GAI model is the multi-linear extension of a p-ary capacity.
C44|Utility Theory of General Lotteries|Generalizing the notions of roulette lotteries, horse lotteries, and quantum lotteries, we introduce maximally general notion of lottery. It uses the theory of ordered vector spaces. Under appropriate conditions on preference relation between lotteries (generalizing the conditions considered by von Neumann, Savage, Aumann and Anscombe) we give a formula for the utility of lotteries. Its ingredients are a utility function of prizes and a belief functional giving probability of events. Inthe second part of paper we discuss the issue about updating of beliefs under receiving additional information. We give a formula for the updated belief (which generates Bayes rule and von Neumann-Luders projection postulate), suppose that the ordered vector space is the real part of C*-algebra.
C44|"Simulación como herramienta de ayuda para la toma de decisiones empresariales. Un caso práctico || Simulation as a business decision making tool. A case study"|"La toma de decisiones empresariales es un factor crítico para la viabilidad de las empresas. En el actual escenario más dinámico, global y competitivo, este factor crítico abre espacio a requerimientos más amplios. El modelado y la simulación tradicionalmente han formado parte sustancial del juego de herramientas utilizadas para el soporte a la toma de decisiones, especialmente en referencia a la toma de decisiones estratégicas. En este trabajo, pretendemos mostrar una perspectiva clara sobre el valor añadido que ofrece la simulación. En primer lugar, se describen las etapas que deben llevarse a cabo para completar un proceso de toma de decisiones mediante simulación. A continuación, se desarrolla un caso práctico en el que se plasman dichas etapas frente a la decisión de implantar un comportamiento PUSH o un comportamiento PULL en una determinada empresa. Los resultados obtenidos nos conducen a la decisión de implantar PULL en favor de PUSH, mostrando que el valor esperado de las técnicas de simulación justifica que una organización madura incorpore tales técnicas en su banco de talento corporativo. || Business decision making is a critical factor to the viability of companies. In the current more dynamic, global and competitive scenario, this critical factor opens space to wider requirements. Modeling and simulation have traditionally formed a substantial part of the toolkit used to support decision making, especially in the business strategy field. In this paper, our aim is to open a clear perspective on the added value offered by simulation. We first introduce an overview about the process of decision making based on simulation. Next, a case study is developed, in which such steps are reflected to decide whether to implement a PUSH behavior or a PULL behavior in a particular company. The results obtained lead us to the decision to implement PULL instead of PUSH, showing that simulation techniques are a very valuable tool to have in the bank of knowledge in any mature company."
C44|"Aplicación de dos técnicas del análisis multivariado en el mercado de valores mexicano || Application of Two Techniques of Multivariate Analysis in the Mexican Stock Market"|"Esta investigación complementa al análisis técnico bursátil y tiene por objetivo clasificar 88 emisoras de la Bolsa Mexicana de Valores, utilizando el análisis de componentes principales y el análisis discriminante lineal (PCA y LDA, respectivamente, por sus siglas en inglés), con la hipótesis de agrupar emisoras en función de su comportamiento bursátil y del sector económico al que pertenecen. La metodología consistió en recabar el volumen de acciones negociadas (variables de entrada) del software Infosel Financiero correspondiente a 88 emisoras durante el periodo de enero 2015 a marzo 2016. Posteriormente, los datos (variables de entrada) se normalizaron para eliminar su variación natural y se aplicó el PCA y LDA obteniendo tres grupos que no atienden un criterio de importancia, es decir el grupo uno no representa mayor importancia que los grupos subsecuentes. Cada grupo mantiene correlación con cada elemento que lo conforma, pero no mantiene correlación con los elementos de otros grupos, es decir si alguna empresa perteneciente a alguno de los grupos presenta movimientos al alza o baja, las demás acciones del mismo grupo también mostraran esa tendencia, pero las empresas de los otros grupos no necesariamente se comportaran así. Los resultados obtenidos representan un aporte significativo para la creación de un portafolio de inversión ya que se tiene un panorama esclarecedor de las empresas analizadas. Sin embargo, se sugiere complementar con el enfoque de análisis fundamental para analizar la parte intrínseca de las emisoras a mayor profundidad, siempre con el objetivo de buscar minimizar los riesgos de inversión. || This article is a supplement to the stock technical analysis and its main objective is to classify 88 companies belonging to the Mexican Stock Exchange. Using principal component analysis (PCA) and linear discriminant analysis (LDA), the input hypothesis is to group companies according to their market performance and the economic sector to they belong. The methodology consisted in collecting the volume of shares traded indicator (input variables) corresponding to 88 companies for the period January 2015 to March 2016, the input data come from Infosel financial software. After that the input data were normalized and subsequently the PCA and LDA methods were applied to obtain three groups that do not meet an importance criterion. Each group has correlation with each element that makes up, but does not maintain correlation with the elements of other groups; so that if any company belonging to one of the groups presents some tendency, the other actions of the same group also showed that same trend, but companies from other groups will not tend necessarily in the same way. The results represent a significant contribution to the creation of investment portfolios. However, the authors suggest complement this analysis with the fundamental analysis approach to study issuers and reduce investment risks."
C44|Improving service quality to local communities via satisfaction measurament in Greece: The MUSA approach|The objective of this paper is to propose a model that improves service quality to local communities via citizen satisfaction measurement. The main argument arises is that citizen satisfaction represents a modern approach for service quality in local communities and serves the development of a truly citizen-focused management and culture. Measuring citizen satisfaction offers an immediate, meaningful and objective feedback about citizens’ preferences and expectations. In this way, service performance may be evaluated in relation to a set of satisfaction dimensions that indicate the strong and the weak points of a Municipality. The study based on primary data collected through questionnaires from 456 respondents- citizens’ users of the municipality services of Skopelos Island in Greece. The main satisfaction criteria were: C.S.C (Citizen's Service Centre), Municipal Roll-Registry Office, Cleanliness -Lighting, Municipal Works, Home Assistance. These criteria are aggregated through an additive value function which is inferred from a set of satisfaction judgments with the use of the MUSA multi-criteria methodology and software. According to the findings of this research, the citizens of the Municipality of Skopelos seem to be satisfied to a large extent, either fully or partly, with the total of the provided services. However, there appeared some fields that need further improvements, such as Cleanliness - Lighting and Municipality works. The findings of the study are also related to particular policy implications regarding the role and the capacity of local authorities and decision makers to provide efficient and operational services to local communities. It’s very important to relate the findings and the case of Skopelos as pilot guide with the degree of organising capacity of local / regional authorities in larger and different municipalities in Greece. For this reason the proposed MUSA approach, we support that offers the framework to extent our analysis and to evaluate local/ regional authorities role and capacity regarding community development issues.
C44|Qwerty, Йцукен И Српска Ћирилица: Треба Ли Нам Нови Стандард?<BR>[QWERTY, ЙЦУКЕН and Serbian keyboard: do we need new standard?]|The article considers the compatibility of the Serbian cyrilic with the script layouts that is on the keyboards of typewriters and computers used. With the short overview on the most popular keyboard layout (so called universal, i.e. QWERTY, that is primarily intended to Anglo-Saxon and German world, and ЙЦУКЕН, intended to the part of the world that use the Cyrillic script), it is pointed that the layout that now as Serbian named is, realy the minor modification of the QWERTY is. In the second part of the article, with the use of Markov chains, on the empirical example of the Migration, one of the most popular novel of the Serbian literature, it was shown the incompatibility of these layouts for the Serbian Cyrillic script, and with this the need for new standard was emphasized. У раду се анализира компатибилност српског ћириличног писма с распоредима слова који се користе на тастатурама писаћих машина, односно компјутера. Уз кратак осврт на најзаступљеније распореде (тзв. универзални, односно QWERTY, намењен превасходно англосаксонском и германском свету, и ЙЦУКЕН, намењен делу света који користи ћирилично писмо), указује се да је распоред који данас важи за „српску тастатуру” заправо мања модификација QWERTY-ја. У другом делу рада, уз коришћење апаратуре Марковљевих ланаца, на емпиријском примеру Сеоба, једног од најпознатијих романа српске књижевности, показана је некомпатибилност ових распореда за српско ћирилично писмо и тиме истакнута потреба тражења новог стандарда.
C44|Stratyfikacja próby badawczej i dobór ekspertów na przykładzie modelu decyzyjnego opartego na metodzie AHP<BR>[Stratification of research target group and selection of experts for AHP based decision making model]|The paper presents methods of stratification of the research target group and selection of experts for modelling of decision-making processes with Analytic Hierarchy Process method. Research aims at finding a proper and effective method of structuring the research target group. Additionally, it searches for possibilities for limiting the number of respondents that assess the significance of particular decision criteria inside the AHP methodology, without losing results credibility. Stratified random sampling and Delphi reasoning have been applied. To assure the representativeness and reliability of decision-making 30 experts have been selected. Ipso facto, the applicability of both methods for described stages of multicriteria decision-making modelling have been proven.
C44|Марковљеви Ланци И Проблем Одређивања Распореда Слова На Тастатури Српске Ћирилице<BR>[Markov chains and the problem of keyboard layout for Serbian cyrilic]|Serbian Abstract: У раду се разматрају проблеми одређивања оптималног распореда слова на тастатури, који мора бити заснован на карактеристикама и специфичностима сваког конкретног језика. Данашњи стандард за српски језик заснован је на тзв. универзалном распореду и као такав неподесан је. Емпиријском анализом на примеру двају класичних романа српске књижевности, уз примену апаратуре Марковљевих ланаца, анализиране су перформансе постојеће, „српске” тастатуре, и указано је на потребу рада на новом стандарду. English Abstract: The paper considers issues related to an optimal keyboard layout which needs to be based on the characteristics and specificities of each particular language. Today’s standard for Serbian language is based on the so-called universal layout and, as such, is inadequate. The performances of the today-existing “Serbian” keyboard were analyzed through an empirical analysis of two Serbian classical novels as well as the use of Markov chains apparatus, and a need for further development of a new standard was emphasized.
C44|What's BEPS got to do with it? Exploring the effectiveness of thin capitalisation rules|In October 2015, the OECD made a best practice recommendation in Action 4 of its BEPS project, suggesting a Fixed Ratio Rule in place of thin capitalisation rules. This review was almost 3 decades in the making, with the most recent OECD report on thin capitalisation rules published in 1986, which omitted guidance on how these rules could best be designed. Thin capitalisation rules’ strong emphasis on revenue base protection has resulted in their exponentially increasing popularity internationally since the 1960s. However, there is a growing body of literature critiquing the effectiveness of thin capitalisation rules. Accordingly, this paper approaches the issue of thin capitalisation from a novel perspective by conceptualising the cross-border debt bias as the ‘disease’ and thin capitalisation as merely the ‘symptom’. Grounded in the tax principle of efficiency, the overarching question guiding this paper is whether, given the opportunity to start over, the tax-induced cross-border debt bias would be better addressed by retaining thin capitalisation rules in their current form or whether an alternative reform would be more suited to dealing with this ‘disease’. The optimisation model developed in this paper shows that the OECD’s Fixed Ratio Rule is more effective than the current regime of thin capitalisation rules at protecting the tax revenue base from the most tax-aggressive multinational enterprises (MNEs). However, the model also indicates that it is ultimately more effective to align the tax treatment of intercompany funding to eliminate the ‘underlying disease’ (the tax incentive for thin capitalisation), rather than adopting rules that mitigate the ‘symptom’ (such as the OECD’s Fixed Ratio Rule). This research presents a unique contribution to the literature by simulating complex cross-border intercompany tax planning strategies. This facilitates a formal analysis of one of the most significant challenges presented by the mobility and fungibility of capital; namely, anticipating how an MNE structures its internal affairs in a tax-optimal manner given the current tax regime and suggesting tax administrative responses to BEPS accordingly.
C44|Encompassing the Work-Life Balance into Early Career Decision-Making of Future Employees Through the Analytic Hierarchy Process|The paper presents the results of ranking of the significance of quality of life determinants by University students that are starting professional activities. Research methodology: literature review; elaboration of an AHP decision-making model; two-stage expert selection; significance rankings by experts and a graphical and descriptive presentation of obtained results. Research sample: 14 experts out of almost 200 University students. Research outcome: a decision-making model that aims at maximizing the life satisfaction of future employees as a function of their individual assessments of significance of particular determinants of quality of life. Research implications: a more accurate adaptation to trends on the labor market and creation of new business models. Research limitation: narrowing the group of experts to University students. Value added of the research: better motivated employees with a satisfactory level of work-life balance will contribute to an increase of societal satisfaction level.
C44|About the problem of selecting models for production line|The article presents the comparative characteristic of models of industrial production lines operating in transition mode, their advantages and disadvantages are analyzed. For these models, the accuracy of calculations of aggregated parameters of industrial production lines is evaluated. The principle of the PDE-models is shown using the statistical theory of describing production systems. The prospects of developing models of industrial production lines that are used in the design of in-line production control systems.
C44|The construction a kinetic equation of the production process|The paper discusses methods of constructing the kinetic equation of the technology process. The article presents a model of the interaction of objects of labour with technological equipment, which is the basis for the derivation of the kinetic equation. To describe the state of the production line introduced numerical characteristics.
C44|Анализ Принципов И Методов Построения Систем Управления Производственным Процессом<BR>[Analysis Of The Principles And Methods Of Construction Control Systems Of Technological Process]|Аннотация. В статье приведен обзор действующих ГОСТов и дан основанный на них краткий анализ основных элементов технологического процесса – труд, средство труда, предмет труда. Представлены определения технологического процесса, изделия, технологической операции; средств труда, свойства и параметра изделия. Рассмотрены элементы производственной структуры предприятия. Дан анализ типов производства и систем управления ими. Определены главные задачи планирования в условиях единичного, серийного и массового производства. Представлены основные микро- и макропараметры (показатели) производственного процесса. Рассмотрены методы организации различных типов производства. Обоснован выбор плановых учетных единиц производственной системы. Рассмотрена формализация разных технологических операций. Показано, что выполнение технологической операции связано с переносом технологических ресурсов на предмет труда с целью изменения его свойств, каждое из которых определяется введенными параметрами. Abstract. The article provides an overview of current state standards and is given an analysis of the main elements of the process - labour, means of labour, objects of labour. Presents the definition of the process, product, technological operation, the means of labour, properties and parameters of the product. The elements of the production structure of the enterprise. The analysis of the types of production and management systems gives. Identify the main planning tasks in a single, serial and mass production. The article presents the basic micro and macro parameters (parameters) of the production process. The article describes the methods of the organization of various types of production. The author justifies the choice of the planned accounting units of the production system. The article deals with the formalization of various technological operations. The researcher suggests that technical operations associated with the transfer of technological resources on the subject of work in order to change its properties, each of which is determined by the parameter.
C44|A Unifying Framework for Farrell Efficiency Measurement Coherent with Profit-maximizing Principle|Measuring profit efficiency is a challenging task. This paper synthesizes existing approaches to form a general Farrell-type model of profit efficiency. Our derivations help us unveil new and interesting relationship between existing profit efficiency measures and the Farrell-type profit efficiency measures. In turn, this helps us establishing a complete framework of studying efficiency behavior of firms, where the profit efficiency measure satisfies some desirable properties and contains Farrell output-oriented or input-oriented measures of technical efficiency and allocative efficiency as multiplicative elements. The new component, revenue efficient allocative efficiency, introduced in this paper can help firms to make decision and has not been studied in the literature before.
