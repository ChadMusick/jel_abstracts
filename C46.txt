C46|Firm-size distribution in Poland – is power law applicable?|The article focuses on power laws and their growing popularity in science in general and in economics specifically. The theoretical mechanisms responsible for their generating are reviewed. We also empirically test whether firm-size distribution of companies in Poland has the characteristics of the Zipf’s law – a special case of a power law. This is confirmed based on an investigation within the sample of 2000 largest companies and a set of alternative estimators of the power law exponent.
C46|Inequality, mobility and the financial accumulation process: a computational economic analysis|Abstract Our computational economic analysis investigates the relationship between inequality, mobility and the financial accumulation process. Extending the baseline model by Levy et al., we characterise the economic process through stylised return structures generating alternative evolutions of income and wealth through time. First, we explore the limited heuristic contribution of one and two-factors models comprising one single stock (capital wealth) and one single flow factor (labour) as pure drivers of income and wealth generation and allocation over time. Second, we introduce heuristic modes of taxation in line with the baseline approach. Our computational economic analysis corroborates that the financial accumulation process featuring compound returns plays a significant role as source of inequality, while institutional arrangements including taxation play a significant role in framing and shaping the aggregate economic process that evolves over socioeconomic space and time.
C46|A Theory of Scenario Generation|We show how distributions can be reduced to low-dimensional scenario trees. Applied to intertemporal distributions, the scenarios and their probabilities become time-varying factors. From S&P 500 options, two or three time-varying scenarios suffice to forecast returns, implied variance or skewness on par, or better, than extant multivariate stochastic volatility jump-diffusion models, while reducing the computational effort to fractions of a second.
C46|Aplicación del modelo estocástico de difusion -salto de merton para la simulación del valor del índice colcap|ABSTRACT This working paper consist in apply the Stochastic Jump-Diffusion model proposed by Merton (1976 MJD model), as well as the process of estimating its parameters, applied to the COLCAP stock index. Authors like (Andersen, Benzoni, & Lund, 2002), (Hanson & Westman, 2002), (Hanson & Zongwu, 2004), (Penagos, Gabriel; Rubio, Gonzalo, 2013) (Tang, 2018) show how the incorporation of jumps, allows to obtain a Probability Density Function (PDF) according with skewed distributions and high kurtosis characterizing data log-returns financial. The results presented will have as reference the model Black & Scholes, 1973 (B&S) which is a pure diffusion model, whose base is the Gaussian distribution. The paper is composed by five parts, in the first one we could find the bibliographic review about the application of the stochastic jump-diffusion models, in the second and third one describes the COLCAP index, as well as its composition and history, the description of the data and the MJD model is defined; the fourth one shows the results and finally the conclusions. ***** RESUMEN Este trabajo de grado tiene como objetivo aplicar el modelo Estocástico de Salto-Difusión propuesto por Merton (1976) (MJD) así como el proceso de estimación de sus parámetros, aplicado al índice bursátil COLCAP. Autores como (Andersen, Benzoni, & Lund, 2002), (Hanson & Westman, 2002) (Hanson & Zongwu, 2004) (Penagos, Gabriel; Rubio, Gonzalo, 2013) y (Tang, 2018) muestran como la incorporación de saltos, permite obtener una Función de Densidad de Probabilidad (PDF) más acorde con distribuciones asimétricas y con curtosis elevadas que caracterizan a los datos de log-retornos financieros. Los resultados presentados tendrán como punto de referencia el modelo de Black and Scholes (B&S), el cual es un modelo de difusión puro, cuya base es la Distribución Gaussiana. El articulo se compone de cinco partes, en la primera se encuentra la revisión bibliográfica acerca de la aplicación de los modelos estocásticos de salto-difusión, en la segunda y tercera se describen el índice COLCAP, su composición e historia, se describen los datos y se define el modelo MJD, en la cuarta parte se muestran los resultados y finalmente se concluye.
C46|A Different Perspective on the Evolution of UK Income Inequality|This paper scrutinizes the conventional wisdom about trends in UK income inequality and also places contemporary inequality in a much longer historical perspective. We combine household survey and income tax data to provide better coverage of all income ranges from the bottom to the very top. We make a case for studying distributions of income between tax units (i.e. not assuming the full income sharing that goes with the use of the household as the unit of analysis) for reasons of principle as well as data harmonization. We present evidence that income inequality in the UK is as least as high today as it was just before the start of World War 2.
C46|Response surface regressions for critical value bounds and approximate p-values in equilibrium correction models|Single-equation conditional equilibrium correction models can be used to test for the existence of a level relationship among the variables of interest. The distributions of the respective test statistics are nonstandard under the null hypothesis of no such relationship and critical values need to be obtained with stochastic simulations. We compute more than 95 billion F -statistics and 57 billion t-statistics for a large number of specifications of the Pesaran, Shin, and Smith (2001, Journal of Applied Econometrics 16: 289Ð326) bounds test. Our large-scale simulations enable us to draw smooth density functions and to estimate response surface models that improve upon and substantially extend the set of available critical values for the bounds test. Besides covering the full range of possible sample sizes and lag orders, our approach notably allows for any number of variables in the long-run level relationship by exploiting the diminishing effect on the distributions of adding another variable to the model. The computation of approximate p-values enables a fine-grained statistical inference and allows us to quantify the finite-sample distortions from using asymptotic critical values. We find that the bounds test can be easily oversized by more than 5 percentage points in small samples.
C46|What Drives the Distributional Dynamics of Client Interest Rates on Consumer Loans in the Czech Republic?|We study determinants of the bank-level distributional dynamics of client interest rates on consumer loans in the Czech Republic in the recent period 2014–2019 when banks started to provide new consumer loans at very low interest rates. We build on the relevant literature in terms of the selected explanatory variables as well as the methodological approach and use regulatory data that enable us to work with the mean, median and the mode of the distribution of client interest rates on consumer loans. We show that development of the market rate, the NPL ratio as well as the unemployment rate facilitated the observed distributional dynamics. Further, using a variety of variables on market competition/market concentration, our analysis reveals that the role of this determinant is limited at best. Our results, especially regarding the pass through from market rates to consumer loan rates, are mostly in line with the international literature but are novel in the Czech context.
C46|Exact tests on returns to scale and comparisons of production frontiers in nonparametric models|When benchmarking production units by non-parametric methods like data envelopment analysis (DEA), an assumption has to be made about the returns to scale of the underlying technology. Moreover, it is often also relevant to compare the frontiers across samples of producers. Until now, no exact tests for examining returns to scale assumptions in DEA, or for test of equality of frontiers, have been available. The few existing tests are based on asymptotic theory relying on large sample sizes, whereas situations with relatively small samples are often encountered in practical applications. In this paper we propose three novel tests based on permutations. The tests are easily implementable from the algorithms provided, and give exact significance probabilities as they are not based on asymptotic properties. The first of the proposed tests is a test for the hypothesis of constant returns to scale in DEA. The others are tests for general frontier differences and whether the production possibility sets are, in fact, nested. The theoretical advantages of permutation tests are that they are appropriate for small samples and have the correct size. Simulation studies show that the proposed tests do, indeed, have the correct size and furthermore higher power than the existing alternative tests based on asymptotic theory.
C46|Robust measures of skewness and kurtosis for macroeconomic and financial time series|The sample skewness and kurtosis of macroeconomic and financial time series are routinely scrutinized in the early stages of model-building and are often the central topic of studies in economics and finance. Notwithstanding the availability of several robust estimators, most scholars in economics rely on method-of-moments estimation that is known to be very sensitive to outliers. We carry out an extensive Monte Carlo analysis to compare the bias and root mean squared error of twelve different estimators of skewness and kurtosis. We consider nine statistical distributions that approximate the range of data generating processes of many macroeconomic and financial time series. Both in independently and identically distributed samples and in data generating processes featuring serial correlation L-moments and trimmed L-moments estimators are particularly resistant to outliers and deliver the lowest root mean squared error. The application to 128 macroeconomic and financial time series sourced from a large, monthly frequency, database (i.e. the FRED-MD of McCracken and Ng, 2016) confirms the findings of the simulation study.
C46|Labour market effects of crowdwork in US and EU: an empirical investigation|Does working on online labour markets have an impact on earnings and working conditions? Do crowdworkers involved in micro-task outsourcing differ in their characteristics from traditional salaried workers of similar ability? Are micro-task crowdworkers similar or different in United States and in Europe? In this paper, we address these questions by comparing outcomes in working quality between online-platform and traditional workers across the United States and Europe in a quasi-experimental approach, exploiting caregiving as an exogenous source of variation influencing participation in crowdwork rounds across the female population. We find evidence that, when controlling for workers’ observed and unobserved ability, traditional workers retain a significant premium in their earnings with respect to platform workers, though this effect is not as large as descriptive statistics may hint. Moreover, labour force in crowdworking arrangements appears to suffer from high levels of under-utilisation, relegating crowdworkers into a new category of idle workers whose human capital is neither fully utilised nor adequately compensated.
C46|Measurement errors and tax evasion in annual incomes: evidence from survey data matched with fiscal data|Individual records, referred to personal interviews of a survey on income carried out in Modena during 2012 and tax year 2011, had been matched with their corresponding records in the Ministry of Finance databases containing the fiscal incomes of tax year 2011. The analysis of the resulting data set suggested that the fiscal income was generally more reliable than surveyed income, but in the literature the exact opposite is often assumed. Moreover, the obtained data set enables identification of the factors determining over- and under-reporting, as well as measurement error, through a comparison of the surveyed income with the fiscal income, only for suitable categories of interviewees: the taxpayers who are obliged to respect the law (the constrained sector), and taxpayers who have many possibilities to evade (the unconstrained sector). The percentage of underreporters (67.3%) was higher than those of over-reporters (32.7%). Level of income, age, and education were the main regressors affecting the measurement errors and the behaviours of taxpayers. Estimations of tax evasion and the impacts of personal factors affecting it were carried out following different approaches. The average of individual propensity to tax evasion was 25.93% of the corresponding fiscal income. The potential total tax evaders were about 10%
C46|Addenda to “Are the log-growth rates of city sizes distributed normally? Empirical evidence for the USA [Empir. Econ. (2017) 53:1109-1123]”|"We update the recently published paper [A. Ramos, Empir. Econ. (2017) 53:1109-1123] on the basis of another important paper [H. S. Kwong and S. Nadarajah, Physica A (2019) 513:55-62]. Specifically, we introduce the 3-normal (3N) and 3-logistic (3L) distributions and compare them with the best of our distributions in the firstly mentioned paper, namely the ""double mixture exponential Generalized Beta of the second kind (dmeGB2)"". The main result is that the dmeGB2 remains to be the best model when studying log-growth rates of USA city populations to date. However, if one does not want to achieve such a high precision when describing the data, the 3L emerges to be a very good model for the same purposes."
C46|Shape Factor Asymptotic Analysis I|"The shape factor defined as kurtosis divided by skewness squared K/S^2 is characterized as the only choice among all factors K/〖|S|〗^α ,α>0 which is greater than or equal to 1 for all probability distributions. For a specific distribution family, there may exists α>2 such that min⁡〖K/〖|S|〗^α 〗≥1. The least upper bound of all such α is defined as the distribution’s characteristic number. The useful extreme values of the shape factor for various distributions which are found numerically before, the Beta, Kumaraswamy, Weibull, and GB2 Distribution, are derived using asymptotic analysis. The match of the numerical and the analytical results can be considered prove of each other. The characteristic numbers of these distributions are also calculated. The study of the boundary value of the shape factor, or the shape factor asymptotic analysis, help reveal properties of the original shape factor, and reveal relationship between distributions, such as between the Kumaraswamy distribution and the Weibull distribution."
C46|Have the log-population processes stationary and independent increments? Empirical evidence for Italy, Spain and the USA along more than a century|We review the classical Gibrat’s process for the population of city sizes. In particular, we are interested in whether the log-population process has stationary and independent (Gibrat’s Law for cities) increments. We have tested these characteristics for the case of the municipalities of Italy and Spain and the places of USA for a time span of more than one century. The results are clear: stationarity and independence are empirically rejected by standard tests. These results open theoretically the way for the observance of other city size distributions other than the lognormal and the double Pareto lognormal, something that in fact has already happened in the literature.
C46|IFRS9 Expected Credit Loss Estimation: Advanced Models for Estimating Portfolio Loss and Weighting Scenario Losses|Estimation of portfolio expected credit loss is required for IFRS9 regulatory purposes. It starts with the estimation of scenario loss at loan level, and then aggregated and summed up by scenario probability weights to obtain portfolio expected loss. This estimated loss can vary significantly, depending on the levels of loss severity generated by the IFSR9 models, and the probability weights chosen. There is a need for a quantitative approach for determining the weights for scenario losses. In this paper, we propose a model to estimate the expected portfolio losses brought by recession risk, and a quantitative approach for determining the scenario weights. The model and approach are validated by an empirical example, where we stress portfolio expected loss by recession risk, and calculate the scenario weights accordingly.
C46|Spillovers in Higher-Order Moments of Bitcoin, Gold, and Oil|In this paper, we extend existing studies by considering the relationships across crude oil, gold, and Bitcoin markets. Using high-frequency data from December 2, 2014 to June 10, 2018, we analyze spillovers in volatility jumps and realized second, third, and fourth moments across crude oil, gold, and Bitcoin markets via Granger causality and generalized impulse response analyses in daily frequency. Results suggest evidence of predictability and emphasize, among others, the need of jointly modeling linkages across those three markets with higher-order moments; otherwise, inaccurate risk assessment and investment inferences may arise. The responses of realized volatility shocks and volatility jump are generally positive. Furthermore, results indicate evidence of a weaker relationship between gold – crude oil, and Bitcoin – crude oil compared to the case of Bitcoin - gold. Practical implications are discussed.
C46|An improved approach for estimating large losses in insurance analytics and operational risk using the g-and-h distribution|In this paper, we study the estimation of parameters for g-and-h distributions. These distributions find applications in modeling highly skewed and fat-tailed data, like extreme losses in the banking and insurance sector. We first introduce two estimation methods: a numerical maximum likelihood technique, and an indirect inference approach with a bootstrap weighting scheme. In a realistic simulation study, we show that indirect inference is computationally more efficient and provides better estimates in case of extreme features of the data. Empirical illustrations on insurance and operational losses illustrate these findings.
C46|House Price Dispersion in Boom-Bust Cycles: Evidence from Tokyo|We investigate the cross-sectional distribution of house prices in the Greater Tokyo Area for the period 1986 to 2009. We find that size-adjusted house prices follow a lognormal distribution except for the period of the housing bubble and its collapse in Tokyo, for which the price distribution has a substantially heavier upper tail than that of a lognormal distribution. We also find that, during the bubble era, sharp price movements were concentrated in particular areas, and this spatial heterogeneity is the source of the fat upper tail. These findings suggest that, during a bubble, prices increase markedly for certain properties but to a much lesser extent for other properties, leading to an increase in price inequality across properties. In other words, the defining property of real estate bubbles is not the rapid price hike itself but an increase in price dispersion. We argue that the shape of cross-sectional house price distributions may contain information useful for the detection of housing bubbles.
C46|Wealth inequality in Central and Eastern Europe: evidence from joined household survey and rich lists’ data|We study how the problem of the ‘missing rich’, the underrepresentation of the wealthiest in household surveys, affects wealth inequality estimates for the post-socialist countries of Central and Eastern Europe (CEE). The survey data from the second wave of the Household Finance and Consumption Survey (HFCS) are joined with the data from the national rich lists for Estonia, Hungary, Latvia, Poland and Slovakia. Pareto distribution is fitted to the joined survey and rich lists’ data to impute the missing observations for the largest wealth values. We provide the first estimates of the top-corrected wealth inequality for the CEE region in 2013/2014. Despite a short period of wealth accumulation during the post-1989 market economy period, our adjustment procedure reveals that wealth inequality in the Baltic countries is comparable to that of Germany (one of the most wealth unequal countries in Europe), while in Poland and Hungary it has reached levels observed in France or Spain. We discuss possible explanations of these findings with reference to the speed and range of privatization processes, extent of income inequality, and the role of inheritances and wealth taxes in the region.
C46|Forecasting with Unknown Unknowns: Censoring and Fat Tails on the Bank of England's Monetary Policy Committee|This paper considers the production and evaluation of density forecasts paying attention to if and how the probabilities of outlying observations are quantiﬁed and communicated. Particular focus is given to the ‘censored’ nature of the Bank of England’s fan charts, given that - which is commonly ignored - they describe only the inner 90% (best critical region) of the forecast distribution. A new estimator is proposed that ﬁts a potentially skewed and fat tailed density to the inner observations, acknowledging that the outlying observations may be drawn from a diﬀerent but unknown distribution. In forecasting applications, motivation for this could reﬂect the view that outlying forecast errors reﬂect (realised) unknown unknowns or events not expected to recur that should be censored before quantifying known unknowns.
C46|The Cross-sectional Distribution of Completed Lifetimes: Some New Inferences from Survival Analysis|The cross-sectional distribution of completed lifetimes (DCL) is a new estimator defined and derived by Dixon (2012) in the general Taylor price model (GTE). DCL can be known as the cross-sectional weighted estimator summing to 1. It is a new statistics applying to describe the data. This paper focuses on the cross-sectional distribution in the survival analysis. The delta method is applied to derive the variance of the of three cumulative distribution functions: the distribution of duration, cross-sectional distribution of age, distribution of duration across rms. The Monte Carlo experiment is applied to do the simulation study. The empirical results show that the asymptotic variance formula of the DCL and distribution of duration performs well when the sample size above 25. With the increasing of the sample size, the bias of the variance is reduced.
C46|What Drives the Distributional Dynamics of Client Interest Rates on Consumer Loans in the Czech Republic? A Bank-level Analysis|We study the bank-level distributional dynamics and factors of client interest rates on consumer loans in the Czech Republic. We take into account that client interest rates can have different fixation periods, focus on the consumer loans category, which exhibits multimodal client interest rate distributions, and employ an alternative measure to the mean interest rate - the mode measure. We show that in recent years, most banks in the Czech Republic have started to provide new consumer loans at unprecedentedly low client interest rates. The bank-level analysis then reveals that reduced market concentration (increased market competition) and to some extent also accommodative monetary policy and changes in the market for housing loans and mortgages have been driving this development. Our results are in line with the international literature but are novel in the Czech context.
C46|Looking for the missing rich: Tracing the top tail of the wealth distribution|We analyze the top tail of the wealth distribution in Germany, France, and Spain based on the first and second wave of the Household Finance and Consumption Survey (HFCS). Since top wealth is likely to be underrepresented in household surveys, we integrate big fortunes from rich lists, estimate a Pareto distribution, and impute the missing rich. In addition to the Forbes list, we rely on national rich lists since they represent a broader base for the big fortunes in those countries. As a result, the top percentile share of household wealth in Germany jumps up from 24 percent to 31 percent in the first and from 24 to 33 percent in the second wave after top wealth imputation. For France and Spain, we find only a small effect of the imputation since rich households are better captured in the survey.
C46|Is the Top Tail of the Wealth Distribution the Missing Link between the Household Finance and Consumption Survey and National Accounts?|The financial accounts of the household sector within the system of national accounts report the aggregate asset holdings and liabilities of all households within a country. In principle, when household wealth surveys are explicitly designed to be representative of all households, aggregating these microdata should correspond to the macro-aggregates. In practice, however, differences are large. We first discuss conceptual and generic differences between those two sources of data. Thereafter, we investigate missing top tail observation from wealth surveys as a source of discrepancy. By fitting a Pareto distribution to the upper tail, we provide an estimate of how much of the gap between the micro- and macrodata is caused by the underestimation of the top tail of the wealth distribution. Conceptual and generic differences, as well as missing top tail observations, explain part of the gap between financial accounts and survey aggregates.
C46|Testing productivity change, frontier shift, and efficiency change|Inference about productivity change over time based on data envelopment (DEA) has focused primarily on the Malmquist index and is based on asymptotic properties of the index. In this paper we propose a novel set of significance tests for DEA based productivity change measures based on permutations and accounting for the inherent correlations when panel data are observed. The tests are easily implementable and give exact significance probabilities as they are not based on asymptotic properties. Tests are formulated both for the geometric means of the Malmquist index, and also of its components, i.e. the frontier shift index and the eciency change index, which together enable analysis of not only the presence of differences, but also gives an indication of whether the productivity change is due to shifts in the frontiers and/or changes in the efficiency distributions. Simulation results show the power of, and suggest how to interpret the results of, the proposed tests. Finally, the tests are illustrated using a data set from the literature.
C46|Is Bitcoin a hedge, a safe haven or a diversifier for oil price movements? A comparison with gold|This study assesses the roles of Bitcoin as a hedge, a safe haven and/or a diversifier against extreme oil price movements, in comparison to the corresponding roles of gold. We use a quantile-on-quantile regression approach to capture the dependence structure between the considered market returns under different Bitcoin market conditions, while considering nuances of oil price movements, compared to gold. Our findings show that both Bitcoin and gold would serve the roles of a hedge, a safe haven and a diversifier for oil price movements. However, this property seems to be sensitive to the Bitcoin's and gold's different (bear, normal or bull) market conditions and to whether the oil price is in a downside, normal or upside regime. By controlling for new and relevant U.S. and global uncertainty indicators, we confirm that both Bitcoin and gold, but not oil, are assets where investors may park their cash during times of political and economic turmoil. The conditional Value-at-Risk (CoVaR) approach to risk management is then conducted, providing robust evidence of the usefulness of each of the Bitcoin and gold in expanded oil portfolios, in terms of diversification opportunities and downside risk reductions.
C46|Looking for the Missing Rich: Tracing the Top Tail of the Wealth Distribution|We analyze the top tail of the wealth distribution in Germany, France, and Spain based on the first and second wave of the Household Finance and Consumption Survey (HFCS). Since top wealth is likely to be underrepresented in household surveys, we integrate big fortunes from rich lists, estimate a Pareto distribution, and impute the missing rich. In addition to the Forbes list, we rely on national rich lists since they represent a broader base for the big fortunes in those countries. As a result, the top percentile share of household wealth in Germany jumps up from 24 percent to 31 percent in the first and from 24 to 33 percent in the second wave after top wealth imputation. For France and Spain, we find only a small effect of the imputation since rich households are better captured in the survey.
C46|Inference for the neighborhood inequality index|The neighborhood inequality (NI) index measures aspects of spatial inequality in the distribution of incomes within the city. The NI index is defi ned as a population average of the normalized income gap between each individual's income (observed at a given location in the city) and the incomes of the neighbors, living within a certain distance range from that individual. This paper provides minimum bounds for the NI index standard error and shows that unbiased estimators can be identifi ed under fairly common hypothesis in spatial statistics. These estimators are shown to depend exclusively on the variogram, a measure of spatial dependence in the data. Rich income data are then used to infer about trends of neighborhood inequality in Chicago, IL over the last 35 years. Results from a Monte Carlo study support the relevance of the standard error approximations.
C46|Network-based macro fluctuations: Evidence from Lithuania|Do inter-sectoral linkages of intermediate products affect the spread of sectoral shocks at the aggregate level in Lithuania, a small and open economy? We answer this question by: i) constructing the domestic sector-by-sector direct requirements table using the Lithuanian interindustry transactions tables, and ii) applying Acemoglu et al. (2012)'s network-based methodology and Gabaix and Ibragimov (2011)'s modified log rank-log size regression to analyse the nature of inter-sectoral linkages. Our results indicate that the direct and indirect inter-sectoral linkages cause aggregate volatility to decay at a rate lower than square root of n - the rate predicted by the standard diversification argument. Furthermore, indirect linkages play an important role in the above-mentioned process, supporting the findings of Acemoglu et al. (2012). These results suggest that the inter-sectoral network of linkages represent a potential propagation mechanism for idiosyncratic shocks throughout the Lithuanian economy.
C46|The Persistent Statistical Structure of the US Input-Output Coefficient Matrices: 1963-2007|The paper finds evidence for the existence of a statistical structure in the US input-output (I-O) coefficient matrices A = f{aij} for 1963-2007. For various aspects of matrices A we find smooth and unimodal empirical frequency distributions (EFD) with a remarkable stability in their functional form for most of the samples. The EFD of all entries, diagonal entries, row sums, and the (left and right) Perron-Frobenius eigenvectors are well described by fat-tailed distributions while the EFD of column sums and eigenvalue moduli are well explained by the normal distribution and the Beta distribution, respectively. The paper provides several economic interpretations of these statistical results based on the recent developments in the I-O analysis and the price of production literature. Our findings question some probabilistic assumptions conventionally adopted in the stochastic I-O analysis literature and call for a statistical approach to the discussion of the structure of I-O matrices.
C46|Ergodicity conditions for a double mixed Poisson autoregression|We propose a double mixed Poisson autoregression in which the intensity, scaled by a unit mean independent and identically distributed (iid) mixing process, has different regime specifications according to the state of a finite unobserved iid chain. Under some contraction in mean conditions, we show that the proposed model is strictly stationary and ergodic with a finite mean. Applications to various count time series models are given.
C46|Persistence of economic uncertainty: a comprehensive analysis| One of the most heavily researched and cited issue in applied economics is the relationship of uncertainty indices with the financial and macroeconomic variables. While the statistical features of financial and macroeconomic variables have been thoroughly examined, virtually nothing has been done to examine uncertainty indices under the statistical perspective. In this paper, we focus on two primary characteristics of uncertainty indices: persistence and chaotic behaviour. In order to evaluate the persistence and the chaotic behaviour we analyse 72 popular uncertainty indices constructed by forecasting models, text mining from news articles and data mining from monetary variables to measure the Hurst and Lyapunov exponents in rolling windows. The examination in rolling windows provides a dynamic evaluation of the specific characteristics revealing significant variations of persistence and chaotic dynamics with time. More specifically, we find that almost all uncertainty indices are persistent, while the chaotic dynamics are detected only sporadically and for certain indices during recessions of economic turbulence. Thus, we suggest that the examination of persistence and chaos should be a prerequisite step before using uncertainty indices in economic policy models.
C46|Are BRICS exchange rates chaotic?| In this paper, we focus on the stochastic (chaotic) attributes of the US dollar-based exchange rates for Brazil, Russia, India, China and South Africa (BRICS) using a long-run monthly dataset covering 1812M01-2017M12, 1814M01-2017M12, 1822M07-2017M12, 1948M08-2017M12, and 1844M01-2017M12, respectively. For our purpose, we consider the Lyapunov exponents, robust to nonlinear and stochastic systems, in both full – samples and in rolling windows. For comparative purposes, we also evaluate a long-run dataset of a developed currency market, namely British pound over the period of 1791M01-2017M12. Our empirical findings detect chaotic behavior only episodically for all countries before the dissolution of the Bretton Woods system, with the exception of the Russian ruble. Overall, our findings suggest that the establishment of the free floating exchange rate system have altered the path of exchange rates removing chaotic dynamics from the phenomenon, and hence, the need for policymakers to intervene in the currency markets for the most important emerging market bloc, should be carefully examined.
C46|Which Are The Most Common Distributions In Social, Health, And Education Sciences?|Statistical analysis is crucial for research and the choice of analytical technique should take into account the specific distribution of data. Although the data obtained from health, educational and social sciences research are often not normally distributed, there are very few studies detailing which distributions are most likely to represent data in these disciplines. The aim of the present study was to determine the frequency of appearance of the most common non-normal distributions in the health, educational and social sciences by means of a systematic review. The search was carried out in the Web of Science (WOS) database, from which we retrieved 984 abstracts of papers published between 2010 and 2015. In the final review, 148 papers from the area of health, 18 from education and 96 from the social sciences were included. The selection was performed independently by two reviewers. The inter-rater reliability for article selection and agreement regarding the type of distribution was high. The results showed that distributions from the exponential family are the most common non-normal distributions ? and more specifically, gamma as a continuous distribution and the negative binomial as a discrete distribution. In addition to identifying the most common distributions for real data these results will help researchers to decide which distributions should be used in simulation studies examining statistical procedures.This research was supported by grant PSI2016-78737-P (AEI/FEDER, UE) from the Spanish Ministry of Economy, Industry and Competitiveness.
C46|Estimating Value-at-Risk for the g-and-h distribution: an indirect inference approach|TThe g-and-h distribution is a flexible model with desirable theoretical properties. Especially, it is able to handle well the complex behavior of loss data and it is suitable for VaR estimation when large skewness and kurtosis are at stake. However, parameter estimation is di cult, because the density cannot be written in closed form. In this paper we develop an indirect inference method using the skewed- t distribution as instrumental model. We show that the skewed-t is a well suited auxiliary model and study the numerical issues related to its implementation. A Monte Carlo analysis and an application to operational losses suggest that the indirect inference estimators of the parameters and of the VaR outperform the quantile-based estimators.
C46|Impact of advertizing on brand’s market-shares in the automobile market:: a multi-channel attraction model with competition and carry-over effects|This article presents a new approach to measure the impact of multi-channel advertising investments on brands’ market shares in the main segment of the French automobile market. We propose a multi-channel attraction model with adstock, in order to take into account the advertising carryover effect and the competition. This model allows to distinguish between short term and long term effect of the advertising. As, from a mathematical point of view, a vector of market shares is a composition belonging to the simplex space, i.e. subject to positivity and summing up to one contraints, we take benefit from the compositional data analysis (CODA) literature to estimate properly this model. We show how to determine the carryover parameters for each channel (outdoor, press, radio and television) in a multivariate way. We consider several model specifications with more or less complexity (cross effects between brands), including Dirichlet models, and we compare them using goodness-of-fit and prediction accuracy measures. We explain how to built confidence and prediction ellipsoids in the space of market shares. The impact of each channel on market shares is measured in terms of direct and cross elasticities. We conclude that in this market, radio only has a contemporaneous impact whereas outdoor, press and television have a large decay effect. Moreover, the advertising elasticities vary across brands and channels, and can be negative. It also turns out that positive interactions do exist between certain brands for certain media.
C46|Analyzing the impacts of socio-economic factors on French departmental elections with CODA methods|The proportions of votes by party on a given subdivision of a territory form a vector called composition (mathematically, a vector belonging to a simplex). It is interesting to model these proportions and study the impact of the characteristics of the territorial units on the outcome of the elections. In the political economy literature, such regression models are generally restricted to the case of two political parties. In the statistical literature, there are regression models adapted to share vectors including CODA models (for COmpositional Data Analysis), but also Dirichlet models, Student models and others. Our goal is to use CODA regression models to generalize political economy models to more than two parties. The models are _tted on French electoral data of the 2015 departmental elections.
C46|Bayesian Inference for TIP curves: An Application to Child Poverty in Germany|TIP curves are cumulative poverty gap curves used for representing the three different aspects of poverty: incidence, intensity and inequality. The paper provides Bayesian inference for TIP curves, linking their expression to a parametric representation of the income distribution using a mixture of lognormal densities. We treat specifically the question of zero-inflated income data and survey weights, which are two important issues in survey analysis. The advantage of the Bayesian approach is that it takes into account all the information contained in the sample and that it provides small sample confidence intervals and tests for TIP dominance. We apply our methodology to evaluate the evolution of child poverty in Germany after 2002, providing thus an update the portrait of child poverty in Germany given in Corak et al. 2008.
C46|Regional effectiveness of innovation – leaders and followers of the EU NUTS 0 and NUTS 2 regions|Innovation constitutes an important factor for growth in all EU countries. Regions of the EU play a principle role in shaping new innovation trajectories and in bringing out the hidden potential for national growth. However, it is not only the level of innovation that diversifies regions, but also the innovative potential and the level of its realization. Therefore, the aim of this paper is to assess the realization of innovative potential, defined as effectiveness, in EU NUTS 0 and, if possible, NUTS 2 regions. To accomplish this goal a relative effectiveness method in used. The DEA (Data Envelopment Analysis) makes it possible to analyse the relative technical effectiveness based on regional inputs and outputs, without incorporating the legal and technological specifications of innovations, thus treating it like a production process. The inputs of the process are employment in technology and knowledge-intensive sectors and R&D expenditure, while the outputs include the number of patents and GDP. All variables are standardized by the size of the economically active population. DEA results divide regions in to two groups – effective, being the leaders; and ineffective, or followers. The DEA approach was combined and extended by ESDA (Exploratory Spatial Data Analysis) in order to pinpoint spatial patterns of innovation efficiency across NUTS 2 regions. Defining the best practices and implementing the learning-from-the-best policy is important in the process of regional development and specialization
C46|An integrated approach for top-corrected Ginis|Household survey data provide a rich information set on income, household context and demographic variables, but tend to under-report incomes at the very top of the distribution. Tax record data offer more precise information on top incomes at the expense of household context details and incomes of non-fillers at the bottom of the distribution. We combine the benefits of the two data sources to improve survey-based Gini coefficients in two ways. First, we incorporate top income share estimates based on tax records with survey-based Ginis for the rest of the population following Atkinson (2007) and Alvaredo (2011). Second, we impute top fractile's income in EU-SILC survey data with the Pareto distribution coefficients obtained from tax records and then calculate the Gini coefficient. We find that both approaches produce rather similar results. The gap between unadjusted and top-corrected Ginis is highest in countries that rely exclusively on survey data as compared to purely register or partly register countries.
C46|Modelling euro banknote quality in circulation|The quality of banknotes in the cash cycles of countries in the Eurosystem varies, despite all of these countries using identical euro banknotes. While it is known that this is dependent on national characteristics, such as public use and the involvement of the central bank in cash processing operations, the influence of all relevant parameters has not yet been established. This paper presents two computer-based models for the simulation of banknote cash cycles. The first model simulates a cash cycle using a theoretical approach based on key figures and models banknote fitness as a one-dimensional profile of fitness levels. The model identifies: (i) the frequency with which banknotes are returned to the central bank; (ii) the fitness threshold used in automated note processing at the central bank; and (iii) the note lifetime as the main drivers of banknote quality in circulation as well as central bank cash cycle costs. Production variations in new banknotes, the fitness threshold applied by commercial cash handlers and the accuracy of the fitness sensors used in the sorting process have been found to have a lower but non-trivial impact. The second model simulates banknotes in circulation as single entities and is oriented towards modelling country-specific cash cycles using available single-note data. The model is constructed using data collected by monitoring banknotes in circulation over the duration of a “circulation trial” carried out in three euro area countries. We compare the predicted quality results of the second data-based model against actual cash cycle data collected outside the circulation trial, discuss the reasons for the deviations found and conclude with considerations for an optimal theoretical national cash cycle. JEL Classification: C46, C63, E42, E58
C46|Pareto Models, Top Incomes and Recent Trends in UK Income Inequality|I determine UK income inequality levels and trends by combining inequality estimates from tax return data (for the ‘rich’) and household survey data (for the ‘non-rich’), taking advantage of the better coverage of top incomes in tax return data (which I demonstrate) and creating income variables in the survey data with the same definitions as in the tax data to enhance comparability. For top income recipients, I estimate inequality and mean income by fitting Pareto models to the tax data, examining specification issues in depth, notably whether to use Pareto I or Pareto II (generalised Pareto) models, and the choice of income threshold above which the Pareto models apply. The preferred specification is a Pareto II model with a threshold set at the 99th or 95th percentile (depending on year). Conclusions about aggregate UK inequality trends since the mid-1990s are robust to the way in which tax data are employed. The Gini coefficient for gross individual income rose by around 7% or 8% between 1996/97 and 2007/08, with most of the increase occurring after 2003/04. The corresponding estimate based wholly on the survey data is around –5%.<br><small>(This abstract was borrowed from another version of this item.)</small>
C46|Computation of the Corrected Cornish-Fisher Expansion using the Response Surface Methodology: Application to V aR and CV aR| The Cornish-Fisher expansion is a simple way to determine quantiles of non- normal distributions. It is frequently used by practitioners and by academics in risk mana- gement, portfolio allocation, and asset liability management. It allows us to consider non- normality and, thus, moments higher than the second moment, using a formula in which terms in higher-order moments appear explicitly. This paper has two primary objectives. First, we resolve the classic confusion between the skewness and kurtosis coefficients of the formula and the actual skewness and kurtosis of the distribution when using the Cornish{ Fisher expansion. Second, we use the response surface approach to estimate a function for these two values. This helps to overcome the difficulties associated with using the Cornish{ Fisher expansion correctly to compute value at risk (V aR). In particular, it allows a direct computation of the quantiles. Our methodology has many practical applications in risk ma- nagement and asset allocation.
C46|Working Paper 14-17 - Modelling unobserved heterogeneity in distribution - Finite mixtures of the Johnson family of distributions|This paper proposes a new model to account for unobserved heterogeneity in empirical modelling. The model extends the well-known Finite Mixture (or Latent Class) Model by using the Johnson family of distributions for the component densities. Due to the great variety of distributional shapes that can be assumed by the Johnson family, the method does not impose the usual a priori assumptions regarding the type of densities that are mixed.
C46|Bayesian Inference for TIP curves: An Application to Child Poverty in Germany|TIP curves are cumulative poverty gap curves used for representing the three different aspects of poverty: incidence, intensity and inequality. The paper provides Bayesian inference for TIP curves, linking their expression to a parametric representation of the income distribution using a mixture of lognormal densities. We treat specifically the question of zero-inflated income data and survey weights, which are two important issues in survey analysis. The advantage of the Bayesian approach is that it takes into account all the information contained in the sample and that it provides small sample confidence intervals and tests for TIP dominance. We apply our methodology to evaluate the evolution of child poverty in Germany after 2002, providing thus an update the portrait of child poverty in Germany given in Corak et al. 2008.
C46|The Reliability of Students' Earnings Expectations|Eliciting expectation and introducing probabilistic questions into surveys have gained important interest. In this study, we focus on the reliability of students’ earnings expectations. To what extent is observed log earnings expectations affected by random measurement error (noise)? A test-retest method is applied and reliability is found to be fairly low; about 0.59 in 2015 and about 0.67 in 2016. Particularly homogeneous samples exaggerate problems of measurement error. The analysis show how these measures of reliability can be adjusted to become more suitable to other studies, where different degrees of homogeneity are present.
C46|The ‘wrong skewness’ problem: a re-specification of stochastic frontiers|Abstract In this paper, we study the ‘wrong skewness phenomenon’ in stochastic frontiers (SF), which consists in the observed difference between the expected and estimated sign of the asymmetry of the composite error, and causes the ‘wrong skewness problem’, for which the estimated inefficiency in the whole industry is zero. We propose a more general and flexible specification of the SF model, introducing dependences between the two error components and asymmetry (positive or negative) of the random error. This re-specification allows us to decompose the third moment of the composite error into three components, namely: (i) the asymmetry of the inefficiency term; (ii) the asymmetry of the random error; and (iii) the structure of dependence between the error components. This decomposition suggests that the wrong skewness anomaly is an ill-posed problem, because we cannot establish ex ante the expected sign of the asymmetry of the composite error. We report a relevant special case that allows us to estimate the three components of the asymmetry of the composite error and, consequently, to interpret the estimated sign. We present two empirical applications. In the first dataset, where the classic SF has the wrong skewness, an estimation of our model rejects the dependence hypothesis, but accepts the asymmetry of the random error, thus justifying the sign of the skewness of the composite error. More importantly, we estimate a non-zero inefficiency, thus solving the wrong skewness problem. In the second dataset, where the classic SF does not yield any anomaly, an estimation of our model provides evidence for the presence of dependence. In such situations, we show that there is a remarkable difference in the efficiency distribution between the classic SF and our class of models.
C46|Dinámica de la inversión extranjera directa en los estados de México: un análisis de cadenas de Markov espaciales|El objetivo de esta investigación consiste en analizar la evolución de la distribución espacial y temporal de la inversión extranjera directa (IED) en las entidades federativas de México. La literatura que aborda el análisis de la IED en México es abundante y diversa; sin embargo, se argumenta que el análisis de la distribución espacio-temporal de la IED condicionada a la interacción espacial en México, aún está ausente. En este sentido, mediante la aplicación del enfoque de cadenas de Markov espaciales propuesto por Rey (2001), se encuentra que la divergencia regional en la captación de IED es un proceso que parece afianzarse cuando se analizan diferentes cortes en el tiempo. En particular, durante el periodo entre 2006 y 2013 el proceso de divergencia hacia estratos de mayor captación estaría impulsado por las entidades federativas que interactúan con entidades contiguas ubicadas en estratos de captación de IED menores.
C46|Dynamic of foreign direct investment in the states of Mexico: An analysis of Markov's spatial chains|The aim of this investigation is to analyze the evolution of the spatio-temporal distribution of foreign direct investment (FDI) across Mexican states. The literature that analyzes foreign direct investment in Mexico is numerous and diverse; however, it is argued that the analysis of the spatio-temporal distribution of FDI conditioned to spatial interaction effects in Mexico is still absent. In this sense, by applying the spatial Markov chain approach as proposed by Rey (2001), we found a divergence process in the FDI inflows among Mexican states that seem to get stronger over time. In particular, during the period from 2006 to 2013, the process of divergence toward higher-FDI inflows quantiles occurs among states spatially associated with neighbors in lower-FDI quantiles.
C46|The accuracy of measures of institutional trust in household surveys: Evidence from the oecd trust database|A key policy concern in recent years has been the decline in levels of trust by citizen in public institutions. Trust is one of the foundations upon which the legitimacy and sustainability of political systems are built. It is crucial to the implementation of a wide range of policies and influences people’s behavioural responses to such policies. However, despite its acknowledged importance, trust in public institutions is poorly understood and is not consistently measured across OECD countries. The OECD Trust Database brings together information from a wide range of different household surveys containing measures of trust and combines this with information on other social and economic outcomes. The size of the database and range of covariates make it possible to identify the underlying patterns captured by survey based measures of trust in institutions and systematically test the accuracy (i.e. reliability and validity) of these measures. Reliability is tested by examining the consistency of measures of institutional trust across different surveys and between different waves of the same survey. Validity is harder to test than reliability. It is however possible to examine the construct validity of institutional trust measures by looking at whether these measures show the expected correlation with other social and economic variables on a cross-country basis. Analysis of item-specific non-response rates provides important additional information on the face validity of institutional trust measures.
C46|Governance statistics in OECD countries and beyond: What exists, and what would be required to assess their quality?|The paper provides a first assessment of the range of governance statistics that are available in OECD countries, reaching three main conclusions. First, while several statistics relating to various aspects of governance are already available, they differ in terms of the underlying concepts, the labels used to describe them, the range of institutions covered, and the detailed aspect or function considered: developing a common conceptual framework for governance is hence a prerequisite for gathering more robust and useful statistics in this field. Second, efforts should be devoted to thoroughly assess the quality of existing governance statistics, as a preliminary step towards providing general advice to statistical producers and users: the model currently used by the OECD with respect to measuring “trust”, based on an assessment of the reliability and validity of existing measures, could be usefully extended to other aspects of governance. Third, while politically sensitive, there are no a priori reasons why NSOs should consider governance statistics as falling outside their remit; these statistics should become part of their routine production, subject to the same quality standards and requirements that apply to other social, economic and environmental statistics. On trouvera dans le présent document une première évaluation de l’éventail des statistiques disponibles au sein des pays de l’OCDE dans le domaine de la gouvernance. Trois grandes conclusions s’en dégagent. Premièrement, si plusieurs statistiques relatives à divers aspects de la gouvernance sont déjà disponibles, elles diffèrent sur le plan de leurs concepts sous-jacents, des appellations employées pour les décrire, de la gamme d’institutions couvertes et des aspects précis pris en compte : l’élaboration d’un cadre conceptuel commun en matière de gouvernance apparaît donc comme une condition préalable pour rassembler des statistiques plus robustes et utiles dans ce domaine. Deuxièmement, il faudrait s’employer à évaluer de façon exhaustive la qualité des statistiques existantes en matière de gouvernance, afin de pouvoir ensuite fournir des conseils d’ordre général aux producteurs et aux utilisateurs de statistiques ; le modèle actuellement utilisé par l'OCDE s’agissant de mesurer la confiance, à la lumière d’une évaluation de la fiabilité et de la validité des éléments de mesure existants, pourrait utilement être étendu à d’autres aspects de la gouvernance. Troisièmement, même si cette question est politiquement sensible, il n’y a aucune raison, a priori, pour que les offices statistiques nationaux n’intègrent pas les statistiques de gouvernance à leur production courante, avec les mêmes exigences et normes de qualité que pour les autres statistiques d’ordre social, économique ou environnemental.
C46|Measuring the Distributions of Public Inflation Perceptions and Expectations in the UK|The Bank of England/GfK NOP Inflation Attitudes Survey asks individuals about their inflation perceptions and expectations in eight ordered categories with known boundaries except for an indifference limen. With enough categories for identification, one can fit a mixture distribution to such data, which can be multi-modal. Thus Bayesian analysis of a normal mixture model for interval data with an indifference limen is of interest. This paper applies the No-U-Turn Sampler (NUTS) for Bayesian computation, and estimates the distributions of public inflation perceptions and expectations in the UK during 2001Q1--2015Q4. The estimated means are useful for measuring information rigidity.
C46|Robust inference in conditionally heteroskedastic autoregressions|We consider robust inference for an autoregressive parameter in a stationary autoregressive model with GARCH innovations when estimation is based on least squares estimation. As the innovations exhibit GARCH, they are by construction heavy-tailed with some tail index $\kappa$. The rate of consistency as well as the limiting distribution of the least squares estimator depend on $\kappa$. In the spirit of Ibragimov and Müller (“t-statistic based correlation and heterogeneity robust inference”, Journal of Business & Economic Statistics, 2010, vol. 28, pp. 453-468), we consider testing a hypothesis about a parameter based on a Student’s t-statistic for a fixed number of subsamples of the original sample. The merit of this approach is that no knowledge about the value of $\kappa$ nor about the rate of consistency and the limiting distribution of the least squares estimator is required. We verify that the one-sided t-test is asymptotically a level $\alpha$ test whenever $\alpha \le $ 5% uniformly over $\kappa \ge 2$, which includes cases where the innovations have infinite variance. A simulation experiment suggests that the finite-sample properties of the test are quite good.
C46|Have Middle-class Earnings Risen In Canada? A Statistical Inference Approach|This paper extends the statistical inference approach developed in Beach (2016) to look at income changes over different regions of an income distribution. Specifically, it looks at relative-mean earnings (RME) ratios and mean earnings levels for lower earners, middle-class (MC) workers and higher earners in Canada since 1970. Formulas are developed for (asymptotic) standard errors of these distributional statistics. The most consistent pattern since 1980 has been the marked decline in RME for MC workers, which has been highly statistically significant. Since 2005, however, real earnings levels have increased significantly and have been broadly shared across these earnings groups.
C46|In-fill Asymptotic Theory for Structural Break Point in Autoregression: A Unified Theory|This paper obtains the exact distribution of the maximum likelihood estimator of structural break point in the Ornstein-Uhlenbeck process when a continuous record is available. The exact distribution is asymmetric, tri-modal, dependent on the initial condition. These three properties are also found in the finite sam- ple distribution of the least squares (LS) estimator of structural break point in autoregressive (AR) models. Motivated by these observations, the paper then develops an in-fill asymptotic theory for the LS estimator of structural break point in the AR(1) coefficient. The in-fill asymptotic distribution is also asymmetric, tri-modal, dependent on the initial condition, and delivers excellent approximations to the finite sample distribution. Unlike the long-span asymptotic theory, which depends on the underlying AR root and hence is tailor-made but is only available in a rather limited number of cases, the in-fill asymptotic theory is continuous in the underlying roots. Monte Carlo studies show that the in-fill asymptotic theory performs better than the long-span asymptotic theory for cases where the long-span theory is available and performs very well for cases where no long-span theory is available.
C46|Using compositional and Dirichlet models for market share regression| When the aim is to model market shares, the marketing literature proposes some regression models which can be qualified as attraction models. They are generally derived from an aggregated version of the multinomial logit model. But aggregated multinomial logit models (MNL) and the so-called generalized multiplicative competitive interaction models (GMCI) present some limitations: in their simpler version they do not specify brand-specific and cross effect parameters. In this paper, we consider alternative models: the Dirichlet model (DIR) and the compositional model (CODA). DIR allows to introduce brand-specific parameters and CODA allows additionally to consider cross effect parameters. We show that these two models can be written in a similar fashion, called attraction form, as the MNL and the GMCI models. As market share models are usually interpreted in terms of elasticities, we also use this notion to interpret the DIR and CODA models. We compare the properties of the models in order to explain why CODA and DIR models can outperform traditional market share models. An application to the automobile market is presented where we model brands market shares as a function of media investments, controlling for the brands price and scrapping incentive. We compare the quality of the models using measures adapted to shares.
C46|Interpreting the impact of explanatory variables in compositional models|Regression models have been developed for the case where the dependent variable is a vector of shares. Some of them, from the marketing literature, are easy to interpret but they are quite simple and can only be complexified at the expense of a very large number of parameters to estimate. Other models, from the mathematical literature, are called compositional regression models and are based on the simplicial geometry (a vector of shares is called a composition, shares are components, and a composition lies in the simplex). These models are transformation models: they use a log-ratio transformation of shares. They are very flexible in terms of explanatory variables and complexity (component-specific and cross-effect parameters), but their interpretation is not straightforward, due to the fact that shares add up to one. This paper combines both literatures in order to obtain a performing market-share model allowing to get relevant and appropriate interpretations, which can be used for decision making in practical cases. For example, we are interested in modeling the impact of media investments on automobile manufacturers sales. In order to take into account the competition, we model the brands market-shares as a function of (relative) media investments. We furthermore focus on compositional models where some explanatory variables are also compositional. Two specifications are possible: in Model A, a unique coefficient is associated to each compositional explanatory variable, whereas in Model B a compositional explanatory variable is associated to component-specific and cross-effect coefficients. Model A and Model B are estimated for our application in the B segment of the French automobile market, from 2003 to 2015. In order to enhance the interpretability of these models, we present different types of impact assessment measures (marginal effects, elasticities and odds ratios) and we show that elasticities are particularly useful to isolate the impact of an explanatory variable on a particular share. We show that elasticities can be equivalently computed from the transformed model and from the model in the simplex and that they are linked to directional C-derivatives of simplex-valued function of a simplex variable. Direct and cross effects of media investments are computed for both models. Model B shows interesting non-symmetric synergies between brands, and Renault seems to be the most elastic brand to its own media investments. In order to determine if component-specific and cross-effect parameters are needed to improve the quality of the model (Model B) or if a global parameter is reasonable (Model A), we compare the goodness-of-fit of the two models using (out-of-sample) quality measures adapted for share data.
C46|Automatic regrouping of strata in the chi-square test|Pearson´s chi-square test is widely employed in social and health science to analyze categorical data and contingency tables and to assess sample representativeness. For the test to be valid the sample size must be big enough to provide a minimum number of expected elements per category. If the researcher chooses to regroup the strata in order to solve the failure on the minimum size requirement, the existence of automatic re-grouping procedures in statistical software would be very useful, especially when tests are applied sequentially. After comprehensively reviewing the software that can carry out this test, we find that, with a few exceptions, there is no automatic regrouping of the strata to meet this requirement, although it would be very useful if this were available. This paper develops some functions for regrouping strata automatically no matter where they are located, thus enabling the test to be performed within an iterative procedure. The functions are written in Excel VBA (Visual Basic for Applications) and in Mathematica, so it would not be hard to implement them in other languages. The utility of these functions is shown by using three different datasets. Finally, the iterative use of the functions is applied to the Continuous Sample of Working Lives, a dataset that has been used in a considerable number of studies, especially on labor economics and the Spanish public pension system.
C46|David Versus Goliath: Fundamental Patterns and Predictions in Modern Wars and Terrorist Campaigns|It is still unknown whether there is some deep structure to modern wars and terrorist campaigns that could allow reliable prediction of future patterns of violent events. Recent war research focuses on size distributions of violent events, with size defined by the number of people killed in each event. Event size distributions within previously available datasets, for both armed conflicts and for global terrorism as a whole, exhibit extraordinary regularities that transcend specifics of time and place. These distributions have been well modelled by a narrow range of power laws that are, in turn, supported by a theory of coalescence and fragmentation of violent groups. We show that the predicted eventsize patterns emerge in a mass of new event data covering conflict in Africa and Asia from 1990 to 2014. Moreover, there are similar regularities in the events generated by individual terrorist organizations, 1997-2014. The existence of such robust empirical patterns hints at the predictability of size distributions of violent events in future wars. We pursue this prospect using split-sample techniques that help us to make useful out-of-sample predictions. Power-law-based prediction systems outperform lognormal-based systems. We conclude that there is indeed evidence from the existing data that fundamental patterns do exist, and that these can allow prediction of future structures in modern wars and terrorist campaigns.
C46|The Reliability of Studentsâ€™ Earnings Expectations| Eliciting expectation and introducing probabilistic questions into surveys have gained important interest. In this study, we focus on the reliability of studentsâ€™ earnings expectations. To what extent is observed log earnings expectations affected by random measurement error (noise)? A test-retest method is applied and reliability is found to be fairly low; about 0.59 in 2015 and about 0.67 in 2016. Particularly homogeneous samples exaggerate problems of measurement error. The analysis show how these measures of reliability can be adjusted to become more suitable to other studies, where different degrees of homogeneity are present. JEL: C46; C83; I26 Keywords: Earnings expectations, reliability, measurement error.
C46|A Note on Improved Estimation for the Topp-Leone Distribution|The Topp-Leone distribution is attractive for reliability studies as it has finite support and a bathtub-shaped hazard function. We compare some properties of the method of moments, maximum likelihood, and bias-adjusted maximum likelihood estimators of its shape parameter. The last of these estimators is very simple to apply and dominates the method of moments estimator in terms of relative bias and mean squared error.
C46|Risk Analysis for Three Precious Metals: An Application of Extreme Value Theory|Gold, and other precious metals, are among the oldest and most widely held commodities used as a hedge against the risk of disruptions in financial markets. The prices of such metals fluctuate substantially, introducing a risk of its own. This paper’s goal is to analyze the risk of investment in gold, silver, and platinum by applying Extreme Value Theory to historical daily data for changes in their prices. The risk measures adopted in this paper are Value at Risk and Expected Shortfall. Estimates of these measures are obtained by fitting the Generalized Pareto Distribution, using the Peaks-Over-Threshold method, to the extreme daily price changes. The robustness of the results to changes in the sample period is discussed. Our results show that silver is the most risky metal among the three considered. For negative daily returns, platinum is riskier than gold; while the converse is true for positive returns.
C46|The Profile of Romanian Urban Inns|This paper is the third of a series of studies dedicated to tourist inns on the Romanian market. The previous papers focused on the identification of the tourist inns that currently function on the domestic market. Further, their potential as rural facilities was highlighted and their authenticity was discussed. The relevance of this research is linked to the fact that in the early 1990s tourist inns were excluded from the lists of lodging and food-serving facilities, ceasing to be officially ranked. Consequently, the inns’ owners were forced to reclassify as other accepted types or, even worse, to function in the shadow economy , without any official ranking. Moreover, the absence of inns on the market and the incoherent development of certain types of lodgings in Romania, have also led to the fact that entrepreneurs and tourists tend to be confused and, sometimes, not able to differentiate one type of accommodation unit from another. The main purpose of this research is to determine the extent to which urban inns can contribute to the authenticity of the Romanian tourism. From a methodological perspective, the paper relies on both official data (collected and processed based on the official Lists of Hospitality Facilities) and on the information available on specialized websites. Thorough analyses have been run in order to identify the tourist structures pretending to be inns, to further categorize and discuss them according to various criteria. The main findings and conclusions of this paper reveal that inns have the potential to contribute to the authenticity of Romania’s hospitality industry .
C46|A Bayesian Look at American Academic Wages: The Case of Michigan State University|The paper investigates academic wage formation inside Michigan State University and develops tools in order to detect the presence of possible superstars. We model wage distributions using a hybrid mixture formed by a lognormal distribution for regular wages and a Pareto distributions for higher wages, using a Bayesian approach, particularly well adapted for inference in hybrid mixtures. The presence of superstars is detected by studying the shape of the Pareto tail. Contrary to usual expectations, we did found some evidence of superstars, but only when recruiting Assistant Professors. When climbing up the wage ladder, superstars disappear. For full professors, we found a phenomenon of wage compression as if there were a higher bound, which is just the contrary of a superstar phenomenon. Moreover, a dynamic analysis shows that many recruited superstars did not fulfill the university expectations as either they were not promoted or left for lower ranked universities.
C46|NEW ECONOMIC WINDOWS ON INCOME AND WEALTH: THE k-GENERALIZED FAMILY OF DISTRIBUTIONS|Over the last decades, the distribution of income and wealth has been deteriorating in many countries, leading to increased inequalities within and between societies. This tendency has revived the interest in the subject greatly, yet it still receives very little attention within the realm of mainstream economic thinking. One reason for this is that the basic paradigm of “standard economics”, the representative-agent General Equilibrium framework, is badly equipped to cope with distributional issues. Here we argue that when the economy is treated as a complex system composed of many heterogeneous interacting agents who give rise to emergent phenomena, to address the main stylized facts of income/wealth distribution requires leaving the toolbox of mainstream economics in favour of alternative approaches. The “?-generalized” family of income/wealth distributions, building on the categories of complexity, is an example of how advances in the field can be achieved within new interdisciplinary research contexts.
C46|Spatial wage inequality in Belarus|This paper studies the wage inequality in Belarus' districts from 2000 till 2015 following the multistep and multi-mechanism framework. The empirical results show: first, that wage disparities across the districts decreased in the 2000-2012 period and then increased from 2013 to 2015; second, there is the spatial dependency in district wages and increasing separation between districts, and between rural and urban population in Belarus; third, the main economic factors that contribute to decrease in district wage inequality are industrial development, retail trade and agricultural development. Finally, from theoretical point of view this research rejects the inverted U-shaped relationship between spatial inequality and economic development for Belarus and supports the hypothesis made by French economist Thomas Piketty that slow growth rates lead to rising inequality.
C46|Cross Border Mergers And Acquisitions - An Overview Of Their Evolution And Trends|The paper aims to explore the world of cross-border mergers and acquisitions (M&As), in order to capture the dynamic image of a global picture encapsulating a quarter of century (1990-2014) of evolution - mostly by analyzing the UNCTAD's data on the subject. The research goals are to identify the main evolutions and trends of the cross-border M&As, on one hand, and to set the backgrounds for future research on the field, on the other hand - considering that the participants to this process, their motivators, specific choices and strategies, as well as their main challenges and expected outcomes are continuously changing, asking for more and more refined and sophisticated approaches.
C46|On the Stock–Yogo Tables|A standard test for weak instruments compares the first-stage F -statistic to a table of critical values obtained by Stock and Yogo (2005) using simulations. We derive a closed-form solution for the expectation from which these critical values are derived, as well as present some second-order asymptotic approximations that may be of value in the presence of multiple endogenous regressors. Inspection of this new result provides insights not available from simulation, and will allow software implementations to be generalised and improved. Finally, we explore the calculation of p -values for the first-stage F -statistic weak instruments test.
C46|Estimation of Health Care Demand and its Implication on Income Effects of Individuals|Zero inflation and over-dispersion issues can significantly affect the predicted probabilities as well as lead to unreliable estimations in count data models. This paper investigates whether considering this issue for German Socioeconomic Panel (1984-1995), used by Riphahn et al (2003), provides any evidence of misspecification in their estimated models for adverse selection and moral hazard effects in health demand market The paper has the following contributions: first, it shows that estimated parameters for adverse selection and moral hazard effects are sensitive to the model choice; second, the random effects panel data as well as standard pooled data models do not provide reliable estimates for health care demand (doctor visits); third, it shows that by appropriately accounting for zero inflation and over-dispersion there is no evidence of adverse selection behaviour and that moral hazard plays a positive and significant role for visiting more doctors. These results are robust for both males and females’ subsamples as well as for the full data sample.
C46|Beyond Inequality: A Novel Measure of Skewness and its Properties|We show that a recent appendix to the Gini-coefficient to make the latter more sensitive to asymmetric income distributions can be viewed as an abstract measure of skewness. We develop some of its properties and apply it to the US-income distribution in 1974 and 2010.
C46|Sequentially testing polynomial model hypotheses using power transforms of regressors|We provide a methodology for testing a polynomial model hypothesis by generalizing the approach and results of Baek, Cho, and Phillips (Journal of Econometrics, 2015, 187, 376–384; BCP), which test for neglected nonlinearity using power transforms of regressors against arbitrary nonlinearity. We use the BCP quasi‐likelihood ratio test and deal with the new multifold identification problem that arises under the null of the polynomial model. The approach leads to convenient asymptotic theory for inference, has omnibus power against general nonlinear alternatives, and allows estimation of an unknown polynomial degree in a model by way of sequential testing, a technique that is useful in the application of sieve approximations. Simulations show good performance in the sequential test procedure in both identifying and estimating unknown polynomial order. The approach, which can be used empirically to test for misspecification, is applied to a Mincer (Journal of Political Economy, 1958, 66, 281–302; Schooling, Experience and Earnings, Columbia University Press, 1974) equation using data from Card (in Christofides, Grant, and Swidinsky (Eds.), Aspects of Labour Market Behaviour: Essays in Honour of John Vanderkamp, University of Toronto Press, 1995, 201‐222) and Bierens and Ginther (Empirical Economics, 2001, 26, 307–324). The results confirm that the standard Mincer log earnings equation is readily shown to be misspecified. The applications consider different datasets and examine the impact of nonlinear effects of experience and schooling on earnings, allowing for flexibility in the respective polynomial representations.
C46|Identifying Consumers’Profiles Concerning Residential Lighting|Reducing electricity consumption, by decreasing residential light- ing, falls in the range of measures aimed to save 20% of primary energy consump- tion in European Union, up to 2020, and further to improve energy efficiency after 2020. Public lighting and appliances is about 14 % of total electricity consump- tion, in Romania. New energy efficient lighting technologies might contribute to a substantial decreasing of household electricity consumption. Data set used to ap- ply the scientific methodology presented in the paper was gathered in a survey re- search, aiming to investigate Romanians attitude and behavior about lighting con- sumption in households. The goals of this research paper are both to identify the factors associated with the replacement of old incandescent lamps, with the new energy efficient compact fluorescent lamps and light emitting diodes, and to identi- fy Romanian typologies of consumers and the patterns of their behavior. In order to accomplish the research goals, a model of analysis, based on Cluster Analysis and Multiple Correspondence Analysis methods has been proposed in the paper.
C46|The large-sample distribution of the maximum Sharpe ratio with and without short sales|In the Markowitz paradigm the portfolio having maximum Sharpe ratio is optimal. Previously the large sample distribution of this statistic has been calculated when short sales are allowed and sample returns and covariance matrix are asymptotically normally distributed. This paper considers the more complex situation when short sales are not allowed, and provides conditions under which the maximum Sharpe ratio is asymptotically normal. This is not always the case, as we show, in particular when the returns have zero mean. For this situation we obtain upper and lower asymptotic bounds (in distribution) on the possible values of the maximum Sharpe ratio which coincide when the returns are asymptotically uncorrelated. We indicate how the asymptotic theory, developed for the case of no short sales, can be extended to handle a more general class of portfolio constraints defined in terms of convex polytopes. Via simulations we examine the rapidity of approach to the limit distributions under various assumptions.
C46|Variance of the truncated negative binomial distribution|Citations to formulas for the moments of the truncated negative binomial distribution usually reference the paper by Gurmu and Trivedi (1992). However their second moments of the truncated negative binomial are incorrect. We derive the correct second moments for both the left and right truncated negative binomial distribution. The second moments of the truncated distributions are written in a form that shows they will converge to the second moment of the un-truncated distribution when the truncated first moment approaches the un-truncated first moment.
C46|Dynamic conditional correlation multiplicative error processes|We introduce a dynamic model for multivariate processes of (non-negative) high-frequency trading variables revealing time-varying conditional variances and correlations. Modeling the variables' conditional mean processes using a multiplicative error model, we map the resulting residuals into a Gaussian domain using a copula-type transformation. Based on high-frequency volatility, cumulative trading volumes, trade counts and market depth of various stocks traded at the NYSE, we show that the proposed transformation is supported by the data and allows capturing (multivariate) dynamics in higher order moments. The latter are modeled using a DCC-GARCH specification. We suggest estimating the model by composite maximum likelihood which is sufficiently flexible to be applicable in high dimensions. Strong empirical evidence for time-varying conditional (co-)variances in trading processes supports the usefulness of the approach. Taking these higher-order dynamics explicitly into account significantly improves the goodness-of-fit and out-of-sample forecasts of the multiplicative error model.
C46|Climate normals and weather normalization for utility regulation|In the regulation of natural gas and electric utilities, the determination of rate revenues commonly involves a sales adjustment to reflect the difference between actual weather and normal weather. This adjustment process, commonly known as weather normalization, is required to properly determine a set of rates which yields the revenue requirement under the assumption of normal weather. Normal weather values that characterize long-term weather patterns are critical component of weather normalization. Conventionally, normal weather values are calculated using the Standard Climate Normal (SCN). The SCN for any given calendar day is the 30-year average of the associated weather observations for that calendar day. In the regulatory process the SCN can inadvertently introduce biases in the weather normalization adjustment. This study investigates the sources and mitigation of these biases.
C46|Dynamic structure of the spot price of crude oil: does time aggregation matter?|This paper assesses nonlinear structures in the time series data generating mechanism of crude oil prices. We apply well-known univariate tests for nonlinearity, with distinct power functions over alternatives, but with different null hypotheses reflecting the existence of different concepts of linearity and nonlinearity in the time series literature. We utilize daily data on crude oil spot price for over 26years, as well as monthly data on crude oil spot price for 41years. Investigating the monthly price of crude oil along with the daily price distinguishes the approach of this paper from existing studies focusing on the time series structure of crude oil price. All the tests detect strong evidence of general nonlinear serial dependence, as well as nonlinearity in the mean, variance, and skewness functions in the daily spot price of crude oil. Since evidence of nonlinear dependence is less dramatic in monthly observations, nonlinear serial dependence is moderated by time aggregation in crude oil prices but not significantly.
C46|A note on optimal portfolios under regime–switching|This paper extends the stochastic dominance rules for normal mixture distributions derived by Levy and Kaplanski (2015). First, the portfolios under consideration are allowed to follow different regime-switching processes. Second, the results are extended from second- to fourth-order stochastic dominance, which is known to be closely related to kurtosis aversion in financial markets and allows to compare mixture distributions with the same overall variance. In particular, when a risk-free asset is available, checking for fourth-order stochastic dominance turns out to amount to a comparison of the regime-specific and overall Sharpe ratios of the portfolios under consideration.
C46|The risk in capital controls|This paper investigates the effect of capital controls in banking outflow funds on financial risk in the Athens Stock Exchange. The study uses daily returns for two sub-periods before and after the capital controls, ranging from 31 March 2011 to 31 March 2016. We focus on the peaks over threshold method to calculate the risk measures. We propose a bootstrap approach to compare risk measures in two sub-periods. The bootstrap results indicate that there is an underestimation of financial risk after the capital controls.
C46|Internal or external devaluation? What does the EC Consumer Survey tell us about macroeconomic adjustment in the Euro area?|This paper explores the dynamics of national inflation expectations within the euro area during the recent crisis. Using the European Commission's Consumer Survey, we find that the strong anchoring of area-wide inflation expectations, which is typically found in the literature, does not extend to individual member states. We also measure the effect of the crisis on national inflation expectations using sovereign bond spreads and find that increases in sovereign risk have a significant negative effect on inflation expectations. This suggests that consumers expect their country to adjust through a process of internal devaluation. In contrast, we find no evidence that tensions in the sovereign bond markets increase national inflation expectations, as one would be expect under an exit or breakup scenario.
C46|Diversity of firm sizes, complexity, and industry structure in the Chinese economy|Among the phenomena in economics that are not yet well-understood is the fat-tailed (power-law) distribution of firm sizes in the world's economies. In the present paper we discuss different mechanisms suggested in the literature to explain this distribution of firm sizes. The paper uses the China Industrial Enterprises Database to study the distribution (firm size in terms of the number of employees, capital, and gross profit) for the provinces of China for the years 1998–2008. We estimate the power-law distribution and confirm its plausibility using the KS test and the log-likelihood ratio vs. lognormal and exponential distributions. The analysis on regional levels allows an assessment of regional effects on differences in the distribution; we discuss possible explanations for the observed patterns in the light of the recent regional economic development and the economic reforms in the PRC.
C46|Accurate Evaluation of Expected Shortfall for Linear Portfolios with Elliptically Distributed Risk Factors|We provide an accurate closed-form expression for the expected shortfall of linear portfolios with elliptically distributed risk factors. Our results aim to correct inaccuracies that originate in Kamdem (2005) and are present also in at least thirty other papers referencing it, including the recent survey by Nadarajah et al. (2014) on estimation methods for expected shortfall. In particular, we show that the correction we provide in the popular multivariate Student t setting eliminates understatement of expected shortfall by a factor varying from at least four to more than 100 across different tail quantiles and degrees of freedom. As such, the resulting economic impact in ﬁnancial risk management applications could be signiﬁcant. We further correct such errors encountered also in closely related results in Kamdem (2007 and 2009) for mixtures of elliptical distributions. More generally, our ﬁndings point to the extra scrutiny required when deploying new methods for expected shortfall estimation in practice.
C46|A Bayesian Look at American Academic Wages: The Case of Michigan State University|The paper investigates academic wage formation, taking as a benchmark the Michigan State University. We model wage distributions using a hybrid mixture formed by a lognormal distribution for regular wages and a Pareto distributions for higher wages, using a Bayesian approach. With this model, we test for the presence of superstars in the Pareto member by comparing inequality in the two members. We found some evidence of superstars when recruiting Assistant Professors. However, a dynamic analysis reveals that they have a higher rate of outing, and, if they stay, a lower rate of wage increase. For full professors, we found a phenomenon of wage compression as if there were a kind of higher bound, which is just the contrary of a superstar phenomenon.
C46|A Flexible Link Function for Discrete-Time Duration Models|This paper proposes a discrete-time hazard regression approach based on the relation between hazard rate models and excess over threshold models, which are frequently encountered in extreme value modelling. The proposed duration model employs a flexible link function and incorporates the grouped-duration analogue of the well-known Cox proportional hazards model and the proportional odds model as special cases. The theoretical setup of the model is motivated, and simulation results are reported, suggesting that the model proposed performs well. The simulation results and an empirical analysis of US import durations also show that the choice of link function in discrete hazard models has important implications for the estimation results, and that severe biases in the results can be avoided when using a flexible link function.
C46|On a Class of Statistical Distance Measures for Sales Distribution: Theory, Simulation and Calibration|While firm-level and micro issue analysis become an important part in research of international trade, only a few work is concerned about the goodness-of-fit for size distribution of firms. In this paper, we revisit the statistical aspects of firm productivity and sales revenue, in order to compare different definitions of statistical distances. We first deduce the exact form of size distribution of firms by only implementing the assumptions of productivity and demand function, and then introduce the famous g-divergence as well as its statistical implications. We also do the simulation and calibration so as to compare those different divergences, moreover, tests the combined assumptions. We conclude that minimizing Pearson x2 and Neyman x2 produces similar results and minimizing Kullback-Leibler divergence is likely to take the expense of other distance measures. Additionally, selection among different statistical distances is much more significant than demand functions.
C46|Volatility capital buffer to prevent the breach of the Solvency II capital requirements|The Solvency II regulation prescribes continuous capital adequacy, despite the fact that insurance companies only determine their capital adequacy in a reliable manner once annually. The volatility capital buffer1 (VCB) is meant to guarantee that, despite the higher volatility arising from the market valuation, at a given ? confidence level the solvency capital of insurers meets the capital requirement on a continuous basis. This paper reduces the problem to the search of the probability distribution quantile belonging to the ? confidence level (VaR?), the 99.5 per cent quantile of which is the solvency capital requirement (SCR) specified in the Solvency II Capital Regulation, and thus the VCB can be expressed as a percentage of the SCR. Without the assumptions related to the distribution, any value may be obtained for the VCB ratio, but it can be squeezed into a relatively narrow band even under natural assumptions. On the one hand, the analysis of these distribution groups may further narrow the possible values, and on the other hand it points out that in the case of fatter-tailed distributions (when major, extreme losses may also occur more frequently) and positive skewness (when the probability of the loss is smaller than that of the profit, but the value thereof is expected to be higher), we obtain a lower VCB ratio.
C46|Pricing of average value options versus European options with stochastic interest rate|This paper proposes a methodology to obtain the price of an Asian option with underlying average through Monte Carlo simulation. It is assumed that the interest rate is driven by a mean reversion process of Vasicek and CIR type with parameters calibrated by maximum likelihood. The simulation includes the quadratic resampling which reduces the use of computational resources, in particular the method improves the generation of variance covariance matrix. The proposed methodology is applied in the valuation of options on the price of AMXL. The results show that by comparing prices of European options, with both simulated and published by MexDer with their Asian counterparts, Asian options prices are lower in the case of call and put options in the money. For put options simulated prices were lower in all cases. Moreover, it was also found that the difference increases as the time to maturity of the option increases.
C46|Informational Performance, Competitive Capital-Market Scaling, and the Frequency Distribution of Tobin’s Q|We develop a systemic interpretation of the functioning of capital markets that formally accounts for the observed frequency distribution of Tobin’s q, reported in Scharfernaker and dos Santos, 2015. Considering Tobin’s q as a ratio of expected total rates of return, we draw on an epistemological understanding of the tools of statistical mechanics to interpret capital markets as a competitive informational system. The strong modality in the distribution of q is taken to be conditioned by the arbitrage operations of corporate insiders. We take the persistent spread in the distribution of q to reflect the presence of obstacles to that agency, which impose an informational constraint on the operation of capital markets. This spread is also shaped by the fact that the measure of Tobin’s q e↵ectively scales the expected returns for an individual corporation relative to those expected of all corporations. This scaling reflects aggregate measures of bullishness in investors’ valuations that insiders do not seek to exploit. In addition to accounting for the frequency distribution of q observed for the past 50 years, this interpretation points to a systemic diagnostic for the presence of speculative equity-price bubbles, and o↵ers a new informational characterization efficiency in capital markets. According to the latter, U.S. capital markets have experienced a steady secular loss in their informational efficiency since the early 1980s.
C46|On distributions of ratios|Inversion formulae are derived that express the density and distribution function of a ratio of random variables in terms of the joint characteristic function of the numerator and denominator. The resulting expressions are amenable to numerical evaluation and lead to simple asymptotic expansions. The expansions reduce to known results when the denominator is almost surely positive. Their accuracy is demonstrated with numerical examples.
C46|Beyond Dimension two: A Test for Higher-Order Tail Risk|In practice, multivariate dependencies between extreme risks are often only assessed in a pairwise way. We propose a test for detecting situations when such pairwise measures are inadequate and give incomplete results. This occurs when a significant portion of the multivariate dependence structure in the tails is of higher dimension than 2. Our test statistic is based on a decomposition of the stable tail dependence function describing multivariate tail dependence. The asymptotic properties of the test are provided and a bootstrap-based finite sample version of the test is proposed. A simulation study documents good size and power properties of the test including settings with time-series components and factor models. In an application to stock indices for non-crisis times, pairwise tail models seem appropriate for global markets while the test finds them not admissible for the tightly interconnected European market. From 2007/2008 on, however, higher order dependencies generally increase and require a multivariate tail model in all cases.
C46|Statistical Analysis Of Business Cycle Fluctuations In Poland Before And After The Crisis|The main objective of the paper is to investigate properties of business cycles in the Polish economy before and after the recent crisis. The essential issue addressed here is whether there is statistical evidence that the recent crisis has affected the properties of the business cycle fluctuations. In order to improve robustness of the results, we do not confine ourselves to any single inference method, but instead use different groups of statistical tools, including non-parametric methods based on subsampling and parametric Bayesian methods. We examine monthly series of industrial production (from January 1995 till December 2014), considering the properties of cycles in growth rates and in deviations from long-run trend. Empirical analysis is based on the sequence of expanding-window samples, with the shortest sample ending in December 2006. The main finding is that the two frequencies driving business cycle fluctuations in Poland correspond to cycles with periods of 2 and 3.5 years, and (perhaps surprisingly) the result holds both before and after the crisis. We, therefore, find no support for the claim that features (in particular frequencies) that characterize Polish business cycle fluctuations have changed after the recent crisis. The conclusion is unanimously supported by various statistical methods that are used in the paper, however, it is based on relatively short series of the data currently available.
C46|Measuring poverty with the Foster, Greer and Thorbecke indexes based on the Gamma distribution|The purpose of this paper is the estimation of the Foster, Greer and Thorbecke family of poverty indexes using the Gamma distribution as a continuous representation of the distribution of incomes. The expressions of this family of poverty indexes associated with the Gamma probability model and their asymptotic distributions are derived in the text, both for an exogenous and a relative (to the mean) poverty line. Finally, a Monte Carlo experiment is performed to compare three different methods of estimation for grouped data.
C46|Fuzzy models in regional statistics|Many regional data are not provided as precise numbers, but they are frequently non-precise (fuzzy). In order to provide realistic statistical information, the imprecision must be described quantitatively. This is possible using special fuzzy subsets of the set of real numbers ℝ, called fuzzy numbers, together with their characterising functions. In this study, the uncertainty of measured data is highlighted through an example of environmental data from a regional study. The generalised statistical methods, through the characterising function and the δ-cut, that are suitable for the situations of fuzzy uni- and multivariate data are described. In addition, useful generalised descriptive statistics and predictive models frequently applicable for analysis of fuzzy data in regional studies as well as the concept of fuzzy data in databases are presented.
C46|The theory of dual co~event means|This work is the third, but not the last, in the cycle begun by the works [23, 22] about the new theory of experience and chance as the theory of co~events. Here I introduce the concepts of two co~event means, which serve as dual co~event characteristics of some co~event. The very idea of dual co~event means has become the development of two concepts mean-measure set [16] and mean-probable event [20, 24], which were first introduced as two independent characteristics of the set of events, so that then, within the framework of the theory of experience and of chance, the idea can finally get the opportunity to appear as two dual faces of the same co~event. I must admit that, precisely, this idea, hopelessly long and lonely stood at the sources of an indecently long string of guesses and insights, did not tire of looming, beckoning to the new co~event description of the dual nature of uncertainty, which I called the theory of experience and chance or the certainty theory. The constructive final push to the idea of dual co~event means has become two surprisingly suitable examples, with which I was fortunate to get acquainted in 2015, each of which is based on the statistics of the experienced-random experiment in the form of a co~event.
C46|Triangle room paradox of negative probabilities of events|Here an improved generalization of Feynman’s paradox of negative probabilities [1, 2] for observing three events is considered. This version of the paradox is directly related to the theory of quantum computing. Imagine a triangular room with three windows, where there are three chairs, on each of which a person can seat [4]. In any of the windows, an observer can see only the corresponding pair of chairs. It is known that if the observer looks at a window (to make a pairwise observation), the picture will be in the probabilistic sense the same for all windows: only one chair from the observed pair is occupied with a probability of 1/2, and there are never busy or free both chairs at once. Paradoxically, existing theories based on Kolmogorov’s probability theory do not answer the question that naturally arises after such pairs of observations of three events: «What is really happening in a triangular room, how many people are there and with what is the probability distribution they are sitting on three chairs?».
C46|Blyth’s paradox «of three pies»: setwise vs. pairwise event preferences|The pairwise independence of events does not entail their setwise independence (Bernstein’s example, 1910-1917). The probability distributions of all pairs of events do not determine the probability distribution of the whole set of events (the triangular room paradox of negative probabilities of events [8, 9, 2001]). The pairwise preferences of events do not determine their setwise preferences (Blyth’s paradox, 1972). The eventological theory of setwise event preferences, proposed in [8, 2007], gives an event justification and extension of the classical theory of preferences and explains Blyth’s paradox «of three pies»1 (that was already well-known to Yule2) by human ability to use triplewise and morewise preferences.
C46|Exchange Rates Forecasting: Can Jump Models Combined with Macroeconomic Fundamentals Help?|Connection between macroeconomic variables and foreign exchange (FX) rates evaluated in the context of out-of-sample forecasting is a well-known problem in economics. We propose a method that utilizes stochastic models based on jump processes (namely the normal inverse Gaussian and Meixner models), combines them with macroeconomic fundamentals, and using a moving (rolling or recursive) regularized estimation procedure produces forecasts of FX rates. These are compared to benchmark models, namely the direct forecast and the Gauss model fore-cast. Empirical out-of-sample experiments are performed on EUR/USD and USD/DKK currencies.
C46|New Distribution Theory for the Estimation of Structural Break Point in Mean|Based on the Girsanov theorem, this paper rst obtains the exact distribution of the maximum likelihood estimator of structural break point in a continuous time model. The exact distribution is asymmetric and tri-modal, indicating that the estimator is seriously biased. These two properties are also found in the nite sample distribution of the least squares estimator of structural break point in the discrete time model. The paper then builds a continuous time approximation to the discrete time model and develops an in- ll asymptotic theory for the least squares estimator. The obtained in- ll asymptotic distribution is asymmetric and tri-modal and delivers good approximations to the nite sample distribution. In order to reduce the bias in the estimation of both the continuous time model and the discrete time model, a simulation-based method based on the indirect estima- tion approach is proposed. Monte Carlo studies show that the indirect estimation method achieves substantial bias reductions. However, since the binding function has a slope less than one, the variance of the indirect estimator is larger than that of the original estimator.
C46|Testing for competitive balance|Abstract We present two tests for the hypothesis that a league is competitively balanced, identifying in each case the appropriate statistic as well as its sampling distribution. We then apply these tests to Major League Baseball and the National Football League. We demonstrate how these tests can be applied to full sports leagues, or to just intradivisional play, and to both single season and multiple season outcomes. We also show how one of our statistics is related to existing measures of competitive balance, and note the possibility that, because our statistics have known sampling distributions, knowledge may ultimately allow them to be transformed into measures that are comparable between leagues of different sizes and season lengths.
C46|A tour of regression models for explaining shares|This paper aims to present and compare statistical modeling methods adapted for shares as dependent variables. Shares are characterized by the following constraints: positivity and sum equal to 1. Four types of models satisfy this requirement: multinomial logit models widely used in discrete choice models of the econometric literature, market-share models from the marketing literature, Dirichlet covariate models and compositional regression models from the statistical literature. We highlight the properties, the similarities and the differences between these models which are coming from the assumptions made on the distribution of the data and from the estimation methods. We prove that all these models can be written in an attraction model form, and that they can be interpreted in terms of direct and cross elasticities. An application to the automobile market is presented where we model brand market-shares as a function of media investments in 6 channels in order to measure their impact, controlling for the brands average price and a scrapping incentive dummy variable. We propose a cross-validation method to choose the best model according to different quality measures.
C46|Optimal Mean Reversion Trading:Mathematical Analysis and Practical Applications|Optimal Mean Reversion Trading: Mathematical Analysis and Practical Applications provides a systematic study to the practical problem of optimal trading in the presence of mean-reverting price dynamics. It is self-contained and organized in its presentation, and provides rigorous mathematical analysis as well as computational methods for trading ETFs, options, futures on commodities or volatility indices, and credit risk derivatives. This book offers a unique financial engineering approach that combines novel analytical methodologies and applications to a wide array of real-world examples. It extracts the mathematical problems from various trading approaches and scenarios, but also addresses the practical aspects of trading problems, such as model estimation, risk premium, risk constraints, and transaction costs. The explanations in the book are detailed enough to capture the interest of the curious student or researcher, and complete enough to give the necessary background material for further exploration into the subject and related literature. This book will be a useful tool for anyone interested in financial engineering, particularly algorithmic trading and commodity trading, and would like to understand the mathematically optimal strategies in different market environments.
C46|Information transmission in high dimensional choice problems: The value of online ratings in the restaurant market|In choice problems with many alternatives and a priori uncertain outcomes, it has long been argued that individuals may use the observed choices of others as information to guide their own decisions. This paper analyses the role of these social interactions in the context of restaurant choice, while at the same time evaluating the value of a restaurants user rating in determining economic outcomes (as measured by restaurant checkins). The model follows a Polya urn logic, with choices being analyzed in a Dirichlet-multinomial framework. Data is provided by the online urban guide Yelp. Results show that correlation across choices of restaurant guests is present and can be modelled as a function of group-level variables related to information exchange. Ratings and other factors such as price categories do play a role, though traditional modelling approaches that ignore social interactions tend to overstate its importance both in terms of statistical and economic significance. The presented results from the restaurant market may well prove to be important in other choice contexts characterized by many alternatives and highly skewed outcome distributions.
C46|Pareto and Piketty: The Macroeconomics of Top Income and Wealth Inequality|Since the early 2000s, research by Thomas Piketty, Emmanuel Saez, and their coauthors has revolutionized our understanding of income and wealth inequality. In this paper, I highlight some of the key empirical facts from this research and comment on how they relate to macroeconomics and to economic theory more generally. One of the key links between data and theory is the Pareto distribution. The paper describes simple mechanisms that give rise to Pareto distributions for income and wealth and considers the economic forces that influence top inequality over time and across countries. For example, it is in this context that the role of the famous r - g expression is best understood.
C46|Uso práctico de la distribución TSP en el método de valoración de las dos betas| Este trabajo tiene dos partes claramente diferenciadas. La primera se dedica a realizar una somera revisión de algunos de los trabajos relacionados con el método de valoración de las dos betas, MDB, ideado por Ballestero al comienzo de los años setenta del siglo anterior. En la segunda se presenta la distribución TSP, introducida por van Dorp y Kotz al comienzo de este siglo, y su uso como modelo probabilístico en el MDB, tanto para la variable índice de calidad del bien a valorar como para la variable valor de mercado del mencionado bien. Finalmente, se ilustra el MDB, con un caso práctivo de la literatura especializada, Ballestero y Rodríguez (1999).
C46|Changes in the spatial pattern of net earnings: Evidence from Serbia|Spatial autocorrelation analysis is an important method that can reveal the structure and patterns of economic spatial variables. It can be used to identify not only global spatial patterns in the country, but also characteristic locations at micro levels. In this research, we used spatial autocorrelation methodologies, including Global Moran’s I and Local Getis—Ord Gi statistics to identify the intensity of the spatial clustering of municipalities in Serbia by the level of average monthly net earnings from 2001 to 2010. We identified and mapped local clusters (hot and cold spots) by the level of average monthly net earnings for the same period. The results show that overall spatial segregation between municipalities with high and low average monthly net earnings was predominantly increasing during the investigated period. Local statistics illustrated that overall spatial segregation followed a broad north—south divide, with a concentration of municipalities with high net earnings in the north of Serbia, and low net earnings in the south. Closer inspection showed that at the beginning of the study period, there were three statistically significant hot spots in the north. As time passed, only one highly clustered hot spot remained — the Belgrade region. One cold spot retained a relatively stable position in the country’s southeast. This research shows that spatial changes of net earnings can be successfully studied with respect to statistically significant global and local spatial associations in the variables using spatial autocorrelation analysis.
C46|A Comparative Analysis of Gibrat’s and Zipf’s Law on Urban Population|The regional economics and geography literature on urban population size has in recent years shown interesting conceptual and methodological contributions on the validity of Gibrat’s Law and Zipf’s Law. Despite distinct modeling features, they express similar fundamental characteristics in an equilibrium situation. Zipf’s law is formalized in a static form, while its associated dynamic process is articulated by Gibrat’s Law. Thus, it is likely that both Zipf’s Law and Gibrat’s Law share a common root. Unfortunately, empirical investigations on the direct relationship between Gibrat’s Law and Zipf’s Law are rather rare and not conclusive. The present paper aims to answer the question whether (a generalisation of) Gibrat’s Law allows us to infer Zipf’s Law, and vice versa? In our conceptual and applied framework, particular attention will be paid to the role of the mean and the variance of city population as key indicators for assessing the (non-) validity of the generalised Gibrat’s Law. Our empirical experiments are based on a comparative analysis between the dynamics of the urban population of four countries with entirely mutually contrasting spatial-economic and geographic characteristics: Botswana, Germany, Hungary and Luxembourg. We arrive at the following results: if (i) the mean is independent of city size (first necessary condition of Gibrat’s law) and (ii) the coefficient of the rank-size rule/Zipf’s Law is different from one, then the variance is dependent on city size.
C46|Multiple Outlier Detection in Samples with Exponential & Pareto Tails: Redeeming the Inward Approach & Detecting Dragon Kings|"We consider the detection of multiple outliers in Exponential and Pareto samples -- as well as general samples that have approximately Exponential or Pareto tails, thanks to Extreme Value Theory. It is shown that a simple ""robust'' modification of common test statistics makes inward sequential testing -- formerly relegated within the literature since the introduction of outward testing -- as powerful as, and potentially less error prone than, outward tests. Moreover, inward testing does not require the complicated type 1 error control of outward tests. A variety of test statistics, employed in both block and sequential tests, are compared for their power and errors, in cases including no outliers, dispersed outliers (the classical slippage alternative), and clustered outliers (a case seldom considered). We advocate a density mixture approach for detecting clustered outliers. Tests are found to be highly sensitive to the correct specification of the main distribution (Exponential/Pareto), exposing high potential for errors in inference. Further, in five case studies -- financial crashes, nuclear power generation accidents, stock market returns, epidemic fatalities, and cities within countries -- significant outliers are detected and related to the concept of ‘Dragon King’ events, defined as meaningful outliers of unique origin."
C46|Secular Bipolar Growth Rate of the Real US GDP Per Capita: Implications for Understanding Past and Future Economic Growth|We present a quantitative characterisation of the fluctuations of the annualized growth rate of the real US GDP per capita growth at many scales, using a wavelet transform analysis of two data sets, quarterly data from 1947 to 2015 and annual data from 1800 to 2010. Our main finding is that the distribution of GDP growth rates can be well approximated by a bimodal function associated to a series of switches between regimes of strong growth rate Phigh and regimes of low growth rate Plow. The succession of such two regimes compounds to produce a remarkably stable long term average real annualized growth rate of 1.6% from 1800 to 2010 and approximately 2.0% since 1950, which is the result of a subtle compensation between the high and low growth regimes that alternate continuously. Thus, the overall growth dynamics of the US economy is punctuated, with phases of strong growth that are intrinsically unsustainable, followed by corrections or consolidation until the next boom starts. We interpret these findings within the theory of “social bubbles” and argue as a consequence that estimations of the cost of the 2008 crisis may be misleading. We also interpret the absence of strong recovery since 2008 as a protracted low growth regime Plow associated with the exceptional nature of the preceding large growth regime.
C46|Is a normal copula the right copula?|We derive computationally simple and intuitive expressions for score tests of Gaussian copulas against Generalised Hyperbolic alternatives, including symmetric and asymmetric Student t, and Hermite polynomial expansions. We decompose our tests into third and fourth moment components, and obtain one-sided Likelihood Ratio analogues, whose asymptotic distribution we provide. We conduct Monte Carlo exercises to assess the finite sample properties of asymptotic and bootstrap versions of our tests. In an empirical application to CRSP stocks, we find that short-term reversals and momentum effects are better captured by non-Gaussian copulas. We estimate their parameters by indirect inference, and devise successful trading strategies.
C46|Behavioral differences in violence: The case of intra-group differences of Paramilitaries and Guerrillas in Colombia|In most studies on civil wars, determinants of conflict have been hitherto explored assuming that actors involved were either unitary or stable. However, if this intra-group homogeneity assumption does not hold, empirical econometric estimates may be biased. We use Fixed Effects Finite Mixture Model (FE-FMM) approach to address this issue that provides a representation of heterogeneity when data originate from different latent classes and the affiliation is unknown. It allows to identify sub-populations within a population as well as the determinants of their behaviors. By combining various data sources for the period 2000-2005, we apply this methodology to the Colombian conflict. Our results highlight a behavioral heterogeneity in guerrilla’s armed groups and their distinct economic correlates. By contrast paramilitaries behave as a rather homogenous group.
C46|Discrete Spectral Analysis. The Case of Industrial Production in Selected European Countries|The aim of this paper is to show the usefulness the discrete spectral analysis in identification cyclical fluctuations. The subsampling procedure was applied to construct the asymptotically consistent test for Fourier coefficient and frequency significance. The case of monthly production in industry in European countries (thirty countries) was considered. Using pro-posed approach the frequencies concerning business fluctuations, seasonal fluctuations and trading-day effects fluctuations were recognized in considered data sets. The comparison with existing procedures was shown.
C46|Is a Normal Copula the Right Copula?|We derive computationally simple and intuitive expressions for score tests of Gaussian copulas against Generalised Hyperbolic alternatives, including symmetric and asymmetric Student t, and Hermite polynomial expansions. We decompose our tests into third and fourth moment components, and obtain one-sided Likelihood Ratio analogues, whose asymptotic distribution we provide. We conduct Monte Carlo exercises to assess the finite sample properties of asymptotic and bootstrap versions of our tests. In an empirical application to CRSP stocks, we find that short-term reversals and momentum effects are better captured by non-Gaussian copulas. We estimate their parameters by indirect inference, and devise successful trading strategies.
C46|Behavioral differences in violence: The case of intra-group differences of Paramilitaries and Guerrillas in Colombia|In most studies on civil wars, determinants of conflict have been hitherto explored assuming that actors involved were either unitary or stable. However, if this intra-group homogeneity assumption does not hold, empirical econometric estimates may be biased. We use Fixed Effects Finite Mixture Model (FE-FMM) approach to address this issue that provides a representation of heterogeneity when data originate from different latent classes and the affiliation is unknown. It allows to identify sub-populations within a population as well as the determinants of their behaviors. By combining various data sources for the period 2000-2005, we apply this methodology to the Colombian conflict. Our results highlight a behavioral heterogeneity in guerrilla’s armed groups and their distinct economic correlates. By contrast paramilitaries behave as a rather homogenous group.
C46|Non-Negativity, Zero Lower Bound and Affine Interest Rate Models|This thesis presents new developments in the literature of non-negative affine interest rate models. The first chapter is devoted to the introduction of the main mathematical tools used in the following chapters. In particular, it presents the so-called affine processes which are extensively employed in no-arbitrage interest rate models. Chapter 2 provides a new filtering and estimation method for linear-quadratic state-space models. This technique is exploited in the 3rd chapter to estimate a positive asset pricing model on the term structure of Euro area interbank spreads. This allows us to decompose the interbank risk into a default risk and a liquidity risk components. Chapter 4 proposes a new recursive method for building general multivariate affine processes from their univariate counterparts. In particular, our method does not impose the conditional independence between the different vector elements. We apply this technique in Chapter 5 to produce multivariate non-negative affine processes where some components can stay at zero for several periods. This process is exploited to build a term structure model consistent with the zero lower bound features.
C46|The Top Tail of the Wealth Distribution in Germany, France, Spain, and Greece|In this study we analyze the top tail of the wealth distribution in Germany and France based on the Household Finance and Consumption Survey (HFCS). Since top wealth is likely to be underrepresented in household surveys we integrate the big fortunes from rich lists, estimate a Pareto distribution, and impute the missing rich. As a result, the top percentile share of household wealth in Germany jumps up from 24 percent based on the HFCS alone to 34 percent after top wealth imputation. For France there is only a small effect of the imputation since rich households are better captured in the survey.
C46|Estimating occupational mobility with covariates|The Altham statistic is often used to calculate intergenerational associations in occupations in studies of historical social mobility. This paper presents a method to incorporate individual covariates into such estimates of social mobility, and to construct corresponding confidence intervals. The method is applied to an intergenerational sample of Norwegian data, showing that estimates of intergenerational mobility are robust to the inclusion of controls for father’s and son’s age.
C46|Testing linearity using power transforms of regressors|We develop a method of testing linearity using power transforms of regressors, allowing for stationary processes and time trends. The linear model is a simplifying hypothesis that derives from the power transform model in three different ways, each producing its own identification problem. We call this modeling difficulty the trifold identification problem and show that it may be overcome using a test based on the quasi-likelihood ratio (QLR) statistic. More specifically, the QLR statistic may be approximated under each identification problem and the separate null approximations may be combined to produce a composite approximation that embodies the linear model hypothesis. The limit theory for the QLR test statistic depends on a Gaussian stochastic process. In the important special case of a linear time trend regressor and martingale difference errors asymptotic critical values of the test are provided. Test power is analyzed and an empirical application to crop-yield distributions is provided. The paper also considers generalizations of the Box–Cox transformation, which are associated with the QLR test statistic.
C46|A Quadratic Kalman Filter|We propose a new filtering and smoothing technique for non-linear state-space models. Observed variables are quadratic functions of latent factors following a Gaussian VAR. Stacking the vector of factors with its vectorized outer-product, we form an augmented state vector whose first two conditional moments are known in closed-form. We also provide analytical formulae for the unconditional moments of this augmented vector. Our new Quadratic Kalman Filter (Qkf) exploits these properties to formulate fast and simple filtering and smoothing algorithms. A simulation study first emphasizes that the Qkf outperforms the extended and unscented approaches in the filtering exercise showing up to 70% RMSEs improvement of filtered values. Second, it provides evidence that Qkf-based maximum-likelihood estimates of model parameters always possess lower bias or lower RMSEs than the alternative estimators.
C46|Predictability of price movements in deregulated electricity markets|In this paper we investigate predictability of electricity prices in the Canadian provinces of Alberta and Ontario, as well as in the US Mid-C market. Using scale-dependent detrended fluctuation analysis, spectral analysis, and the probability distribution analysis we show that the studied markets exhibit strongly anti-persistent properties suggesting that their dynamics can be predicted based on historic price records across the range of time scales from 1h to one month. For both Canadian markets, the price movements reveal three types of correlated behavior which can be used for forecasting. The discovered scenarios remain the same on different time scales up to one month as well as for on- and off-peak electricity data. These scenarios represent sharp increases of prices and are not present in the Mid-C market due to its lower volatility. We argue that extreme price movements in this market should follow the same tendency as the more volatile Canadian markets. The estimated values of the Pareto indices suggest that the prediction of these events can be statistically stable. The results obtained provide new relevant information for managing financial risks associated with the dynamics of electricity derivatives over time frame exceeding one day.
C46|Liquidity and resolution of uncertainty in the European carbon futures market|We investigate whether liquidity introduces or helps resolve uncertainty in Phase I and the first year of Phase II of the European carbon futures market. We propose a distinction between ‘absolute’ or overall liquidity and that which is ‘relative’ to a benchmark. For this purpose, we suggest volume-weighted duration as a natural measure of trading intensity as a proxy for liquidity, and we model it as a rescaled temporal point process. The new model is called Autoregressive Conditional Weighted Duration (ACWD) and is shown to outperform its discrete modelling counterparts. Liquidity is found to play a dual role, with higher relative liquidity introducing uncertainty and higher absolute liquidity accelerating uncertainty resolution, thus, enhancing market efficiency.
C46|The instability of the Pearson correlation coefficient in the presence of coincidental outliers|It is well known that any statistic based on sample averages can be sensitive to outliers. Some examples are the conventional moments-based statistics such as the sample mean, the sample variance, or the sample covariance of a set of observations on two variables. Given that sample correlation is defined as sample covariance divided by the product of sample standard deviations, one might suspect that the impact of outliers on the correlation coefficient may be neither present nor noticeable because of a ‘dampening effect’ i.e., the effects of outliers on both the numerator and the denominator of the correlation coefficient can cancel each other. In this paper, we formally investigate this issue. Contrary to such an expectation, we show analytically and by simulations that the distortion caused by outliers in the behavior of the correlation coefficient can be fairly large in some cases, especially when outliers are present in both variables at the same time. These outliers are called ‘coincidental outliers.’ We consider some robust alternative measures and compare their performance in the presence of such coincidental outliers.
C46|Learning About Consumer Uncertainty from Qualitative Surveys: As Uncertain As Ever|We study diffusion indices constructed from qualitative surveys to provide real-time assessments of various aspects of economic activity. In particular, we highlight the role of diffusion indices as estimates of change in a quasi extensive margin, and characterize their distribution, focusing on the uncertainty implied by both sampling and the polarization of participants' responses. Because qualitative tendency surveys generally cover multiple questions around a topic, a key aspect of this uncertainty concerns the coincidence of responses, or the degree to which polarization comoves, across individual questions. We illustrate these results using micro data on individual responses underlying different composite indices published by the Michigan Survey of Consumers. We find a secular rise in consumer uncertainty starting around 2000, following a decade-long decline, and higher agreement among respondents in prior periods. Six years after the Great Recession, uncertainty arising from the polarization of responses in the Michigan Survey stands today at its highest level since 1978, coinciding with the weakest recovery in U.S. post-war history. The formulas we derive allow for simple computations of approximate confidence intervals, thus affording a more complete real-time assessment of economic conditions using qualitative surveys.
C46|Correcting for the Missing Rich: An Application to Wealth Survey Data|It is a well-known criticism that if the distribution of wealth is highly concentrated, survey data are hardly reliable when it comes to analyzing the richest parts of society. This paper addresses this criticism by providing a general rationale of the underlying methodological problem as well as by proposing a specific methodological approach tailored to correcting the arising bias. We illustrate the latter approach by using Austrian data from the Household Finance and Consumption Survey. Specifically, we identify suitable parameter combinations by using a series of maximum-likelihood estimates and appropriate goodness-of-fit tests to avoid arbitrariness with respect to the fitting of the Pareto distribution. Our results suggest that the alleged non-observation bias is considerable, accounting for about one quarter of total net wealth in the case of Austria. The method developed in this paper can easily be applied to other countries where survey data on wealth are available.<br><small>(This abstract was borrowed from another version of this item.)</small>
C46|Conditional Systemic Risk with Penalized Copula|Financial contagion and systemic risk measures are commonly derived from conditional quantiles by using imposed model assumptions such as a linear parametrization. In this paper, we provide model free measures for contagion and systemic risk which are independent of the speci - cation of conditional quantiles and simple to interpret. The proposed systemic risk measure relies on the contagion measure, whose tail behavior is theoretically studied. To emphasize contagion from extreme events, conditional quantiles are speci ed via hierarchical Archimedean copula. The parameters and structure of this copula are simultaneously estimated by imposing a non-concave penalty on the structure. Asymptotic properties of this sparse estimator are derived and small sample properties illustrated using simulations. We apply the proposed framework to investigate the interconnectedness between American, European and Australasian stock market indices, providing new and interesting insights into the relationship between systemic risk and contagion. In particular, our ndings suggest that the systemic risk contribution from contagion in tail areas is typically lower during times of nancial turmoil, while it can be signi cantly higher during periods of low volatility.
C46|Effects of Higher Education on the Unconditional Distribution of Financial Literacy|In this paper, the effect of the higher-education ratio on the distribution of ﬁnancial literacy in Taiwan is investigated with the unconditional quantile estimation suggested by Firpo et al. (2009). Our empirical data are obtained from three surveys conducted in 2007, 2009, and 2011 by the Financial Supervisory Commision, R.O.C. Using the method of factor analysis suggested by van Rooij et al. (2011), the ﬁnancial literacy is measured from 18 questionnaires regarding the knowledge of management of cash, savings, credit, and loans. In total, 3,155 individuals were surveyed, namely, 1,005 in 2007, 919 in 2009, and 1,231 in 2011. Our empirical results conclude that an increase in higher education not only increases the acquisition of ﬁnancial literacy (which is more signiﬁcant at quantiles smaller than 0.52), but also reduces the dispersion of the ﬁnancial literacy distribution. This conclusion provides evidence in support of the policy of higher education expansion. Given the positive eﬀect of ﬁnancial literacy on capital income and retirement plans, the level of financial literacy will be increased and its dispersion will be decreased with the expansion of higher education, so that income inequality as a result will be reduced.
C46|The success of art galleries: a dynamic model with competition and information effects|An intrinsic characteristic of cultural goods is the unpredictability of their economic success. Arts goods in particular share characteristics with credence, inspection, and experience goods. Accordingly, art collectors rely on the experience and the reputation of art galleries when investing in artwork. Some qualitative sociological studies have found that only a few very successful galleries represent the bulk of the most visible and most successful artists (e.g., Crane in The transformation of the avant-garde: the New York art world, 1940–1985. The University of Chicago Press, Chicago, 1989 ; Currid in The Warhol economy: How fashion, art and music drive New York City. Princeton University Press, Princeton, 2007 ). This paper investigates the success of art galleries in a dynamic model, which elaborates different statistical processes that allow us to analyze the development of different types of success distributions in the market for art galleries. Instead of applying standard economic analysis only, we employ methods from statistical physics to construct a model of gallery investment and competition. Our model entails information, competition, and innovation effects. Subsequently, art market data are used to test which version of the model fits best. We find that the lognormal distribution provides the best fit and conclude that the data generating process is compatible with the version of the model, which entails an inhomogeneous geometric Brownian motion. Hence, the success of art galleries depends strongly on information and innovation effects, but is hardly affected by competition effects. We argue that the superstar effect in the case of art galleries can be understood as an appropriation of search and entrance costs, which emerge whenever consumption requires special knowledge and social inclusion. Copyright Springer Science+Business Media New York 2015
C46|Are all firms inefficient?|In the usual stochastic frontier model, all firms are inefficient, because inefficiency is non-negative and the probability that inefficiency is exactly zero equals zero. We modify this model by adding a parameter p which equals the probability that a firm is fully efficient. We can estimate this model by MLE and obtain estimates of the fraction of firms that are fully efficient and of the distribution of inefficiency for the inefficient firms. This model has also been considered by Kumbhakar et al. (J Econ 172:66–76, 2013 ). We extend their paper in several ways. We discuss some identification issues that arise if all firms are inefficient or no firms are inefficient. We show that results like those of Waldman (J Econ 18:275–279, 1982 ) hold for this model, that is, that the likelihood has a stationary point at parameters that indicate no inefficiency and that this point is a local maximum if the OLS residuals are positively skewed. Finally, we consider problems involved in testing the hypothesis that p = 0. We also provide some simulations and an empirical example. Copyright Springer Science+Business Media New York 2015
C46|Too many skew normal distributions? The practitioner’s perspective|The paper tackles the issue of possible misspecification in fitting skew normal distributions to empirical data. It is shown, through numerical experiments, that it is easy to choose a distribution which is different from this which actually generated the sample, if the minimum distance criterion is used. It is suggested that, in case of similar values of distance measures obtained for different distributions, the choice should be made on the grounds of parameters’ interpretation rather than the goodness of fit. This is supported by empirical evidence of fitting different skew normal distributions to the estimated monthly inflation uncertainties for Belarus, Poland, Russia and Ukraine.
C46|Migration Processes in European Cities: A Spatio-Temporal Analysis Using Different Spatial Weights Matrices/Procesos migratorios entre ciudades europeas: Un análisis espacio-temporal usando diferentes matrices de pesos|The aim of the article is to verify a hypothesis on the occurrence of spatial interactions in foreign migrations in selected European cities by applying different spatial weights matrices that define the multidimensional spatial dependences. The analysis used GIS, ESDA, geostatistical, spatial statistics and econometric tools to recognise and examine these interactions. The main part of the study was a specification of six spatial weights matrices that described relations occurring between cities in different ways: two adjacency and directional adjacency matrices, two geographical distance matrices and economic distance matrix. The study covered 271 European cities in the years 2005-2012. The analysed variable was net migration per 1000 people. El objetivo de este trabajo es contrastar el supuesto de interacción espacial entre los inmigrantes no nacionales, en una muestra representativa de ciudades europeas, utilizando un conjunto de matrices de contactos diseñadas para capturar la estructura multidimensional de las dependencias. En el análisis se utilizan diferentes instrumentos de geoestadística espacial, con base GIS, junto a indicadores ESDA y herramientas de econometría espacial. En la primera parte del trabajo se especifican hasta seis matrices de ponderaciones espaciales para conectar las ciudades de la muestra; dos se basan en el criterio de adyacencia, en otras dos se utilizan medidas de adyacencia direccional y las dos últimas están basadas en sendas medidas de distancia económica. El estudio incluye 271 ciudades europeas en los años 2005-2012. La variable analizada es la migración neta por 1.000 personas.
C46|Shapley Allocation, Diversification and Services in Operational Risk|A method of allocating Operational Risk regulatory capital using the Shapley method for a large number of business units, supported by a service, is proposed. A closed-form formula for Shapley allocations is developed under two principal assumptions. First, if business units form coalitions, the value added to the coalition by a new entrant depends on a constant proportionality factor. This factor represents the diversification that can be achieved by combining operational risk losses. Second, that the service should reduce the capital payable by business units, and that this reduction is calculated as an integral part of the allocation process. We ensure that allocations of capital charges are acceptable to and are understandable by both risk and senior managers. The results derived are applied to recent loss data
C46|Statistical Study on the Need for a Preliminary Assessment of the Effectiveness of the Implementation Process of ERP-Systems in Bulgarian SMEs|This paper presents a statistical analysis of data obtained from an empirical study of a sample of Bulgarian SMEs to establish the need for a preliminary assessment of the effectiveness of the implementation process of ERP-systems in such enterprises. The analysis is carried out based on one-dimensional and two-dimensional empirical distributions and verification of statistical hypotheses formulated. The conclusions highlight the need to develop an original methodology and its implementation in software for evaluation of the effectiveness of implementation of ERP systems in SMEs
C46|Beyond location and dispersion models: The Generalized Structural Time Series Model with Applications|In many settings of empirical interest, time variation in the distribution parameters is important for capturing the dynamic behaviour of time series processes. Although the fitting of heavy tail distributions has become easier due to computational advances, the joint and explicit modelling of time-varying conditional skewness and kurtosis is a challenging task. We propose a class of parameter-driven time series models referred to as the generalized structural time series (GEST) model. The GEST model extends Gaussian structural time series models by a) allowing the distribution of the dependent variable to come from any parametric distribution, including highly skewed and kurtotic distributions (and mixed distributions) and b) expanding the systematic part of parameter-driven time series models to allow the joint and explicit modelling of all the distribution parameters as structural terms and (smoothed) functions of independent variables. The paper makes an applied contribution in the development of a fast local estimation algorithm for the evaluation of a penalised likelihood function to update the distribution parameters over time \textit{without} the need for evaluation of a high-dimensional integral based on simulation methods.
C46|Flexible statistical models: Methods for the ordering and comparison of theoretical distributions|Statistical models usually rely on the assumption that the shape of the distribution is fixed and that it is only the mean and volatility that varies. Although the fitting of heavy tail distributions has become easier due to computational advances, the fitting of the appropriate heavy tail distribution requires knowledge of the properties of the different theoretical distributions. The selection of the appropriate theoretical distribution is not trivial. Therefore, this paper provides methods for the ordering and comparison of continuous distributions by making a threefold contribution. Firstly, it provides an ordering of the heaviness of distribution tails of continuous distributions. The resulting classification of over 30 important distributions is given. Secondly it provides guidance on choosing the appropriate tail for a given variable. As an example, we use the USA box-office revenues, an industry characterised by extreme events affecting the supply schedule of the films, to illustrate how the theoretical distribution could be selected. Finally, since moment based measures may not exist or may be unreliable, the paper uses centile based measures of skewness and kurtosis to compare distributions. The paper therefore makes a substantial methodological contribution towards the development of conditional densities for statistical model in the presence of heavy tails.
C46|US city size distribution revisited: Theory and empirical evidence|We develop a urban economic model in which agents locate in cities of different size so as to maximize the net output of the whole system of cities in a country. From this model two new city size distributions are exactly derived. We call these functions “threshold double Pareto Generalized Beta of the second kind” and “double mixture Pareto Generalized Beta of the second kind”. In order to test empirically the theory, we analyze the US urban system and consider three types of data (incorporated places from 1900 to 2000, all places in 2000 and 2010 and City Cluster Algorithm nuclei in 1991 and 2000). The results are encouraging because the new distributions clearly outperform the lognormal and the double Pareto lognormal for all data samples. We consider a number of different tests and statistical criteria and the results are robust to all of them. Thus, the new distributions describe accurately the US city size distribution and, therefore, support the validity of the theoretical model.
C46|Are the log-growth rates of city sizes normally distributed? Empirical evidence for the US|We study the decennial log-growth population rate distributions of the US incorporated places (resp., all places) for the period 1990-2000 (resp. 2000-2010) and the recently constructed US City Clustering Algorithm (CCA) population data in the period 1991-2000. It is obtained an excellent parametric description of these log-growth rates by means of a newly introduced distribution called “double mixture exponential Generalized Beta 2”. The normal distribution is not the one empirically observed for the same datasets.
C46|Log-growth distributions of US city sizes and non-Lévy processes|We study whether the hypothesis that the log-population of US cities follows a Lévy process can be rejected or not. The result seems to be rejection. As a consequence, the cited process seems not to be described by a standard Brownian motion with drift (with a Yule process), thus explaining in another way the rejection of the lognormal and double Pareto lognormal distributions for US city size in recent studies. The datasets employed are that of US incorporated places on the period 1890-2000. However, we recall a way of obtaining a family of stochastic Itô differential equations whose sample paths are associated to the time-dependent probability density functions for city size that in principle could be observed empirically
C46|An improvement over the normal distribution for log-growth rates of city sizes: Empirical evidence for France, Germany, Italy and Spain|We study the decennial log-growth population rate distributions of France (1990-2009), Germany (1996-2006), Italy (1951-1961, 2001-2011) and Spain (1950-1960, 2001-2010). It is obtained an excellent parametric description of these log-growth rates by means of a modification of the normal distribution in that the tails are mixed by means of convex linear combinations with exponential distributions, giving rise to the so called “double mixture exponential normal”. The normal distribution is not the one empirically observed for the same datasets.
C46|Quantum strategy creation by interlocking interconnecting directors in boards of directors in modern organizations at time of globalization|The research explores a scientific problem on the quantum strategy creation and implementation by the interlocking interconnecting directors in the boards of directors in the modern organizations at the time of globalization, solving the winning virtuous strategy search, the most effective strategy selection and the organizational strategy optimization paradigms. We know that, having the different mindset architectures, the directors in the interlocking interconnecting directors networks in the boards of directors in the modern organizations complete the information sensing, filtering, processing, resonant absorption, analysis, strategy decision making processes with the aim to create and implement the most effective optimal winning virtuous organization development strategy, applying the early researched deductive, inductive and abductive logics. We propose that the new quantum strategy can be considered as a most effective optimal winning virtuous organizational strategy, allowing the board of directors to set a right direction vector toward the business development, to establish a necessary actions plan and to reach the sustainable business profitability goals in the economies of the scales and scopes. We explain that the quantum strategy can be formulated, going from: 1) the new quantum logic principles in the Copenhagen interpretation in the quantum mechanics science, 2) the deductive, inductive and abductive logics existing knowledge in the philosophy science, and 3) the modern strategy research findings in the business administration science. We think that the directors in the boards of directors in the complex organizations can implement the quantum strategies with the ultimate goal to build the prosperous organizations in the economies of the scale and scope at the time of the disruptive changes and opportunities by the globalization.
C46|Economic stratification - The remedy and demise of humanity|The paper shows that the world economic stratification, as well as that of all natural, social and political systems is a natural law and a prerequisite for progress. Stratification itself may mean, however, both the remedy, but also the demise of systems, by exacerbating social inequalities. The database includes the evolution of gross domestic product per capita for different time frames at European level and worldwide. The main methods employed in processing database are the indices method and expectation values of position (quartile values) used to assess the structure of Europe and world countries according to the size of the gross domestic product. In Europe, for a century, stratification has increased visibly. If in 1913, the richest country in Europe achieved a GDP per capita of 3.94 times higher compared to the poorest country, in 2013 the ratio is 13.82 to one. The status of key statistical indicators that characterize the polarization of the world by size of gross domestic product, demonstrates that stratification is less pronounced inside continents, becoming however severe, worldwide. In this regard, it is alarming that in 1994, 75% of world countries were making only 7% of the GDP per capita in the richest country (Monaco). Given that information has now become increasingly more fluid, one can include among beneficiaries, the least developed countries. Circulation of information is, however, under the command of polarizing forces, belonging to the same great powers of the world. In this way, by means of more refined methods, the benefits of progress preserve world hierarchies.
C46|Dezvoltarea economica endogena la nivel regional. Cazul Romaniei<BR>[Endogenous economic development at regional level. The case of Romania]|Under the current global economic circumstances, characterized by growing uncertainties and the endurance of financial crisis effects, the approach of endogenous factors that contribute to the economic growth and welfare improvement is of major interest for academics and decision makers. The endogenous forces of development have a significant impact at both national and regional levels under the impact of recovery measures and actions related to different areas of economic policy. This book focuses on the complex dialectics between endogenous and exogenous components of local economic development, highlighting the main factors behind endogenous processes and emphasizing the particular importance of activating the regional endogenous potential in the case of Romania. The authors’ work is based on the interpretation of the main theories on endogenous development at national and regional level, completed by a multifaceted analysis based on statistical methods and econometric techniques for assessing competitive and comparative advantages to support economic growth and social development. The study has highlighted the need of a policy mix that would support the development of areas with a relatively lower development level, under the circumstances of a polycentric regional development. An optimal combination between top-down and bottom-up interventions may prove to be the most successful, offering an effective compatibility of decentralization with the coordination and monitoring requirements, supporting the smart specialization at regional level, that could create spillover effects at national level. The authors have pointed out the strategic orientations of regional growth for 2014-2020 resulting from Romania’s Territorial Agenda and the Regional Operational Programme, revealing the most important challenges in relation to endogenous development factors and the perspectives of regional convergence at EU level.
C46|Vícerozměrný pravděpodobnostní model rozdělení příjmů českých domácností<BR>[Multivariate Probability Model For Incomes of the Czech Households]|The equivalised total net annual incomes of the Czech households (in CZK) in 2007-2010 are analysed in the text. The set of all households is very nonhomogeneous (with respect to incomes) and the aim of the analysis is to determine more homogeneous subsets (components) and to describe the distribution of incomes in these components. The components are supposed to be artificial, the membership of households in components is not known (or observable). A multivariate mixture of normal distributions (four dimensional component distributions) is used to describe a vector of logarithms of incomes, models with 2 to 9 components are fitted. Maximum likelihood estimates of unknown parameters were found with the use of EM algorithm. Akaike information criterion was used (accompanied by bootstraped test) and models with 3 or 4 components were selected to be acceptable for the description of distribution of incomes. Cluster analysis was performed in order to classify households into components and good performance of the model was found.
C46|Empirical Properties of the Credit and Equity Cycle within Almost Periodically Correlated Stochastic Processes - the Case of Poland, UK and USA|We discuss the notion of the financial cycle making a clear indication that the thorough study of its empirical properties in case of developing economies is still missing. We focus on the observed series of credit and equity and make formal statistical inference about the properties of the cycles in case of Polish economy. The non-standard subsampling procedure and discrete spectral characteristics of almost periodically correlated time series are applied to make formal statistical inference about the cycle. We compare the results with those obtained for UK and USA. We extract the cyclical component and confront empirical properties of the financial cycle for small open economy with those established so far in case of developed economies.
C46|Portafolio óptimo y productos estructurados en mercados a-estables: un enfoque de minimización de riesgo|This paper is aimed at studying the optimal portfolio problem when the assets have returns from a-stable distributions. The optimal portfolio contains a riskless asset and various risky assets, including structured notes. The basic statistics of the assets are calculated and both the a-stable distribution parameters and the covariation matrix are estimated through maximum likelihood. Finally, it is shown that by including structured notes in the a-stable optimal portfolio it is obtained higher returns, lower risk and better performance than Gaussian optimal portfolio.
C46|A Brief Analysis Of Seasonality in the Romanian Car Market|This article presents a brief seasonality analysis based on monthly statistics of some variables specific to the total sales of home-produced and imported cars focusing on the data series of the domestic market over the last three years, distinctly structured on three significant subpopulations (supply and delivery of car running on petrol, supply and delivery of diesel cars, and delivery of the leading car makes/brands). The statistical analysis of seasonality is focused on the faster assessment of the average structure, generating more interesting information regarding the data processed, which is specific to the complex type of thinking of statistics, and the aggregate and subsequently structured study, giving original indicators (absolute and relative gap of structural coefficients of seasonality), which the authors consider an interesting investigation solution providing a higher degree of overall simplicity and accessibility. The article also used the E-Views software package to reveal seasonality through special econometric graphs, which becomes a second proof of originality, comparable to the classical statistical charting, as in modern econometric modeling, immortalized in a mean dynamics (the moving average method).
C46|The Influence of Higher Moments and Non-Normality on the Sharpe Ratio: A South African Perspective|Although the general assumption is that daily and monthly returns data are normally distributed (Aparicio & Estrada, 2001), the correct statistical distribution of returns must first be established (Linden, 2001), as it constitutes one of the elementary building blocks that will ensure accurate financial analyses (Taylor, 1986). The assumption of normality is also critical when constructing reference intervals for variables (Royston, 1991). By evaluating the pre-, during and post- 2007-2009 financial crisis periods, this paper found that non-normality can be present in all data frequencies, especially in higher data frequencies. Further evidence also showed that the deviation from normality escalated over the crisis period and remained higher after the crisis, compared to the pre-crisis period. By comparing the traditional Sharpe ratio with adjusted versions, based on Gatfaouiâ€™s (2012) methodology, this paper accentuates the fact that the presence of non-normality and higher moments can influence the Sharpe ratioâ€™s performance rankings
C46|Zipf law and the firm size distribution: a critical discussion of popular estimators|The upper tail of the firm size distribution is often assumed to follow a Power Law. Several recent papers, using different estimators and different data sets, conclude that the Zipf Law, in particular, provides a good fit, implying that the fraction of firms with size above a given value is inversely proportional to the value itself. In this article we compare the asymptotic and small sample properties of different methods through which this conclusion has been reached. We find that the family of estimators most widely adopted, based on an OLS regression, is in fact unreliable and basically useless for appropriate inference. This finding raises doubts about previously identified Zipf behavior. Based on extensive numerical analysis, we recommend the adoption of the Hill estimator over any other method when individual observations are available. Copyright Springer-Verlag Berlin Heidelberg 2015
C46|Relative Risk Aversion and Power‐Law Distribution of Macroeconomic Disasters|The coefficient of relative risk aversion (CRRA) is notoriously difficult to estimate. Recently, Barro and Jin (On the size distribution of macroeconomic disasters, Econometrica 2011; 79(3): 434–455) have come up with a new estimation approach that fits a power-law model to the tail of distribution of macroeconomic disasters. We show that their results can be successfully replicated using a more refined power-law fitting methodology and a more comprehensive data set.<br><small>(This abstract was borrowed from another version of this item.)</small>
C46|"We provide mathematical proofs for the results in ""Testing Linearity Using Power Transforms of Regressors"""|"We provide mathematical proofs for the results in ""Testing Linearity Using Power Transforms of Regressors"" by Baek, Cho, and Phillips (2014)."
C46|The Top Tail of the Wealth Distribution in Germany, France, Spain, and Greece|We analyze the top tail of the wealth distribution in Germany, France, Spain, and Greece based on the Household Finance and Consumption Survey (HFCS). Since top wealth is likely to be underrepresented in household surveys we integrate the big fortunes from rich lists, estimate a Pareto distribution, and impute the missing rich. Instead of the Forbes list we mainly rely on national rich lists since they represent a broader base for the big fortunes. As a result, the top percentile share of household wealth in Germany jumps up from 24 percent in the HFCS alone to 33 percent after top wealth imputation. For France and Spain we find only a small effect of the imputation since rich households are better captured in the survey. The results for Greece are ambiguous since the data do not show clear concentration patterns.
C46|Testing heteroskedastic time series for normality|Normality testing is an evergreen topic in statistics and econometrics and other disciplines. The paper focuses on testing economic time series for normality in a robust way, taking specific data features such as serial dependence and time-varying volatility into account. Here, we suggest tests based on raw moments of probability integral transform of standardized time series. The use of raw moments is advantageous as they are quite sensitive to deviations from the null other than asymmetry and excess kurtosis. To standardize the series, nonparametric estimators of the (time-varying) variance may be used, but the mean as a function of time has to be estimated parametrically. Short-run dynamics is taken into account using the Heteroskedasticity and Autocorrelation Robust [HAR] approach of Kiefer and Vogelsang (2005, ET). The effect of estimation uncertainty arising from estimated standardization is accounted for by providing a necessary modification. In a simulation study, we compare the suggested tests to a benchmark test by Bai and Ng (2005, JBES). The results show that the new tests are performing well in terms of size (which is mainly due to the adopted fixed-b framework for long-run covariance estimation), but also in terms of power. An empirical application to G7 industrial production growth rates sheds further light on the empirical usefulness and limitations of the proposed test.
C46|Welfare and Trade without Pareto|Quantifications of gains from trade in heterogeneous firm models assume that productivity is Pareto distributed. Replacing this assumption with log-normal heterogeneity retains some useful Pareto features, while providing a substantially better fit to sales distributions-especially in the left tail. The cost of log-normal is that gains from trade depend on the method of calibrating the fixed cost and productivity distribution parameters. When set to match the size distribution of firm sales in a given market, the log-normal assumption delivers gains from trade in a symmetric two-country model that can be twice as large as under the Pareto assumption.
C46|Medical Tourism in Romania. The Case Study of Cardiovascular Rehabilitation in Covasna|Romania has one of the highest mortality rates in Europe for ischemic heart disease and, especially, for cerebrovascular disease. Taking into account the actual prevalence of cardiovascular diseases, an augmentation of the demand for specialized medical services is expected. As this paper argues, this situation can have an important impact on medical tourism. We analyze original data on the case study of a hospital, specialized in cardiovascular treatment, in the Romanian county of Covasna, which is offering specific balneal procedures, such as CO2 .hydrotherapy, alongside regular rehabilitation programs. The aim of our study is to evaluate the demographic characteristics, and the pathology of the hospitalized patients, as well as the specific rehabilitation procedures. Our findings suggest that the interest of patients, with cardiovascular diseases, for medical tourism can be influenced by accessibility, by some particularities of the location, but also by the holistic nature of the rehabilitation procedures.
C46|Analysis of Romania’s and Transylvania’s Tourist Supply Development and Performance|The paper’s premise is that a strong link exists between Romania’s poor tourism performance and the quality of its hospitality supply. In this respect, a comparative analysis of Romania’s and Transylvania’s tourist offering is carried out aiming at identifying the particularities of their development and the causes of their poor performance. The motivation for having chosen Transylvania resides in this region’s attractiveness among foreign tourists; it is a destination with a high potential favoring the development of some tourist products competitive on the international market. Thus, based on a comprehensive secondary data analysis regarding the public lodging facilities, the paper presents their geographic dispersion, the development of the accommodation capacity by levels of classification, the number of available beds, the types of units, the structures’ international affiliation, the main Romanian owners, etc. The dispersion by levels of classification, together with the net usage indexes of the accommodation capacity, characterize the performance of this industry. The research study emphasizes the fact that despite a high tourist potential, the present-day hospitality supply cannot support a healthy development of the tourist activity, and presents some possible causes.
C46|Hotel Chain’s Strategic Options to Penetrate the Romanian Market|The European hotel industry is dominated by small companies that own and operate a hotel. The rest of the market contains hotel chains. Most of the international hotel chains realized the potential of the market and that is why they intend to increase the number of hotels opened in Romania, but for the moment, only 3.5% belong to international hotel chains. The objectives of the study are identifying the level of attractiveness of Romania for international hotel chains, establishing the options of penetration chosen by them and the main influence factors upon the penetration on the market. The decision made by a hotel company to carry out its activity in a foreign country and the penetration strategy into the market can be motivated by many reasons. The study revealed that Romania ranks 24 out of 27 in terms of the opportunities of American international hotel chain that wants to enter the market. Analysing the 20 international hotel chains present on the Romanian market was found that the preferred penetration strategy is contract management. Research has indicated that the country of origin of the international hotel chain, chain size, the number of hotel stars belonging to a chain and the hotel establishment year has a statistically significant influence upon the strategy used to penetrate Romanian hotel market.
C46|An Inter-, Trans-, Cross- and Multidisciplinary Approach to Higher Education in the Field of Business Studies|Modern academic education in economics, and especially that in the field of business, frequently tries, and rather seldom manages to combine, interfere and unify, to varying degrees, distinct disciplines with practical interests usually similar or even seemingly adverse; its concrete action is an example of the successful attempt at prompt adaptation to the entrepreneurial reality, which is constantly changing, and also to the more and more dynamic economic environment, perhaps excessively so in the last two decades. This paper attempts to clarify the conceptual differences and the relatively common parts of inter-, trans-, cross- and multidisciplinarity in general, and in particular in education, focusing on business education. A brief introduction structures the inter-, trans-, cross- and multidisciplinary investigative approach within an educational context in business, and a review of the scientific literature section clarifies both the meaning of those major concepts, extending from simple paradigm of each concept to their multiple aggregative paradigm, and some modern educational trends in economics. A second section is devoted to the method of investigating opinions, based on the volume of a sample determined statistically and from a scientific methodology of observation and recording, by collecting individual data, validating the rigor of research on the diversity in business education approaches, by simply and consistently noting them by the students and graduates of faculties of economics. The study of the graduates’ opinions focused on an original questionnaire, presented in the a annexes of the paper, and the resulting databases were subjected to a descriptive statistical analysis, made with the software package Eviews, with emphasis on the homogeneity, symmetry, skewness and normality of distributions. The third section is a brief analysis of the research results, supplemented naturally by some clarifying discussions, which outline the current option of the graduates towards early multidisciplinarity as a necessary goal resulting from the content analysis of opinions. The conclusions briefly describe the expected trends of inter-, trans-, cross- and multidisciplinary education in economics, relating to business, approached as any job well done, from several scientific viewpoints.
C46|Calibrating the Italian Smile with Time-Varying Volatility and Heavy-Tailed Models|Abstract In this paper, we consider several time-varying volatility and/or heavy-tailed models to explain the dynamics of return time series and to fit the volatility smile for exchange-traded options where the underlying is the main Italian stock index. Given observed prices for the time period we investigate, we calibrate both continuous-time and discrete-time models. First, we estimate the models from a time-series perspective (i.e. under the historical probability measure) by investigating more than 10 years of daily index price log-returns. Then, we explore the risk-neutral measure by fitting the values of the implied volatility for numerous strikes and maturities during the highly volatile period from April 1, 2007 (prior to the subprime mortgage crisis in the US) to March 30, 2012. We assess the extent to which time-varying volatility and heavy-tailed distributions are needed to explain the behavior of the most important stock index of the Italian market.
C46|Are the log-returns of Italian open-end mutual funds normally distributed? A risk assessment perspective|In this paper we conduct an empirical analysis of daily log-returns of Italian open-end mutual funds and their respective benchmarks in the period from February 2007 to June 2013. First, we estimate the classical normal-based model on the log-returns of a large set of funds. Then we compare it with three models allowing for asymmetry and heavy tails. We empirically assess that both the value at risk and the average value at risk are model-dependent and we show that the difference between models should be taken into consideration in the evaluation of risk measures.
C46|Scale-free tails in Colombian financial indexes: a primer|A maximum likelihood method for estimating the power-law exponent verifies that the positive and negative tails of the Colombian stock market index (IGBC) and the Colombian peso exchange rate (TRM) approximate a scale-free distribution, whereas none of the heavy tails of a local sovereign securities index (IDXTES) are a plausible case for such distribution. Results also (i) support critiques regarding the flaws of ordinary least squares estimation methods for scale-free distributions; (ii) question the validity of Zipf’s law; (iii) suggest that IGBC and TRM display the scale-free nature documented as a stylized fact of financial returns, and that they may be following a gradually truncated Lévy flight; and (v) suggest that local financial markets are self-organized systems.
C46|Benford's Law, families of distributions and a test basis|Benford's Law is used to test for data irregularities. While novel, there are two weaknesses in the current methodology. First, test values used in practice are too conservative and the test values of this paper are more powerful and hold for fairly small samples. Second, testing requires Benford's Law to hold, which it often does not. I present a simple method to transform distributions to satisfy the Law with arbitrary precision and induce scale invariance, freeing tests from the choice of units. I additionally derive a rate of convergence to Benford's Law. Finally, the results are applied to common distributions.
C46|Optimizing Waste Costs in Production Management|In case of the investment process, the optimization is based on a mathematical model or a range of mathematical models which describe the process. The Choice of the most appropriate mathematical model is the guarantee to obtain useful information for investors. This paper presents a classic analytical method of decreasing the investment cost in the batch production systems that are specific to the engineering industry.
C46|A Cross Matrix for Modeling Open Innovation in Production Management|Innovation has become the industrial religion of the late 20th century. Business sees it as the key to increasing profits and market share. Governments automatically reach for it when trying to fix the economy. Around the world, the rhetoric of innovation has replaced the post-war language of welfare economics. Innovation: nothing new? Recent years have seen much focus on how innovation can lead to improvements in productivity assisting in economic development The article present the big difference between making culture in a particular field and practicing it. Innovation is the instrument of entrepreneurship. It invests resources with a new capacity to produce prosperity.
C46|Scale-free tails in Colombian financial indexes: A primer|A maximum likelihood method for estimating the power-law exponent verifies that the positive and negative tails of the Colombian stock market index (IGBC) and the Colombian peso exchange rate (TRM) approximate a scale-free distribution, whereas none of the heavy tails of a local sovereign securities index (IDXTES) are a plausible case for such distribution. Results also (i) support critiques regarding the flaws of ordinary least squares estimation methods for scale-free distributions; (ii) question the validity of Zipf’s law; (iii) suggest that IGBC and TRM display the scale-free nature documented as a stylized fact of financial returns, and that they may be following a gradually truncated Lévy flight; and (iv) suggest that local financial markets are self-organized systems.
C46|Income distribution in urban China: An overlooked data inconsistency issue|The Urban Household Income and Expenditure Surveys, conducted by the National Bureau of Statistics, are extensively explored in income distribution studies. However, we find that a survey coverage expansion that includes migrant residents in the urban sample may induce serious data inconsistency before and after the year 2002. To further unveil the inconsistency, we construct a random walk hierarchical beta-2 distribution model, estimated by the unscented Kalman filter, to investigate the magnitude of the structural break. Results show that the gaps of Gini coefficients and the shape of the distribution can be bridged by a counterfactual time series that coherently measures the urban China income distribution.
C46|Estimating GARCH-type models with symmetric stable innovations: Indirect inference versus maximum likelihood|Financial returns exhibit conditional heteroscedasticity, asymmetric responses of their volatility to negative and positive returns (leverage effects) and fat tails. The α-stable distribution is a natural candidate for capturing the tail-thickness of the conditional distribution of financial returns, while the GARCH-type models are very popular in depicting the conditional heteroscedasticity and leverage effects. However, practical implementation of α-stable distribution in finance applications has been limited by its estimation difficulties. The performance of the indirect inference approach using GARCH models with Student’s t distributed errors as auxiliary models is compared to the maximum likelihood approach for estimating GARCH-type models with symmetric α-stable innovations. It is shown that the expected efficiency gains of the maximum likelihood approach come at high computational costs compared to the indirect inference method.
C46|Modeling tails of aggregate economic processes in a stochastic growth model|An annual sequence of wages in England starting in 1245 is used. It is shown that a standard AK-type growth model with capital externality and stochastic productivity shocks is unable to explain important features of the data. Random returns to scale are then considered. Moderate episodes of increasing returns to scale and growth are shown to be compatible with convergence of wage’s process towards a unique stationary distribution. This holds true for other relevant values such as GDP and/or capital stock. Furthermore, random returns to scale generate heteroskedasticity, a feature common to macroeconomic time series. Finally, the limit distribution of real wages displays fat tails if returns to scale are episodically increasing. Several inference results supporting randomness of returns to scale are provided.
C46|A comparison of city size distributions for China and India from 1950 to 2010|We examine the distributions of Chinese and Indian city sizes for seven decades (1950s to 2010s) using lognormal, Pareto, and general Pareto distributions. We ascertain which distribution fits the data and how the city size distributions change during these periods. The Chinese city size distribution is represented by lognormal in the early periods (1950–1990) and by Pareto in 2010, but is not characterized by Zipf, which could be attributed to Chinese government’s restrictions of migration from rural to urban areas and the one-child policy. In contrast, the Indian city size distribution transitions from lognormal in the earlier periods to Zipf in the later periods.
C46|Tail risk in energy portfolios|This article analyzes the tail behavior of energy price risk using a multivariate approach, in which the exposure to energy markets is given by a portfolio of oil, gas, coal, and electricity. To accommodate various dependence and tail decay patterns, this study models energy returns using different generalized hyperbolic conditional distributions and time-varying conditional mean and covariance. Employing daily energy futures data from August 2005 to March 2012, the authors recursively estimate the models and evaluate tail risk measures for the portfolio's profit-and-loss distribution for long and short positions at various horizons and confidence levels. Both in-sample and out-of-sample analyses applied to different energy portfolios show the importance of heavy tails and positive asymmetry in the distribution of energy risk factors. Thus, tail risk measures for energy portfolios based on standard methods (e.g. normality, constant covariance matrix) and on models with exponential tail decay underestimate actual tail risk, especially for short positions and short time horizons.
C46|Empirical modeling of the impact factor distribution|The distribution of impact factors has been modeled in the recent informetric literature using two-exponent law proposed by Mansilla, Köppen, Cocho, and Miramontes (2007). This paper shows that two distributions widely-used in economics, namely the Dagum and Singh-Maddala models, possess several advantages over the two-exponent model. Compared to the latter, the former models give as good as or slightly better fit to data on impact factors in eight important scientific fields. In contrast to the two-exponent model, both proposed distributions have closed-from probability density functions and cumulative distribution functions, which facilitates fitting these distributions to data and deriving their statistical properties.
C46|Measuring systemic risk-adjusted liquidity (SRL)—A model approach|Little progress has been made so far in addressing—in a comprehensive way—the negative externalities caused by excessive maturity transformation and the implications for effective liquidity regulation of banks. The SRL model combines option pricing theory with market information and balance sheet data to generate probabilistic measure of systemic liquidity risk. It enhances price-based liquidity regulation by linking a bank’s maturity mismatch impacting the stability of its funding with those characteristics of other banks, subject to individual changes in risk profiles and common changes in market conditions impacting funding and market liquidity risk. This approach can then be used (i) to quantify an individual institution’s time-varying contribution to expected losses from system-wide liquidity shortfalls and (ii) to price insurance premia that provide incentives for banks to internalize the social cost of their individual funding decisions.
C46|Lévy jump risk: Evidence from options and returns|Using index options and returns from 1996 to 2009, I estimate discrete-time models where asset returns follow a Brownian increment and a Lévy jump. Time variations in these models are generated with an affine GARCH, which facilitates the empirical implementation. I find that the risk premium implied by infinite-activity jumps contributes to more than half of the total equity premium and dominates that of the Brownian increments suggesting that it is more representative of the risks present in the economy. Overall, my findings suggest that infinite-activity jumps, instead of the Brownian increments, should be the default modeling choice in asset pricing models.
C46|Earnings and labour market volatility in Britain, with a transatlantic comparison|We contribute new evidence about earnings and labour market volatility in Britain over the period 1992–2008, for women as well as men, and provide transatlantic comparisons (Most research about volatility refers to earnings volatility for US men.). Earnings volatility declined slightly for both men and women over the period but the changes are not statistically significant. When we look at labour market volatility, i.e. also including individuals with zero earnings in the calculations, there is a statistically significant decline in volatility for both women and men, with the fall greater for men. Using variance decompositions, we demonstrate that the fall in labour market volatility is largely accounted for by changes in employment attachment rates. We show that volatility trends in Britain, and what contributes to them, differ from their US counterparts in several respects.
C46|Benford's Law, Families of Distributions and a Test Basis|Benford's Law is used to test for data irregularities. While novel, there are two weaknesses in the current methodology. First, test values used in practice are too conservative and the test values of this paper are more powerful and hold for fairly small samples. Second, testing requires Benford's Law to hold, which it often does not. I present a simple method to transform distributions to satisfy the Law with arbitrary precision and induce scale invariance, freeing tests from the choice of units. I additionally derive a rate of convergence to Benford's Law. Finally, the results are applied to common distributions.
C46|Medical tourism in Romania: the case study of cardiovascular rehabilitation in Covasna|Romania has one of the highest mortality rates in Europe for ischemic heart disease and, especially, for cerebrovascular disease. Taking into account the actual prevalence of cardiovascular diseases, an augmentation of the demand for specialized medical services is expected. As this paper argues, this situation can have an important impact on medical tourism. We analyze original data on the case study of a hospital, specialized in cardiovascular treatment, in the Romanian county of Covasna, which is offering specific balneal procedures, such as CO2 .hydrotherapy, alongside regular rehabilitation programs. The aim of our study is to evaluate the demographic characteristics, and the pathology of the hospitalized patients, as well as the specific rehabilitation procedures. Our findings suggest that the interest of patients, with cardiovascular diseases, for medical tourism can be influenced by accessibility, by some particularities of the location, but also by the holistic nature of the rehabilitation procedures.
C46|La cópula GED bivariada. Una aplicación en entornos de crisis|The General Error Distribution (GED) has been extensively used in time series econometrics applications, due to its great flexibility in the estimation of financial stylized facts. However, there has been no attempt to employ this statistical distribution in the construction of copulas. Copulas are probability functions that link one multivariate distribution to univariate distribution functions called marginals. These marginals are continuous and follow a uniform behavior within [0,1]. In this paper we introduce the bivariate GED copula to investigate financial contagion in Latinamerica during the 2008 crisis. We examine contagion in foreign exchange, equity, bonds and sovereign markets in Latinamerica. Standard decision criteria provide strong evidence in favor of the GED copula, against other widely used elliptical and arquimidean alternatives.// Mientras que la distribución de error general (GED) ha sido usada extensamente en aplicaciones de series de tiempo y ha demostrado una gran flexibilidad en la estimación de series de tiempo financieras, no se ha intentado utilizarla en la construcción de cópulas. Las cópulas son funciones de probabilidad que unen una función de distribución multivariada a funciones de distribución univariadas llamadas marginales. Se parte del supuesto de que las marginales son continuas y uniformes en el intervalo [0,1]. En este artículo proponemos la cópula GED bivariada, la cual, de acuerdo con nuestra revisión, no ha sido usada en la bibliografía. Esta función abarca otras funciones de distribución, como la gausiana o la doble exponencial, empleadas frecuentemente en el análisis de fenómenos financieros. Con el fin de probar el desempeño de esta nueva cópula investigamos el contagio financiero en la crisis de 2008 empleando tipos de cambio, acciones, bonos y mercados de deuda soberana en América Latina. Los criterios usuales de decisión proveen fuerte evidencia a favor de la cópula GED sobre otras alternativas elípticas o arquimideanas.
C46|Exploring Network Behavior Using Cluster Analysis|Innovation occurs in network environments. Identifying the important players in the innovative Â process, Â namely Â â€œthe Â innovatorsâ€ , Â is Â key to understanding the process of innovation. Doing this requires flexible analysis tools tailored to work well with complex datasets generated within such environments. One such tool, cluster analysis, organizes a large data set into discrete groups based on patterns of similarity. It can be used to discover data patterns in networks without requiring strong ex ante assumptions about the properties of either the data generating process or the environment. This paper reviews key procedures and algorithms related to cluster analysis. Further, it demonstrates how to choose among these methods to identify the characteristics of players in a network experiment where innovation emerges endogenously. Length: 30
C46|High-dimensional CLTs for individual Mahalanobis distances|In this paper we derive central limit theorems for two different types of Mahalanobis distances in situations where the dimension of the parent variable increases proportionally with the sample size. It is shown that although the two estimators are closely related and behave similarly in nite dimensions, they have different convergence rates and are also centred at two different points in high-dimensional settings. The limiting distributions are shown to be valid under some general moment conditions and hence available in a wide range of applications.
C46|Estimating Individual Mahalanobis Distance in High-Dimensional Data|This paper treats the problem of estimating individual Mahalanobis distances (MD) in cases when the dimension of the variable p is proportional to the sample size n. Asymptotic expected values are derived under the assumption p/n->c, 0
C46|Search Costs, Information Exchange and Sales Concentration in the Digital Music Industry|It is often assumed that consumers benefit from the internet because it offers a “long tail” with more variety of products to choose from. However, search costs may block the long tail effect and result in the dominance of superstars. This paper examines the variety hypothesis in the entire online market for digital music downloads in 17 countries over the period 2006-2011. First, we show that the entire distribution of legal music downloads is heavily skewed. Second, we hypothesise that a wide range of online information channels (sales and discovery platforms) play a role in this market. We find that the reduction of search costs implied by the generalisation of online information tools transforms demand as a result of changes in the dispersion of preferences. Ubiquitous and very popular discovery channels such as Facebook and iTunes tend to push consumers towards the superstars by shifting the demand curve but also towards the long-tail since they also generate rotations that promote niches. Conssequently, both the superstar and the long tail effects emerge even in mature digital markets.
C46|Correcting wealth survey data for the missing rich: The case of Austria|It is a well-known criticism that due to its exponential distribution, survey data on wealth is hardly reliable when it comes to analyzing the richest parts of society. This paper addresses this criticism using Austrian data from the Household Finance and Consumption Survey (HFCS). In doing so we apply the assumption of a Pareto distribution to obtain estimates for the number of households possessing a net wealth greater than four million Euros as well as their aggregate wealth holdings. Thereby, we identify suitable parameter combinations by using a series of maximum-likelihood estimates and appropriate goodness-of-fit tests to avoid arbitrariness with respect to the fitting of the Pareto-Distribution. Our results suggest that the alleged non-observation bias is considerable, accounting for about one quarter of total net wealth. The method developed in this paper can easily be applied to other countries where survey data on wealth are available.
C46|On the distribution of government bond returns: evidence from the EMU|This paper assesses the statistical distribution of daily EMU bond returns for the period 1999–2012. The normality assumption is tested and clearly rejected for all European countries and maturities. Although skewness plays a minor role in this departure from normality, it is mainly due to the excess kurtosis of bond returns. Therefore, we test the Student’s t, skewed Student’s t, and stable distribution that exhibit this feature. The financial crisis leads to a structural break in the time series. We account for this and retest the alternative distributions. A value-at-risk application underlines the importance of our findings for investors. In sum, excess kurtosis in bond returns is essential for risk management, and the stable distribution captures this feature best. Copyright Swiss Society for Financial Market Research 2014
C46|Rank-based Markov chains for regional income distribution dynamics|Markov chains have become a mainstay in the literature on regional income distribution dynamics and convergence. Despite its growing popularity, the Markov framework has some restrictive characteristics associated with the underlying discretization income distributions. This paper introduces several new approaches designed to mitigate some of the issues arising from discretization. Based on the examination of rank distributions, two new Markov-based chains are developed. The first explores the movement of individual economies through the income rank distribution over time. The second provides insight on the movements of ranks over geographical space and time. These also serve as the foundation for two new tests of spatial dynamics or the extent to which movements in the rank distribution are spatially clustered. An illustration of these new methods is included using income data for the lower 48 US states for the years 1929–2009. Copyright Springer-Verlag Berlin Heidelberg 2014
C46|Entropy, complexity, and spatial information|We pose the central problem of defining a measure of complexity, specifically for spatial systems in general, city systems in particular. The measures we adopt are based on Shannon’s (in Bell Syst Tech J 27:379–423, 623–656, 1948 ) definition of information. We introduce this measure and argue that increasing information is equivalent to increasing complexity, and we show that for spatial distributions, this involves a trade-off between the density of the distribution and the number of events that characterize it; as cities get bigger and are characterized by more events—more places or locations, information increases, all other things being equal. But sometimes the distribution changes at a faster rate than the number of events and thus information can decrease even if a city grows. We develop these ideas using various information measures. We first demonstrate their applicability to various distributions of population in London over the last 100 years, then to a wider region of London which is divided into bands of zones at increasing distances from the core, and finally to the evolution of the street system that characterizes the built-up area of London from 1786 to the present day. We conclude by arguing that we need to relate these measures to other measures of complexity, to choose a wider array of examples, and to extend the analysis to two-dimensional spatial systems. Copyright The Author(s) 2014
C46|The Process of Creating Economic Value Added: Causes, Factors and Implications|The economic value added is formed under the influence of two major factors: the difference between the economic profitability and the weighted average cost of capital, called the rate of economic value generation and the value of the economic asset exploited by the company. To stimulate the process of creation of economic value added, the company must act in the sense of increasing the rate of economic value generation and development of the volume of the economic asset.
C46|Intangible Assets and Strategic Positioning of Company|Company’s profitability is the result of the use of all available assets, although in many cases, the intangible assets are those that give the company the largest part of the strategic substance, contributing decisively to the increase of its overall value over the value of the tangible assets they possess. Intangible assets represent a support for the tangible ones and are at the origin of profitability surplus (super profit) obtained by the enterprise above normal profitability of the sector.
C46|A skew test on financial returns in the Colombian market|The characterization of financial returns depends heavily on probabilistic behavior, which can be ill-fitted, thus leading to inappropriate economic decisions concerning asset pricing, portfolio allocation and/or the measurement of market risk. In this paper, we propose a test to determine the adjustment of the returns of the General Index of the Colombian Stock Exchange (IGBC) to the following distributions: Normal, Skew Normal, and Skew T. In addition, we measure the level of bias and compare our test’s performance with an alternative test used only for detecting asymmetry. We find that the proposed test allows for characterizing returns in the Colombian stock market with one of the probability distributions, unlike the other test, which only provides a warning about the existence of bias and does not ascertain the distribution representing its behavior.
C46|The Intensity and Shape of Inequality: The ABG Method of Distributional Analysis|Inequality is anisotropic: its intensity varies by income level. We here develop a new tool, the isograph, to focus on local inequality and illustrate these variations. This method yields three coefficients which summarize the shape of inequality: a main coefficient, Alpha, which measures inequality at the median, and two correction coefficients, Beta and Gamma, which pick up any differential curvature at the top and bottom of the distribution. The analysis of a set of 232 microdata samples from 41 different countries in the LIS datacenter archive allows us to provide a systematic overview of the properties of the ABG (Alpha Beta Gamma) coefficients, which are compared both to a set of standard indices (Atkinson indices, generalized entropy, Wolfson polarization, etc.) and the GB2 distribution. This method also provides a smoothing tool that reveals the differences in the shape of distributions (the strobiloid) and how these have changed over time.
C46|Testing Spatial Causality in Cross-section Data|The paper shows a new non-parametric test, based on symbolic entropy, which permits detect spatial causality in cross-section data. The test is robust to the functional form of the relation and has a good behaviour in samples of medium to large size. We illustrate the use of test with the case of relationship between migration and unemployment, using data on 3,108 U.S. counties for the period 2003-2008.
C46|On the winning virtuous strategies for ultra high frequency electronic trading in foreign currencies exchange markets|In the Schumpeterian creative disruption age, the authors firmly believe that an increasing application of electronic technologies in the finances opens a big number of new unlimited opportunities toward a new era of the ultra high frequency electronic trading in the foreign currencies exchange markets in the conditions of the discrete information absorption processes in the diffusion - type financial systems with the induced nonlinearities. Going from the academic literature, we discuss the probability theory and the statistics theory application to accurately characterize the trends in the foreign currencies exchange rates dynamics in the short and long time periods. We consider the financial analysis methods, including the macroeconomic analysis, market microstructure analysis and order flow analysis, to forecast the volatility in the foreign currencies exchange rates dynamics in the short and long time periods. We discuss the application of the Stratanovich-Kalman-Bucy filtering algorithm in the Stratanovich – Kalman – Bucy filter and the particle filter to accurately estimate the time series and predict the trends in the foreign currencies exchange rates dynamics in the short and long time periods. We research the influence by discrete information absorption on the ultra high frequency electronic trading strategies creation and execution during the electronic trading in the foreign currencies exchange markets. We formulate the Ledenyov law on the limiting frequency (the cut-off frequency) for the ultra high frequency electronic trading in the foreign currencies exchange markets.
C46|Estimation of Fractal Parameters of Tehran Stock Market Groups Time Series Using Discrete Wavelet Transform|Nowadays financial markets such as stock markets, gold and currency because of their significant returns are the investors’ main target. Their aim is to invest in a way that they can earn the highest profit. Among these markets, the stock market is of utmost importance since it deals with buying and selling the shares of diverse companies. Thus using the approaches that yield the highest profit and the lowest risk is the greatest priority of investors. This paper wants to calculate the chaotic indicators in different groups of Tehran’s stock market using Discrete Wavelet Transform. For this purpose, by utilizing the wavelet toolbox of Matlab software, Hurst exponent and Fractal Dimension and Predictability index of Tehran stock market’s groups time series were estimated. Results prove that almost all of the group’s time series are demonstrating Non-Gaussian behavior. And the type of time series’ memories whether they are short-term or long-term were identified. Furthermore, Predictability indices of time series were calculated which is also useful in investor’s decision making.
C46|Rescue costs and financial risk|Following Taleb/Tapiero (2009) , the hypotheses are contrasted based on partial information of firms had losses (including external risk factors); the policy implications of this analysis are projected after evaluating two fundamental issues that continue to preoccupy the public opinion: how failures occur in markets in the case of large firms, corporations or companies, and what are the criteria for regulation and rescue available to governments, institutions and citizens to control them.
C46|On the winning virtuous strategies for ultra high frequency electronic trading in foreign currencies exchange markets|In the Schumpeterian creative disruption age, the authors firmly believe that an increasing application of electronic technologies in the finances opens a big number of new unlimited opportunities toward a new era of the ultra high frequency electronic trading in the foreign currencies exchange markets in the conditions of the discrete information absorption processes in the diffusion - type financial systems with the induced nonlinearities. Going from the academic literature, we discuss the probability theory and the statistics theory application to accurately characterize the trends in the foreign currencies exchange rates dynamics in the short and long time periods. We consider the financial analysis methods, including the macroeconomic analysis, market microstructure analysis and order flow analysis, to forecast the volatility in the foreign currencies exchange rates dynamics in the short and long time periods. We discuss the application of the Stratanovich-Kalman-Bucy filtering algorithm in the Stratanovich – Kalman – Bucy filter and the particle filter to accurately estimate the time series and predict the trends in the foreign currencies exchange rates dynamics in the short and long time periods. We research the influence by discrete information absorption on the ultra high frequency electronic trading strategies creation and execution during the electronic trading in the foreign currencies exchange markets. We formulate the Ledenyov law on the limiting frequency (the cut-off frequency) for the ultra high frequency electronic trading in the foreign currencies exchange markets.
C46|Random Variables, Their Properties, and Deviational Ellipses: In Map Point and Excel, v 4.0|This book is a practical reference guide accompanied with an Excel Workbook. This book gives an elementary introduction of the weighted standard deviational ellipse. This book also presents the computational aspects of the weighted exponential distributions as well. For the examples given, calculations are performed using VBA for Excel. This book makes comparisons (and shows the computations via VBA for Excel) using the likelihood functions with spatial data of the weighted ellipses. Lastly, the book covers spherical statistics. Throughout the text, the reader can see how to perform these difficult calculations and learn to adapt the code for his research.
C46|Random Variables, Their Properties, and Deviational Ellipses: In Map Point and Excel, v 4.0|This book is a practical reference guide accompanied with an Excel Workbook. This book gives an elementary introduction of the weighted standard deviational ellipse. This book also presents the computational aspects of the weighted exponential distributions as well. For the examples given, calculations are performed using VBA for Excel. This book makes comparisons (and shows the computations via VBA for Excel) using the likelihood functions with spatial data of the weighted ellipses. Lastly, the book covers spherical statistics. Throughout the text, the reader can see how to perform these difficult calculations and learn to adapt the code for his research.
C46|Synthetic data: an endogeneity simulation|This paper uses synthetic data and different scenarios to test treatments for endogeneity problems under different parameter settings. The model uses initial conditions and provides the solution for a hypothetical equation system with an embedded endogeneity problem. The behavioral and statistical assumptions are underlined as they are used through this research. A methodology is proposed for constructing and computing simulation scenarios. The econometric modeling of the scenarios is developed accordingly with the feedback obtained from previous scenarios. The inputs for these scenarios are synthetic data, which are constructed using random number machines and/or Monte Carlo simulations. The outputs of the scenarios are the model estimators. The research results demonstrated that a treatment for endogeneity can be developed as the sample size increases.
C46|Regional resilience and fat tails: A stochastic analysis of firm growth rate distributions of German regions|This paper breaks down the distributional analysis of firm growth rates to the domain of regions. Extreme growth events, i.e. fat tails, are conceptualized as an indicator of competitive regional environments which enable processes like structural adaptation or technological re-orientation. An understanding of the heterogeneous dynamics at the level of firms, the â€œturbulence underneath the big calmâ€ (Dosi et al. 2012), provides a micro-funded empirical perspective on the evolutionary dimension of regional resilience. Therefore, the flexible Asymmetric Exponential Power (AEP) density is fitted to firm data for each German region during the years of economic downturn (2008-2010). Peculiarities of employment growth are explicitly taken into account by applying a new maximum likelihood estimation procedure with order statistics (Bottazzi 2012). The estimated parameters, which measure the tailsâ€™ fatness, are then related to various region-specific factors that are discussed in the literature on regional resilience. Results show that firm growth rate distributions remain asymmetric and fat tailed at the spatially disaggregated level, but their shape markedly differ across regions. Extreme growth events, i.e. firm-level turbulences, are primarily a phenomenon of economically better performing regions at the aggregate level and further intensified by the presence of a higher qualified workforce. Besides, the fatness of the tails depends on the regionsâ€™ industrial structure.
C46|Statistical Delimitation of the Profile of Local Elections Candidate – An Applied Statistics Research|This article devoted to the delimitation of candidate profile, by means of an opinion survey representative of various categories of voters placed in the same area or locality, vitally depended on the perception of the people the questionnaire-based research was addressed to, on the categories of questions, solutions and interpretations that are most likely to be targets of the reader’s interest; they were presented in the kind of language that all of them are used to, thus aiming to maximize the utility and impact of the research through its usefulness, and anticipating its regular resuming, as well as a permanent adequacy of the form, and even the style of the paper.
C46|On Bias in the Estimation of Structural Break Points|Based on the Girsanov theorem, this paper obtains the exact finite sample distribution of the maximum likelihood estimator of structural break points in a continuous time model. The exact finite sample theory suggests that, in empirically realistic situations, there is a strong finite sample bias in the estimator of structural break points. This property is shared by least squares estimator of both the absolute structural break point and the fractional structural break point in discrete time models. A simulation-based method based on the indirect estimation approach is proposed to reduce the bias both in continuous time and discrete time models. Monte Carlo studies show that the indirect estimation method achieves substantial bias reductions. However, since the binding function has a slope less than one, the variance of the indirect estimator is larger than that of the original estimator.
C46|A note on the null distribution of the local spatial heteroscedasticity (LOSH) statistic|Recently, Ord and Getis (Ann Reg Sci 48:529–539, 2012 ) developed a local statistic $$H_i$$ H i , called local spatial heteroscedasticity statistic, to identify boundaries of clusters and to describe the nature of heteroscedasticity within clusters. Furthermore, in order to implement the hypothesis testing, Ord and Getis suggested a chi-square approximation method to approximate the null distribution of $$H_i$$ H i , but they said that the validity of the chi-square approximation remains to be investigated and some other approximation methods are still worthy of being developed. Motivated by this suggestion, we propose in this paper a bootstrap procedure to approximate the null distribution of $$H_i$$ H i and conduct some simulation to empirically assess the validity of the bootstrap and chi-square methods. The results demonstrate that the bootstrap method can provide a more accurate approximation than the chi-square method at the cost of more computation time. Moreover, the power of $$H_i$$ H i in identifying boundaries of clusters is empirically examined using the proposed bootstrap method to compute $$p$$ p values of the tests, and the multiple comparison issue is also discussed. Copyright Springer-Verlag Berlin Heidelberg 2014
C46|Business concentration through the eyes of the HHI|This paper examines the understanding of business concentration through the Her findahl-Hirschman Index (HHI), by showing that this index is conceptually a model according to which this concentration is the consequence of a renewal process. This process is prompted by firms engaging in different types of economic activity as the means by which to vie for market share. The resultant rivalry produces departures between the market shares of firms. These departures ultimately transmit into differing concentration levels attributable only to the economic activity with which firms vie. As a consequence, while the HHI is commonly interpreted to be a screening indicator of market structure, it is in fact first and foremost a screening indicator of market conduct, which incidentally doubles-up as an indicator of market structure. As part of this, the paper shows that while the HHI cannot identify the exact economic conduct that produces the corresponding business concentration of the observed market structure, it does reveal that whatever this conduct is, it is always subordinated to some type of regenerative or revitalising process.
