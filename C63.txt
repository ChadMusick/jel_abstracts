C63|The pure effect of social preferences on regional location choices: The evolving dynamics of convergence to a steady state population distribution|This paper tracks the consequences of individuals' desire to align their location with their social preferences. The social preference studied in the paper is distaste for relative deprivation, measured in a cardinal manner. Location is conceived as social space, with individuals choosing to relocate if, as a result, their relative deprivation will be reduced, holding their incomes constant. Conditions are provided under which the associated dynamics reaches a spatial steady state, the number of periods it takes to reach a steady state is specified, and light is shed on the robustness of the steady state outcome. By way of simulation it is shown that for large populations, a steady state of the relocation dynamics is almost always reached, typically in one period, and that cycles are more likely to occur when the populations' income distributions are more equal.
C63|Macro and Micro Prudential Policies: Sweet and Lowdown in a Credit Network Agent Based Model|The paper presents an agent based model reproducing a stylized credit network that evolves endogenously through the individual choices of rms and banks. We introduce in this framework a anancial stability authority in order to test the e ects of different prudential policy measures designed to improve the resilience of the economic system. Simulations show that a combination of micro and macro prudential policies reduces systemic risk, but at the cost of increasing banks' capital volatility. Moreover, agent based methodology allows us to implement an alternative meso regulatory framework that takes into consideration the connections between firms and banks. This policy targets only the more connected banks, increasing their capital requirement in order to reduce the di usion of local shocks. Our results support the idea that the meso prudential policy is able to reduce systemic risk without a ecting the stability of banks'capital structure.
C63|Phillips' averaging procedure as a 'crude' version of the Haar wavelet filter|The aim of this study is to investigate the exact nature of Phillips' (1958) findings. We show that the application of the simplest type of wavelet basis function developed by Haar in 1910 allows to replicate the output of Phillips' data transformation procedure, i.e. the six mean coordinates. Specifically, the resemblance between the coarsest scale level coefficients from the Haar wavelet filter and the six crosses suggests the long-term nature of Phillips' (wage-unemployment) relationship. The application of the Haar wavelet filter allows us to examine the effects of two main features of Phillips' 'unorthodox' averaging procedure: the arbitrarily choice of variable-width intervals and the choice of sorting observations in ascending order of unemployment rate values. Our results show that the arbitrary selection of intervals affects only the smoothness (regularity) of the nonlinear pattern of the wage-unemployment relationship, but not its shape which is determined by sorting and grouping unemployment rate values in ascending order. Indeed, when observations are ordered according to a chronological sequence a simple linear relationship is evident. These findings are robust to different samples, 1861-1913 and 1861-1958.
C63|Inequality, mobility and the financial accumulation process: a computational economic analysis|Abstract Our computational economic analysis investigates the relationship between inequality, mobility and the financial accumulation process. Extending the baseline model by Levy et al., we characterise the economic process through stylised return structures generating alternative evolutions of income and wealth through time. First, we explore the limited heuristic contribution of one and two-factors models comprising one single stock (capital wealth) and one single flow factor (labour) as pure drivers of income and wealth generation and allocation over time. Second, we introduce heuristic modes of taxation in line with the baseline approach. Our computational economic analysis corroborates that the financial accumulation process featuring compound returns plays a significant role as source of inequality, while institutional arrangements including taxation play a significant role in framing and shaping the aggregate economic process that evolves over socioeconomic space and time.
C63|Search Complementarities, Aggregate Fluctuations, and Fiscal Policy|We develop a quantitative business cycle model with search complementarities in the inter-firm matching process that entails a multiplicity of equilibria. An active static equilibrium with strong joint venture formation, large output, and low unemployment can coexist with a passive static equilibrium with low joint venture formation, low output, and high unemployment. Changes in fundamentals move the system between the two static equilibria, generating large and persistent business cycle fluctuations. The volatility of shocks is important for the selection and duration of each static equilibrium. Sufficiently adverse shocks in periods of low macroeconomic volatility trigger severe and protracted downturns. The magnitude of government intervention is critical to foster economic recovery in the passive static equilibrium, while it plays a limited role in the active static equilibrium.
C63|Firm-level Investment Under Imperfect Capital Markets in Ukraine|This paper develops and estimates a model of firm-level fixed capital investment when firms face borrowing constraints. Dynamically optimal investment functions are derived for the firms with and without financial constraints. These policy functions are then used to construct the likelihood of observing each of the investment regimes in the data. Structural parameters are estimated using data from the Ukrainian manufacturing sector in 1993–1998. I provide empirical evidence of the role of market and ownership structure for firm-level investment behavior. I also discuss the effects of international trade exposure and involvement in non-monetary transactions on the probability of facing financial constraints and the resulting fixed capital accumulation path. Estimation results are used to illustrate the welfare implications of financial constraints in the Ukrainian manufacturing sector.
C63|Assessing the Resilience of the Canadian Banking System|The stability of the Canadian financial system, as well as its ability to support the Canadian economy, depends on the ability of financial institutions to absorb and manage major shocks. This is especially true for large banks, which perform services essential to the Canadian economy.
C63|Évaluer la résilience du système bancaire canadien|La stabilité du système financier canadien et son rôle de pilier de l’économie du pays tiennent à la capacité des institutions financières à absorber et à gérer les chocs majeurs. Cela est particulièrement vrai pour les grandes banques, dont les services sont essentiels à l’économie canadienne.
C63|Colombian liberalization and integration to world trade markets: Much ado about nothing|The objective of this paper is to study the evolution of Colombian liberalization and integration to world trade from 1995 to 2016. We achieve our objective by measuring Colombia’s importance in the world trade network. We employ several types of network centrality metrics to measure importance (i.e. degree, strength, hub, authority), and examine their dynamics against a set of regional peers that serve as benchmark countries. Consistent with previous literature, more than two decades of dedicated trade policies and institutional changes resulted in increased exports and imports. However, when compared to regional peers such as Chile, Brazil, Mexico, and Peru, and China and the United States as trade leading countries, Colombia’s centrality in the world trade network did not improve accordingly. Absolute changes in the evolution of trade did not materialize in an enhanced integration to world markets. Colombia’s ranking in the world trade network did not improve materially, whereas that of some of her regional peers did manifestly (i.e. Peru and Chile). Results highlight the perils of analyzing a country’s trade dynamics in isolation, and emphasizes the usefulness of examining the world trade network. From the economic policy and institutional perspectives, results underscore the challenges ahead to better integrate to world markets and to achieve long-term economic growth from trade. **** RESUMEN: El objetivo del documento es estudiar la evolución de la apertura e integración de Colombia al comercio mundial entre 1995 y 2016 y evaluar su importancia en la red de comercio mundial. El documento emplea varios tipos de métricas de centralidad de red (es decir, grado, valor de los flujos de comercio, centro, autoridad), y examina su dinámica y las compara con las de pares regionales que sirven como países de referencia. De acuerdo con la literatura colombiana, más de dos décadas de políticas comerciales y cambios institucionales resultaron en un aumento de las exportaciones e importaciones. Sin embargo, en comparación con Chile, Brasil, México y Perú, y China y Estados Unidos como países líderes en el comercio mundial, la centralidad de Colombia en la red mundial no mejoró. Los cambios absolutos en los flujos de exportaciones e importaciones no se materializaron en una mayor integración a los mercados mundiales. La posición de Colombia en la red de comercio mundial no mejoró sustancialmente, mientras que la de algunos de sus pares regionales sí lo hizo (es decir, Perú y Chile). Desde una perspectiva de política económica e institucional, los resultados resaltan los desafíos futuros de Colombia para integrarse mejor en los mercados mundiales y lograr un mayor crecimiento económico de largo plazo derivado del comercio internacional.
C63|Measuring contagion risk in international banking|We propose a distress measure for national banking systems that incorporates not only banks' CDS spreads, but also how they interact with the rest of the global financial system via multiple linkage types. The measure is based on a tensor decomposition method that extracts an adjacency matrix from a multi-layer network, measured using banks' foreign exposures obtained from the BIS international banking statistics. Based on this adjacency matrix, we develop a new network centrality measure that can be interpreted in terms of a banking system's credit risk or funding risk.
C63|Market-implied systemic risk and shadow capital adequacy|This paper presents a forward-looking approach to measure systemic solvency risk using contingent claims analysis (CCA) as a theoretical foundation for determining an institution’s default risk based on the uncertainty in its asset value relative to promised debt payments over time. Default risk can be quantified as market-implied expected losses calculated from integrating equity market and balance sheet information in a structural default risk model. The expected losses of multiple banks and their non-parametric dependence structure define a multivariate distribution that generates portfolio-based estimates of the joint default risk using the aggregation technique of the Systemic CCA framework (Jobst and Gray, 2013). This market-implied valuation approach (‘shadow capital adequacy’) endogenises bank solvency as a probabilistic concept based on the perceived default risk (in contrast to accounting-based prudential measures of capital adequacy). The presented model adds to the literature of analytical tools estimating market-implied systemic risk by augmenting the CCA approach with a jump diffusion process of asset changes to inform a more comprehensive and flexible assessment of common vulnerabilities to tail risks of the four largest UK commercial banks.
C63|U.S. Macroeconomic Policy Evaluation in an Open Economy Context using Wavelet Decomposed Optimal Control Methods|It is widely recognized that the policy objectives of fiscal and monetary policymakers usually have different time horizons, and this feature may not be captured by traditional econometric techniques. In this paper, we first decompose U.S macroeconomic data using a time-frequency domain technique, namely discrete wavelet analysis. We then model the behavior of the U.S. economy over each wavelet frequency range and use our estimated parameters to construct a tracking model. To illustrate the usefulness of this approach, we simulate jointly optimal fiscal and monetary policy with different short-term targets: an inflation target, a money growth target, an interest rate target, and a real exchange rate target. The results determine the reaction in fiscal and monetary policy that is required to achieve an inflation target in a low inflation environment, and when both fiscal and monetary policy are concerned with meeting certain economic growth objectives. The combination of wavelet decomposition in an optimal control framework can also provide a new approach to macroeconomic forecasting.
C63|Density Forecasting|This paper reviews different methods to construct density forecasts and to aggregate forecasts from many sources. Density evaluation tools to measure the accuracy of density forecasts are reviewed and calibration methods for improving the accuracy of forecasts are presented. The manuscript provides some numerical simulation tools to approximate predictive densities with a focus on parallel computing on graphical process units. Some simple examples are proposed to illustrate the methods.
C63|How BLUE is the Sky? Estimating the Air Quality Data in Beijing During the Blue Sky Day Period (2008-2012) by the Bayesian LSTM Approach|Over the last three decades, air pollution has become a major environmental challenge in many of the fast growing cities in China, including Beijing. Given that any long-term exposure to high-levels of air pollution has devastating health consequences, accurately monitoring and reporting air pollution information to the public is critical for ensuring public health and safety and facilitating rigorous air pollution and health-related scientific research. Recent statistical research examining China’s air quality data has posed questions regarding data accuracy, especially data reported during the Blue Sky Day (BSD) period (2000 – 2012), though the accuracy of publicly available air quality data in China has improved gradually over the recent years (2013 – 2017). To the best of our understanding, no attempt has been made to re-estimate the air quality data during the BSD period. In this paper, we put forward a machine-learning model to re-estimate the official air quality data during the BSD period of 2008 – 2012, based on the PM2.5 data of the Beijing US Embassy, and the proxy data covering Aerosol Optical Depth (AOD) and meteorology. Results have shown that the average re-estimated daily air quality values are respectively 64% and 61% higher than the official values, for air quality index (AQI) and AQI equivalent PM2.5, during the BSD period of 2008 to 2012. Moreover, the re-estimated BSD air quality data exhibit reduced statistical discontinuity and irregularity, based on our validation tests. The results suggest that the proposed data re-estimation methodology has the potential to provide more justifiable historical air quality data for evidence-based environmental decision-making in China.<br><small>(This abstract was borrowed from another version of this item.)</small>
C63|A Unit Commitment and Economic Dispatch Model of the GB Electricity Market – Formulation and Application to Hydro Pumped Storage|We present a well calibrated unit commitment and economic dispatch model of the GB electricity market and applied it to the economic analysis of the four existing hydro pumped storage (PS) stations in GB. We found that with more wind on the system PS arbitrage revenue increases: with every percentage point (p.p) increase in wind capacity the total PS arbitrage profit increases by 0.21 p.p.. However, under a range of wind capacity, the PS’ modelled revenue from price arbitrage is not enough to cover their ongoing fixed costs. Analysing the 2015-18 GB balancing and ancillary services data suggests that PS stations were not active in managing transmission constraints and in fact about 60% of constraint payments went to gas-fired units. However, the PS stations are active in provision of ancillary services such as fast reserve, response and other reserve services with a combined market share of at least 30% in 2018. Stacking up the modelled revenue from price arbitrage with the 2018 balancing and ancillary services revenues against the ongoing fixed costs suggests that the four existing PS stations are profitable. Most of the revenue comes from balancing and ancillary services markets – about 75% – whereas only 25% comes from price arbitrage. However, the revenues will not be enough to cover capex and opex of a new 600 MW PS station. The gap in financing will have to come from balancing and ancillary services market opportunities and less so from purely price arbitrage. Finally, we found that the marginal contribution of most of the existing PS stations to gas and coal plant profitability is negative, while from the system point of view, PS stations do contribute to minimizing the total operating cost.
C63|The mirror does not lie: Endogenous ?scal limits for Slovakia|We study the interactions among ?scal policy, ?scal limits and the associated sovereign risk premium. The ?scal limit distribution, which measures the ability of the government to service its debt, arises endogenously from dynamic Laffer curves. We assume a feedback loop between the ?scal limit distribution and the risk premium and determine them simultaneously using and ef?cient iterative scheme. A nonlinear relationship between the sovereign risk premium and the level of government debt then emerges in equilibrium. The model is calibrated to Slovak data assuming steeply growing age-related transfers and volatile business cycle. We study the impact of various model parameters on the conditional (state-dependent) and unconditional distributions of the ?scal limit. Fiscal limit distributions obtained via Markov–Chain–Monte–Carlo regime switching algorithm depend on the rate of growth of government transfers, the degree of countercyclicality of policy, and the distribution of the underlying economic conditions. We ?nd that both distributions are considerably more heavy-tailed compared with those usually obtained in the literature for advanced economies, and are very sensitive to the size and rate of growth of transfers, the business cycle phase and the ?scal policy credibility. The main policy message is that the Maastricht debt limit of 60 percent of GDP is not safe enough for Slovakia. Furthermore, credible reforms reining in age-related spending and thus stabilising public ?nance in the long-run, should be a priority.
C63|CBOâ€™s Medicare Beneficiary Cost-Sharing Model: A Technical Description: Working Paper 2019-08|CBO uses a model to estimate the federal budgetary effects of proposed changes to the cost-sharing structure of the Medicare fee-for-service program. This paper describes that model, the analyses it can support, and an illustrative option for changing Medicareâ€™s cost-sharing structure.
C63|On the Solution of High-Dimensional Macro Models with Distributional Channels|Importance of distributional channels in macroeconomic dynamics has been the object of considerable attention in empirical studies. Despite significant amount of effort aimed at incorporating heterogeneity into macroeconomics, however, their explicit inclusion in the standard policy toolbox is far from widespread. A relevant obstacle, in such cases, is the computation of equilibria. I propose a global solution method for the computation of infinite-horizon, heterogeneous agent macroeconomic models with aggregate uncertainty. Details of the algorithm are illustrated by presenting its application to a an example model: in it, aggregate dynamics depends explicitly on firm entry and exit, and individual choices are often constrained by a form of market incompleteness. Existing computational strategies are either unfeasible or provide inaccurate solutions. Moreover, global solutions are computationally expensive because the minimal representation of the aggregate state space - and thus the aggregate law of motion - faces the curse of dimensionality. The proposed strategy thus combines adaptive sparse grids with a cross-sectional density approximation, and introduces a framework for solving the more general class of dynamic models with firm or household heterogeneity accurately.
C63|Predicting criminal behavior with Lévy flights using real data from Bogotá|I use residential burglary data from Bogota, Colombia, to fit an agent-based model following truncated L´evy flights (Pan et al., 2018) elucidating criminal rational behavior and validating repeat/near-repeat victimization and broken windows effects. The estimated parameters suggest that if an average house or its neighbors have never been attacked, and it is suddenly burglarized, the probability of a new attack the next day increases, due to the crime event, in 79 percentage points. Moreover, the following day its neighbors will also face an increment in the probability of crime of 79 percentage points. This effect persists for a long time span. The model presents an area under the Cumulative Accuracy Profile (CAP) curve, of 0.8 performing similarly or better than state-of-the-art crime prediction models. Public policies seeking to reduce criminal activity and its negative consequences must take into account these mechanisms and the self-exciting nature of crime to effectively make criminal hotspots safer.
C63|Aplicación del modelo estocástico de difusion -salto de merton para la simulación del valor del índice colcap|ABSTRACT This working paper consist in apply the Stochastic Jump-Diffusion model proposed by Merton (1976 MJD model), as well as the process of estimating its parameters, applied to the COLCAP stock index. Authors like (Andersen, Benzoni, & Lund, 2002), (Hanson & Westman, 2002), (Hanson & Zongwu, 2004), (Penagos, Gabriel; Rubio, Gonzalo, 2013) (Tang, 2018) show how the incorporation of jumps, allows to obtain a Probability Density Function (PDF) according with skewed distributions and high kurtosis characterizing data log-returns financial. The results presented will have as reference the model Black & Scholes, 1973 (B&S) which is a pure diffusion model, whose base is the Gaussian distribution. The paper is composed by five parts, in the first one we could find the bibliographic review about the application of the stochastic jump-diffusion models, in the second and third one describes the COLCAP index, as well as its composition and history, the description of the data and the MJD model is defined; the fourth one shows the results and finally the conclusions. ***** RESUMEN Este trabajo de grado tiene como objetivo aplicar el modelo Estocástico de Salto-Difusión propuesto por Merton (1976) (MJD) así como el proceso de estimación de sus parámetros, aplicado al índice bursátil COLCAP. Autores como (Andersen, Benzoni, & Lund, 2002), (Hanson & Westman, 2002) (Hanson & Zongwu, 2004) (Penagos, Gabriel; Rubio, Gonzalo, 2013) y (Tang, 2018) muestran como la incorporación de saltos, permite obtener una Función de Densidad de Probabilidad (PDF) más acorde con distribuciones asimétricas y con curtosis elevadas que caracterizan a los datos de log-retornos financieros. Los resultados presentados tendrán como punto de referencia el modelo de Black and Scholes (B&S), el cual es un modelo de difusión puro, cuya base es la Distribución Gaussiana. El articulo se compone de cinco partes, en la primera se encuentra la revisión bibliográfica acerca de la aplicación de los modelos estocásticos de salto-difusión, en la segunda y tercera se describen el índice COLCAP, su composición e historia, se describen los datos y se define el modelo MJD, en la cuarta parte se muestran los resultados y finalmente se concluye.
C63|Predicting criminal behavior with Levy flights using real data from Bogota|I use residential burglary data from Bogota, Colombia, to fit an agent-based modelfollowing truncated Lévy flights (Pan et al., 2018) elucidating criminal rational behaviorand validating repeat/near-repeat victimization and broken windows effects. The estimatedparameters suggest that if an average house or its neighbors have never been attacked,and it is suddenly burglarized, the probability of a new attack the next day increases, dueto the crime event, in 79 percentage points. Moreover, the following day its neighborswill also face an increment in the probability of crime of 79 percentage points. This effectpersists for a long time span. The model presents an area under the Cumulative AccuracyProfile (CAP) curve, of 0.8 performing similarly or better than state-of-the-art crimeprediction models. Public policies seeking to reduce criminal activity and its negativeconsequences must take into account these mechanisms and the self-exciting nature ofcrime to effectively make criminal hotspots safer
C63|Integrating a Global Supply Chain Model With a Computable General Equilibrium Model|Global supply chain (GSC) trade results from decisions by firms producing final goods to allocate underlying tasks to dedicated facilities in different countries. These decisions create cross-border flows of products at various stages of completion. We demonstrate a divide-and-conquer approach to integrating GSC and computable general equilibrium (CGE) models: the models are solved separately and information is passed between them. A stylized integrated model suggests that by providing low-skilled jobs in developing countries, GSC trade accelerates the transfer of labour out of low-marginal-productivity agriculture in these countries into higher-marginal-productivity manufacturing. At the same time, GSC trade can leave high-income countries having to transfer considerable fractions of their workforce out of manufacturing and into services. After potentially expensive structural adjustment, high-income countries may be left in the long run with no more than a small equilibrium welfare gain or even a loss.
C63|Using the Sequence-Space Jacobian to Solve and Estimate Heterogeneous-Agent Models|We propose a general and highly efficient method for solving and estimating general equilibrium heterogeneous-agent models with aggregate shocks in discrete time. Our approach relies on the rapid computation and composition of sequence-space Jacobians—the derivatives of perfect-foresight equilibrium mappings between aggregate sequences around the steady state. We provide a fast algorithm for computing Jacobians for heterogeneous agents, a technique to substantially reduce dimensionality, a rapid procedure for likelihood-based estimation, a determinacy condition for the sequence space, and a method to solve nonlinear perfect-foresight transitions. We apply our methods to three canonical heterogeneous-agent models: a neoclassical model, a New Keynesian model with one asset, and a New Keynesian model with two assets.
C63|A Structural Model for the Coevolution of Networks and Behavior|This paper introduces a structural model for the coevolution of networks and behavior. The microfoundation of our model is a network game where agents adjust actions and network links in a stochastic best-response dynamics with a utility function allowing for both strategic externalities and unobserved heterogeneity. We show the network game admits a potential function and the coevolution process converges to a unique stationary distribution characterized by a Gibbs measure. To bypass the evaluation of the intractable normalizing constant in the Gibbs measure, we adopt the Double Metropolis-Hastings algorithm to sample from the posterior distribution of the structural parameters. To illustrate the empirical relevance of our structural model, we apply it to study R&D investment and collaboration decisions in the chemicals and pharmaceutical industry and find a positive knowledge spillover effect. Finally, our structural model provides a tractable framework for a long-run key player analysis.
C63|Dynamic Social Interactions and Health Risk Behavior|We study risky behavior of adolescents. Concentrating on smoking and alcohol use, we structurally estimate a dynamic social interaction model in the context of students' school networks included in the National Longitudinal Study of Adolescent Health (Add Health). The model allows for forward-looking behavior of agents, addiction effects, and social interactions in the form of preferences for conformity in the social network. We find strong evidence for forward looking dynamics and addiction effects. We also find that social interactions in the estimated dynamic model are quantitatively large. A misspecified static model would fit data substantially worse, while producing a much smaller estimate of the social interaction effect. With the estimated dynamic model, a temporary shock to students' preferences in the 10th grade has effects on their behavior in grades 10, 11, 12, with estimated social multipliers 1:53, 1:03, and 0:76, respectively. The multiplier effect of a permanent shock is much larger, up to 3:7 in grade 12. Moreover (semi-) elasticities of a permanent change in the availability of alcohol or cigarettes at home on child risky behavior implied by the dynamic equilibrium are 25%, 63%, and 79%, in grades 10, 11, 12, respectively.
C63|Search Complementarities, Aggregate Fluctuations, and Fiscal Policy|We develop a quantitative business cycle model with search complementarities in the inter-firm matching process that entails a multiplicity of equilibria. An active static equilibrium with strong joint venture formation, large output, and low unemployment can coexist with a passive static equilibrium with low joint venture formation, low output, and high unemployment. Changes in fundamentals move the system between the two static equilibria, generating large and persistent business cycle fluctuations. The volatility of shocks is important for the selection and duration of each static equilibrium. Sufficiently adverse shocks in periods of low macroeconomic volatility trigger severe and protracted downturns. The magnitude of government intervention is critical to foster economic recovery in the passive static equilibrium, while it plays a limited role in the active static equilibrium.
C63|Financial Frictions and the Wealth Distribution|This paper investigates how, in a heterogeneous agents model with financial frictions, idiosyncratic individual shocks interact with exogenous aggregate shocks to generate time-varying levels of leverage and endogenous aggregate risk. To do so, we show how such a model can be efficiently computed, despite its substantial nonlinearities, using tools from machine learning. We also illustrate how the model can be structurally estimated with a likelihood function, using tools from inference with diffusions. We document, first, the strong nonlinearities created by financial frictions. Second, we report the existence of multiple stochastic steady states with properties that differ from the deterministic steady state along important dimensions. Third, we illustrate how the generalized impulse response functions of the model are highly state-dependent. In particular, we find that the recovery after a negative aggregate shock is more sluggish when the economy is more leveraged. Fourth, we prove that wealth heterogeneity matters in this economy because of the asymmetric responses of household consumption decisions to aggregate shocks.
C63|When the U.S. catches a cold, Canada sneezes: a lower-bound tale told by deep learning|"The Canadian economy was not initially hit by the 2007-2009 Great Recession but ended up having a prolonged episode of the effective lower bound (ELB) on nominal interest rates. To investigate the Canadian ELB experience, we build a ""baby"" ToTEM model -- a scaled-down version of the Terms of Trade Economic Model (ToTEM) of the Bank of Canada. Our model includes 49 nonlinear equations and 21 state variables. To solve such a high-dimensional model, we develop a projection deep learning algorithm -- a combination of unsupervised and supervised (deep) machine learning techniques. Our findings are as follows: The Canadian ELB episode was contaminated from abroad via large foreign demand shocks. Prolonged ELB episodes are easy to generate in open-economy models, unlike in closed-economy models. Nonlinearities associated with the ELB constraint have virtually no impact on the Canadian economy but other nonlinearities do, in particular, the degree of uncertainty and specific closing condition used to induce the model's stationarity."
C63|Bilateral Defaultable Financial Derivatives Pricing and Credit Valuation Adjustment|The one-side defaultable financial derivatives valuation problems have been studied extensively, but the valuation of bilateral derivatives with asymmetric credit qualities is still lacking convincing mechanism. This paper presents an analytical model for valuing derivatives subject to default by both counterparties. The default-free interest rates are modeled by the Market Models, while the default time is modeled by the reduced-form model as the first jump of a time-inhomogeneous Poisson process. All quantities modeled are market-observable. The closed-form solution gives us a better understanding of the impact of the credit asymmetry on swap value, credit value adjustment, swap rate and swap spread.
C63|CDS index options in Markov chain models|We study CDS index options in a credit risk model where the defaults times have intensities which are driven by a finite-state Markov chain representing the underlying economy. In this setting we derive compact computationally tractable formulas for the CDS index spread and the price of a CDS index option. In particular, the evaluation of the CDS index option is handled by translating the Cox-framework into a bivariate Markov chain. Due to the potentially very large, but extremely sparse matrices obtained in this reformulating, special treatment is needed to efficiently compute the matrix exponential arising from the Kolmogorov Equation. We provide details of these computational methods as well as numerical results. The finite-state Markov chain model is calibrated to data with perfect fits, and several numerical studies are performed. In particular we show that under same exogenous circumstances, the CDS index options prices in the Markov chain framework can be close to or sometimes larger than prices in models which assume that the CDS index spreads follows a log-normal process. We also study the different default risk components in the option prices generated by the Markov model, an investigation which is difficult to do in models where the CDS index spreads follows a log-normal process.
C63|Samuelson's Approach to Revealed Preference Theory: Some Recent Advances|Since Paul Samuelson introduced the theory of revealed preference, it has become one of the most important concepts in economics. This chapter surveys some recent contributions in the revealed preference literature. We depart from Afriat's theorem, which provides the conditions for a data set to be consistent with the utility maximization hypothesis. We provide and motivate a new condition, which we call the Varian inequalities. The advantage of the Varian inequalities is that they can be formulated as a set of mixed integer linear inequalities, which are linear in the quantity and price data. We show how the Varian inequalities can be used to derive revealed preference tests for weak separability, and show how it can be used to formulate tests of the collective household model. Finally, we discuss measurement errors in the observed data and measures of goodness-of-fit, power and predictive success.
C63|Brexit: Everyone Loses, but Britain Loses the Most|This paper examines 12 economic simulation models that estimate the impact of Brexit. We provide their range of results and explain their associated assumptions and methodologies (macroeconometric models, computable general equilibrium [CGE] models, or mixed approaches). CGE models simulate the operation of market economies, solving for changes in equilibrium prices and quantities (production, employment, demand, and international trade) for all sectors in the economy. Macroeconometric models focus on economic aggregates and macro shocks, such as interest rates, the exchange rate, inflation, risk, uncertainty, and government expenditure/revenue. Most of the studies find adverse effects for the UK and the EU-27. The UK's GDP losses from a hard Brexit (reversion to World Trade Organization rules due to a lack of UK-EU agreement) range from –1.2 to –4.5 percent in most of the models analyzed. A soft Brexit (e.g., Norway arrangement, which seems in line with the nonbinding text of the political declaration of November 14, 2018 on the future EU-UK relationship) has about half the negative impact of a hard Brexit. Only two of the models derive gains for the UK after Brexit because they are based on unrealistic assumptions. We analyze more deeply a CGE model that includes productivity and firms' selection effects within manufacturing sectors à la Melitz (2003) and the operations of foreign multinationals in services. Based on this latest model, we provide a complete overview and explanation of the likely economic impact of Brexit on a wide range of macroeconomic variables, namely GDP, wages, private consumption, capital remuneration, aggregate exports, aggregate imports, and the consumer price index. The data underlying this analysis are available at https://piie.com/system/files/documents/wp19-5.zip.
C63|FIDELIO 3 manual: Equations and data sources|"FIDELIO (Fully Interregional Dynamic Econometric Long-term Input-Output) is a multi-sectoral model developed by the unit B.5 of the Directorate General Joint Research Centre (JRC) â€” the circular economy and industrial leadership unit. Compared to neoclassical CGE models â€” which assume that the perfect flexibility of prices and quantities ensures the full use of the factors of production at all times â€” FIDELIO integrates some new-Keynesian features: consumption adjusts slowly to its optimal level according to an error correction model and wages do not clear the labour market. The assumptions that prices do not clear the markets and market ""imperfections"" exist generate the dynamics of the model that is solved sequentially (recursive dynamic). In addition, FIDELIO is an econometric model since the calibration of most of the behavioural parameters of the model (dynamic adjustment lags of prices and quantities, and elasticities) is based on econometric estimations.This technical report illustrates the third version of the FIDELIO model, FIDELIO 3. The changes introduced in the subsequent versions of the model have two main objectives. The first one is to increase the coverage of the model. The second one is to improve the efficiency and the capacity of the model to evaluate sustainable production and consumption policies. The aim of this report is twofold. First, it contains all the equations of the current version of the model; second, it illustrates the characteristics of the data used by FIDELIO 3."
C63|The pure effect of social preferences on regional location choices: The evolving dynamics of convergence to a steady state population distribution| This paper tracks the consequences of individuals’ desire to align their location with their social preferences. The social preference studied in the paper is distaste for relative deprivation, measured in a cardinal manner. Location is conceived as social space, with individuals choosing to relocate if, as a result, their relative deprivation will be reduced, holding their incomes constant. Conditions are provided under which the associated dynamics reaches a spatial steady state, the number of periods it takes to reach a steady state is specified, and light is shed on the robustness of the steady state outcome. By way of simulation it is shown that for large populations, a steady state of the relocation dynamics is almost always reached, typically in one period, and that cycles are more likely to occur when the populations’ income distributions are more equal.
C63|Age-Income Dynamics Over The Life Course: Cohort Transition Patterns In Relative Income Based On Canadian Tax Returns|This paper is concerned with patterns of cohort aging and income progression. We take a new approach to the characterization of relative income, explore the dynamics of age/income progression through the use of state transition matrices, consider alternative cohort definitions, and introduce an artificial cross-section cohort based on the transition matrices. Our applications make use of individual income records from the Statistics Canada Longitudinal Administrative Database. Relative income is defined by how an individual of a given age is positioned in the overall distribution of income in a given year. We derive the proportionate distribution of individuals in each decile group at each representative age, starting at 24, and the transition matrices then show the movements from the distribution at one age to the distribution five years later.
C63|Traders, forecasters and financial instability: A model of individual learning of anchor-and-adjustment heuristics|Behavioral and experimental literature on financial instability focuses on either subjective price expectations (Learning-to-Forecast experiments) or individual trading (Learning-to-Optimize experiments). Bao et al. (2017) have shown that subjects have problems with both tasks. In this paper, I explore these experimental results by investigating a model in which financial traders individually learn how to use forecasting and/or trading anchor-and-adjustment heuristics by updating them with Genetic Algorithms. The model replicates the main outcomes of these two threads of the experimental finance literature. It shows that both forecasters and traders coordinate on chasing asset price trends, which in turn causes substantial and self-fulfilling price oscillations, albeit larger and faster in the case of trading markets. When agents have to learn both tasks, financial instability becomes more persistent.
C63|Trend followers, contrarians and fundamentalists: Explaining the dynamics of financial markets|We propose an empirically motivated financial market model in which speculators rely on trend-following, contrarian and fundamental trading rules to determine their orders. Speculators' probabilistic rule-selection behavior - the only type of randomness in our model - depends on past and future performance indicators. For a large number of speculators, the model's intrinsic noise vanishes and its dynamics is driven by an analytically tractable nonlinear map. An in-depth investigation into this map provides the key to understanding how the model functions. Since our model is able to match a number of important stylized facts concerning financial markets, it may be regarded as validated.
C63|A flexible state-space model with lagged states and lagged dependent variables: Simulation smoothing|We provide a simulation smoother to a exible state-space model with lagged states and lagged dependent variables. Qian (2014) has introduced this state-space model and proposes a fast Kalman filter with time-varying state dimension in the presence of missing observations in the data. In this paper, we derive the corresponding Kalman smoother moments and propose an efficient simulation smoother, which relies on mean corrections for unconditional vectors. When applied to a factor model, the proposed simulation smoother for the states is efficient compared to other state-space models without lagged states and/or lagged dependent variables in terms of computing time.
C63|Beyond quantified ignorance: Rebuilding rationality without the bias bias|If we reassess the rationality question under the assumption that the uncertainty of the natural world is largely unquantifiable, where do we end up? In this article the author argues that we arrive at a statistical, normative, and cognitive theory of ecological rationality. The main casualty of this rebuilding process is optimality. Once we view optimality as a formal implication of quantified uncertainty rather than an ecologically meaningful objective, the rationality question shifts from being axiomatic/probabilistic in nature to being algorithmic/ predictive in nature. These distinct views on rationalitymirror fundamental and longstanding divisions in statistics.
C63|Job duration and inequality|As suggested by recent empirical evidence, one of the causes behind the widespread rise of inequality experienced by OECD countries in the last few decades may have been the increased flexibility of labor markets. The authors explore this hypothesis through the analysis of a stock-flow consistent agent-based macroeconomic model able to reproduce with good statistical precision several empirical regularities. To this scope they employ three different sensitivity analysis techniques, which indicate that increasing job contract duration (i.e. decreasing flexibility) has the effect of reducing income and wealth inequality. However, the authors also find that this effect is diminished by tight monetary policy and low credit supply. This result suggests that the final outcome of structural reforms aimed at changing labor flexibility can depend on the macroeconomic environment in which these are implemented.
C63|Arbeitsangebotsmodul zum IW-Mikrosimulationsmodell STATS: Dokumentation Version 1.0|Mit dem dargestellten Arbeitsangebotsmodul wird das Steuer-, Abgaben- und Transfer-Mikrosimulationsmodell des IW Köln um eine tragende Komponente erweitert, die die Simulation von Zweitrundeneffekten in Folge von Änderungen des deutschen Steuer- und Transfersystems auf unterschiedliche Zielgrößen ermöglicht. Am Beispiel einer (exogenen) Erhöhung der Bruttostundenlöhne von Männern und Frauen um jeweils 10 Prozent ließ sich zeigen, wie sich das Arbeitsangebot von unterschiedlichen Haushaltstypen anpasst. Die geschätzten unkompensierten Arbeitsangebotselastizitäten liegen dabei zwischen 0,02 und 0,08. Die Arbeitsangebotseffekte sind dabei für Single-Haushalte tendenziell größer als für Paarhaushalte und für Frauen größer als für Männer. Dabei sind dem Modell jedoch auch Grenzen gesetzt: So bezieht sich die Simulation von Arbeitsangebotseffekten ausschließlich auf abhängig Beschäftigte im erwerbsfähigen Alter zwischen 20 und 65 Jahren, die auf veränderte Arbeitsanreize auf dem Arbeitsmarkt reagieren können. Das Verhalten von Beamten, Rentnern oder Selbständigen wird aufgrund von andersartigen Arbeitsmarkt- und Beschäftigungsbedingungen in der derzeitigen Modellversion nicht abgebildet. Dies steht ebenfalls in Zusammenhang mit unterschiedlichen Präferenzen dieser Gruppen, die nicht oder nur schwer beobachtet werden können. Zudem unterliegt die Modellschätzung zum Teil strikten Annahme bezüglich des nutzenoptimalen Verhaltens der Haushalte und der darin lebenden Individuen: So kann beispielsweise die Annahme diskutiert werden, ob Paarhaushalte stets eine gemeinsame Nutzenfunktion maximieren - wie es angenommen wird - oder ob die Individuen nicht ihren eigenen Nutzen unter Berücksichtigung der Entscheidungen des Partners optimieren. Trotz dieser Einschränkungen stellen diskrete Arbeitsangebotsmodule bisweilen die beste und flexibelste Möglichkeit zur Modellierung von Arbeitsangebotsentscheidungen dar und sind ein wertvolles Instrument zur Evaluierung unterschiedlicher sozialpolitischer Reformvorhaben.
C63|"Das Modul ""Arbeitslosengeld"" als Element des IW-Mikrosimulationsmodells STATS: Version 1.0"|"Das Modul ""Arbeitslosengeld"" ist ein optionaler Baustein des Mikrosimulationsmodells des IW Köln (STATS). Es zielt darauf ab, die Modellierung einer Arbeitsangebotssimulation zu unterstützen, indem der Anspruch auf Arbeitslosengeld (ALG) abgebildet wird. Dazu werden die Ansprüche von ALG-Empfängern im Status Quo ebenso berücksichtigt wie auch die ALG-Ansprüche von sozialversicherungspflichtig Beschäftigten simuliert, wenn diese arbeitslos würden. Zum anderen können aber auch ad-hoc Analysen von bestimmten Reformoptionen des Arbeitslosengelds unabhängig von der Integration in das STATS durchgeführt werden, beispielsweise wenn die Auswirkungen von Änderungen der Rahmenfrist auf den Anspruch von verschiedenen Personengruppen auf ALG von Interesse sind und mögliche Angebotsreaktionen vernachlässigt werden. Um die Validität des Moduls zu überprüfen, werden ausgewählte Ergebnisse auf Basis des vorliegenden Moduls mit amtlichen Daten verglichen."
C63|Endogenous segregation dynamics and housing market interactions: An ABM approach|In contrast to previous research, I hypothesize that residential segregation patterns do not only result from an individual's perception of different ethnicities, but is rather affected by housing market interactions and socioeconomic endowment, like income and education. I implement a theoretical agent-based model, which contains three main features: agents' socioeconomic endowment, the quantification of one's Willingness-to-Stay within a neighborhood and housing market interactions if an agent decides to move. The results indicate that housing market interactions, the valuation of socioeconomic factors, but also the increasing share of minority groups diminish the absolute level of racial segregation. The analysis shows that house price clusters dominate urban areas, since individuals have an incentive to stay in more expensive neighborhoods in which they made a bargain. An increase in house price segregation can be observed if individuals strongly undervalue their own house and if individuals have higher access to credit. I can show that these market interactions lead to lock-in effects for low-income individuals, since they lack the necessary budget and suffer under negative equity. Thus, residential segregation shows a strong dependency on housing market interactions and is more complex than presumed by Schelling's Spatial Model or the White Flight Hypothesis.
C63|Potential economic effects of a global trade conflict: Projecting the medium-run effects with the WTO global trade model|The WTO Global Trade Model is employed to project the medium-run economic effects of a global trade conflict. The trade conflict scenario is based on recent estimates in the literature of the difference between cooperative and non-cooperative tariffs. The study provides three main insights. First, the projected macroeconomic effects in the medium run are considerable. A global trade conflict started in 2019 would lead to a reduction in global GDP in 2022 of about 1.96% and a reduction in global trade of about 17% compared to the baseline. For context global GDP fell about 2.1% and global trade 12.4% in the global financial crisis of 2009. Second, behind the single-digit aggregate production effects there are much larger, double-digit sectoral production effects in many countries, leading to a painful adjustment process. In general, a global trade conflict leads to a reallocation of resources away from the most efficient allocation based on comparative advantage. Third, the large swings in sectoral production lead to substantial labour displacement. On average 1.15% and 1.74% of high-skilled and low-skilled workers respectively would leave their initial sector of employment.
C63|Multi-battle contests, finite automata, and the tug-of-war|This paper examines multi-battle contests whose extensive form can be represented in terms of a finite state machine. We start by showing that any contest that satisfies our assumptions decomposes into two phases, a principal phase (in which states cannot be revisited) and a concluding tie-breaking phase (in which all non-terminal states can be revisited). Degenerate cases are the finite-horizon contests on the one hand (e.g., the match race), and the tug-of-war on the other. Next, assuming a probabilistic technology in each battle, we show that any contest satisfying our assumptions, with either finite or infinite horizon, admits a unique symmetric and interior Markov perfect equilibrium. This entails, in particular, a complete characterization of the equilibrium in the tug-of-war. Finally, we explore, both analytically and numerically, the intricate problem of a contest designer that maximizes expected total effort. In the absence of a complexity constraint, the revenue-maximizing contest is always a match race, where the optimal length of the race increases as the technology of the component contest becomes more noisy. If, however, the complexity constraint is binding, then the optimal contest is typically (but not always) a tug-of-war.
C63|Experimental Evidence on the Impact of Replacing the Incurred Credit Loss Model of Bank Loan Loss Provisions with the International or US Accounting Standards Boards’ Expected Credit Loss Models|Our objective is to test-bed the new Expected Credit Loss (ECL) and Current Expected Credit Loss (CECL) models for bank credit loss accounting to identify the potential consequences of their implementation. In particular, whether and how ECL and CECL approaches could lead to divergence in credit loss accounting practices in the U.S. relative to the rest of the world is an unanswered question. To do this, we develop a stylized bank-loan setting in a controlled laboratory environment with eight different secured personal-loan portfolios. Fifty-six senior accounting students take the role of loan managers responsible for making annual loan-loss reserve decisions in a between-subjects design under the rules of either the ECL or CECL models. We examine the effects of mandating the ECL or CECL model in terms of their impacts on the adequacy of loan-loss reserves, the comparability and predictability of loan-loss reserves and the volatility of reported profit.
C63|Using the Sequence-Space Jacobian to Solve and Estimate Heterogeneous-Agent Models|We propose a general and highly efficient method for solving and estimating general equilibrium heterogeneous-agent models with aggregate shocks in discrete time. Our approach relies on the rapid computation and composition of sequence-space Jacobians-the derivatives of perfect-foresight equilibrium mappings between aggregate sequences around the steady state. We provide a fast algorithm for computing Jacobians for heterogeneous agents, a technique to substantially reduce dimensionality, a rapid procedure for likelihood-based estimation, a determinacy condition for the sequence space, and a method to solve nonlinear perfect-foresight transitions. We apply our methods to three canonical heterogeneous-agent models: a neoclassical model, a New Keynesian model with one asset, and a New Keynesian model with two assets.
C63|Dynamic Social Interactions and Health Risk Behavior|We study risky behavior of adolescents. Concentrating on smoking and alcohol use, we structurally estimate a dynamic social interaction model in the context of students' school networks included in the National Longitudinal Study of Adolescent Health (Add Health). The model allows for forward-looking behavior of agents, addiction effects, and social interactions in the form of preferences for conformity in the social network. We find strong evidence for forward looking dynamics and addiction effects. We also find that social interactions in the estimated dynamic model are quantitatively large. A misspecified static model would fit data substantially worse, while producing a much smaller estimate of the social interaction effect. With the estimated dynamic model, a temporary shock to students' preferences in the 10th grade has effects on their behavior in grades 10, 11, 12, with estimated social multipliers 1.53, 1.03, and 0.76, respectively. The multiplier effect of a permanent shock is much larger, up to 3.7 in grade 12. Moreover, (semi-) elasticities of a permanent change in the availability of alcohol or cigarettes at home on child risky behavior implied by the dynamic equilibrium are 25%, 63%, and 79%, in grades 10, 11, 12, respectively.
C63|Financial Frictions and the Wealth Distribution|This paper investigates how, in a heterogeneous agents model with ?nancial frictions, idiosyncratic individual shocks interact with exogenous aggregate shocks to generate time-varying levels of leverage and endogenous aggregate risk. To do so, we show how such a model can be e?ciently computed, despite its substantial nonlinearities, using tools from machine learning. We also illustrate how the model can be structurally estimated with a likelihood function, using tools from inference with di?usions. We document, ?rst, the strong nonlinearities created by ?nancial frictions. Second, we report the existence of multiple stochastic steady states with properties that di?er from the deterministic steady state along important dimensions. Third, we illustrate how the generalized impulse re-sponse functions of the model are highly state-dependent. In particular, we ?nd that the recovery after a negative aggregate shock is more sluggish when the economy is more lever-aged. Fourth, we prove that wealth heterogeneity matters in this economy because of the asymmetric responses of household consumption decisions to aggregate shocks.
C63|Financial Frictions and the Wealth Distribution|This paper investigates how, in a heterogeneous agents model with financial frictions, idiosyncratic individual shocks interact with exogenous aggregate shocks to generate time-varying levels of leverage and endogenous aggregate risk. To do so, we show how such a model can be efficiently computed, despite its substantial nonlinearities, using tools from machine learning. We also illustrate how the model can be structurally estimated with a likelihood function, using tools from inference with diffusions. We document, first, the strong nonlinearities created by financial frictions. Second, we report the existence of multiple stochastic steady states with properties that differ from the deterministic steady state along important dimensions. Third, we illustrate how the generalized impulse response functions of the model are highly state-dependent. In particular, we find that the recovery after a negative aggregate shock is more sluggish when the economy is more leveraged. Fourth, we prove that wealth heterogeneity matters in this economy because of the asymmetric responses of household consumption decisions to aggregate shocks.
C63|Search Complementarities, Aggregate Fluctuations, and Fiscal Policy|We develop a quantitative business cycle model with search complementarities in the inter-?rm matching process that entails a multiplicity of equilibria. An active static equilibrium with strong joint venture formation, large output, and low unemployment can coexist with a passive static equilibrium with low joint venture formation, low output, and high unemployment. Changes in fundamentals move the system between the two static equilibria, generating large and persistent business cycle ?uctuations. The volatility of shocks is important for the selection and duration of each static equilibrium. Su?ciently adverse shocks in periods of low macroeconomic volatility trigger severe and protracted downturns. The magnitude of government intervention is critical to foster economic recovery in the passive static equilibrium, while it plays a limited role in the active static equilibrium.
C63|How BLUE is the Sky? Estimating the Air Quality Data in Beijing During the Blue Sky Day Period (2008-2012) by the Bayesian LSTM Approach|Over the last three decades, air pollution has become a major environmental challenge in many of the fast growing cities in China, including Beijing. Given that any long-term exposure to high-levels of air pollution has devastating health consequences, accurately monitoring and reporting air pollution information to the public is critical for ensuring public health and safety and facilitating rigorous air pollution and health-related scientific research. Recent statistical research examining China’s air quality data has posed questions regarding data accuracy, especially data reported during the Blue Sky Day (BSD) period (2000 – 2012), though the accuracy of publicly available air quality data in China has improved gradually over the recent years (2013 – 2017). To the best of our understanding, no attempt has been made to re-estimate the air quality data during the BSD period. In this paper, we put forward a machine-learning model to re-estimate the official air quality data during the BSD period of 2008 – 2012, based on the PM2.5 data of the Beijing US Embassy, and the proxy data covering Aerosol Optical Depth (AOD) and meteorology. Results have shown that the average re-estimated daily air quality values are respectively 64% and 61% higher than the official values, for air quality index (AQI) and AQI equivalent PM2.5, during the BSD period of 2008 to 2012. Moreover, the re-estimated BSD air quality data exhibit reduced statistical discontinuity and irregularity, based on our validation tests. The results suggest that the proposed data re-estimation methodology has the potential to provide more justifiable historical air quality data for evidence-based environmental decision-making in China.
C63|Buffering Volatility: Storage Investments and Technology-Specific Renewable Energy Support|Mitigating climate change will require integrating large amounts of highly intermittent renewable energy (RE) sources in future electricity markets. Considerable uncertainties exist about the cost and availability of future large-scale storage to alleviate the potential mismatch between demand and supply. This paper examines the suitability of regulatory (public policy) mechanisms for coping with the volatility induced by intermittent RE sources, using a numerical equilibrium model of a future wholesale electricity market. We find that the optimal RE subsidies are technology-specific reflecting the heterogeneous value for system integration. Differentiated RE subsidies reduce the curtailment of excess production, thereby preventing costly investments in energy storage. Using a simple cost-benefit framework, we show that a “smart” design of RE support policies significantly reduces the level of optimal storage. We further find that the marginal benefits of storage rapidly decrease for short-term (intra-day) storage and are small for long-term (seasonal) storage independent of the storage level. This suggests that storage is not likely to be the limiting factor for decarbonizing the electricity sector.
C63|Response surface regressions for critical value bounds and approximate p-values in equilibrium correction models|Single-equation conditional equilibrium correction models can be used to test for the existence of a level relationship among the variables of interest. The distributions of the respective test statistics are nonstandard under the null hypothesis of no such relationship and critical values need to be obtained with stochastic simulations. We compute more than 95 billion F -statistics and 57 billion t-statistics for a large number of specifications of the Pesaran, Shin, and Smith (2001, Journal of Applied Econometrics 16: 289Ð326) bounds test. Our large-scale simulations enable us to draw smooth density functions and to estimate response surface models that improve upon and substantially extend the set of available critical values for the bounds test. Besides covering the full range of possible sample sizes and lag orders, our approach notably allows for any number of variables in the long-run level relationship by exploiting the diminishing effect on the distributions of adding another variable to the model. The computation of approximate p-values enables a fine-grained statistical inference and allows us to quantify the finite-sample distortions from using asymptotic critical values. We find that the bounds test can be easily oversized by more than 5 percentage points in small samples.
C63|What Wavelet-Based Quantiles Can Suggest about the Stocks-Bond Interaction in the Emerging East Asian Economies?|This paper investigates bidirectional interdependence between 10Y bond yields and stock returns in the eight emerging East Asian economies. The method of choice is wavelet-based quantile approach, which can provide an answer about spillover effect in different market conditions and in different time horizons. We find that shock spillover effect is much more intense from the bond markets to the stock markets in all the selected economies, than vice-versa. Also, the nexus is dominantly positive in the more developed financial markets in both tranquil and crisis periods, particularly in the short and midterm horizons, which is an indication that capital reallocation takes place between these markets in a search for safer and more profitable investments. As for the less developed East Asian economies, we find negative quantile parameters in all quantiles and in all wavelet scales, which suggests that dividend discount model is a decisive factor that drives the stock-bond interdependence in all time horizons.
C63|Risk Management for Sovereign Debt Financing with Sustainability Conditions|We develop a model of debt sustainability analysis with optimal financing decisions in the presence of macroeconomic, financial and fiscal uncertainty. We define a coherent measure of refinancing risk, and trade off the risks of debt stock and flow dynamics, subject to debt sustainability constraints and endogenous risk and term premia. We optimize both static and dynamic financing strategies, compare them with several simple rules and consol financing to demonstrate economically significant effects of optimal financing, and show that the stock-flow tradeoff can be critical for sustainability. We quantify the minimum refinancing risk and the maximum rate of debt reduction that a sovereign can achieve given its economic fundamentals, and extend the model to identify optimal timing for debt flow adjustments that allow the sovereign to go beyond these limits. We put the model to the data on three real-world cases: a representative euro zone crisis country, a low-debt country (Netherlands) and a high-debt country (Italy). These applications illustrate the use of the model in informing diverse policy decisions on sustainable public finance. The model is part of the European Stability Mechanism toolkit to assess debt sustainability and repayment capacity of member states in the context of financial assistance.
C63|Optimal monetary policy regime switches|Given regime switches in the economy’s growth rate, optimal monetary policy rules may respond by switching policy parameters. These optimized parameters differ across regimes and from the optimal choice under fixed regimes, particularly in the inflation target and interest rate inertia. Optimal switching rules produce welfare gains relative to constant rules, with switches in the implicit real interest rate used for policy and the degree of interest rate inertia producing the largest gains. However, gains from switching rules decrease if the monetary authority trades-off the probability of low rates, or if it may misidentify the regime.
C63|Likelihood Evaluation of Models with Occasionally Binding Constraints|Applied researchers interested in estimating key parameters of DSGE models face an array of choices regarding numerical solution and estimation methods. We focus on the likelihood evaluation of models with occasionally binding constraints. We document how solution approximation errors and likelihood misspecification, related to the treatment of measurement errors, can interact and compound each other.
C63|A Generalized Approach to Indeterminacy in Linear Rational Expectations Models|We propose a novel approach to deal with the problem of indeterminacy in Linear Rational Expectations models. The method consists of augmenting the original state space with a set of auxiliary exogenous equations to provide the adequate number of explosive roots in presence of indeterminacy. The solution in this expanded state space, if it exists, is always determinate, and is identical to the indeterminate solution of the original model. The proposed approach accommodates determinacy and any degree of indeterminacy, and it can be implemented even when the boundaries of the determinacy region are unknown. Thus, the researcher can estimate the model using standard packages without restricting the estimates to the determinacy region. We apply our method to estimate the New-Keynesian model with rational bubbles by Galí (2017) over the period 1982:Q4 until 2007:Q3. We find that the data support the presence of two degrees of indeterminacy, implying that the central bank was not reacting strongly enough to the bubble component.
C63|Efficient Computation with Taste Shocks|Taste shocks result in nondegenerate choice probabilities, smooth policy functions, continuous demand correspondences, and reduced computational errors. They also cause significant computational cost when the number of choices is large. However, I show that, in many economic models, a numerically equivalent approximation may be obtained extremely efficiently. If the objective function has increasing differences (a condition closely tied to policy function monotonicity) or is concave in a discrete sense, the proposed algorithms are O(n log n) for n states and n choice--a drastic improvement over the naive algorithm's O(n2) cost. If both hold, the cost can be further reduced to O(n). Additionally, with increasing differences in two state variables, I propose an algorithm that in some cases is O(n2) even without concavity (in contrast to the O(n3) naive algorithm). I illustrate the usefulness of the proposed approach in an incomplete markets economy and a long-term sovereign debt model, the latter requiring taste shocks for convergence. For grid sizes of 500 points, the algorithms are up to 200 times faster than the naive approach.
C63|NIESIM: A Simulation-based Application for Estimating the Value of Information in Mobile Network Management|In this paper we introduce NIESIM (Network Information Economics SIMulation), a software for simulating a mobile communications scenario and studying the value of information in the context of mobile network management. The modelling principles and the simulation strategy used to design and develop NIESIM software are introduced, along with simulation results. We consider an application of NIESIM to support the definition of the grade of service in a network that is exposed to failures. An exploratory discussion of the findings and their implications to future work is also presented.
C63|Dura lex sed lex: why implementation gaps in environmental policy matter?|We investigate implementation gaps observed in environmental regulations in the specific case of dangerous chemical substances such as targeted by the REACH regulation. An agent-based model is developed as an exploratory tool to examine to what extent significant implementation gaps between stringency requirements and real but conditional enforcement jeopardize the transition to safer substitutes, by affecting the way heterogeneous actors perceive the regulatory threat and their innovation strategy. We show that the combination of the most severe regulation with the strictest enforcement and the shortest timing would not necessarily lead to the highest frequency of bans on dangerous substances, because it may alter the competitive process that is vital to preserving diversity in innovation strategies and to developing safer substitutes. Opting for a very severe regulation should be combined with concessions on enforcement in order to preserve diversity and to give green pioneering competitors enough time to expand. From a reverse angle, if authorities are keen to apply the regulation strictly and are prepared to face higher market concentration, then they should release the degree of stringency in order to enhance the prospects of transition to safer substitutes.
C63|Structural changes and growth regimes|Abstract We study the relation between income distribution and growth, mediated by structural changes on the demand and supply sides. Using the results from a multi-sector growth model, we compare two growth regimes that differ in three aspects: labour relations, competition and consumption patterns. Regime one, similar to Fordism, is assumed to be relatively less unequal, more competitive and to have more homogeneous consumers than regime two, which is similar to post-Fordism. We analyse the parameters that define the two regimes to study the role of the economy’s exogenous institutional features and endogenous structural features on output growth, income distribution, and their relation. We find that regime one exhibits significantly lower inequality, higher output and productivity and lower unemployment compared to regime two, and that both institutional and structural features explain these differences. Most prominent amongst the first group are wage differences, accompanied by capital income and the distribution of bonuses to top managers. The concentration of production magnifies the effect of wage differences on income distribution and output growth, suggesting the relevance of competition norms. Amongst structural determinants, firm organisation and the structure of demand are particularly relevant. The way that final demand is distributed across sectors influences competition and overall market concentration; demand from the least wealthy classes is especially important. We show also the tight linking between institutional and structural determinants. Based on this linking, we conclude by discussing a number of policy implications that emerge from our model.
C63|Measuring the External Stability of the One-to-One Matching Generated by the Deferred Acceptance Algorithm|In this paper, we consider a one-to-one matching model where the population expands with the arrival of a man and a woman. Individuals in this population are matched, before and after the expansion, according to a version of the deferred acceptance algorithm (Gale and Shapley, 1962) where men propose and women reject or (tentatively or permanently) accept. Using computer simulations of this model, we study how the percentage of matches disrupted (undisrupted) with the expansion of the population is affected when the initial size of the population and the size of correlation in the preferences of individuals change.
C63|The Success of the Deferred Acceptance Algorithm under Heterogenous Preferences with Endogenous Aspirations|In this paper, we consider a one-to-one matching model with two phases; an adolescence phase where individuals meet a number of dates and learn about their aspirations, followed by a matching phase where individuals are matched according to a version of Gale and Shapley's (1962) deferred acceptance (DA) algorithm. Using simulations of this model, we study how the likelihoods of matching and divorce, and also the balancedness and the speed of matching associated with the outcome of the DA algorithm are affected by the size of correlation in the preferences of individuals and by the frequency individuals update their aspirations in the adolescence phase.
C63|Modeling the Internal Revenue Code in a heterogeneous-agent framework: An application to TCJA|Macroeconomic models used for tax policy analysis often simultaneously abstract from two features of the US tax code: special tax treatment for preferential capital income, and the joint tax treatment of ordinary capital and labor income. In this paper, we explore the extent to which explicitly accounting for these tax details has macroeconomic implications within a heterogeneous-agent model. We do this by expanding the Moore and Pecoraro (2018) overlapping generations model to include distinct corporate and non-corporate firms so that the business income distributed to households can be separated into ordinary and preferred capital income. Household income tax treatment is then determined by an internal tax calculator that fully accounts for interaction among income bases while conditioning on idiosyncratic household characteristics. Relative to a conventional approach where household income taxation is determined by independent labor and capital income tax functions that do not distinguish between ordinary and preferred capital income, we find that our innovations have implications for household behavior and economic aggregates - especially the tax consequences of changes to the returns to labor and capital - when analyzing a subset of tax provisions from the recently enacted “Tax Cuts and Jobs Act”. Our findings imply that the abstracting from tax detail may come at the expense of correctly accounting for incentives and estimating macroeconomic responses to tax policy changes.
C63|Analysis of Sustainable Procurement in SMEs in Developing Countries|The purpose of the paper is to integrate supply base consolidation, rationalization, and buyer’s perspective about its suppliers to reveal more insight to implement sustainable procurement in small and medium enterprises (SMEs) in developing countries like India. In this paper an attempt has been made to integrate Constrained Optimization of Frobenius Norm by Genetic Algorithm (COFGA) with traditional spend, and value risk analysis to consolidate and rationalize supply base w.r.t fifteen triple bottom line indicators (TBL). This paper shows that spend analysis is justified in crisp domain and becomes myopic in limited data environment. Spend analysis becomes more ineffective to deal imprecise and vague qualitative data. Integrated approach of multiple criteria decision analysis,spend analysis, and value risk analysis, thus, an alternative approach to give better insight to sustainable procurement in fuzzy environment. Finally, a case study is discussed to use proposed method.
C63|Resolutions to flip-over credit risk and beyond|Abstract Given a risk outcome y over a rating system {R_i }_(i=1)^k for a portfolio, we show in this paper that the maximum likelihood estimates with monotonic constraints, when y is binary (the Bernoulli likelihood) or takes values in the interval 0≤y≤1 (the quasi-Bernoulli likelihood), are each given by the average of the observed outcomes for some consecutive rating indexes. These estimates are in average equal to the sample average risk over the portfolio and coincide with the estimates by least squares with the same monotonic constraints. These results are the exact solution of the corresponding constrained optimization. A non-parametric algorithm for the exact solution is proposed. For the least squares estimates, this algorithm is compared with “pool adjacent violators” algorithm for isotonic regression. The proposed approaches provide a resolution to flip-over credit risk and a tool to determine the fair risk scales over a rating system.
C63|Monotonic Estimation for the Survival Probability over a Risk-Rated Portfolio by Discrete-Time Hazard Rate Models|Monotonic estimation for the survival probability of a loan in a risk-rated portfolio is based on the observation arising, for example, from loan pricing that a loan with a lower credit risk rating is more likely to survive than a loan with a higher credit risk rating, given the same additional risk covariates. Two probit-type discrete-time hazard rate models that generate monotonic survival probabilities are proposed in this paper. The first model calculates the discrete-time hazard rate conditional on systematic risk factors. As for the Cox proportion hazard rate model, the model formulates the discrete-time hazard rate by including a baseline component. This baseline component can be estimated outside the model in the absence of model covariates using the long-run average discrete-time hazard rate. This results in a significant reduction in the number of parameters to be otherwise estimated inside the model. The second model is a general form model where loan level factors can be included. Parameter estimation algorithms are also proposed. The models and algorithms proposed in this paper can be used for loan pricing, stress testing, expected credit loss estimation, and modeling of the probability of default term structure.
C63|Monotonic Estimation for Probability Distribution and Multivariate Risk Scales by Constrained Minimum Generalized Cross-Entropy|Minimum cross-entropy estimation is an extension to the maximum likelihood estimation for multinomial probabilities. Given a probability distribution {r_i }_(i=1)^k, we show in this paper that the monotonic estimates {p_i }_(i=1)^k for the probability distribution by minimum cross-entropy are each given by the simple average of the given distribution values over some consecutive indexes. Results extend to the monotonic estimation for multivariate outcomes by generalized cross-entropy. These estimates are the exact solution for the corresponding constrained optimization and coincide with the monotonic estimates by least squares. A non-parametric algorithm for the exact solution is proposed. The algorithm is compared to the “pool adjacent violators” algorithm in least squares case for the isotonic regression problem. Applications to monotonic estimation of migration matrices and risk scales for multivariate outcomes are discussed.
C63|Addressing urban sprawl from the complexity sciences|Urban sprawl is nowadays a pervasive topic that is subject of a contentious debate among planners and researchers, who still fail to reach consensual solutions. This paper reviews controversies of the sprawl debate and argues that they owe to a failure of the employed methods to appraise its complexity, especially the notion that urban form emerges from multiple overlapping interactions between households, firms and governmental bodies. To address such issues, this review focuses on recent approaches to study urban spatial dynamics. Firstly, spatial metrics from landscape ecology provide means of quantifying urban sprawl in terms of increasing fragmentation and diversity of land use patches. Secondly, cellular automata and agent-based models suggest that the prevalence of urban sprawl and fragmentation at the urban fringe emerge from negative spatial interaction between residential agents, which seem accentuated as the agent’s preferences become more heterogeneous. Then, the review turns to practical applications that employ such models to spatially inform urban planning and assess future scenarios. A concluding discussion summarizes potential contributions to the debate on urban sprawl as well as some epistemological implications.
C63|Decomposition of intra-household disparity sensitive fuzzy multi-dimensional poverty index: A study of vulnerability through Machine Learning|The traditional multi-dimensional measures have failed to properly project the vulnerability of human-beings towards poverty. Some of the reasons behind this inability may be the failure of the existing measures to recognise the graduality inside the concept of poverty and the disparities within the household in wealth distribution. So this work wants to develop a measure to estimate the vulnerability of households in becoming poor in a multidimensional perspective through incorporating the intra-household disparities and graduality within the causal factors. Dimensional decomposition of the developed vulnerability measure is also under the purview of this work. To estimate the vulnerability and dimensional influences with the help of artificial intelligence an integrated mathematical framework is developed.
C63|A probabilistic interpretation of the constant gain algorithm|This paper proposes a novel interpretation of the constant gain learning algorithm through a probabilistic setting with Bayesian updating. Such framework allows to understand the gain coefficient in terms of the probability of changes in the estimated quantity.
C63|An economy under the digital transformation|During the last twenty years, we have witnessed the deep development of digital technologies. Artificial intelligence, software and algorithms have started to impact more and more frequently in our daily lives and most people didn't notice it. Recently, economists seem to have perceived that this new technological wave could have some consequences, but which one are they? Will they be positive or negative? In this paper we try to give a possible answer to these questions through an agent based computational approach; more specifically we enriched the large-scale macroeconomics model EURACE with the concept of digital technologies in order to investigate the effect that their business dynamics have at a macroeconomic level. Our preliminary results show that this productivity increase could be a double-edged sword: notwithstanding the development of the digital technologies sector can create new job opportunities, at the same time, these products could jeopardize the employment inside the traditional mass-production system.
C63|Investigating on Hydrodynamic Behavior of Slotted Breakwater Walls Under Sea Waves|Breakwater walls are buildings that are built to prevent the collapse of the soil or other granular materials and the safety of the sea. One of the destructive phenomena in these structures is the impact of sea wave forces on the overturning phenomenon and instability of the coastal wall, which has damaged the structures existing on these sites. The pattern of interaction between water and seas is complex in coastal structures. In this research, the influence of the different wall heights and soil type changes on wall stability and water pressure distribution in the coastal wall have been investigated. Also, studies will be done on the investigation and optimization of the wall and Finally, by comparing the results obtained with classical methods, the strengths and weaknesses of the classical methods have been analyzed and the effectiveness of these methods (classical) has been evaluated. These walls are made in two types of weighted and flexible (mainly metal) types, in which flexible performance is considered in this research. The behavior of metal shields in front of the water will be examined using the ANSYS software. Several methods for calculating wave forces on perforated coastal walls are also reviewed. In this study, the behavior of the elastic wall is assumed. Coastal walls have been investigated in different hardships and the distribution of pressure and anchor due to hydrodynamic pressure of water on the wall have been investigated. The walls are different in terms of material and amount of rigidity.
C63|A simulation of the insurance industry: The problem of risk model homogeneity|We develop an agent-based simulation of the catastrophe insurance and reinsurance industry and use it to study the problem of risk model homogeneity. The model simulates the balance sheets of insurance firms, who collect premiums from clients in return for ensuring them against intermittent, heavy-tailed risks. Firms manage their capital and pay dividends to their investors, and use either reinsurance contracts or cat bonds to hedge their tail risk. The model generates plausible time series of profits and losses and recovers stylized facts, such as the insurance cycle and the emergence of asymmetric, long tailed firm size distributions. We use the model to investigate the problem of risk model homogeneity. Under Solvency II, insurance companies are required to use only certified risk models. This has led to a situation in which only a few firms provide risk models, creating a systemic fragility to the errors in these models. We demonstrate that using too few models increases the risk of nonpayment and default while lowering profits for the industry as a whole. The presence of the reinsurance industry ameliorates the problem but does not remove it. Our results suggest that it would be valuable for regulators to incentivize model diversity. The framework we develop here provides a first step toward a simulation model of the insurance industry for testing policies and strategies for better capital management.
C63|Rating firms and sensitivity analysis|This paper introduces a model for rating a firm's default risk based on fuzzy logic and expert system and an associated model of sensitivity analysis (SA) for managerial purposes. The rating model automatically replicates the evaluation process of default risk performed by human experts. It makes use of a modular approach based on rules blocks and conditional implications. The SA model investigates the change in the firm's default risk under changes in the model inputs and employs recent results in the engineering literature of Sensitivity Analysis. In particular, it (i) allows the decomposition of the historical variation of default risk, (ii) identifies the most relevant parameters for the risk variation, and (iii) suggests managerial actions to be undertaken for improving the firm's rating.
C63|Numerical Study of the Gap at the Base of the Bridge on the River Flow Parameters|In this study, substrate surface channel and bridge piers is modeled by utilization of Ansys Fluent. Continuity and Momentum equations is solved and for the pressure-velocity linkage SIMPLE algorithm is utilized. In order to model turbulence due to relatively high Reynolds value, K-ε is being used. Effect of wall shear stress, velocity magnitude and static pressure is being investigated at different heights thorough substrate longitudinal lines. It was found that different heights of longitudinal lines won’t affect static pressure, while it affects velocity magnitude and wall shear stress significantly.
C63|A Generalized Endogenous Grid Method for Models with the Option to Default|We develop an endogenous grid method for models with the option to default in which price schedules are endogenously determined in equilibrium and depend on individuals’ states. The algorithm has noticeable computational benefits in efficiency and accuracy. We obtain these computational benefits by combining Fella’s (2014) identification for non-concave regions with our algorithm that numerically searches for risky borrowing limits. These two procedures identify the region of solution sets to which Carroll’s (2006) endogenous grid method is applicable. To demonstrate the method, we apply our method to Nakajima and Rios-Rull’s(2014) model. In terms of computation time, this method is seven to twenty-seven times faster than the conventional grid search method. Moreover, various types of accuracy tests indicate that our method yields more accurate results than the grid search method.
C63|Stochastic Structural Change|We propose a tractable algorithm to solve stochastic growth models of structural change. Under general conditions, structural change implies an unbalanced growth path. This property prevents the use of local solution techniques when uncertainty is introduced, and requires the adoption of global methods. Our algorithm relies on the Parameterized Expectations Approximation and we apply it to a stochastic version of a three-sector structural transformation growth model with Stone-Geary preferences. We use the calibrated solution to show that in this class of models there exists a tension between the long- and the short-run properties of the economy. This tension is due to the non-homothetic components of the various types of consumption, which are needed to fit long-run structural change, but imply a counterfactually high volatility of services, and counterfactually low volatilities of manufacturing and agriculture in the short-run.
C63|Rise and fall of calendar anomalies over a century|In this paper, we conduct a comprehensive investigation of calendar anomaly evolution in the US stock market (given by the Dow Jones Industrial Average) for the 1900–2018 period. We employ various statistical techniques (average analysis, Student’s t-test, ANOVA, the Kruskal-Wallis and Mann-Whitney tests, modified cumulative abnormal returns approach), R/S analysis, and the trading simulation approach to analyse the evolution of the following calendar anomalies: day of the week effect, turn of the month effect, turn of the year effect, and the holiday effect. The results revealed that ‘golden age’ of calendar anomalies was in the middle of the 20th century. However, since the 1980s all calendar anomalies disappeared. This is consistent with the Efficient Market Hypothesis.
C63|Halloween Effect in Developed Stock Markets: A US Perspective|In this paper, we conduct a comprehensive investigation of the Halloween effect evolution in the US stock market over its entire history. We employ various statistical techniques (average analysis, Student’s t-test, ANOVA, and the Mann-Whitney test) and the trading simulation approach to analyse the evolution of the Halloween effect. The results suggest that in the US stock market the Halloween effect became more persistent since the middle of the 20th century. Despite the decline in its prevalence since that time, nowadays it is still present in the US stock market and provides opportunities to build a trading strategy which can beat the market. These results are well in line with other developed stock markets. Therefore, in the main, our results are inconsistent with the Efficient Market Hypothesis.
C63|Historical Evolution of Monthly Anomalies in International Stock Markets|This paper is a comprehensive investigation of the evolution of various monthly anomalies (January effect, December effect, and the Mark Twain effect) in the US stock market for its entire history. This is done using various statistical techniques (average analysis, Student’s t-test, ANOVA, the Mann-Whitney test) and a trading simulation approach). To confirm our results we extended the analysis to the UK, Japan, Canada, France, Switzerland, Germany and Italy stock markets. The results indicate that the January effect was most prevalent in the US and that the December effect and the Mark Twain effect were never prevalent in the US. This result was confirmed in other markets as well. The January effect was most prevalent in the middle of the 20th century but has since disappeared. Furthermore, the January effect provided exploitable profit opportunities. Our results are consistent and add to the existing literature through the use of a complete history of the US market. Overall, the US stock market is consistent with the Adaptive Market Hypothesis.
C63|Price Gap Anomaly in the US Stock Market: The Whole Story|This paper analyses the price gap anomaly in the US stock market (comprised of the DJI, S&P 500 and NASDAQ) covering the period 1928 to 2018. This paper aims to investigate whether or not price gaps create market inefficiencies. Price gaps occur when the current day’s opening price is different from the previous day’s closing price due orders placed before the opening of the market. Several hypotheses are tested using various statistical tests (Student’s t-test, ANOVA, Mann-Whitney test), regression analysis, and special methods, that is, the modified cumulative returns and the trading simulation approaches. We find strong evidence in favour of abnormal price movements after price gaps. We observe that during a gap day prices tend to change in the direction of the gap. A trading strategy based on this anomaly was efficient in that its results were not random, indicating that this market was not efficient. The momentum effect was found to be temporary and no evidence of seasonality in price gaps was found. Lastly, our results were also contrary to the myth that price gaps tend to get filled.
C63|Mapping Thailand's Financial Landscape: A Perspective through Balance Sheet Linkages and Contagion|This paper conducts in-depth profiling of players and interlinkages in the Thai financial system based on sectoral balance sheet data and disaggregated supervisory data on banks and mutual funds. Several aspects of Thailand ' s financial landscape have been documented. We find that financial interconnectedness has risen and become more complex, with the financial landscape increasingly tilted toward non-bank intermediaries. Network topology suggests a segmented landscape, with the presence of a core cluster where key players including households, firms, large domestic banks, and mutual funds of large banks' asset management arms are located, indicating their tight interconnections. Leveraging on entity-level balance sheet profiles, we develop a stress-testing framework that is based on a network model of financial contagion. Two types of shocks are studied. For industry shocks, we find that losses generally propagate via the liability and ownership channel and the reverse liquidity channel. But when the losses are large enough, the fire-sale effects dominate. For bank reputational shocks, we simulate a loss of confidence in major banks via deposit withdrawal and fund redemption. While the overall losses are much smaller than those of industry shocks, these risks cannot be ignored since the mutual fund industry stands to suffer and panic selling could amplify the losses.
C63|Search Complementarities, Aggregate Fluctuations and Fiscal Policy|This paper develops a model with search complementarities in the inter-firm matching process that entails two steady-state equilibria. An active equilibrium with strong partnership formation, large production, and low unemployment and a passive equilibrium with low partnership formation, low production, and high unemployment. Changes in fundamentals move the system between the two equilibria, generating large and persistent business cycle fluctuations. Suciently adverse shocks in periods of low macroeconomic volatility trigger severe and protracted downturns. The magnitude of government intervention is critical to foster economic recovery in the passive equilibrium while it plays a limited role in the active equilibrium. Fiscal policy has a non-monotonic effect on output in the passive equilibrium and the scal multiplier depends on the interplay between macroeconomic volatility and the magnitude of government spending.
C63|The Shift in Global Crude Oil Market Structure: A model-based analysis of the period 2013–2017|The global crude oil market has gone through two important phases over the recent years. The ﬁrst one was the price collapse that started in the third quarter of 2014 and continued until mid-2016. The second phase occurred in late 2016, after major producers within and outside OPEC agreed to cut production in order to adjust the ongoing fall in oil prices, which is now known as the OPEC+ agreement. This paper analyzes the eﬀects of these recent developments on the market structure and on the behavior of major producers in the market. To this end, we develop a partial equilibrium model with a spatial structure for the global crude oil market and simulate the market for the period between 2013 and 2017 under oligopolistic, cartel and perfectly competitive market structure setups. The simulation results reveal that, although the oligopolistic market structures ﬁt overall well to the realized market outcomes, they are not successful at explaining the low prices during 2015 and 2016, which instead are closer to estimated competitive levels. Moreover, we further suggest that from 2014 onward, the market power potential of major suppliers has shrunk considerably, supporting the view that the market has become more competitive. We also analyze the Saudi Arabia- and Russia-led OPEC+ agreement, and ﬁnd that planned production cuts in 2017, particularly of Saudi Arabia (486 thousand barrels/day) and Russia (300 thousand barrels/day), were below the levels of estimated non-competitive market structure setups. This explains why the oil prices did not recover to pre-2014 levels although a temporary adjustment was observed in 2017.
C63|Developing an Adaptive Chinese Near-Synonym Corpus for Word of Mouth Classification|Word of mouth (WOM) is the subjective opinion of consumers for a brand, a product or a service. Its impact on consumer?s purchasing decision is greater than the marketing activities of a product. Word of mouth classification is an effective means for document organization in an era of big data. However, the existing tasks of WOM classification are mainly dependent on the bag of word (BOW) in the vector space model (VSM), which usually suffers from the curse of dimensionality while dealing with large amounts of documents. We compared characters, context, and homophones, and integrated thesauruses to establish an adaptable Chinese near-synonym corpus. Subsequently, lexical replacement was applied, and the adaptable Chinese near-synonym corpus was created for classifying WOM documents. Two static corpora, the Ministry of Education?s Revised Mandarin Chinese Dictionary and the Extended Chinese Synonym Forest, were used as the benchmarks of comparison for the proposed adaptable near-synonym corpus in the classification and evaluation stage. Evaluations were conducted by calculating recall, precision, F-measure, accuracy, and area under receiver operating characteristic curves (AUC). The results indicate that the classification accuracy of the adaptable near-synonym corpus proposed in the research exceeds that of static corpora when used in the fields of movie, leisure and travel, food, and cosmetics.
C63|Employment of advanced approach to control inventory level by monitoring Safety Stock in Supply Chain under Uncertain environment|In order to overcome uncertainty situation and inability to meet with customers' demand due to uncertainty, the organizations tend to keep a certain safety stock level. In this paper, the researcher used soft computing to identify optimal safety stock level (SSL), the fuzzy model uses dynamic concept to cope with high complexity environment status and control the inventory. The proposed approach deals with demand stability level, raw material availability level, and on hand inventory level by using fuzzy logic to obtain SSL. In this approach, demand stability, raw material, and on hand inventory are described linguistically and treated by inference rules of fuzzy model to extract best level of safety stock. The numerical dairy industry case study was applied with yogurt 200 gm cup product.
C63|The IT-value stream model for Hospital Networks|"Hospitals, especially in regional settings, often struggle to fulfil all medical demands placed on them due their size and capacity. Thus, it is helpful to cooperate with other hospitals, perhaps in the neighbourhood as a network to exchange resources such as technical equipment or medi-cal staff such as qualified nurses who are in high demand. To solve this problem we can trans-fer some theoretical aspects from other similar situations such as network cooperation in con-nected industry-companies. This only works by using modern Information Technology based on internet technology and connected IT-Processes. In Germany, the government has implemented an extensive scientific and industrial project called ?Industry 4.0?.The idea behind ""Industry 4.0"" is that all industry processes use digital methods, which are connected but decentralized.Through the use of Smart technology (computer, smartphone or personal digital assistant - PDA), a Digital Twin representing a complete digital footprint of all units, products and re-sources which are not digital (i.e. human employees) can be created and all digital processes work together (cloud based) to solve problems in real time.This idea of ""Industry 4.0"" is useful for Hospital Networks, which often have similar structures. To develop Industry 4.0 for Hospital Networks, we need a special modelling method. An exten-sion of the value stream model (the IT-value stream model) can fulfil this task. In this publica-tion, we demonstrate how an extension of the value stream model can solve the resource prob-lems in Hospital Networks."
C63|An algorithm for construction of a portfolio with a fundamental criterion|The classical models for construction of investment portfolio do not take into account fundamental values of considered companies. In our approach we extend the portfolio choice by adding this dimension to the classical criteria of profitability and risk. It is assumed that an investor selects stock according to their attractiveness, measured by some fundamental values of companies. In this approach portfolios are assessed according to three criteria: their profitability, risk (measured by variance of returns) and fundamental value (measured by some indicators of fundamental value). In this article we consider earnings to price ratio as the measure of the fundamental value of a company. In the paper we consider an algorithm for constructing portfolios with fundamental criterion based on analytical solutions for appropriate optimization problems. In the optimization problem we consider minimizing variance with constrains on expected return and attractiveness of investment, measured with some indicators of fundamental values of companies in a portfolio. We also present empirical examples of calculating effective portfolios of stocks listed on the Warsaw Stock Exchange.
C63|Teaching computer programming for industrial engineering without teacher|"The C programming language is widely used in computer and industrial engineering. Because of that, such programming language is also widely used as a language to teach programming to industrial engineering students. In Spain, many universities use this language compulsory in the first year, or even in higher courses. Our experience shows that learning computer programming in four months is an arduous task, but curricula require it. Such learning process is also tough by the fact that many students can not attend classes regularly and, even if they attend, sometimes the class is no longer at the level they require. In this work we develop a series of files in ""presentation"" format (.ppsx) that allows students to see several explanations about the most complicated programming C topics: functions, arrays, structures, strings, arrays with structures... This multimedia material includes explanations (voice-over), and animations with examples. Students can watch and listen to the explanations whenever they want and wherever they want (tablet, PC, phone?). Surveys made to students reveal that it is also interesting for students who regularly attend classes, and they prefer to use this course material only at home, outside of regular classes."
C63|Spatial Distribution of Logistics Services in Brazil: a Potential Market Analysis|The main objective of this work is to investigate the present and long-term spatial distribution of the logistics sector in Brazil. In order to do so, this study seeks to answer the following questions: i) how is the logistics sector organized in Brazil? that is, is there evidence of logistics clusters?; ii) what is the long-term perspective of this organization? The Logistics Potential Mapping Model (MapLog), inspired by Krugman's Core-Periphery Model, will serve as an analytical tool to verify the long-term spatialization of the logistics sector. The results point to a change within five decades (2015-2065) of the locational pattern of the logistics sector focused on industry but not for the logistics sector focused on agriculture.
C63|Deep Haar Scattering Networks in Unidimensional Pattern Recognition Problems|The aim of this paper is to discuss the use of Haar scattering networks, which is a very simple architecture that naturally supports a large number of stacked layers, yet with very few parameters, in a relatively broad set of pattern recognition problems, including regression and classification tasks. This architecture, basically, consists of stacking convolutional filters, that can be thought as a generalization of Haar wavelets, followed by nonlinear operators which aim to extract symmetries and invariances that are later fed in a classification/regression algorithm. We show that good results can be obtained with the proposed method for both kind of tasks. We outperformed the best available algorithms in 4 out of 18 important data classification problems, and obtained a more robust performance than ARIMA and ETS time series methods in regression problems for data with invariances and symmetries, with desirable features, such as possibility to evaluate parameter stability and easy structural assessment.
C63|Macroeconomic Impacts of Trade Credit: An Agent-Based Modeling Exploration|This paper explores the effects of trade credit by assessing its macroeconomic impacts on several dimensions. To that end, we develop an agent-based model (ABM) with two types of firms: downstream firms, which produce a final good for consumption purposes using intermediate goods, and upstream firms, which produce and supply those intermediate goods to the downstream firms. Upstream firms can act as trade credit suppliers, by allowing delayed payment of a share of their sales to downstream firms. Our results suggest a potential trade-off between financial robustness as measured by the proportion of non-performing loans and the average output level. The intuitive reason is that greater availability of trade credit, which however does not necessarily imply proportionately greater actual use of it by downstream firms, allows more financial resources to remain in the real sector, favoring the latterâ€™s financial robustness. Yet, given that trade credit is proportionally more beneficial to smaller downstream firms, it enhances market competition. This results in a decrease in markups and thereby in profits and dividends, which contributes negatively to aggregate demand formation
C63|Asset Dynamics, Liquidity, And Inequality In Decentralized Markets|The Kiyotaki and Wright model has exerted a considerable influence on the monetary search literature. We argue that the model also delivers important insights into a broader range of macroeconomic and development issues. The analysis studies how market frictions and the liquidity of assets affect the distribution of income. Experiments illustrate how the economy adjusts to shocks to asset returns and to the matching technology. They also deal with long‐run transition. An experiment interprets the reversal of fortune hypothesis as a situation in which an economy with a low‐return asset takes over a similar economy with a high‐return asset. (JEL C61, C63, E41, E27, D63)
C63|Transformed Perturbation Solutions for Dynamic Stochastic General Equilibrium Models|This paper introduces a new solution method for Dynamic Stochastic General Equilibrium (DSGE) models that produces non explosive paths. The proposed solution method is as fast as standard perturbation methods and can be easily implemented in existing software packages like Dynare as it is obtained directly as a transformation of existing perturbation solutions proposed by Judd and Guu (1997) and Schmitt-Grohe and Uribe (2004), among others. The transformed perturbation method shares the same advantageous function approximation properties as standard higher order perturbation methods and, in contrast to those methods, generates stable sample paths that are stationary, geometrically ergodic and absolutely regular. Additionally, moments are shown to be bounded. The method is an alternative to the pruning method as proposed in Kim et al. (2008). The advantages of our approach are that, unlike pruning, it does not need to sacrifice accuracy around the steady state by ignoring higher order effects and it delivers a policy function. Moreover, the newly proposed solution is always more accurate globally than standard perturbation methods. We demonstrate the superior accuracy of our method in a range of examples.
C63|Copula Multivariate GARCH Model with Constrained Hamiltonian Monte Carlo|The Copula Multivariate GARCH (CMGARCH) model is based on a dynamic copula function with time-varying parameters. It is particularly suited for modelling dynamic dependence of non-elliptically distributed financial returns series. The model allows for capturing more flexible dependence patterns than a multivariate GARCH model and also generalizes static copula dependence models. Nonetheless, the model is subject to a number of parameter constraints that ensure positivity of variances and covariance stationarity of the modeled stochastic processes. As such, the resulting distribution of parameters of interest is highly irregular, characterized by skewness, asymmetry, and truncation, hindering the applicability and accuracy of asymptotic inference. In this paper, we propose Bayesian analysis of the CMGARCH model based on Constrained Hamiltonian Monte Carlo (CHMC), which has been shown in other contexts to yield efficient inference on complicated constrained dependence structures. In the CMGARCH context, we contrast CHMC with traditional random-walk sampling used in the previous literature and highlight the benefits of CHMC for applied researchers. We estimate the posterior mean, median and Bayesian confidence intervals for the coefficients of tail dependence. The analysis is performed in an application to a recent portfolio of S&P500 financial asset returns.
C63|Static use of options in dynamic portfolio optimization under transaction costs and solvency constraints|We study a dynamic portfolio optimization problem where it is possible to invest in a risk-free bond, in a risky stock modeled by a lognormal diffusion and in call options written on the stock. The use of the options is limited to static strategies at the beginning of the investment period. The investor faces transaction costs with a fixed component and solvency constraints and the objective is to maximize the expected utility of the final wealth. We characterize the value function as a constrained viscosity solution of the associated quasi-variational inequality and we prove the local uniform convergence of a Markov chain approximation scheme to compute numerically the optimal solution. Because of transaction costs and solvency constraints the options cannot be pefectly replicated and despite the restriction to static policies our numerical results show that in most cases the investor will keep a significant part of his portfolio invested in options.
C63|Macroeconomic Models with Incomplete Information and Endogenous Signals|This paper characterizes a general class of macroeconomic models with incomplete information, when the information process includes endogenous variables. I derive conditions for existence and uniqueness of equilibrium, which apply even when the model contains endogenous state variables, and I introduce an algorithm to solve the general model. As an application I consider a business cycle model with capital where firms must make inferences about aggregate shocks through the movements of endogenous prices. In this model, the central bank's policy rule determines the real effects of nominal shocks, by controlling how informative prices are about the aggregate state. The optimal policy targets acyclical inflation, which makes money neutral. Finally, I demonstrate an advantage of models with endogenous information: the noisy signals are driven by fundamental shocks, rather than ad hoc noise, so data can discipline the information structure. Accordingly, I calibrate the model using US industry-level panel data.
C63|Macroeconomic simulation comparison with a multivariate extension of the Markov Information Criterion|Comparison of macroeconomic simulation models, particularly agent-based models (ABMs), with more traditional approaches such as VAR and DSGE models has long been identified as an important yet problematic issue in the literature. This is due to the fact that many such simulations have been developed following the great recession with a clear aim to inform policy, yet the methodological tools required for validating these models on empirical data are still in their infancy. The paper aims to address this issue by developing and testing a comparison framework for macroeconomic simulation models based on a multivariate extension of the Markov Information Criterion (MIC) originally developed in Barde (2017). The MIC is designed to measure the informational distance between a set of models and some empirical data by mapping the simulated data to the markov transition matrix of the underlying data generating process, and is proven to perform optimally (i.e. the measurement is unbiased in expectation) for all models reducible to a markov process. As a result, not only can the MIC provide an accurate measure of distance solely on the basis of simulated data, but it can do it for a very wide class of data generating processes. The paper first presents the strategies adopted to address the computational challenges that arise from extending the methodology to multivariate settings and validates the extension on VAR and DGSE models. The paper then carries out a comparison of the benchmark ABM of Caiani et al. (2016) and the DGSE framework of Smets and Wouters (2007), which to our knowledge, is the first direct comparison between a macroeconomic ABM and a DGSE model.
C63|Structural Transformations and Cumulative Causation: Towards an Evolutionary Micro-foundation of the Kaldorian Growth Model|We build upon the evolutionary model developed in prior works (Ciarli, Lorentz, Savona and Valente 2010b), which formalises the links between production, organisation and functional composition of the employment on the supply side and the endogenous evolution of consumption patterns on the demand side. The main contribution resulting from the exercise proposed here is to derive the Kaldorian cumulative causation mechanism as an emergent property of the dynamics generated by the micro-founded model. More precisely, we discuss the main transition dynamics to a self-sustained growth regime in a two-stage growth patterns generated through the numerical simulation of the model. We then show that these mechanisms lead to the emergence of a Kaldor-Verdoorn law. Finally we show that the structure of demand (among others the heterogeneity in consumption behaviour) itself shapes the type of growth regime emerging from the endogenous structural changes, fostering or hampering the emergence of the Kaldor Verdoorn law.
C63|Beyond RCP8.5: Marginal Mitigation Using Quasi-Representative Concentration Pathways|Assessments of decreases in economic damages from climate change mitigation typically rely on climate output from computationally expensive precomputed runs of general circulation models (GCMs) under a handful of scenarios with discretely varying targets, such as the four representative concentration pathways (RCPs) for CO2 and other anthropogenically emitted gases. Although such analyses are extremely valuable in informing scientists and policymakers about specific, well-known, and massive mitigation goals, we add to the literature by considering potential outcomes from more modest policy changes that may not be represented by any concentration pathway or GCM output. We construct computationally efficient Quasi-representative Concentration Pathways (QCPs) in order to leverage existing scenarios featuring plausible concentration pathways. Computational efficiency allows for common statistical methods for assessing model uncertainty based on iterative replication, such as bootstrapping. We illustrate by feeding two QCPs through a computationally efficient statistical emulator and dose response functions extrapolated from estimates in the recent literature in order to gauge effects of mitigation on the relative risk of heat stress mortality.
C63|Naïve imitation and partial cooperation in a local public goods model|In a local interaction model agents situated on a circle play bilateral prisoners’ dilemmas with their immediate neighbors and have three possible strategies: cooperate in all interactions (altruistic), defect in all interactions (egoistic), or cooperate with one immediate neighbor with probability 1=2 (partial cooperation). After each period the agents adopt the strategy with the highest average payoff in their observed local neighborhood (naïve imitation). The absorbing states of the process are outlined and analysed. There does not exist an absorbing state in which the partially cooperative strategy coexists with any of the other strategies. The partially cooperative strategy limits the diffusion of altruistic behavior in the population. Even though clustering of altruists is generally beneficial for sustaining altruism, relatively big groups of altruists at the onset actually enable the spread of the partially cooperative strategy.
C63|The potential economic role of regenerative therapy in the treatment of knee osteoarthritis|Background. Osteoarthritis (OA) is a chronic and degenerative pathology that affects joints in particular hands, knees, hip and lower back. The prevalence and incidence is in continuous increase for the advanced ageing of the population and the increasing presence of risk factors such as obesity. OA burden of disease implies high care costs and has an important social impact because it limits people’s every day activities causing pain and mobility reduction. Objectives. The aim of this work is to carry out an economic evaluation of the intra-articular (i.a.) use of the Platelet-Rich Plasma (PRP) therapy in the treatment for knee osteoarthritis. Recently the scientific literature has shown the effectiveness of this treatment. The comparator adopted is the Hyaluronic acid (HA) which represents the standard i.a. therapy. Both therapies can reduce pain, improving patient quality of life, and can help the patient to delay the joint surgery, that represents a high cost for the National Health System. Methods. A cost-effectiveness analysis (CEA) were performed using a decision tree model. The effectiveness outcomes are reported in terms of Quality Adjusted Life Year (QALY). The costs are reported in Euro (€) currency evaluated in 2016. The adopted perspective is the Healthcare System and only direct cost have been included in the analyses. Deterministic and probabilistic sensibility analyses are reported in order to evaluate the robustness of the results and account for the different sources of uncertainty.The analyses have been carried out for three European country: Germany, Italy and France Results. The PRP therapy results more costly but also more effective than HA. Using a Willingness to pay thresholds of € 10,000/QALY, the PRP result the cost-effective therapy with respect to HA, for patient with moderate to severe knee OA.
C63|Positive Liquidity Spillovers from Sovereign Bond-Backed Securities|This paper contributes to the debate concerning the benefits and disadvantages of introducing a European Sovereign Bond-Backed Securitisation (SBBS) to address the need for a common safe asset that would break destabilising bank-sovereign linkages. The analysis focuses on assessing the effectiveness of hedges incurred while making markets in individual euro area sovereign bonds by taking offsetting positions in one or more of the SBBS tranches. Tranche yields are estimated using a simulation approach. This involves the generation of sovereign defaults and allocation of the combined credit risk premium of all the sovereigns, at the end of each day, to the SBBS tranches according to the seniority of claims under the proposed securitisation. Optimal hedging with SBBS is found to reduce risk exposures substantially in normal market conditions. In volatile conditions, hedging is not very effective but leaves dealers exposed to mostly idiosyncratic risks. These remaining risks largely disappear if dealers are diversified in providing liquidity across country-specific secondary markets and SBBS tranches. Hedging each of the long positions in a portfolio of individual sovereigns results in a risk exposure as low as that borne by holding the safest individual sovereign bond (the Bund).
C63|Public Procurement and Reputation: An Agent-Based Model|Based on the literature on public procurement regulation, we use an Agent-Based Model to assess the performance of different selection procedures. Specifically, we aim at investigating whether and how the inclusion of reputation of firms in the public procurement selection process affects the final cost of the contract. The model defines two types of actors: i) firms potentially competing to win the contract; ii) a contracting authority, aiming at minimizing procurement costs. These actors respond to environmental conditions affecting the actual costs of carrying on the project and unknown to firms at the time of bidding and to the contracting authority. The results from the model are generated through simulations by considering different congurations and varying some parameters of the model, such as the firms' skills, the level of opportunistic rebate, the relative weight of reputation and rebate. The main conclusion is that reputation matters and some policy implications are drawn.
C63|What if supply-side policies are not enough? The perverse interaction of flexibility and austerity|In this work we develop a set of labour market and fiscal policy experiments upon the labour- and credit- augmented “Schumpeter meeting Keynes” agent-based model. The labour market is declined under two institutional variants, the “Fordist” and the “Competitive” set-ups meant to capture the historical transition from the post-WWII toward the post Thatcher-Reagan period. Inside these two regimes, we study the different effects of supply-side active labour market policies (ALMPs) vs. demand-management, passive labour market ones (PLMPs). In particular, we analyse the effects of ALMPs aimed at promoting job search, and at providing training to unemployed people. Next, we compare the effects of these policies with unemployment benefits meant to sustain income and therefore aggregate demand. Considering the burden of unemployment benefits in terms of the public budget, we link such provision with the objectives of the European Stability and Growth Pact. Our results show that (i) an appropriate level of skills is not enough to sustain growth when workers face adverse labour demand; (ii) supply-side policies are not able to reverse the negative interaction between flexibility and austerity; (iii) PLMPs outperform ALMPs in reducing unemployment and workers’ skill deterioration; and (iv) demand-management policies are better suited to mitigate inequality and to sustain long-run growth.
C63|An agent-based model of intra day financial markets dynamics|We build an agent based model of a financial market that is able to jointly reproduce many of the stylized facts at different time-scales. These include properties related to returns (leptokurtosis, absence of linear autocorrelation, volatility clustering), trading volumes (volume clustering, correlation betwenn volume and volatility), and timing of trades (number of price changes, autocorrelation of durations between subsequent trades, heavy tails in their distribution, order-side clustering). With respect to previous contributions we introduce a strict event scheduling borrowed from the Euronext exchange, and an endogenous rule for traders participation. We show that such a rule is crucial to match stylized facts.
C63|And then he wasn't a she : Climate change and green transitions in an agent-based integrated assessment model|In this work, we employ an agent-based integrated assessment model to study the likelihood of transition to green, sustainable growth in presence of climate damages. The model comprises heterogeneous fossil-fuel and renewable plants, capital- and consumption-good firms and a climate box linking greenhouse gasses emission to temperature dynamics and microeconomic climate shocks affecting labour productivity and energy demand of firms. Simulation results show that the economy possesses two statistical equilibria: a carbon-intensive lock-in and a sustainable growth path characterized by better macroeconomic performances. Once climate damages are accounted for, the likelihood of a green transition depends on the damage function employed. In particular, aggregate and quadratic damage functions overlook the impact of climate change on the transition to sustainability; to the contrary, more realistic micro-level damages are found to deeply influence the chances of a transition. Finally, we run a series of policy experiments on carbon (fossil fuel) taxes and green subsidies. We find that the effectiveness of such marketbased instruments depends on the different channels climate change affects the economy through, and complementary policies might be required to avoid carbon-intensive lock-ins.
C63|Debunking the granular origins of aggregate fluctuations: from real business cycles back to Keynes|Abstract In this work we study the granular origins of business cycles and their possible underlying drivers. As shown by Gabaix (Econometrica 79:733–772, 2011), the skewed nature of firm size distributions implies that idiosyncratic (and independent) firm-level shocks may account for a significant portion of aggregate volatility. Yet, we question the original view grounded on “supply granularity”, as proxied by productivity growth shocks – in line with the Real Business Cycle framework–, and we provide empirical evidence of a “demand granularity”, based on investment growth shocks instead. The role of demand in explaining aggregate fluctuations is further corroborated by means of a macroeconomic Agent-Based Model of the “Schumpeter meeting Keynes” family Dosi et al. (J Econ Dyn Control 52:166–189, 2015). Indeed, the investigation of the possible microfoundation of RBC has led us to the identification of a sort of microfounded Keynesian multiplier.
C63|The labour-augmented K+S model: a laboratory for the analysis of institutional and policy regimes|In this work we discuss the research findings from the labour-augmented Schumpeter meeting Keynes (K+S) agent-based model. It comprises comparative dynamics experiments on an artificial economy populated by heterogeneous, interacting agents, as workers, firms, banks and the government. The exercises are characterised by different degrees of labour flexibility, or by institutional shocks entailing labour market structural reforms, wherein the phenomenon of hysteresis is endogenous and pervasive. The K+S model constitutes a laboratory to evaluate the effects of new institutional arrangements as active/passive labour market policies, and fiscal austerity. In this perspective, the model allows mimicking many of the customary policy responses which the European Union and many Latin American countries have embraced in reaction to the recent economic crises. The obtained results seem to indicate, however, that most of the proposed policies are likely inadequate to tackle the short-term crises consequences, and even risk demoting the long-run economic prospects. More objectively, the conclusions offer a possible explanation to the negative path traversed by economies like Brazil, where many of the mentioned policies were applied in a short period, and hint about some risks ahead.
C63|Inequality, Business Cycles, and Monetary-Fiscal Policy|We study optimal monetary and fiscal policy in a model with heterogeneous agents, incomplete markets, and nominal rigidities. We develop numerical techniques to approximate Ramsey plans and apply them to a calibrated economy to compute optimal responses of nominal interest rates and labor tax rates to aggregate shocks. Responses differ qualitatively from those in a representative agent economy and are an order of magnitude larger. Taylor rules poorly approximate the Ramsey optimal nominal interest rate. Conventional price stabilization motives are swamped by an across person insurance motive that arises from heterogeneity and incomplete markets.
C63|Pricing and hedging GDP-linked bonds in incomplete markets|We model the super-replication of payoffs linked to a country’s GDP as a stochastic linear program on a discrete time and state-space scenario tree to price GDP-linked bonds. As a byproduct of the model we obtain a hedging portfolio. Using linear programming duality we compute also the risk premium. The model applies to coupon-indexed and principal-indexed bonds, and allows the analysis of bonds with different design parameters (coupon, target GDP growth rate, and maturity). We calibrate for UK and US instruments, and carry out sensitivity analysis of prices and risk premia to the risk factors and bond design parameters. We also compare coupon-indexed and principal-indexed bonds.
C63|Risk management for sovereign financing within a debt sustainability framework|The mix of instruments used to finance a sovereign is a key determinant of debt sustainability through its effect on funding costs and risks. We extend standard debt sustainability analysis to incorporate debt-financing decisions in the presence of macroeconomic, financial, and fiscal risks. We optimize the maturity of debt instruments to trade off borrowing costs with refinancing risk. Risk is quantified with a coherent measure of tail risk of financing needs, conditional Flow-at-Risk. A constraint on the pace of reduction of debt stocks is also imposed, and we model the effect of debt stocks on the yield curve through endogenous risk and term premia. On a simulated economy, we show that the cost-risk and flow-stock trade-offs embedded in issuance decisions are key determinants of the evolution of debt dynamics and are economically significant. Comparing three alternative optimizing strategies and some simple fixed-issuance rules, we also draw lessons on when and why optimizing matters the most. This depends on the risk tolerance level, the size, cost, and maturity of legacy debt, and the sensitivity of interest rates to debt. Our model quantifies thresholds for the minimum level of refinancing risks and the maximum pace of debt reduction that a sovereign could reach given its economic fundamentals. Going beyond those thresholds is only feasible through adjustments of gross financing needs, and an extension of the baseline model identifies the hot spots for these adjustments, computing their minimum size and optimal timing. Our findings inform policy decisions concerning both official sector borrowing and public finance, with a focus not only on minimizing interest payments but also on managing refinancing risks and increasing debt dynamics.
C63|Effects of Taxes and Safety Net Pensions on life-cycle Labor Supply, Savings and Human Capital: the Case of Australia|In this paper we structurally estimate a life-cycle model of consumption/savings, labor supply and retirement, using data from the Australian HILDA panel. We use the model to evaluate effects of the Australian aged pension system and tax policy on labor supply, consumption and retirement decisions. Our model accounts for human capital accumulation via learning by doing, as well as wealth accumulation and decumulation over the life cycle, uninsurable wage risk, credit constraints, a non-absorbing retirement decision, and labor market frictions. We account for the “bunching” of hours by discretizing job offers into several hours levels, allowing us to investigate labor supply on both intensive and extensive margins. Our model allows us to quantify the effects of anticipated and unanticipated tax and pension policy changes at different points of the life cycle. Our results imply that the Australian Aged Pension system as currently designed is very poorly targeted, so that means testing and other program rules could be improved.
C63|Naive Learning in Social Networks with Random Communication|We study social learning in a social network setting where agents receive independent noisy signals about the truth. Agents naïvely update beliefs by repeatedly taking weighted averages of neighbors' opinions. The weights are fixed in the sense of representing average frequency and intensity of social interaction. However, the way people communicate is random such that agents do not update their belief in exactly the same way at every point in time. We show that even if the social network does not privilege any agent in terms of influence, a large society almost always fails to converge to the truth. We conclude that wisdom of crowds is an illusive concept and bares the danger of mistaking consensus for truth.
C63|Leverage and evolving heterogeneous beliefs in a simple agent-based financial market|Recent research has acknowledged the crucial role of financial intermediariesâ€™ balance sheet variables â€“ namely, wealth and leverage â€“ in the dynamics of asset prices. In this paper we use a prototypical â€œsmall-typeâ€ artificial financial market model with heterogeneous interacting traders to pin down how asset prices are affected by the complex interaction between balance sheet constraints and the endogenous evolution of trading rules.
C63|Bilateral netting and systemic liquidity shortages in banking networks|The cross holding of interbank deposits represents an optimal ex-ante coinsurance arrangement whenever the uncertainty concerning banksâ€™ liquidity needs is idiosyncratic and imperfectly correlated. When a shock to aggregate liquidity demand occurs, however, such an arrangement could be detrimental â€“ depending on the topological structure of interlinkages - as financial exposures become a means to spread risk. If the ex-post facto is an excess demand for liquidity, therefore, regulators could severe potential channels of contagion by forcing banks to net their mutual debt obligations. Starting from these premises we employ simulation techniques with simple interbank structures to obtain two results. First, a state-contingent mandatory policy to bilaterally net mutual interbank exposures comes with a trade-off between the benefits of thwarting the channels of contagion and the harms of a greater concentration of the remaining netted expositions. Second, the balance between the two prongs of the trade-off depends on the metric used by regulators to define financial stability and the topological structure characterizing the interbank market.
C63|Analyse scientométrique de la crise économique|En s’appuyant sur les techniques de cartographie, en se basant sur l’analyse textuelle et l’analyse des réseaux de citations nous avons analysé le développement de la thématique de la crise économique. L’objectif est de montrer dans un premier temps les courants de pensée et les auteurs influents. Dans un second temps comprendre son évolution à travers le temps. Pour ce faire, nous avons extrait de l’interface du WOS en ligne l’ensemble des publications contenant le mot « crisis » dans les catégories disciplinaires « economics » et « business & finance ». Notre requête renvoie plus de 24000 publications. Nos résultats nous ont permis de montrer les différents courants de pensée dominant l’analyse de la crise économique ainsi que les auteurs les plus influents. L’analyse textuelle des termes présents dans les titres, résumé et mots-clés montre des changements majeurs dans la façon dont les économistes traitent le sujet. Désormais, une bonne partie des publications traitant la thématique de la crise économique cherche non pas à traiter les conséquences ou à proposer des solutions, mais plutôt prévoir l’avènement des crises à travers l’analyse des différents risques qui conduiraient à une crise. Nous avons montré également que cette thématique est très fortement dominée par la finance tant au niveau microéconomique que macroéconomique.
C63|Heterogeneity, distribution and financial fragility of non-financial firms: an agent-based stock-flow consistent (AB-SFC) model|In Minsky's Financial Instability Hypothesis (FIH), financial fragility of non-financial firms tends to increase endogenously over the cycle along with the macroeconomic leverage ratio. This analysis has been criticized for two main complementary reasons: firstly, it does not duly consider the aggregate pro-cyclicallity of profits; secondly, due to an overly aggregate analysis, some inferences about the relation between aggregate leverage and systemic fragility are potentially misleading. In this paper, we take these criticisms into account by building an agent-based stock-flow consistent model which integrates the real and financial sides of the economy in a fundamentally dynamic environment. We calibrate and simulate our model and show that the dynamics generated are in line with empirical evidence both at the micro and the macro levels. We create a financial fragility index and examine how systemic financial fragility relates to the aggregate leverage along the cycle. We show that our model yields both Min-skian regimes, in which the aggregate leverage increases along with investment, and Steindlian regimes, where investment brings leverage down. Our key findings are that the sensitivity of financial fragility to aggregate leverage is not as big as assumed in the literature; and that the distribution of profits amongst firms does matter for the stability of the system, both statically (immediately for financial fragility) and dynamically (because of the dynamics of leverage).
C63|How active is active learning: value function method vs an approximation method|In a previous paper Amman and Tucci (2018) compare the two dominant approaches for solving models with optimal experimentation (also called active learning), i.e. the value function and the approximation method. By using the same model and dataset as in Beck and Wieland (2002), theyfind that the approximation method produces solutions close to those generated by the value function approach and identify some elements of the model specifications which affect the difference between the two solutions. They conclude that differences are small when the effects of learning are limited. However the dataset used in the experiment describes a situation where the controller is dealing with a nonstationary process and there is no penalty on the control. The goal of this paper is to see if their conclusions hold in the more commonly studied case of a controller facing a stationary process and a positive penalty on the control.
C63|Hotelling-Bertrand duopoly competition under firrm-specific network e effects|When dealing with consumer choices, social pressure plays a crucial role; also in the context of market competition, the impact of network/social effects has been largely recognized. However, the effects of firm-specifc social recognition on market equilibria has never been addressed so far. In this paper, we consider a duopoly where competing firms are differentiated solely by the level of social (or network) externality they induce on consumers' perceived utility. We fully characterize the subgame perfect Nash equilibria in locations, prices and market shares. Under a scenario of weak social externality, the firms opt for maximal differentiation and the one with the highest social recognition has a relative advantage in terms of profits. Surprisingly, this outcome is not persistent; excessive social recognition may lead to adverse coordination of consumers: the strongest firm can eventually be thrown out of the market with positive probability. This scenario is related to a Pareto inefficient trap of no differentiation.
C63|On the significance of borders|We propose a prototype model of market dynamics in which all functional relationships are linear. We take into account three borders, defined by linear functions, which are intrinsic to the economic reasoning: non-negativity of prices; downward rigidity of capacity (depreciation) and a capacity constraint for the production decision. Given the linear specification, the borders are the only source for the emerging of cyclical and more complex dynamics. In particular, we discuss centre bifurcations, border collision bifurcations and degenerate flip bifurcations - dynamic phenomena the occurrence of which are intimately related to the existence of borders.
C63|The impact of Brexit on trade patterns and industry location: a NEG analysis|We explore the effects of Brexit on trade patterns and on the spatial distribution of industry between the United Kingdom and the European Union and within the EU. Our study adopts a new economic geography (NEG) perspective developing a linear model with three regions, the UK and two separated regions composing the EU. The 3-region framework and linear demands allow for different trade patterns. Two possible ante-Brexit situations are possible, depending on the interplay between local market size, local competition and trade costs: industrial agglomeration or dispersion. Considering a soft and a hard Brexit scenario, the ante-Brexit situation is altered substantially, depending on which scenario prevails. UK firms could move to the larger EU market, even in the peripheral region, reacting to the higher trade barriers, relocation representing a substitute for trade. Alternatively, some EU firms could move in the more isolated UK market finding shelter from the competition inside the EU. We also consider the post-Brexit scenario of deeper EU integration, leading to a weakening of trade links between the EU and the UK. Our analysis also reveals a highly complex bifurcation sequence leading to many instances of multistability, intricate basins of attraction and cyclical and chaotic dynamics.
C63|On the significance of borders|We propose a prototype model of market dynamics in which all functional relationships are linear. We take into account three borders, defined by linear functions, which are intrinsic to the economic reasoning: non-negativity of prices; downward rigidity of capacity (depreciation) and a capacity constraint for the production decision. Given the linear specification, the borders are the only source for the emerging of cyclical and more complex dynamics. In particular, we discuss centre bifurcations, border collision bifurcations and degenerate flip bifurcations - dynamic phenomena the occurrence of which are intimately related to the existence of borders.
C63|The impact of Brexit on trade patterns and industry location: a NEG analysis|We explore the effects of Brexit on trade patterns and on the spatial distribution of industry between the United Kingdom and the European Union and within the EU. Our study adopts a new economic geography (NEG) perspective developing a linear model with three regions, the UK and two separated regions composing the EU. The 3-region framework and linear demands allow for different trade patterns. Two possible ante-Brexit situations are possible, depending on the interplay between local market size, local competition and trade costs: industrial agglomeration or dispersion. Considering a soft and a hard Brexit scenario, the ante-Brexit situation is altered substantially, depending on which scenario prevails. UK firms could move to the larger EU market, even in the peripheral region, reacting to the higher trade barriers, relocation representing a substitute for trade. Alternatively, some EU firms could move in the more isolated UK market finding shelter from the competition inside the EU. We also consider the post-Brexit scenario of deeper EU integration, leading to a weakening of trade links between the EU and the UK. Our analysis also reveals a highly complex bifurcation sequence leading to many instances of multistability, intricate basins of attraction and cyclical and chaotic dynamics.
C63|“Tracking economic growth by evolving expectations via genetic programming: A two-step approach”|The main objective of this study is to present a two-step approach to generate estimates of economic growth based on agents’ expectations from tendency surveys. First, we design a genetic programming experiment to derive mathematical functional forms that approximate the target variable by combining survey data on expectations about different economic variables. We use evolutionary algorithms to estimate a symbolic regression that links survey-based expectations to a quantitative variable used as a yardstick (economic growth). In a second step, this set of empirically-generated proxies of economic growth are linearly combined to track the evolution of GDP. To evaluate the forecasting performance of the generated estimates of GDP, we use them to assess the impact of the 2008 financial crisis on the accuracy of agents' expectations about the evolution of the economic activity in 28 countries of the OECD. While in most economies we find an improvement in the capacity of agents' to anticipate the evolution of GDP after the crisis, predictive accuracy worsens in relation to the period prior to the crisis. The most accurate GDP forecasts are obtained for Sweden, Austria and Finland.
C63|Heterogeneous expectations and asset price dynamics|Within the seminal asset-pricing model by Brock and Hommes (1998), heterogeneous boundedly rational agents choose between a fixed number of expectation rules to forecast asset prices. However, agents' heterogeneity is limited in the sense that they typically switch between a representative technical and a representative fundamental expectation rule. Here we generalize their framework by considering that all agents follow their own time-varying technical and fundamental expectation rules. Estimating our model using the method of simulated moments reveals that it is able to explain the statistical properties of the daily behavior of the S&P500 quite well. Moreover, our analysis reveals that heterogeneity is not only a realistic model property but clearly helps to explain the intricate dynamics of financial markets.
C63|The labour Augmented K+S model : a laboratory for the analysis of institutional and policy regimes|In this work we discuss the research findings from the labour-augmented Schumpeter meeting Keynes (K+S) agent-based model. It comprises comparative dynamics experiments on an artificial economy populated by heterogeneous, interacting agents, as workers, firms, banks and the government. The exercises are characterised by different degrees of labour flexibility, or by institutional shocks entailing labour market structural reforms, wherein the phenomenon of hysteresis is endogenous and pervasive. The K+S model constitutes a laboratory to evaluate the effects of new institutional arrangements as active/passive labour market policies, and fiscal austerity. In this perspective, the model allows mimicking many of the customary policy responses which the European Union and many Latin American countries have embraced in reaction to the recent economic crises. The obtained results seem to indicate, however, that most of the proposed policies are likely inadequate to tackle the short-term crises consequences, and even risk demoting the long-run economic prospects. More objectively, the conclusions offer a possible explanation to the negative path traversed by economies like Brazil, where many of the mentioned policies were applied in a short period, and hint about some risks ahead.
C63|Computational evidence on the distributive properties of monetary policy|Empirical studies have pointed out that monetary policy may significantly affect income and wealth inequality. To investigate the distributive properties of monetary policy the authors resort to an agent-based macroeconomic model where firms, households and one bank interact on the basis of limited information and adaptive rules-of-thumb. Simulations show that the model can replicate fairly well a number of stylized facts, especially those relative to the business cycle. The authors address the issue using three types of computational experiments, including a global sensitivity analysis carried out through a novel methodology which greatly reduces the computational burden of simulations. The result emerges that a more restrictive monetary policy increases inequality, even though this effect may differ across groups of households. In addition, it appears to be attenuated if the bank's willingness to lend is lower. The overall analysis suggests that inequality can constitute valuable information also for central banks.
C63|The Aggregate Consequences of Tax Evasion|There is a sizable overall tax gap in the U.S., albeit tax non-compliance differs sharply across income types. While only small percentages of wages and salaries are underreported, the estimated misreporting rate of self-employment business income is substantial. This paper studies how tax evasion in the self-employment sector affects aggregate outcomes and welfare. We develop a dynamic general equilibrium model with incomplete markets in which heterogeneous agents choose between being a worker or self-employed. Self-employed agents may hide a share of their business income but face the risk of being detected by the tax authority. Our model replicates important quantitative features of the U.S. economy in terms of income, wealth, self-employment, and tax evasion. Our quantitative ndings suggest that tax evasion leads to a larger self-employment sector but it depresses the average size and productivity of self-employed businesses. Tax evasion generates positive aggregate welfare effects because it acts as a subsidy for the self-employed. Workers, however, suffer from substantial welfare losses.
C63|Forward Guidance at the Zero Lower Bound: Curse and Blessing of Time-Inconsistency|Forward guidance as a tool of unconventional monetary policy can be highly efficient to support aggregate demand and to steer the economy out of the zero lower bound (ZLB). However, the effect that stimulates the economy can give rise to a time-inconsistency problem: if the central bank promises to keep interest rates at the ZLB for long, the sub-sequent increase in inflation and economic activity may create a motive for the central bank to forego its promise and to exit the ZLB earlier than announced. We illustrate the time-inconsistency problem in a New Keynesian model with hand-to-mouth consumers. Using a novel and analytically tractable method for handling occasionally binding constraints, we contrast the case of commitment to forward guidance with the case in which monetary policy allows for an early exit of the ZLB. Our method is able to provide results on uniqueness and existence of (ZLB) equilibria. We study the equilibrium selection given different scenarios and conclude that central bankers should be careful when choosing the number of periods with low interest rates in order to avoid the inconsistency problem. Furthermore, we calculate government spending multipliers and argue that the multiplier is even larger if combined with forward guidance.
C63|Weighted Committee Games|Players in a committee, council, or electoral college often wield asymmetric numbers of votes. Binary decision environments are then conventionally modeled as weighted voting games. We introduce weighted committee games in order to describe decisions on three or more alternatives in similarly succinct fashion. We compare different voting weight configurations for plurality, Borda, Copeland, and antiplurality rule. The respective geometries and very different numbers of structurally non-equivalent committees have escaped notice so far. They determine voting equilibria, the distribution of power, and other aspects of collective choice.
C63|A Practical Guide to Parallelization in Economics|This guide provides a practical introduction to parallel computing in economics. After a brief introduction to the basic ideas of parallelization, we show how to parallelize a prototypical application in economics using, on CPUs, Julia, Matlab, R, Python, C++-OpenMP, Rcpp–OpenMP, and C++-MPI, and, on GPUs, CUDA and OpenACC. We provide code that the user can download and fork, present comparative results, and explain the strengths and weaknesses of each approach. We conclude with some additional remarks about alternative approaches.
C63|Solving heterogeneous agent models in discrete time with many idiosyncratic states by perturbation methods|This paper describes a method for solving heterogeneous agent models with aggregate risk and many idiosyncratic states formulated in discrete time. It extends the method proposed by Reiter (2009) and complements recent work by Ahn et al. (2017) on how to solve such models in continuous time. We suggest first solving for the stationary equilibrium of the model without aggregate risk. We then write the functionals that describe the recursive equilibrium as sparse expansions around their stationary equilibrium counterparts. Finally we use the perturbation method of Schmitt-Grohé and Uribe (2004) to approximate the aggregate dynamics of the model.
C63|Network formation with local complements and global substitutes: the case of R&D networks|In this paper we introduce a stochastic network formation model where agents choose both actions and links. Neighbors in the network benefit from each other’s action levels through local complementarities and there exists a global interaction effect reflecting a strategic substitutability in actions. The tractability of the model allows us to provide a complete equilibrium characterization in the form of a Gibbs measure, and we show that the structural features of equilibrium networks are consistent with empirically observed networks. We then use our equilibrium characterization to show that the model can be conveniently estimated even for large networks. The policy relevance is demonstrated with examples of firm exit, mergers and acquisitions and subsidies in the context of R&D collaboration networks.
C63|Matlab, Python, Julia: What to Choose in Economics?|We perform a comparison of Matlab, Python and Julia as programming languages to be used for implementing global nonlinear solution techniques. We consider two popular applications: a neoclassical growth model and a new Keynesian model. The goal of our analysis is twofold: First, it is aimed at helping researchers in economics to choose the programming language that is best suited to their applications and, if needed, help them transit from one programming language to another. Second, our collections of routines can be viewed as a toolbox with a special emphasis on techniques for dealing with high dimensional economic problems. We provide the routines in the three languages for constructing random and quasi-random grids, low-cost monomial integration, various global solution methods, routines for checking the accuracy of the solutions, etc. Our global solution methods are not only accurate but also fast. Solving a new Keynesian model with eight state variables only takes a few seconds, even in the presence of active zero lower bound on nominal interest rates. This speed is important because it then allows the model to be solved repeatedly as one would require in order to do estimation.
C63|Forward Guidance: Is It Useful After the Crisis?|During recent economic crisis, when nominal interest rates were at their effective lower bounds, central banks used forward guidance -- announcements about future policy rates -- to conduct their monetary policy. Many policymakers believe that forward guidance will remain in use after the end of the crisis, however, there is uncertainty about its effectiveness. In this paper, we study the impact of forward guidance in a stylized new Keynesian economy away from the effective lower bound on nominal interest rates. Using closed-form solutions, we show that the impact of forward guidance on the economy depends critically on a specific monetary policy rule, ranging from non-existing to immediate and unrealistically large, the so-called forward guidance puzzle. We show that the puzzle occurs under very special -- empirically implausible and socially suboptimal -- monetary policy rules, whereas empirically relevant Taylor rules lead to sensible implications.
C63|Continuous Time Versus Discrete Time in the New Keynesian Model: Closed-Form Solutions and Implications for Liquidity Trap|Economists often use interchangeably the discrete- and continuous-time versions of the Keynesian model. In the paper, I ask whether or not the two versions effectively lead to the same implications. I analyze several alternative monetary policies, including a Taylor rule, discretionary interest rate choice and forward guidance. I show that the answer depends on a specific scenario and parameterization considered. In particular, in the presence of liquidity trap, the discrete-time analysis helps overcome some negative implications of the continuous-time model, such as excessively strong impact of price stickiness on inflation and output, unrealistically large government multipliers, as well as implausibly large effects of forward guidance.
C63|Exploitation, skills, and inequality| This paper uses a computational framework to analyse the equilibrium dynamics of exploitation and inequality in accumulation economies with heterogeneous labour. A novel index is presented which measures the intensity of exploitation at the individual level and the dynamics of the distribution of exploitation intensity is analysed. The e ects of technical change and evolving social norms on exploitation and inequalities are also considered and an interesting phenomenon of exploitation cycles is identi ed. Various taxation schemes are analysed which may reduce exploitation or inequalities in income and wealth. It is shown that relatively small taxation rates may have significant cumulative effects on wealth and income inequalities. Further, taxation schemes that eliminate exploitation also reduce disparities in income and wealth but in the presence of heterogeneous skills, do not necessarily eliminate them. The inegalitarian effects of different abilities need to be tackled with a progressive education policy that compensates for unfavourable circumstances.
C63|Price Overreactions in the Cryptocurrency Market|This paper examines price overreactions in the case of the following cryptocurrencies: BitCoin, LiteCoin, Ripple and Dash. A number of parametric (t-test, ANOVA, regression analysis with dummy variables) and non-parametric (Mann–Whitney U test) tests confirm the presence of price patterns after overreactions: the next-day price changes in both directions are bigger than after “normal” days. A trading robot approach is then used to establish whether these statistical anomalies can be exploited to generate profits. The results suggest that a strategy based on counter-movements after overreactions is not profitable, whilst one based on inertia appears to be profitable but produces outcomes not statistically different from the random ones. Therefore the overreactions detected in the cryptocurrency market do not give rise to exploitable profit opportunities (possibly because of transaction costs) and cannot be seen as evidence against the Efficient Market Hypothesis (EMH).
C63|“Tracking economic growth by evolving expectations via genetic programming: A two-step approach”|The main objective of this study is to present a two-step approach to generate estimates of economic growth based on agents’ expectations from tendency surveys. First, we design a genetic programming experiment to derive mathematical functional forms that approximate the target variable by combining survey data on expectations about different economic variables. We use evolutionary algorithms to estimate a symbolic regression that links survey-based expectations to a quantitative variable used as a yardstick (economic growth). In a second step, this set of empirically-generated proxies of economic growth are linearly combined to track the evolution of GDP. To evaluate the forecasting performance of the generated estimates of GDP, we use them to assess the impact of the 2008 financial crisis on the accuracy of agents' expectations about the evolution of the economic activity in 28 countries of the OECD. While in most economies we find an improvement in the capacity of agents' to anticipate the evolution of GDP after the crisis, predictive accuracy worsens in relation to the period prior to the crisis. The most accurate GDP forecasts are obtained for Sweden, Austria and Finland.
C63|“A regional perspective on the accuracy of machine learning forecasts of tourism demand based on data characteristics”|In this work we assess the role of data characteristics in the accuracy of machine learning (ML) tourism forecasts from a spatial perspective. First, we apply a seasonal-trend decomposition procedure based on non-parametric regression to isolate the different components of the time series of international tourism demand to all Spanish regions. This approach allows us to compute a set of measures to describe the features of the data. Second, we analyse the performance of several ML models in a recursive multiple-step-ahead forecasting experiment. In a third step, we rank all seventeen regions according to their characteristics and the obtained forecasting performance, and use the rankings as the input for a multivariate analysis to evaluate the interactions between time series features and the accuracy of the predictions. By means of dimensionality reduction techniques we summarise all the information into two components and project all Spanish regions into perceptual maps. We find that entropy and dispersion show a negative relation with accuracy, while the effect of other data characteristics on forecast accuracy is heavily dependent on the forecast horizon.
C63|“A geometric approach to proxy economic uncertainty by a metric of disagreement among qualitative expectations”|In this study we present a geometric approach to proxy economic uncertainty. We design a positional indicator of disagreement among survey-based agents' expectations about the state of the economy. Previous dispersion-based uncertainty indicators derived from business and consumer surveys exclusively make use of the two extreme pieces of information coming from the respondents expecting a variable to rise and to fall. With the aim of also incorporating the information coming from the share of respondents expecting a variable to remain constant, we propose a geometrical framework and use a barycentric coordinate system to generate a measure of disagreement, referred to as a discrepancy indicator. We assess its performance, both empirically and experimentally, by comparing it to the standard deviation of the share of positive and negative responses, which has been used by Bachman et al. (2013) as a proxy for economic uncertainty. When applied in sixteen European countries, we find that both time-varying metrics co-evolve in most countries for expectations about the country's overall economic situation in the present, but not in the future. Additionally, we obtain their simulated sampling distributions and we find that the proposed indicator gravitates uniformly towards the three vertices of the simplex representing the three answering categories, as opposed to the standard deviation, which tends to overestimate the level of uncertainty as a result of ignoring the no-change responses. Consequently, we find evidence that the information coming from agents expecting a variable to remain constant has an effect on the measurement of disagreement.
C63|An impossibility theorem for wealth in heterogeneous-agent models with limited heterogeneity|It has been conjectured that canonical Bewley–Huggett–Aiyagari heterogeneous-agent models cannot explain the joint distribution of income and wealth. The results stated below verify this conjecture and clarify its implications under very general conditions. We show in particular that if (i) agents are infinitely-lived, (ii) saving is risk-free, and (iii) agents have constant discount factors, then the wealth distribution inherits the tail behavior of income shocks (e.g., light-tailedness or the Pareto exponent). Our restrictions on utility require only that relative risk aversion is bounded, and a large variety of income processes are admitted. Our results show conclusively that it is necessary to go beyond standard models to explain the empirical fact that wealth is heavier-tailed than income. We demonstrate through examples that relaxing any of the above three conditions can generate Pareto tails.
C63|Climate Policy under Cooperation and Competition between Regions with Spatial Heat Transport|We build a novel stochastic dynamic regional integrated assessment model (IAM) of the climate and economic system including a number of important climate science elements that are missing in most IAMs. These elements are spatial heat transport from the Equator to the Poles, sea level rise, permafrost thaw and tipping points. We study optimal policies under cooperation and noncooperation between two regions (the North and the Tropic-South) in the face of risks and recursive utility. We introduce a new general computational algorithm to find feedback Nash equilibrium. Our results suggest that when the elements of climate science are ignored, important policy variables such as the optimal regional carbon tax and adaptation could be seriously biased. We also find the regional carbon tax is significantly smaller in the feedback Nash equilibrium than in the social planner’s problem in each region, and the North has higher carbon taxes than the Tropic-South.
C63|Business Cycle Uncertainty and Economic Welfare Revisited|Cho, Cooley, and Kim (RED, 2015) (CCK) consider the welfare effects of removing multiplicative productivity shocks from real business cycle models. In a model that admits an analytical solution they argue convincingly that the positive welfare effect of removing uncertainty can be dominated by a negative mean effect arising from the optimal response of household labor supply. While the presentation of this model is quite elaborate, the details of their subsequent quantitative analysis of several versions of the standard real business cycle model remain vague. We lay out the general procedure of computing second-order accurate approximations of welfare gains or losses in the canonical dynamic stochastic general equilibrium model. In order to be able to consider mean preserving increases in the size of shocks we extend the computation of second-order approximations of the policy functions pioneered by Schmitt-GrohÃ© and Uribe (JEDC, 2004). Our computations show that different from the results reported in CCK the mean effect never dominates the fluctuations effect. Welfare measures computed from weighted residuals methods confirm the logic behind our perturbation approach and verify the accuracy of our estimates.
C63|Risk-Adjusted Linearizations of Dynamic Equilibrium Models|We propose a simple risk-adjusted linear approximation to solve a large class of dynamic models with time-varying and non-Gaussian risk. Our approach generalizes lognormal affine approximations commonly used in the macro-finance literature and can be seen as a first-order perturbation around the risky steady state. Therefore, we unify coexisting theories of risk-adjusted linearizations. We provide a formal foundation for approximation methods that remained so far heuristic, and offer explicit formulas for approximate equilibrium objects and conditions for their local existence and uniqueness. Affine approximations are not nested in conventional perturbations of arbitrary order. We apply this technique to models featuring Campbell-Cochrane habits, recursive preferences, and time-varying disaster risk. The proposed affine approximation performs similarly to global solution methods in many applications; risk pricing is accurate at all investment horizons, thereby capturing the main properties of investors’ marginal utility of wealth and measures of welfare costs of fluctuations.
C63|Bayesian Forecasting of Electoral Outcomes with new Parties' Competition|We propose a new methodology for predicting electoral results that combines a fundamental model and national polls within an evidence synthesis framework. Although novel, the methodology builds upon basic statistical structures, largely modern analysis of variance type models, and it is carried out in open-source software. The methodology is largely motivated by the specific challenges of forecasting elections with the participation of new political parties, which is becoming increasingly common in the post-2008 European panorama. Our methodology is also particularly useful for the allocation of parliamentary seats, since the vast majority of available opinion polls predict at the national level whereas seats are allocated at local level. We illustrate the advantages of our approach relative to recent competing approaches using the 2015 Spanish Congressional Election. In general, the predictions of our model outperform the alternative specifications, including hybrid models that combine fundamental and polls' models. Our forecasts are, in relative terms, particularly accurate to predict the seats obtained by each political party.
C63|When are credit gap estimates reliable?|We evaluate the reliability of credit gap measures estimated over time samples of different lengths. We augment our empirical analysis (which turned out to be somewhat inconclusive) with Monte Carlo experiments. For this purpose we build an agent-based model that realistically reproduces credit cycles and use it to generate the artificial data set. We found that 12-15 years of available data is sufficient for the estimation of reliable credit gaps (i.e. the reliability of credit gap estimates will not improve substantially as more data are added to the sample).
C63|Forecasting the implications of foreign exchange reserve accumulation with an agent-based model|We develop a stock-flow-consistent agent-based model that comprises a realistic mechanism of money creation and parametrize it to fit actual data. The model is used to make out-of-sample projections of broad money and credit developments under the commencement/termination of foreign reserve accumulation by the Bank of Russia. We use direct forecasts from the agent-based model as well as the two-step approach, which implies the use of artificial data to pre-train the Bayesian vector autoregression model. We conclude that the suggested approach is competitive in forecasting and yields promising results.
C63|Identification of interbank loans and interest rates from interbank payments – A reliability assessment|We investigate the reliability of the `Furfine filter' often used to identify interbank loans and interest rates from interbank payments settled at central banks. To this end, we have been granted access to records of all unsecured overnight interbank loans during a month from the banks that participated in Norges Bank’s real-time gross settlement system. The filter applied was able to identify each of these loans and correctly derive the associated interest rates. The filter's reliability is also supported by additional evidence based on the Norwegian Overnight Weighted Average (NOWA) interest rates beyond the survey month. Sensitivity analyses suggest the share of false or overlooked loans may remain small if the filter design largely incorporates interbank market conventions regarding loan size requests and interests rate quotes.
C63|The deeds of speed: an agent-based model of market liquidity and flash episodes|This paper examines the role of high-frequency traders in flash episodes in electronic financial markets. To do so, we construct an agent-based model of a market for a financial asset in which trading occurs through a central limit order book. The model consists of heterogeneous agents with different trading strategies and frequencies, and is calibrated to high-frequency time series data on the sterling-US dollar exchange rate. Flash episodes occur in the model due to the procyclical behaviour of high-frequency market participants. This is aligned with some empirical evidence as to the drivers of real-world flash crashes. We find that the prevalence of flash episodes increases with the frequency with which high-frequency market participants trade compared to their low-frequency counterparts. This provides tentative theoretical evidence that the recent growth in high-frequency trading across some markets has led to flash episodes. Furthermore, we adapt the model so that large movements in price trigger temporary halts in trading (ie circuit breakers). This is found to reduce the magnitude and frequency of flash episodes.
C63|Simulation and Evaluation of Zonal Electricity Market Design|Zonal pricing with countertrading (a market-based redispatch) gives arbitrage opportunities to the power producers located in the export-constrained nodes. They can increase their profit by increasing the output in the dayahead market and decrease it in the real-time market (the inc-dec game). We show that this leads to large inefficiencies in a standard zonal market. We also show how the inefficiencies can be significantly mitigated by changing the design of the real-time market. We consider a two-stage game with oligopoly producers, wind-power shocks and real-time shocks. The game is formulated as a two-stage stochastic equilibrium problem with equilibrium constraints (EPEC), which we recast into a two-stage stochastic Mixed-Integer Bilinear Program (MIBLP). We present numerical results for a six-node and the IEEE 24-node system.<br><small>(This abstract was borrowed from another version of this item.)</small>
C63|Increase-Decrease Game under Imperfect Competition in Two-stage Zonal Power Markets – Part I: Concept Analysis|This paper is part I of a two-part paper. It proposes a two-stage game to analyze imperfect competition of producers in zonal power markets with a day-ahead and a real-time market. We consider strategic producers in both markets. They need to take both markets into account when deciding what to bid in each market. The demand shocks between these markets are modeled by several scenarios. The two-stage game is formulated as a Twostage Stochastic Equilibrium Problem with Equilibrium Constraints (TS-EPEC). Then it is further reformulated as a two-stage stochastic Mixed-Integer Linear Program (MILP). The solution of this MILP gives the Subgame Perfect Nash Equilibrium (SPNE). To tackle multiple SPNE, we design a procedure which _nds all SPNE with di_erent total dispatch costs. The proposed MILP model is solved using Benders decomposition embedded in the CPLEX solver. The proposed MILP model is demonstrated on the 6-node and the IEEE 30-node example systems.<br><small>(This abstract was borrowed from another version of this item.)</small>
C63|Increase-Decrease Game under Imperfect Competition in Two-stage Zonal Power Markets – Part II: Solution Algorithm|In part I of this paper, we proposed a Mixed-Integer Linear Program (MILP) to analyse imperfect competition of oligopoly producers in two-stage zonal power markets. In part II of this paper, we propose a solution algorithm which decomposes the proposed MILP model into several subproblems and solve them in parallel and iteratively. Our solution algorithm reduces the solution time of the MILP model and it allows us to analyze largescale examples. To tackle the multiple Subgame Perfect Nash Equilibria (SPNE) situation, we propose a SPNE-band approach. The SPNE band is split into several subintervals and the proposed solution algorithm finds a representative SPNE in each subinterval. Each subinterval is independent from each other, so this structure enables us to use parallel computing. We also design a pre-feasibility test to identify the subintervals without SPNE. Our proposed solution algorithm and our SPNE-band approach are demonstrated on the 6-node and the modified IEEE 30-node example systems. The computational tractability of our solution algorithm is illustrated for the IEEE 118-node and 300-node systems.<br><small>(This abstract was borrowed from another version of this item.)</small>
C63|Saddle Cycles: Solving Rational Expectations Models Featuring Limit Cycles (or Chaos) Using Perturbation Methods|Unlike their linear counterparts, non-linear models of the business cycle can generate sustained economic fluctuations even in the absence of shocks (e.g., via limit cycles or chaos). A popular approach to solving non-linear models is the use of perturbation methods. I show that, as typically implemented, these methods are generally incapable of finding solutions that feature limit cycles or chaos, a fact that does not appear to be recognized in the existing literature. Standard algorithms only seek solutions that feature converge to the steady state, which is stronger than the standard definitional requirement that a solution simply cannot explode. Because of this, in estimation exercises any parameterization that involves limit cycles would typically (and incorrectly) be discarded. I propose a modification to standard algorithms that does not impose the overly strong requirement that solutions involve convergence.
C63|The Evaluation of Fiscal Consolidation Strategies|: In this paper, we present a framework and perform an assessment of different fiscal consolidation strategies both on the revenue as well as on the expenditure sides of the budget in the context of Slovakia. The model we use for simulations is a behavioural general-equilibrium what-if model. We analyse the simulated impacts of consolidation strategies on growth and on fiscal balance (both in short- and long- term). The microsimulation approach allows us also to evaluate the distributional impacts. In addition, the approach permits to compare the statutory with the resulting tax incidence in the long-run. We simulate strategies based on taxing labour income, taxing consumption as well as cutting expenditures on social transfers. We document that corporate and labour taxes are more unfavourable to output growth, while consumption taxes belong to less damaging instruments for consolidation. We show that spending cuts may promote employment and are not detrimental to output growth.
C63|A heterogeneous-agent model of growth and inequality for the UK|This paper analyses the effect of wealth inequality on UK economic growth in recent decades with a heterogeneous-agent growth model where agents can enhance individual productivity growth by undertaking entrepreneurship. The model assumes wealthy people are more able to afford the costs of entrepreneurship. Wealth concentration therefore stimulates entrepreneurship among the rich and so aggregate growth, whose fruits in turn are largely captured by the rich. This process creates a mechanism by which inequality and growth are correlated. The model is estimated and tested by indirect inference and is not rejected. Policy-makers face a trade-off between redistribution and growth.
C63|Price Overreactions in the Cryptocurrency Market|This paper examines price overreactions in the case of the following cryptocurrencies: BitCoin, LiteCoin, Ripple and Dash. A number of parametric (t-test, ANOVA, regression analysis with dummy variables) and non-parametric (Mann–Whitney U test) tests confirm the presence of price patterns after overreactions: the next-day price changes in both directions are bigger than after “normal” days. A trading robot approach is then used to establish whether these statistical anomalies can be exploited to generate profits. The results suggest that a strategy based on counter-movements after overreactions is not profitable, whilst one based on inertia appears to be profitable but produces outcomes not statistically different from the random ones. Therefore the overreactions detected in the cryptocurrency market do not give rise to exploitable profit opportunities (possibly because of transaction costs) and cannot be seen as evidence against the Efficient Market Hypothesis (EMH).
C63|On the Frequency of Price Overreactions|This paper explores the frequency of price overreactions in the US stock market by focusing on the Dow Jones Industrial Index over the period 1990-2017. It uses two different methods (static and dynamic) to detect overreactions and then carries out various statistical tests (both parametric and non-parametric) including correlation analysis, augmented Dickey–Fuller tests (ADF), Granger causality tests, and regression analysis with dummy variables. The following hypotheses are tested: whether or not the frequency of overreactions varies over time (H1), is informative about crises (H2) and/or price movements (H3), and exhibits seasonality (H4). The null cannot be rejected except for H4, i.e. no seasonality is found. On the whole it appears that the frequency of overreactions can provide useful information about market developments and for designing trading strategies.
C63|Bitcoin fluctuations and the frequency of price overreactions|Abstract This paper investigates the role of the frequency of price overreactions in the cryptocurrency market in the case of BitCoin over the period 2013–2018. Specifically, it uses a static approach to detect overreactions and then carries out hypothesis testing by means of a variety of statistical methods (both parametric and non-parametric) including ADF tests, Granger causality tests, correlation analysis, regression analysis with dummy variables, ARIMA and ARMAX models, neural net models, and VAR models. Specifically, the hypotheses tested are whether or not the frequency of overreactions (i) is informative about Bitcoin price movements (H1) and (ii) exhibits no seasonality (H2). On the whole, the results suggest that it can provide useful information to predict price dynamics in the cryptocurrency market and for designing trading strategies (H1 cannot be rejected), whilst there is no evidence of seasonality (H2 cannot be rejected).
C63|Patience is a Virtue - In Value Investing|This note illustrates a simple but important insight for financial investment. In a heterogeneous agent-based evolutionary finance market model with long-lived assets, markets are stable if clients of fundamental ('value') investment funds are more patient than clients of other funds.
C63|A Corporate Financing-Based Asset Pricing Model|I show that an asset pricing model for the equity claims of a value-maximizing firm can be constructed from its optimal financial contracting behavior. I study a dynamic contracting model in which firms trade off the costs and benefits of a given promise to pay external lenders in a specific economic state. Deals between firms and financiers reveal the importance of that state for firm's equity value, namely the stochastic discount factor the firm responds to. I empirically evaluate the model in the cross section of expected equity returns. I find that the financial contracting approach goes a long way in rationalizing observed cross-sectional differences in average returns, also in comparison to leading asset pricing models. In addition, the model discloses that two easily measured variables, the growth rates on net worth and profitability, generate sizeable cross-sectional spreads in returns. Finally, a calibrated version of the model is broadly consistent with observed corporate policies of US listed firms.
C63|Making sense of Piketty's 'Fundamental Laws' in a Post-Keynesian Framework: The transitional dynamics of wealth inequality|If Piketty's main theoretical prediction (r>g leads to rising wealth inequality) is taken to its radical conclusion, then a small elite will own all wealth if capitalism is left to its own devices. We formulate and calibrate a Post-Keynesian model with an endogenous distribution of wealth between workers and capitalists which permits such a corner solution of all wealth held by capitalists. However, it also shows interior solutions with a stable, non-zero wealth share of workers, a stable wealth-to-income ratio, and a stable and positive gap between the profit and the growth rate determined by the Cambridge equation. More importantly, simulations show that the model conforms to Piketty's empirical findings during a transitional phase of increasing wealth inequality, which characterizes the current state of high-income countries: The wealth share of capitalists rises to over 60%, the wealth-to-income ratio increases, and income inequality rises. Finally, we show that the introduction of a wealth tax as suggested by Piketty could neutralize this rise in wealth concentration predicted by our model.
C63|Dynamic Macroeconomics: A Didactic Numeric Model|Teaching Dynamic Macroeconomics at undergraduate courses relies exclusively on intuitive prose and graphics depicting behaviours and steady states of the main markets of the economy. But when the case of forward-looking agents and the macroeconomic implications of their actions are discussed, intuitions and graphical representations offered to students may lead to unsupported conclusions. This happens even if the teacher and students use the chapter upon a dynamic macroeconomic model of one of the most didactic and ordered texts ever published: Williamson (2014). In this paper we try to sustain this assertion.
C63|Tendencias actuales en la evaluación de políticas públicas|No abstract is available for this item.
C63|Optimal Taxation of Secondary Earners in the Netherlands: Has Equity Lost Ground?|The Netherlands witnessed major reforms in the taxation of (potential) secondary earners over the past decade. Using the inverse-optimal method of optimal taxation we recover the implicit social welfare weights of single- and dual-earner couples over time. The social welfare weights are grosso modo well-behaved before the reforms. However, after the reforms, they are no longer monotonically declining in income and sometimes negative, suggesting that Pareto-improving reforms are possible. Taken at face value, these results suggest an imbalance between equity and efficiency. However, other considerations may rationalize these fi ndings, like differences in preferences over formal income and informal care.
C63|Aide et Croissance dans les pays de l’Union Economique et Monétaire Ouest Africaine (UEMOA) : retour sur une relation controversée<BR>[Aid and Growth in West African Economic and Monetary Union (WAEMU) countries : a return back to a controversial relationship]|The main purpose of this paper is to analyze threshold effects of official development assistance (ODA) on economic growth in WAEMU zone countries. To achieve this, the study is based on OECD and WDI data covering the period 1980-2015 and used Hansen’s Panel Threshold Regression (PTR) model to “bootstrap” aid threshold above which its effectiveness is effective. The evidence strongly supports the view that the relationship between aid and economic growth is non-linear with a unique threshold which is 12.74% GDP. Above this value, the marginal effect of aid is 0.69 points, “all things being equal to otherwise”. One of the main contribution of this paper is to show that WAEMU countries need investments that could be covered by the foreign aid. This later one should be considered just as a complementary resource. Thus, WEAMU countries should continue to strengthen their efforts in internal resource mobilization in order to fulfil this need.
C63|New and simple algorithms for stable flow problems|Stable flows generalize the well-known concept of stable matchings to markets in which transactions may involve several agents, forwarding flow from one to another. An instance of the problem consists of a capacitated directed network in which vertices express their preferences over their incident edges. A network flow is stable if there is no group of vertices that all could benefit from rerouting the flow along a walk. Fleiner established that a stable flow always exists by reducing it to the stable allocation problem.We present an augmenting path algorithm for computing a stable flow, the first algorithm that achieves polynomial running time for this problem without using stable allocations as a black-box subroutine. We further consider the problem of finding a stable flow such that the flow value on every edge is within a given interval. For this problem, we present an elegant graph transformation and based on this, we devise a simple and fast algorithm, which also can be used to find a solution to the stable marriage problem with forced and forbidden edges. Finally, we study the stable multicommodity flow model introduced by Király and Pap. The original model is highly involved and allows for commoditydependent preference lists at the vertices and commodity-specific edge capacities. We present several graph-based reductions that show equivalence to a significantly simpler model. We further show that it is NP-complete to decide whether an integral solution exists.
C63|The complexity of cake cutting with unequal shares|An unceasing problem of our prevailing society is the fair division of goods. The problem of proportional cake cutting focuses on dividing a heterogeneous and divisible resource, the cake, among n players who value pieces according to their own measure function. The goal is to assign each player a not necessarily connected part of the cake that the player evaluates at least as much as her proportional share. In this paper, we investigate the problem of proportional division with unequal shares, where each player is entitled to receive a predetermined portion of the cake. Our main contribution is threefold. First we present a protocol for integer demands that delivers a proportional solution in fewer queries than all known algorithms. Then we show that our protocol is asymptotically the fastest possible by giving a matching lower bound. Finally, we turn to irrational demands and solve the proportional cake cutting problem by reducing it to the same problem with integer demands only. All results remain valid in a highly general cake cutting model, which can be of independent interest.
C63|Paths to stable allocations|The stable allocation problem is one of the broadest extensions of the well-known stable marriage problem. In an allocation problem, edges of a bipartite graph have capacities and vertices have quotas to fill. Here we investigate the case of uncoordinated processes in stable allocation instances. In this setting, a feasible allocation is given and the aim is to reach a stable allocation by raising the value of the allocation along blocking edges and reducing it on worse edges if needed. Do such myopic changes lead to a stable solution? In our present work, we analyze both better and best response dynamics from an algorithmic point of view. With the help of two deterministic algorithms we show that random procedures reach a stable solution with probability one for all rational input data in both cases. Surprisingly, while there is a polynomial path to stability when better response strategies are played (even for irrational input data), the more intuitive best response steps may require exponential time. We also study the special case of correlated markets. There, random best response strategies lead to a stable allocation in expected polynomial time.
C63|Simulation and Evaluation of Zonal Electricity Market Designs|Zonal pricing with countertrading (a market-based redispatch) gives arbitrage opportunities to the power producers located in the export-constrained nodes. They can increase their profit by increasing the output in the day-ahead market and decrease it in the real-time market (the inc-dec game). We show that this leads to large inefficiencies in a standard zonal market. We also show how the inefficiencies can be significantly mitigated by changing the design of the real-time market. We consider a two-stage game with oligopoly producers, wind-power shocks and real-time shocks. The game is formulated as a two-stage stochastic equilibrium problem with equilibrium constraints (EPEC), which we recast into a two-stage stochastic Mixed-Integer Bilinear Program (MIBLP). We present numerical results for a six-node and the IEEE 24-node system.
C63|Increase-Decrease Game under Imperfect Competition in Two-stage Zonal Power Markets –​ Part I: Concept Analysis|This paper is part I of a two-part paper. It proposes a two-stage game to analyze imperfect competition of producers in zonal power markets with a day-ahead and a real-time market. We consider strategic producers in both markets. They need to take both markets into account when deciding what to bid in each market. The demand shocks between these markets are modeled by several scenarios. The two-stage game is formulated as a Twostage Stochastic Equilibrium Problem with Equilibrium Constraints (TS-EPEC). Then it is further reformulated as a two-stage stochastic Mixed-Integer Linear Program (MILP). The solution of this MILP gives the Subgame Perfect Nash Equilibrium (SPNE). To tackle multiple SPNE, we design a procedure which finds all SPNE with different total dispatch costs. The proposed MILP model is solved using Benders decomposition embedded in the CPLEX solver. The proposed MILP model is demonstrated on the 6-node and the IEEE 30-node example systems.
C63|Increase-Decrease Game under Imperfect Competition in Two-stage Zonal Power Markets –​ Part II: Solution Algorithm|In part I of this paper, we proposed a Mixed-Integer Linear Program (MILP) to analyze imperfect competition of oligopoly producers in two-stage zonal power markets. In part II of this paper, we propose a solution algorithm which decomposes the proposed MILP model into several subproblems and solve them in parallel and iteratively. Our solution algorithm reduces the solution time of the MILP model and it allows us to analyze largescale examples. To tackle the multiple Subgame Perfect Nash Equilibria (SPNE) situation, we propose a SPNE-band approach. The SPNE band is split into several subintervals and the proposed solution algorithm finds a representative SPNE in each subinterval. Each subinterval is independent from each other, so this structure enables us to use parallel computing. We also design a pre-feasibility test to identify the subintervals without SPNE. Our proposed solution algorithm and our SPNE-band approach are demonstrated on the 6-node and the modified IEEE 30-node example systems. The computational tractability of our solution algorithm is illustrated for the IEEE 118-node and 300-node systems.
C63|Exploitation, skills, and inequality|This paper uses a computational framework to analyse the equilibrium dynamics of exploitation and inequality in accumulation economies with heterogeneous labour. A novel index is presented which measures the intensity of exploitation at the individual level and the dynamics of the distribution of exploitation intensity is analysed. The effects of technical change and evolving social norms on exploitation and inequalities are also considered and an interesting phenomenon of exploitation cycles is identified. Various taxation schemes are analysed which may reduce exploitation or inequalities in income and wealth. It is shown that relatively small taxation rates may have significant cumulative effects on wealth and income inequalities. Further, taxation schemes that eliminate exploitation also reduce disparities in income and wealth but in the presence of heterogeneous skills, do not necessarily eliminate them. The inegalitarian effects of different abilities need to be tackled with a progressive education policy that compensates for unfavourable circumstances.
C63|Forecasting Inflation Expectations from the CESifo World Economic Survey: An Empirical Application in Inflation Targeting|The purpose of this paper is twofold. First, we evaluate the responses to the questions on inflation expectations in the World Economic Survey for sixteen inflation targeting countries. Second, we compare inflation expectation forecasts across countries by using a two-step approach that selects the most accurate linear or non-linear forecasting method for each country. Then, using Self Organizing Maps, we cluster the inflation expectations, setting June 2014 as a benchmark. At this time there was a sharp decline in oil prices and by analyzing inflation expectations in the context of this price change, we can discriminate between countries that anticipated the oil shock smoothly and those that had to significantly adjust their expectations. Our main findings from the in-sample comparison of the WES surveys suggest that expert forecasts of inflation expectations are systematically distorted in 83 percent of the countries in the sample. On the other hand, our out of sample forecast analysis indicates that Non-linear Artificial Neural Networks combined with Bayesian regularization outperform ARIMA linear models for longer forecasting horizons. This holds true for countries with both soft and brisk changes of expectations. However, when forecasting one step ahead, the performance between the two methods is similar.
C63|Making Sense of Piketty’s ‘Fundamental Laws’ in a Post-Keynesian Framework: The Transitional Dynamics of Wealth Inequality|If Piketty's main theoretical prediction (r>g leads to rising wealth inequality) is taken to its radical conclusion, then a small elite will own all wealth if capitalism is left to its own devices. We formulate and calibrate a Post-Keynesian model with an endogenous distribution of wealth between workers and capitalists which permits such a corner solution of all wealth held by capitalists. However, it also shows interior solutions with a stable, non-zero wealth share of workers, a stable wealth-to-income ratio, and a stable and positive gap between the profit and the growth rate determined by the Cambridge equation. More importantly, simulations show that the model conforms to Piketty's empirical findings during a transitional phase of increasing wealth inequality, which characterizes the current state of high-income countries: The wealth share of capitalists rises to over 60%, the wealth-to-income ratio increases, and income inequality rises. Finally, we show that the introduction of a wealth tax as suggested by Piketty could neutralize this rise in wealth concentration predicted by our model.<br><small>(This abstract was borrowed from another version of this item.)</small>
C63|“A regional perspective on the accuracy of machine learning forecasts of tourism demand based on data characteristics”|In this work we assess the role of data characteristics in the accuracy of machine learning (ML) tourism forecasts from a spatial perspective. First, we apply a seasonal-trend decomposition procedure based on non-parametric regression to isolate the different components of the time series of international tourism demand to all Spanish regions. This approach allows us to compute a set of measures to describe the features of the data. Second, we analyse the performance of several ML models in a recursive multiple-step-ahead forecasting experiment. In a third step, we rank all seventeen regions according to their characteristics and the obtained forecasting performance, and use the rankings as the input for a multivariate analysis to evaluate the interactions between time series features and the accuracy of the predictions. By means of dimensionality reduction techniques we summarise all the information into two components and project all Spanish regions into perceptual maps. We find that entropy and dispersion show a negative relation with accuracy, while the effect of other data characteristics on forecast accuracy is heavily dependent on the forecast horizon.
C63|“A geometric approach to proxy economic uncertainty by a metric of disagreement among qualitative expectations”|In this study we present a geometric approach to proxy economic uncertainty. We design a positional indicator of disagreement among survey-based agents' expectations about the state of the economy. Previous dispersion-based uncertainty indicators derived from business and consumer surveys exclusively make use of the two extreme pieces of information coming from the respondents expecting a variable to rise and to fall. With the aim of also incorporating the information coming from the share of respondents expecting a variable to remain constant, we propose a geometrical framework and use a barycentric coordinate system to generate a measure of disagreement, referred to as a discrepancy indicator. We assess its performance, both empirically and experimentally, by comparing it to the standard deviation of the share of positive and negative responses, which has been used by Bachman et al. (2013) as a proxy for economic uncertainty. When applied in sixteen European countries, we find that both time-varying metrics co-evolve in most countries for expectations about the country's overall economic situation in the present, but not in the future. Additionally, we obtain their simulated sampling distributions and we find that the proposed indicator gravitates uniformly towards the three vertices of the simplex representing the three answering categories, as opposed to the standard deviation, which tends to overestimate the level of uncertainty as a result of ignoring the no-change responses. Consequently, we find evidence that the information coming from agents expecting a variable to remain constant has an effect on the measurement of disagreement.
C63|Smooth income tax schedules: derivation and consequences|Existing tax schedules are often overly complex and characterized by discontinuities in the marginal tax burden. In this paper we propose a class of progressive smooth functions to replace personal income tax schedules. These functions depend only on three meaningful parameters, and avoid the drawbacks of defining tax schedules through various tax brackets. Based on representative micro data, we derive revenue-neutral parameters for four different types of tax regimes (Austria, Germany, Hungary and Spain). We then analyze possible implications from a hypothetical switch to smoother income tax tariffs. We find that smooth tax functions eliminate the most extreme cases of bracket creep, while the impact on income inequality is mostly negligible, but uniformly reducing.
C63|Robust-yet-fragile: A simulation model on exposure and concentration at interbank networks|This paper presents a layered simulation model and the results from its initial employment. In this study, we focus on financial contagion due to debt exposure and structural concentration at interbank networks. Our results suggest that a medium density of connections in regular networks is already sufficient to induce a ’robust-yet-fragile’ response to insolvency shocks, while the same occurs in star networks only when the centralization is very high. The simulation model enables us to create stock-flow-consistent interbank networks with desired level of network connectivity and centralization. A parsimonious set of network configuration parameters can be employed not only to create stylized network structures with exact connectivity and centralization features but also random core-periphery network representations of a two-tier banking system. Our generic setup decouples the steps of a research on financial contagion. The layers of the simulator covers phases of a research from interbank network configuration to probing the details of a contagion. The presented version enables researchers (i) to create an interbank system of a desired network structure, (ii) to initialize bank balance sheets where the network in previous step can optionally be used as an input, (iii) to configure a controlled or randomized sequence of exogenous shock vectors, (iv) to simulate and inspect detailed process of a single contagion process via tables, graphs and plots generated by the simulator, (v) to design and run automated Monte Carlo simulations, (vi) to analyze results of Monte Carlo simulations via tools from the simulation analysis library.
C63|Convergence of Computed Dynamic Models with Unbounded Shock|The purpose of this paper is to provide the conditions for the convergence of invariant measure obtained from numerical simulations to the exact invariant measure. Santos and Peralta-Alva (2005) have studied the convergence of computed invariant measure of economic models which cannot be solved analytically and must be solved numerically or with some other form of approximation. However, they assume that the state space is compact and therefore, the support of the shock of dynamical system is assumed to be bounded. This paper is to relax the compactness assumption for the convergence of the approximated invariant measure.
C63|The Aggregate Consequences of Tax Evasion|There is a sizeable overall tax gap in the U.S., albeit tax noncompliance differs sharply across income types. While only small percentages of wages and salaries are underreported, the estimated misreporting rate of self-employment business income is substantial. This paper studies how tax evasion in the self-employment sector affects aggregate outcomes and inequality. To this end, we develop a dynamic general equilibrium model with incomplete markets in which heterogeneous agents choose between being a worker and being self-employed. Self-employed agents may hide a share of their business income but are confronted with the probability of being detected by the tax authority. Our model replicates important quantitative features of U.S. data, in particular, the misreporting rate, wealth inequality, and the firm size distribution. Our quantitative findings suggest that tax evasion induces self-employed businesses to stay small. In the aggregate, tax evasion increases the size but decreases the productivity of the self-employment sector. Moreover, it increases aggregate savings and reduces wealth inequality. We show that tax revenues follow a Laffer curve in the size of the tax evasion penalty.
C63|Impacto económico del empredimiento en una economía regional: el caso de Andalucía| Although traditionally entrepreneurship has been considered as one of the engines of economic activity, it has not been until recent years that public authorities have made a planned and organized effort to support the entrepreneurial initiative. However, even though many millions of euros are invested annually in this support, the effectiveness of such investment is rarely measured in terms of the impact of entrepreneurial activity on the economy. For this reason, in this paper we analyze the effect of this activity (entrepreneurship) on a regional economy and its impact on it. To do so, we develop a Computable General Equilibrium (CGE) model for the Andalusian economy for 2015, within a top-down approach. The model is based in the Andalusian Social Accounting Matrix (SAM) updated for the year 2015. A SAM is a statistical-accounting instrument that collects all the information of an economic system and, in addition, closes the circular flow of incomes, considering direct, indirect and induced effects. This gives an overview of the implications of the economic flows on the different sectors of activity and at the same time details and completes them. The SAM for Andalusia 2015 has a disaggregation level of 35 economic activities (27 productive sectors plus 8 endogenous accounts that include items such as capital, consumption, labor, investment, taxes, public sector and sector Exterior). In order to obtain the impact vector for the entrepreneurial activity, necessary to make the estimates for each one of the activity sector, the statistical official information available about business creation in Andalusia has been used. The results will show the effects on Gross domestic product, Productive Output and employment creation and its distribution by sectors of Activity.
C63|Energy policy tools in Luxembourg - Assessing their impact on households’ space heating energy consumption and CO2 emissions by means of the LuxHEI model|In the Grand Duchy of Luxembourg, the residential building sector is a major energy consumer and greenhouse gases emitter that is considered key in achieving the country’s climate goals. The purpose of this paper is to assess the effectiveness of the most important policy instruments in achieving savings in the final energy consumption and direct CO2 emissions of Luxembourgish households. Our study is based on the LuxHEI model, which is an enhanced and upgraded version of the well-known French simulation model Res-IRF. This variant has been adjusted to the particular problems of a small country with growing economy and a quickly increasing population. The LuxHEI model goes beyond standard energy-economy models by incorporating global warming as a decision-making factor. The model outcomes reveal that total environmental and economic effectiveness increases if energy policy tools are applied concurrently. In 2060, and compared to the no-policy baseline scenario, the most aspirational policy mix enables energy savings of 42% and an emission mitigation of 60%. From our results, we can draw the following policy implications: for a significant improvement of the sector’s energy efficiency and sufficiency, (1) the implementation of a remediation duty for existing buildings and (2) the tightening of the performance standards for new constructions, (3) combined with a national carbon tax, are crucial.
C63|Pricing Credit Default Swap Subject to Counterparty Risk and Collateralization|This article presents a new model for valuing a credit default swap (CDS) contract that is affected by multiple credit risks of the buyer, seller and reference entity. We show that default dependency has a significant impact on asset pricing. In fact, correlated default risk is one of the most pervasive threats in financial markets. We also show that a fully collateralized CDS is not equivalent to a risk-free one. In other words, full collateralization cannot eliminate counterparty risk completely in the CDS market.
C63|Aide et Croissance dans les pays de l’Union Economique et Monétaire Ouest Africaine (UEMOA) : retour sur une relation controversée|"The main purpose of this paper is to analyze threshold effects of official development assistance (ODA) on economic growth in WAEMU zone countries. To achieve this, the study is based on OECD and WDI data covering the period 1980-2015 and used Hansen's Panel Threshold Regression (PTR) model to "" bootstrap "" aid threshold above which its effectiveness is effective. The evidence strongly supports the view that the relationship between aid and economic growth is non-linear with a unique threshold which is 12.74% GDP. Above this value, the marginal effect of aid is 0.69 points, "" all things being equal to otherwise "". One of the main contribution of this paper is to show that WAEMU countries need investments that could be covered by the foreign aid. This later one should be considered just as a complementary resource. Thus, WEAMU countries should continue to strengthen their efforts in internal resource mobilization in order to fulfil this need."
C63|Pricing Financial Derivatives Subject to Counterparty Risk and Credit Value Adjustment|This article presents a generic model for pricing financial derivatives subject to counterparty credit risk. Both unilateral and bilateral types of credit risks are considered. Our study shows that credit risk should be modeled as American style options in most cases, which require a backward induction valuation. To correct a common mistake in the literature, we emphasize that the market value of a defaultable derivative is actually a risky value rather than a risk-free value. Credit value adjustment (CVA) is also elaborated. A practical framework is developed for pricing defaultable derivatives and calculating their CVAs at a portfolio level.
C63|Spatiotemporal distribution of inclusive wealth data: An illustrated guide|In this paper we develop an illustrated guide for IWR2017 data. Graphical representations aim to reveal the multi-layer nature of IWR data with self-explanatory schemes. There are four parts of the analysis. In the first part, we present the spatial distribution of the three types of capitals - natural, human and produced - associated to social well-being. In the second part, we illustrate capitals’ temporal variation over 1990-2014, on different geographical and economic backgrounds. We investigate the dynamic evolution of capital assets and capture the key trend among different geographical regions and among regions with different economic growth. The third part makes an additional focus on natural capital and its spatial distribution over different income levels and regions. The forth part examines the causal relation between pollution and wealth. All four research questions are confronted with ease, clarity, and accuracy, with digital methods for mapping. A variety of graphical styles and/or forms is employed to indicate the resource use, capital exploitation trends of countries of different economic integration, uncover policies per income level.
C63|Macroeconomic Implications of Modeling the Internal Revenue Code in a Heterogeneous-Agent Framework|Fiscal policy analysis in heterogeneous-agent models typically involves the use of smooth tax functions to approximate present tax law and proposed reforms. We argue that the tax detail omitted under this conventional approach has macroeconomic implications relevant for policy analysis. In this paper, we develop an alternative approach by embedding an internal tax calculator into a large-scale overlapping generations model that explicitly models key provisions in the Internal Revenue Code applied to labor income. While both approaches generate similar policy-induced patterns of economic activity, we find that the similarities mask differences in key economic aggregates and welfare due to variation in the underlying distribution of household labor supply responses. Absent sufficient tax detail, analysis of specific policy changes - particularly those involving large, discrete effects on a relatively small group of households - using heterogeneous-agent models can be unreliable.
C63|Reporting the natural environmental hazards occurrences and fatalities over the last century|This paper presents the occurrences and fatalities of natural environmental hazards drawing an initial picture of concentration, if there is any. For that reason, the authors use aggregate regional tables as well as map visualizations created in R- studio. As it is shown, there appears to be a space concentration on the natural environmental hazards that need to be deeply examined with the use of advanced econometric techniques.
C63|Simulating the potential of swarm grids for pre-electrified communities - A case study from Yemen|Swarm grids are an emerging approach for electrification in the Global South that interconnects individual household generation and storage to a small electricity network for making full use of existing generation capacities. Using a simulation tool for demand, weather, and power flows, we analyse the potential of an AC swarm grid for a large preelectrified village in rural Yemen. Service quality and financial indicators are compared to the cases of individual supply and a centralised micro grid. While the swarm grid would, in fact, improve supply security from currently 12.4 % (Tier 2) to 81.7 % (Tier 3) at lower levelised costs, it would be inferior to the micro grid in both service (Tier 4) and costs. This is mainly driven by the large pre-installed fossil-fuel generator and storage capacities in our case study. However, this situation may be representative for other relevant locations. Under these conditions, a swarm grid poses the danger to create (possibly-undesired) incentives to invest in diesel generators, and it may fail to support prosumerism effectively. Nevertheless, the swarm’s evolutionary nature with the possibility for staggered investments (e.g. in smaller yet complementary groups of consumers) poses a central advantage over micro grids in the short-term alleviation of energy poverty.
C63|How Internal Violence Lowers Economic Growth: A Theoretical and Empirical Study|In this paper, we introduce a new variable called Internal Violence Index (IVI) and study its effects on economic growth both theoretically and empirically. The first part builds a stochastic endogenous growth model which demonstrates that Internal Violence harms economic growth. On the theoretical side, this paper is the first to introduce a fully-micro-founded endogenous economic growth model that illustrates the explicit effect of Internal Violence on long-run growth in a stochastic dynamic optimization in continuous time framework. On the empirical side, this paper is also the first to employ Linear Regressions and Instrumental Variables Estimations techniques to empirically study the impact of Internal Violence on economic growth. The empirical results corroborate the theoretical predictions that Internal Violence acts negatively on economic growth. The negative impact of Internal Violence on growth are maintained when we use alternative measurements of Internal Violence and subsamples of Least Developed Countries (LDCs) and Non Least Developed Countries.
C63|Aux prémices des humanités numériques? La première analyse automatisée d'un réseau économique ancien (Gardin Garelli, 1961). Réalisation, conceptualisation, réception<BR>[A Precursor of Digital Humanities ? The First Automated Analysis of an Ancient Economic Network (Gardin & Garelli, 1961). Implementation, Theorization, Reception]|From as early as the 1950s, J.C. Gardin's work spanned both archaeology and the emerging automation of numerical computation and documentation. In 1961, with P. Garelli, he published the first automated application of graph theory to historical materials, working from Assyrian cuneiform tablets documenting economic relations. This work was then widely ignored both in archeology and network analysis. However, in the past twenty years, socio-epistemic claims related to the growth of the Internet and computing (digital humanities, computational archaeology, etc.) have brought a surge of interest in Gardin's work, which is now regarded as pioneering. Working from archive materials and publications, this paper shows how a historical sociology of scientific writings can be relevant to the history of automation in historical sciences. The paper examines Gardin's recognition as an influential forerunner of computational archeology, showing that : 1) although Gardin had access to resources (financial, instrumental, etc.) that were rare at the time, and could have provided material for the foundation of a school or a specialty, he did not however pursue this ambition; 2) the demonstrative purposes pursued by Gardin with his study of 1961 economic networks varied between the 1960s (demonstrating the relevance of non-numerical computation) and the 1980s (legitimizing simulation in the social sciences), but were never concerned with network analysis as such.
C63|Dynamic Scoring: An Assessment of Fiscal Closing Assumptions|Analysis of fiscal policy changes using general equilibrium models with forward-looking agents typically requires the modeler to assume a counterfactual adjustment to some fiscal instrument in order to achieve the debt sustainability implied by the government's intertemporal budget constraint. Since the fiscal instrument chosen to close the model can induce economic behavior unrelated to the policy change in models where Ricardian Equivalence does not hold, noise may be introduced into the analysis. In this paper we use such an overlapping generations framework to examine the impact of alternative fiscal closing assumptions on projected changes to economic aggregates over the ten-year `budget window' following a change in tax policy, assessing the extent to which the noise associated with a particular fiscal instrument can be mitigated. We find that while quantitative differences in projected macroeconomic activity can be observed across alternative fiscal instruments, these differences tend to shrink as the date that fiscal instruments begin to adjust is delayed into the future. Since the particular fiscal instrument chosen to achieve debt sustainability can then become relatively unimportant, the reliability of policy analysis obtained using this class of models may be improved.
C63|Income inequality, consumption, credit and credit risk in a data-driven agent-based model|The issue of income inequality occupies a prominent position in the research agenda of academic and policy circles alike, especially after the crisis of 2008, due to its potential causal link with the development of credit bubbles and therefore the emergence of financial crises. This paper examines the long-run effect of income inequality on consumption, consumer credit and non-performing loans through the means of a data-driven agent-based model. The data-driven nature of the model enhances its ability to match historical series and thus makes it suitable for policy simulations tailored for specific economies. The analysis indicates that higher income inequality has a detrimental impact on consumption and is associated with lower volumes of consumer credit. However, the ratio of non-performing loans as a share of total loans seems to be independent of income inequality.
