C11|The Effect of Aspirations on Inequality: Evidence from the German Reunification using Bayesian Growth Incidence Curves|A long-standing literature has investigated the formation of aspirations and how they shape human behaviours but a recent interest has been devoted on the interplay between aspirations and inequality. Because aspirations are socially determined, household investment decisions tend to be reproduced according to the social context which fosters inequality to persist. We empirically examine the role of aspirations on inequality using a natural experiment. We exploit an exogenous variation of social aspirations determined by the exposure to Western German TV broadcasts in the GDR before the reunification. We measure the treatment effect on wage inequality by comparing inequality changes between the treatment and the control regions after reunification. We use an heteroskedastic parametric model for income with a treatment effect and sample selection into the labour market. We derive analytical formulae for the growth incidence curve of Ravallion and Chen (2003) and poverty growth curve of Son (2004) for the log-normal distribution. Based on those curves, we provide Bayesian inference and a set of tests related to stochastic dominance criteria. We find evidences that aspirations-through exposure to Western German broadcasts-have significantly affected inequality. We find that this effect was detrimental in terms of inequality and poverty. However, we cannot conclude about the persistence of the effect after 1995.
C11|Deciding with judgment|Non sample information is hidden in frequentist statistics in the choice of the hypothesis to be tested and of the confidence level. Explicit treatment of these elements provides the connection between Bayesian and frequentist statistics. A frequentist decision maker starts from a judgmental decision and moves to the closest boundary of the confidence interval of the first order conditions, for a given loss function. This statistical decision rule does not perform worse than the judgmental decision with a probability equal to the confidence level. For any given prior, there is a mapping from the sample realization to the confidence level which makes Bayesian and frequentist decision rules equivalent. Frequentist decision rules can be interpreted as decisions under ambiguity. JEL Classification: C1, C11, C12, C13, D81
C11|Bayesian MIDAS penalized regressions: estimation, selection, and prediction|We propose a new approach to mixed-frequency regressions in a high-dimensional environment that resorts to Group Lasso penalization and Bayesian techniques for estimation and inference. To improve the sparse recovery ability of the model, we also consider a Group Lasso with a spike-and-slab prior. Penalty hyper-parameters governing the model shrinkage are automatically tuned via an adaptive MCMC algorithm. Simulations show that the proposed models have good selection and forecasting performance, even when the design matrix presents high cross-correlation. When applied to U.S. GDP data, the results suggest that financial variables may have some, although limited, short-term predictive content.
C11|Inducing Sparsity and Shrinkage in Time-Varying Parameter Models|Time-varying parameter (TVP) models have the potential to be over-parameterized, particularly when the number of variables in the model is large. Global-local priors are increasingly used to induce shrink- age in such models. But the estimates produced by these priors can still have appreciable uncertainty. Sparsification has the potential to remove this uncertainty and improve forecasts. In this paper, we develop computationally simple methods which both shrink and sparsify TVP models. In a simulated data exercise we show the benefits of our shrink-then-sparsify approach in a variety of sparse and dense TVP regressions. In a macroeconomic forecast exercise, we find our approach to substantially improve forecast performance relative to shrinkage alone.
C11|Fluctuations in Global Macro Volatility|We rely on a hierarchical volatility factor approach to estimate and decompose timevarying second moments of output growth into global, regional and idiosyncratic contributions. We document a “global moderation” of international business cycles, defined as a persistent decline in macroeconomic volatility across the main world economies. This decline in volatility was induced by a reduction in the underlying global component, uncovering a new level of interconnection of the world economy. After assessing the importance of different economic factors, we find that the reduction in overall countries macroeconomic volatility can be mainly explained by the increasing trade openness exhibited in recent decades. Likewise, the idiosyncratic component of countries volatility is also influenced by domestic monetary policies.
C11|The euro-area output gap through the lens of a DSGE model|The paper provides estimates of the euro-area output gap, based on a relatively standard medium scale DSGE model estimated recursively with Bayesian techniques over the period 1985-2016. The main findings can be summarized as follows. First, our measure of output gap identifies episodes of expansion and recession generally in line with the official business cycle dating of the CEPR. Second, unlike measures of output gap obtained by means of statistical filtering techniques, real-time DSGE-based estimates are remarkably stable and hence are less prone to ex-post revisions. According to our results, the euro-area output gap was -3.4% in 2016, more negative than assessed by most economic analysts and institutions (spanning a range between from 0 and to -2%), but arguably more consistent with the still weak dynamics of both labour costs and core inflation.
C11|Bayesian MIDAS Penalized Regressions: Estimation, Selection, and Prediction|We propose a new approach to mixed-frequency regressions in a high-dimensional environment that resorts to Group Lasso penalization and Bayesian techniques for estimation and inference. To improve the sparse recovery ability of the model, we also consider a Group Lasso with a spike-and-slab prior. Penalty hyper-parameters governing the model shrinkage are automatically tuned via an adaptive MCMC algorithm. Simulations show that the proposed models have good selection and forecasting performance, even when the design matrix presents high cross-correlation. When applied to U.S. GDP data, the results suggest that financial variables may have some, although limited, short-term predictive content.
C11|Bayesian Inference for Markov-switching Skewed Autoregressive Models|We examine Markov-switching autoregressive models where the commonly used Gaussian assumption for disturbances is replaced with a skew-normal distribution. This allows us to detect regime changes not only in the mean and the variance of a specified time series, but also in its skewness. A Bayesian framework is developed based on Markov chain Monte Carlo sampling. Our informative prior distributions lead to closed-form full conditional posterior distributions, whose sampling can be efficiently conducted within a Gibbs sampling scheme. The usefulness of the methodology is illustrated with a real-data example from U.S. stock markets.
C11|Steady-state growth|We compute steady-state economic growth - defined as the rate of growth that the economy would converge to in the absence of new shocks. This rate can be computed in real-time by means of a parsimonious time-varying parameter (TVP) VAR model. Our procedure offers a relatively agnostic estimation of benchmark equilibrium growth rates. Estimates show that the steady-state GDP growth rate in the case of the United States declined from just above 3% per year in the 1990s to 2.4% at present. Results for other six advanced economies and the euro area indicate that the steady-state growth rate, which is consistent with stable inflation and financial conditions, has been relatively stable since 2010 in most cases in spite of a recent slowdown in actual GDP growth rates.
C11|Multivariate Bayesian Predictive Synthesis in Macroeconomic Forecasting|We present new methodology and a case study in use of a class of Bayesian predictive synthesis (BPS) models for multivariate time series forecasting. This extends the foundational BPS framework to the multivariate setting, with detailed application in the topical and challenging context of multi-step macroeconomic forecasting in a monetary policy setting. BPS evaluates– sequentially and adaptively over time– varying forecast biases and facets of miscalibration of individual forecast densities for multiple time series, and– critically– their time-varying interdependencies. We define BPS methodology for a new class of dynamic multivariate latent factor models implied by BPS theory. Structured dynamic latent factor BPS is here motivated by the application context– sequential forecasting of multiple US macroeconomic time series with forecasts generated from several traditional econometric time series models. The case study highlights the potential of BPS to improve of forecasts of multiple series at multiple forecast horizons, and its use in learning dynamic relationships among forecasting models or agents.
C11|News-driven inflation expectations and information rigidities|We investigate the role played by the media in the expectations formation process of households. Using a news-topic-based approach we show that news types the media choose to report on, e.g., (Internet) technology, health, and politics, are good predictors of households' stated in ation expectations. In turn, in a noisy information model setting, augmented with a simple media channel, we document that the underlying time series properties of relevant news topics explain the timevarying information rigidity among households. As such, we not only provide a novel estimate showing the degree to which information rigidities among households vary across time, but also provide, using a large news corpus and machine learning algorithms, robust and new evidence highlighting the role of the media for understanding infl ation expectations and information rigidities.
C11|The Shale Oil Boom and the U.S. Economy: Spillovers and Time-Varying Effects|We analyze if the transmission of oil price shocks on the U.S. economy has changed as a result of the shale oil boom. To do so we allow for spillovers at the state level, as well as aggregate country level effects. We identify and quantify these spillovers using a factor-augmented vector autoregressive (VAR) model, allowing for time-varying changes. In contrast to previous results, we find considerable changes in the way oil price shocks are transmitted: there are now positive spillovers to non-oil investment, employment and production in many U.S. states from an increase in the oil price - effects that were not present before the shale oil boom.
C11|Multivariate Bayesian Predictive Synthesis in Macroeconomic Forecasting|We present new methodology and a case study in use of a class of Bayesian predictive synthesis (BPS) models for multivariate time series forecasting. This extends the foundational BPS framework to the multivariate setting, with detailed application in the topical and challenging context of multi-step macroeconomic forecasting in a monetary policy setting. BPS evaluates - sequentially and adaptively over time – varying forecast biases and facets of miscalibration of individual forecast densities for multiple time series, and – critically – their time-varying interdependencies. We define BPS methodology for a new class of dynamic multivariate latent factor models implied by BPS theory. Structured dynamic latent factor BPS is here motivated by the application context–sequential forecasting of multiple US macroeconomic time series with forecasts generated from several traditional econometric time series models. The case study highlights the potential of BPS to improve of forecasts of multiple series at multiple forecast horizons, and its use in learning dynamic relationships among forecasting models or agents.
C11|News-driven inflation expectations and information rigidities|We investigate the role played by the media in the expectations formation process of households. Using a novel news-topic-based approach we show that news types the media choose to report on, e.g., fiscal policy, health, and politics, are good predictors of households' stated inflation expectations. In turn, in a noisy information model setting, augmented with a simple media channel, we document that the underlying time series properties of relevant news topics explain the time-varying information rigidity among households. As such, we not only provide a novel estimate showing the degree to which information rigidities among households varies across time, but also provide, using a large news corpus and machine learning algorithms, robust and new evidence highlighting the role of the media for understanding inflation expectations and information rigidities.
C11|Trend and cycle shocks in Bayesian unobserved components models for UK productivity|This paper presents a range of unobserved components models to study productivity dynamics in the United Kingdom. We introduce a set of univariate and bivariate models that allow for shocks between the trend and the cycle to be correlated, and use Bayesian sampling techniques to estimate the models. We show that the size of the priors on the trend and cycle shock has an effect on the results, suggesting that a range of priors need to be considered for policy-making purposes. If the prior is set to a smooth trend, then models with little correlation between the trend and cycle shocks are the likeliest to fit the data. On the other hand, if there is a prior belief that the trend shock is allowed to vary relatively freely, the results suggest that there is a negative correlation between trend and cycle shocks to LIK productivity. This is consistent with real-business cycle type narratives, where trend shocks are the main driver of productivity dynamics. Finally, our evidence suggests that the trend productivity growth rate in the UK has been weaker since the financial crisis. There is also a significant positive correlation between shocks to UK trend productivity and those of other advanced economies.
C11|Assessing reliability of aggregated inflation views in the European Commission consumer survey|Using a novel approach based on micro-level survey responses, we assess the reliability of aggregated inflation expectations estimates in the European Commission Consumer Survey. We identify the share of consumers, whose qualitative and quantitative views on expected increase of prices do not match each other. Then we consider the impact of inconsistent survey responses on balance statistics and mean values of quantitative inflation expectations. We also analyze expectations’ formation estimating the sticky-information models. The results, based on Finnish and Polish data, suggest that even if the fraction of inconsistent survey responses is non-negligible, it matters neither for the aggregated figures of inflation views, nor for understanding of the formation of inflation expectations by consumers. We conclude that micro-level inconsistencies do not reduce the reliability of the current EC Consumer Survey dataset. Our results also indicate that inconsistent responses are not important drivers of the inflation overestimation bias displayed in the data.
C11|Optimism in Financial Markets: Stock Market Returns and Investor Sentiments|This paper investigates how investor sentiment affects stock market returns and evaluates the predictability power of sentiment indices on U.S. and EU stock market returns. As regards the American example, evidence shows that investor sentiment indices have a negative influence on stock market returns. Concerning the European market instead, investigation provides weak results. Moreover, comparing the two markets, where investor sentiment of U.S. market tries to predict the European stock market returns, and vice versa, the analyses indicate a spillover effect from the U.S. to Europe.
C11|Density Forecasting|This paper reviews different methods to construct density forecasts and to aggregate forecasts from many sources. Density evaluation tools to measure the accuracy of density forecasts are reviewed and calibration methods for improving the accuracy of forecasts are presented. The manuscript provides some numerical simulation tools to approximate predictive densities with a focus on parallel computing on graphical process units. Some simple examples are proposed to illustrate the methods.
C11|Applying Bayesian Model Averaging to Characterise Urban Residential Stock Turnover Dynamics|Building stock is a key determinant in building energy and China is the largest producer of CO2 emissions and the largest consumer of energy and building energy, so any effective energy and climate policy will need to address this key driver of energy use. However, official statistics on total floor area of urban residential stock in China only exist up to 2006. Previous studies estimating Chinese urban residential stock size and energy use made various questionable methodological assumptions and only produced deterministic results. We present a Bayesian approach to characterise the stock turnover dynamics and estimate stock size uncertainties. Firstly, a probabilistic dynamic building stock turnover model is developed to describe the building aging and demolition process governed by a hazard function specified by a parametric survival model. Secondly, using five candidate parametric survival models, the building stock turnover model is simulated through Markov Chain Monte Carlo (MCMC) to obtain posterior distributions of model-specific parameters, estimate marginal likelihood, and make predictions on stock size. Finally, Bayesian Model Averaging (BMA) is applied to create a model ensemble that combines the model-specific posterior predictive distributions of the stock evolution pathway in proportion to posterior model probabilities. This Bayesian modelling framework and its results in the form of probability distributions of annual total stock and age-specific substocks, can provide a solid basis for further modelling and analysis of policy trade-offs across embodied-versus-operational energy consumption and carbon emissions of buildings in the context of sector-wide transitions aimed at decarbonising buildings.
C11|Estimated human capital externalities in an endogenous growth framework|To better understand the quantitative implications of human capital externalities at the aggregate level, we estimate a two-sector endogenous growth model with knowledge spill-overs. To achieve this, we account for trend growth in a model consistent fashion and employ a Markov-chain Monte-Carlo (MCMC) algorithm to estimate the model's posterior parameter distributions. Using U.S. quarterly data from 1964-2017, we find significant positive externalities to aggregate human capital. Our analysis further shows that eliminating this market failure leads to sizeable increases in education-time, endogenous growth and aggregate welfare.
C11|The Origins and Effects of Macroeconomic Uncertainty|We construct and estimate a dynamic stochastic general equilibrium model that features demand- and supply-side uncertainty. Using term structure and macroeconomic data, we find sizable effects of uncertainty on risk premia and business cycle fluctuations. Both demand-side and supply-side uncertainty imply large contractions in real activity and an increase in term premia, but supply-side uncertainty has larger effects on inflation and investment. We introduce a novel analytical decomposition to illustrate how multiple distinct risk propagation channels account for these differences. Supply and demand uncertainty are strongly correlated in the beginning of our sample, but decouple in the aftermath of the Great Recession.
C11|A Structural Model for the Coevolution of Networks and Behavior|This paper introduces a structural model for the coevolution of networks and behavior. The microfoundation of our model is a network game where agents adjust actions and network links in a stochastic best-response dynamics with a utility function allowing for both strategic externalities and unobserved heterogeneity. We show the network game admits a potential function and the coevolution process converges to a unique stationary distribution characterized by a Gibbs measure. To bypass the evaluation of the intractable normalizing constant in the Gibbs measure, we adopt the Double Metropolis-Hastings algorithm to sample from the posterior distribution of the structural parameters. To illustrate the empirical relevance of our structural model, we apply it to study R&D investment and collaboration decisions in the chemicals and pharmaceutical industry and find a positive knowledge spillover effect. Finally, our structural model provides a tractable framework for a long-run key player analysis.
C11|Assessing International Commonality in Macroeconomic Uncertainty and Its Effects|This paper uses a large vector autoregression to measure international macroeconomic uncertainty and its effects on major economies. We provide evidence of signi cant commonality in macroeconomic volatility, with one common factor driving strong comovement across economies and variables. We measure uncertainty and its effects with a large model in which the error volatilities feature a factor structure containing time-varying global components and idiosyncratic components. Global uncertainty contemporaneously affects both the levels and volatilities of the included variables. Our new estimates of international macroeconomic uncertainty indicate that surprise increases in uncertainty reduce output and stock prices, adversely affect labor market conditions, and in some economies lead to an easing of monetary policy.
C11|The ECB’s monetary pillar after the financial crisis|"We apply a structural topic model (STM) to analyze European Central Bank (ECB) communication regarding the monetary pillar of its monetary policy strategy. We do so by quantifying the transcripts of the ECB Presidents introductory statements at the press conferences that accompany the regular meetings of the ECB Governing Council. Our evidence shows that, within its monetary pillar, the ECB has gradually shifted its focus away from a genuine monetary analysis towards monitoring the stability of the European financial system. We go on to augment a standard Taylor rule by quantitative indicators obtained from the STM to assess whether the monetary pillar in general, and the shift in focus in particular, has had a measurable impact on the ECBs monetary policy stance. We find weak evidence that the monetary analysis has had a bearing on the ECBs interest rate setting in the early years of the ECB's existence, but this influence completely disappears in the latter years of the sample. We also find that after the financial crisis, the monetary policy response to its financial sentiment communication has been accommodative rather than ""leaning against the wind""."
C11|Bayesian Structural VAR models: a new approach for prior beliefs on impulse responses| Structural VAR models are frequently identified using sign restrictions on impulse responses. Moving beyond the popular but restrictive Normal-inverse-Wishart-Uniform prior, we develop a methodology that can handle almost any prior distribution on contemporaneous responses. We then propose a new sampler that explores the posterior just as efficiently as done by the existing algorithm for the Normal-inverse-Wishart-Uniform case. We use this exible and tractable framework to combine sign restrictions with information on the volatility of the data, giving less prior mass to impulse effects that are inconsistent with the data from a training sample. This approach sharpens posterior bands and makes sign restrictions more informative. We apply the methodology to the oil market and show that oil supply shocks have a non-negligible effect on oil price dynamics.
C11|Local Constant-Quality Housing Market Liquidity Indices|The average time on market (TOM) of sold properties is frequently used by practitioners and policymakers as a market liquidity indicator. This figure might be misleading as the average TOM only considers properties that have been sold. Furthermore, traded properties are heterogeneous. Since these features differ over the cycle, the average TOM could provide wrong signals about market liquidity. These problems are more severe in markets where properties trade infrequently. In this paper, a methodology is provided that allows for the construction of constant-quality housing market liquidity indices in thin markets that can be estimated up to the end of the sample. The latter is particularly important since market watchers are generally interested in the most recent information regarding market liquidity and less in historical information. Using individual transactions data on three different types of Dutch municipalities (small, medium, and large) it is shown that the average TOM overestimates market liquidity in bad times and underestimates market liquidity in good times. The option to withdraw is the most important reason why the average TOM is misleading. Furthermore, constant-quality liquidity leads the average TOM and price changes. The indices not only show that illiquidity is higher during busts, but also that liquidity risk is higher. Additional results suggest that setting a high list price relative to the estimated value results in a higher TOM, but this effect differs over time. Both the list price premium and the effect on sale probability are higher during busts. Differences in housing quality over the cycle, however, also play a significant role. Finally, the method allows for the construction of indices that are more robust to revisions, especially in thinner markets.
C11|Behavioral learning equilibria in the New Keynesian model|We introduce the concept of behavioral learning equilibrium (BLE) into a high dimensional linear framework and apply it to the standard New Keynesian model. For each endogenous variable, boundedly rational agents use a simple, but optimal AR(1) forecasting rule with parameters consistent with the observed sample mean and autocorrelation of past data. The main contributions of our paper are fourfold: (1) we derive existence and stability conditions of BLE in a general linear framework, (2) we provide a general method for Bayesian likelihood estimation of BLE, (3) we estimate the baseline NK model based on U.S. data and show that the relative model fit is better under BLE than REE, (4) we analyze optimal monetary policy under BLE and show that it differs from REE. In particular, we find that the transmission channel of monetary policy is stronger under BLE at the estimated parameter values.
C11|Do public wages in the euro area explain private wage developments? An empirical investigation|This paper investigates the relationship between public and private wages in the five largest euro area countries for the period 1997-2017. The analysis shows that there exists a positive and significant response of private wages to a public wage shock. This effect is found to be temporary and to differ across countries (positive and significant in France, Spain, Italy and non-significant in Germany and the Netherlands). Interestingly, the response of private wages is found to be asymmetric: a positive and statistically significant response is found in case of a positive shock to public wages, while no statistically significant effects are detected in case of a cut to public wages. As the public wage containment policies adopted during the sovereign debt crisis are expected to be gradually lifted in several euro area countries, the findings of this paper suggest that knock-on effects on private sector wages cannot be excluded in the years to come. JEL Classification: E24, E62, J30, C33, C11
C11|Forecasting daily electricity prices with monthly macroeconomic variables|We analyse the importance of macroeconomic information, such as industrial production index and oil price, for forecasting daily electricity prices in two of the main European markets, Germany and Italy. We do that by means of mixed-frequency models, introducing a Bayesian approach to reverse unrestricted MIDAS models (RU-MIDAS). We study the forecasting accuracy for different horizons (from 1 day ahead to 28 days ahead) and by considering different specifications of the models. We find gains around 20% at short horizons and around 10% at long horizons. Therefore, it turns out that the macroeconomic low frequency variables are more important for short horizons than for longer horizons. The benchmark is almost never included in the model confidence set. JEL Classification: C11, C53, Q43, Q47
C11|Taylor-rule consistent estimates of the natural rate of interest|We estimate the natural rate of interest for the US and the euro area in a semi-structural model comprising a Taylor rule. Our estimates feature key elements of Laubach and Williams (2003), but are more consistent with using conventional policy rules: we model inflation to be stationary, with the output gap pinning down deviations of inflation from its objective (rather than relative to a random walk). We relax some constraints on the correlation of latent factor shocks to make the original unobserved-components framework more amenable to structural interpretation and to reduce filtering uncertainty. We show that resulting natural rate metrics are more consistent with estimates from structural models. JEL Classification: C11, E32, E43, E52
C11|Uncertainty Shocks, Monetary Policy and Long-Term Interest Rates|"We study the relationship between monetary policy and long-term rates in a structural, general equilibrium model estimated on both macro and yields data from the United States. Regime shifts in the conditional variance of productivity shocks, or ""uncertainty shocks"", are an important model ingredient. First, they account for countercyclical movements in risk premia. Second, they induce changes in the demand for precautionary saving, which affects expected future real rates. Through changes in both risk-premia and expected future real rates, uncertainty shocks account for about 1/2 of the variance of long-term nominal yields over long horizons. The remaining driver of long-term yields are changes in in ation expectations induced by conventional, autoregressive shocks. Long-term in ation expectations implied by our model are in line with those based on survey data over the 1980s and 1990s, but less dogmatically anchored in the 2000s."
C11|Determinants of German outward FDI: variable selection using Bayesian statistical|"This paper provides new evidence on the drivers of German outward foreign direct investment (FDI) stocks for the period 1996-2012. In contrast to previous empirical studies, we adopt a Bayesian model averaging (BMA) approach for a robust selection of those variables. We find evidence that determinants that are associated with horizontal FDI appear to be dominant for explaining bilateral FDI with developed countries while for the group of developing countries covariates associated with vertical FDI motives play a larger role. Within Europe, while the majority of FDI is horizontally driven in “core"" countries, for peripheral ones the vertical motivation for FDI seems to prevail. Moreover, our results are compatible with more complex FDI models where vertical determinants and institutional variables are gaining prominence, in parallel with the development of global value chains (GVC). Our results can provide hints for policymakers’ strategies to attract German investment."
C11|New Evidence on the Effects of Quantitative Easing|Have the macroeconomic effects of QE programs been overestimated empirically? Using a large set of model specifications that differ in the degree of time-variation in parameters, the answer is yes. Our forecasting exercise suggests that it is crucial to allow for time-variation in parameters, but not for stochastic volatility to improve the fit with data. Having a more reliable specification, we find that the portfolio balance and signaling channels had sizable contributions to the transmission of QE programs. Finally, our identified structural shocks show that QE1 had larger macroeconomic effects than QE2 and QE3, but much smaller than usually found in the literature.
C11|A flexible approach to age dependence in organizational mortality. Comparing the life duration for cooperative and non-cooperative enterprises using a Bayesian Generalized Additive Discrete Time Survival Model|This paper proposes a new estimation model to capture the complex effect of age on organization survival. Testing various theoretical propositions on organizational mortality, we study the survival of French agricultural cooperatives in comparison with other firms with which they compete. The relationship between age and mortality in organizations is analyzed using a Bayesian Generalized discrete-time semi-parametric hazard model with correlated random effects, incorporating unobserved heterogeneity and isolating the various effects of time. This analysis emphasizes the specificity of the temporal dynamics of cooperatives in relation to their special role in agriculture.
C11|Application of the economic theory of self-control to model energy conservation behavioral change in households|Smart meters and in-house displays hold a promise of energy conservation for those who invest in such technology. Research has shown that households only have a limited interest in such technology and information is thus often neglected, with rather limited energy savings. Surprisingly few empirical investigations have a theoretical foundation that may explain what is going on from a behavioral perspective. In this study the economic theory of self-control is used to model energy-eﬃcient behavior in middle-income households in Sweden. Our results show that diﬀerent levels of energy-eﬃcient behavior do not really have any impact on the actual consumption levels of electricity. Instead, diﬀerent beliefs exist of being energy-eﬃcient, but the households do not act accordingly. Our results suggest that the payment time period should be changed to stimulate the monitoring of bills and to introduce a gaming strategy to change incentives for energy conservation.
C11|The Relation between the Corporate Bond-Yield Spread and the Real Economy: Stable or TimeVarying?|In this paper we assess whether the relation between the corporate bond-yield spread and the real economy has been stable over time. Using quarterly US data from 1953Q1 to 2018Q2, we estimate Bayesian VAR models which allow for drifting parameters and/or stochastic volatility and conduct formal model selection in a Bayesian setting. Our results indicate that the relation between the variables has been stable; we do, however, find strong support for stochastic volatility. We conclude that the corporate bond-yield spread’s usefulness for predicting real economic activity has not changed to a relevant extent after the Great Reces-sion.
C11|The Interaction Between Fiscal and Monetary Policies: Evidence from Sweden|This paper estimates the interaction between monetary- and fiscal policy using a structural VAR model with time-varying parameters. For demand and supply shocks, the two policies are estimated to be complementary, while for monetary and fiscal policies shocks the two policies act as substitutes. The budget elasticity varies between 0.3–0.6, indicating that an economic downturn can get a non-negligible negative impact on public finances. The fiscal multiplier is estimated to be stable and higher than one suggesting that fiscal policy can be used to support monetary policy to stabilize the economy in case monetary policy is constrained by the lower effective bound.
C11|Subsampling Sequential Monte Carlo for Static Bayesian Models|We show how to speed up Sequential Monte Carlo (SMC) for Bayesian inference in large data problems by data subsampling. SMC sequentially updates a cloud of particles through a sequence of distributions, beginning with a distribution that is easy to sample from such as the prior and ending with the posterior distribution. Each update of the particle cloud consists of three steps: reweighting, resampling, and moving. In the move step, each particle is moved using a Markov kernel and this is typically the most computation- ally expensive part, particularly when the dataset is large. It is crucial to have an efficient move step to ensure particle diversity. Our article makes two important contributions. First, in order to speed up the SMC computation, we use an approximately unbiased and efficient annealed likelihood estimator based on data subsampling. The subsampling approach is more memory effi- cient than the corresponding full data SMC, which is an advantage for parallel computation. Second, we use a Metropolis within Gibbs kernel with two con- ditional updates. A Hamiltonian Monte Carlo update makes distant moves for the model parameters, and a block pseudo-marginal proposal is used for the particles corresponding to the auxiliary variables for the data subsampling. We demonstrate the usefulness of the methodology for estimating three gen- eralized linear models and a generalized additive model with large datasets.
C11|State Dependence in Labor Market Fluctuations: Evidence, Theory, and Policy Implications|This paper documents state dependence in labor market ï¬‚uctuations. Using a Threshold Vector-Autoregression model, we establish that the unemployment rate, the job separation rate and the job ï¬ nding rate exhibit a larger response to productivity shocks during periods with low aggregate productivity. A Diamond-Mortensen-Pissarides model with endogenous job separation and on-the-job search replicates these empirical regularities well. The transition rates into and out of employment embed state dependence through the interaction of reservation productivity levels and the distribution of match-speciï¬ c idiosyncratic productivity. State dependence implies that the eï¬€ect of labor market reforms is diï¬€erent across phases of the business cycle. A permanent removal of layoï¬€ taxes is welfare enhancing in the long run, but it involves distinct short-run costs depending on the initial state of the economy. The welfare gain of a tax removal implemented in a low-productivity state is 4.9 percent larger than the same reform enacted in a state with high aggregate productivity.
C11|Modeling of Economic and Financial Conditions for Nowcasting and Forecasting Recessions: A Unified Approach|This paper puts forward a unified framework for the joint estimation of the indexes that can broadly capture economic and financiall conditions together with their cyclical regimes of recession and expansion. We do this by utilizing a dynamic factor model together with Markov regime switching dynamics of model parameters that specifically exploit the temporal link between the cyclical behavior of economic and financial factors. This is achieved by constructing the cycle in the financial factor using the cycle in the economic factor together with phase shifts. The resulting framework allows the financial cycle to potentially lead/lag the business cycle in a systematic manner and exploits the information in economic and financial variables for estimation of both economic and financial conditions as well as their cyclical behavior in an efficient way. We examine the potential of the model using a mixed frequency and mixed time span ragged-edge dataset for Turkey. Comparison of our framework with more conventional polar cases imposing a single common cyclical dynamics as well as independent cyclical dynamics for economic and financial conditions reveal that the proposedspecification provides precise estimates of economic and financial conditions and it delivers quite accurate probabilities of recessions that match with stylized facts. We further conduct a recursive real-time exercise of nowcasting and forecasting business cycle turning points. The results show convincing evidence of superior predictive power of our specification by signaling oncoming recessions (expansions) as early as 3.5 (3.4) months ahead of the actual realization.
C11|Time, space and hedonic prediction accuracy evidence from the Corsican apartment market|In this study, we propose a hedonic housing model to address spa- tial and temporal latent structures simultaneously. With the development of spatial econometrics and spatial statistics, economists can now better assess the impact of spatial correlation on house prices. How- ever, the simultaneous handling of spatial and temporal correlation is still under development. Since spatial econometric models are limited to account for two kinds of cor- relation simultaneously, we propose using a hierarchical spatiotemporal model from spatial statistics. Based on a Bayesian framework and a stochastic par- tial differential equation (SPDE) approach, the estimation is carried out via INLA. We then perform an empirical study on apartment transaction prices in Corsica (France) using the proposed model. The empirical results demonstrate that the prediction performance of the hierarchical spatiotemporal model is the best among all candidate models. Moreover, the hedonic housing estimates are affected by spatial effects and temporal effects. Ignoring these effects could result in serious forecasting issues.
C11|Updating Variational Bayes: Fast Sequential Posterior Inference|Variational Bayesian (VB) methods usually produce posterior inference in a time frame considerably smaller than traditional Markov Chain Monte Carlo approaches. Although the VB posterior is an approximation, it has been shown to produce good parameter estimates and predicted values when a rich class of approximating distributions are considered. In this paper we propose Updating VB (UVB), a recursive algorithm used to update a sequence of VB posterior approximations in an online setting, with the computation of each posterior update requiring only the data observed since the previous update. An extension to the proposed algorithm, named UVB-IS, allows the user to trade accuracy for a substantial increase in computational speed through the use of importance sampling. The two methods and their properties are detailed in two separate simulation studies. Two empirical illustrations of the proposed UVB methods are provided, including one where a Dirichlet Process Mixture model is repeatedly updated in the context of predicting the future behaviour of vehicles on a stretch of the US Highway 101.
C11|Institutional determinants of export competitiveness among the EU countries: evidence from Bayesian model averaging|Although the impact of institutions has been broadly studied in the literature on economic growth, their impact on international trade is less well-established. We aim to fill this gap by creating an extended database that, apart from price and non-price factors traditionally analyzed as determinants of exports, also includes measures of institutional development. Next, we introduce the Bayesian Model Averaging to establish which factors play the most important role for the export performance. Our results show that institutions have two types of effects on exports: a direct positive effect on the overall export performance (e.g. regulation) as well as a transformational impact on the export structure (from less to more technologically advanced exports, e.g. freedom to trade internationally). Our results also confirm that technological factors (e.g. patents) have a much greater impact on export performance than price factors. Moreover, some technological factors only have a significant transformational impact on the export structure (e.g. R&D expenditure). Human capital also seems to have only a transformational, rather than direct, impact on exports.
C11|Variational Bayesian inference in large Vector Autoregressions with hierarchical shrinkage|Many recent papers in macroeconomics have used large Vector Autoregressions (VARs) involving a hundred or more dependent variables. With so many parameters to estimate, Bayesian prior shrinkage is vital in achieving reasonable results. Computational concerns currently limit the range of priors used and render difficult the addition of empirically important features such as stochastic volatility to the large VAR. In this paper, we develop variational Bayes methods for large VARs which overcome the computational hurdle and allow for Bayesian inference in large VARs with a range of hierarchical shrinkage priors and with time-varying volatilities. We demonstrate the computational feasibility and good forecast performance of our methods in an empirical application involving a large quarterly US macroeconomic data set.
C11|Launch of a product and patents: evidence from the US cardiovascular pharmaceutical sector|Recent literature on the role of patents in shaping competition between incumbents and new entrants shows mixed evidence, as patents can discourage entry into markets but may also encourage potential entrants by increasing profitability from research and development. The increasing use of patents as strategic weapons motivates this investigation of the impact of innovation on competition. In a case study of US pharmaceutical cardiovascular submarkets over the period 1988-1998, we use a panel probit model to study the impact of a firm’s patents and rivals’ patents in the firm’s decision to launch new products. Our results show that the number of a firm’s lagged patents encourages the firm’s entry with new products, while rivals’ initial stock of patents discourages entry, but more recent patents promote entry by opening new technological opportunities.
C11|Transmission of sectoral debt shocks in OECD countries: Evidence from the income channel|We examine the propagation of debt shocks across sectors of the economy for OECD countries. Our focus lies on assessing the importance of the income channel as a main transmission mechanism of such shocks. Employing a Bayesian Panel VAR, we find strong debt contagion effects across sectors, which work through the income channel. Higher non-financial corporate debt drives down household incomes, increasing pressures for household deleveraging. By contrast, an increase in household debt boosts real incomes and domestic demand, and results in higher corporate leverage. Finally, we find that growth effects of sectoral debt shocks are conditional on country idiosyncrasies.
C11|Online Estimation of DSGE Models|This paper illustrates the usefulness of sequential Monte Carlo (SMC) methods in approximating DSGE model posterior distributions. We show how the tempering schedule can be chosen adaptively, explore the benefits of an SMC variant we call generalized tempering for “online” estimation, and provide examples of multimodal posteriors that are well captured by SMC methods. We then use the online estimation of the DSGE model to compute pseudo-out-of-sample density forecasts of DSGE models with and without financial frictions and document the benefits of conditioning DSGE model forecasts on nowcasts of macroeconomic variables and interest rate expectations. We also study whether the predictive ability of DSGE models changes when we use priors that are substantially looser than those commonly adopted in the literature.
C11|Sources of Economic Growth: A Global Perspective|The main goal of this paper is to determine the factors responsible for economic growth at the global level. The indication of the sources of economic growth may be an important element of the sustainable economic policy for development. The novelty of this research lies in employing an analysis based on data, which consist of an average growth rate of the Gross Domestic Product (GDP) for 168 countries for the years 2002–2013. The Bayesian model averaging approach is used to identify potential factors responsible for differences in countries’ GDPs. Additionally, a jointness analysis is performed to assess the potential independence, substitutability, and complementarity of the factors of economic growth. The robustness of the results is confirmed by Bayesian averaging of classical estimates. We identify the most probable factors of economic growth, and we find that the most important determinants are variables associated with the so-called “Asian development model”.
C11|A Bayesian approach for correcting bias of data envelopment analysis estimators|The validity of data envelopment analysis (DEA) efficiency estimators depends on the robustness of the production frontier to measurement errors, specification errors and the dimension of the input-output space. It has been proven that DEA estimators, within the interval (0, 1], are overestimated when finite samples are used while asymptotically this bias reduces to zero. The non-parametric literature dealing with bias correction of efficiencies solely refers to estimators that do not exceed one. We prove that efficiency estimators, both lower and higher than one, are biased. A Bayesian DEA method is developed to correct bias of efficiency estimators. This is a two-stage procedure of super-efficiency DEA followed by a Bayesian approach relying on consistent efficiency estimators. This method is applicable to ‘small’ and ‘medium’ samples. The new Bayesian DEA method is applied to two data sets of 50 and 100 E.U. banks. The mean square error, root mean square error and mean absolute error of the new method reduce as the sample size increases.
C11|Bayesian multivariate Beveridge--Nelson decomposition of I(1) and I(2) series with cointegration|The consumption Euler equation implies that the output growth rate and the real interest rate are of the same order of integration; thus if the real interest rate is I(1), then so is the output growth rate with possible cointegration, and log output is I(2). This paper extends the multivariate Beveridge--Nelson decomposition to such a case, and develops a Bayesian method to obtain error bands. The paper applies the method to US data to estimate the natural rates (or their permanent components) and gaps of output, inflation, interest, and unemployment jointly, and finds that allowing for cointegration gives much bigger estimates of all gaps.
C11|Monetary policy in oil exporting countries with fixed exchange rate and open capital account: expectations matter|Nominal interest rate is generally assumed to follow an UIP condition when the exchange rate is fixed, and the capital account is opened. Consequently, domestic interest rate is determined by foreign rates and the risk premium. This paper shows that for an oil exporting country like UAE, adjusting nominal interest rate only to foreign rate could be economically inconsistent. In fact, what really matters with exchange rate is expectations, and for an oil exporter country like UAE these expectations are significantly impacted by oil prices. By incorporating a market-expected exchange rate mechanism in a semi-structural New Keynesian Model, this paper highlights the importance of this mechanism and provides a consistent analytical framework.
C11|Exchange Rate Pass-through to Prices : Bayesian VAR Evidence for Ghana|Using quarterly data from 2006q3 to 2017q4, this paper employed sign restrictions with rejection method in a Vector Autoregression to estimate the pass-through of exchange rate dynamics to domestic prices in Ghana. The priors of the model belongs to the flat Normal inverted-Wishart family. Markov Chain Monte Carlo (MCMC) is used to collect 1000 draws from the posterior distribution of the SVAR parameters that satisfy the sign restrictions. The model specification included some idiosyncratic features of the Ghanaian economy such as the dependence on primary export commodities for foreign exchange revenue and the dependence on foreign aid. Impulse response functions was used to analyze exchange rate pass-through whilst variance decomposition was used to explain the most dominant source of inflation in the study sample. The impulse response showed a fairly large but not unitary pass-through of exchange rate dynamics to domestic prices. The implication herein is that exchange rate depreciation led to upsurge in prices in Ghana albeit, the impact is incomplete. Results from the variance decomposition indicated a monetary expansion was most dominant in explaining inflationary pressures in Ghana. For inflation to be lowered, policy directives should be geared towards exchange rate stability as well as ensuring a stable interest rate environment.
C11|Insulating property of the flexible exchange rate regime: A case of Central and Eastern European countries|We examine the insulating property of flexible exchange rate in CEE economies using the fact that they have adopted different regimes. A set of Bayesian structural VAR models with common serial correlations is estimated on data spanning 1998q1-2015q4. The long-term identifying restrictions are derived from a macroeconomic model. We find that irrespective of the exchange rate regime output is driven mainly by real shocks. Its reactions to these shocks, however, are substantially stronger under less flexible regimes, whereas the responses to nominal shocks are similar. Hence, the insulating property of flexible regimes can reduce the costs from economic shocks.
C11|Productive Performance and Technology Gaps using a Bayesian Metafrontier Production Function: A cross-country comparison|Growth theory argues on the role of heterogeneity that can lead to multiple regimes examining countries’ performance. A meta-production stochastic function under a Bayesian perspective has been developed to estimate technical efficiencies across countries over a time period. The metafrontier model is used to highlight heterogeneity among cluster of countries revealing catch up phenomena. The estimation procedure relies on the solution of an optimization problem and on the concept of the upper orthant order of two multinormal random variables. The proposed models are applied in a real dataset consisting of 109 countries for a 20-year period from 1995-2014. The productive performance differential and the associated technology gaps were investigated using two distinct frontiers (OECD vs non-OECD countries). Empirical results reveal that heterogeneity indeed plays a significant and distinctive role in determining technological gaps.
C11|Forecasting with many predictors using message passing algorithms|Machine learning methods are becoming increasingly popular in economics, due to the increased availability of large datasets. In this paper I evaluate a recently proposed algorithm called Generalized Approximate Message Passing (GAMP) , which has been very popular in signal processing and compressive sensing. I show how this algorithm can be combined with Bayesian hierarchical shrinkage priors typically used in economic forecasting, resulting in computationally efficient schemes for estimating high-dimensional regression models. Using Monte Carlo simulations I establish that in certain scenarios GAMP can achieve estimation accuracy comparable to traditional Markov chain Monte Carlo methods, at a tiny fraction of the computing time. In a forecasting exercise involving a large set of orthogonal macroeconomic predictors, I show that Bayesian shrinkage estimators based on GAMP perform very well compared to a large set of alternatives.
C11|The Role of Real Estate Uncertainty in Predicting US Home Sales Growth: Evidence from a Quantiles-Based Bayesian Model Averaging Approach|This paper investigates the role of real estate-specific uncertainty in predicting the conditional distribution of US home sales growth over the monthly period of 1970:07 to 2017:12, based on Bayesian Model Averaging (BMA) to account for model uncertainty. After controlling for standard predictors of home sales (housing price, mortgage rate, personal disposable income, unemployment rate, building permits, and housing starts), and macroeconomic and financial uncertainties, our results from the quantile BMA (QBMA) model show that real estate uncertainty has predictive content for the lower and upper quantiles of the conditional distribution of home sales growth.
C11|Time-Varying Local Projections| In recent years local projections have become a more and more popular methodology for the estimation of impulse responses. Besides being relatively easy to implement, the main strength of this approach relative to the traditional VAR one is that there is no need to impose any specific assumption on the dynamics of the data. This paper models local projections in a time-varying framework and provides a Gibbs sampler routine to estimate them. A simulation study shows how the performance of the algorithm is satisfactory while the usefulness of the model developed here is shown through an application to fiscal policy shocks.
C11|Belief updating: Does the 'good-news, bad-news' asymmetry extend to purely financial domains?|Bayes' statistical rule remains the status quo for modeling belief updating in both normative and descriptive models of behavior under uncertainty. Recent research has questioned the use of Bayes' rule in descriptive models of behavior, presenting evidence that people overweight 'good news' relative to 'bad news' when updating ego-relevant beliefs. In this paper, we present experimental evidence testing whether this 'good-news, bad-news' effect extends to belief updating in the domain of financial decision making, i.e. the domain of most applied economic decision making. We find no evidence of asymmetric updating in this domain. In contrast, the average participant in our experiment is strikingly close to Bayesian in her belief updating. However, we show that this average behavior masks the existence of three distinct types of updating behavior - each of which is distinct from Bayesian, but none of which displays the 'good-news, bad-news' effect.
C11|Investment Specific Technology Shocks and Emerging Market Business Cycle Dynamics|This article explores the role played by investment-specific technology (IST) shocks in emerging market business cycle fluctuations. The analysis is motivated by two key empirical facts; the presence of IST change in the post-war US economy combined with the importance of US investment goods in the emerging market imports. The goal is to quantify the contribution of US IST change for the business cycles of an emerging country in the context of a two-country, two-sector international real business cycle framework with investment and consumption goods sectors. Specifically, I estimate the model using Mexican and US data and find that a permanent US-originating IST shock is important in explaining Mexican business cycle dynamics. Shocks to investment sector technology explain around 60% of the investment, 44% of the consumption and 52% of the output variability. I argue that both a shock that captures financial frictions and a permanent US-originating IST shock are necessary to account for the key business cycle features in the data. (Copyright: Elsevier)
C11|Variational Bayesian Inference in Large Vector Autoregressions with Hierarchical Shrinkage|Many recent papers in macroeconomics have used large Vector Autoregressions (VARs) involving a hundred or more dependent variables. With so many parameters to estimate, Bayesian prior shrinkage is vital in achieving reasonable results. Computational concerns currently limit the range of priors used and render difficult the addition of empirically important features such as stochastic volatility to the large VAR. In this paper, we develop variational Bayes methods for large VARs which overcome the computational hurdle and allow for Bayesian inference in large VARs with a range of hierarchical shrinkage priors and with time-varying volatilities. We demonstrate the computational feasibility and good forecast performance of our methods in an empirical application involving a large quarterly US macroeconomic data set.
C11|Monetary Policy, Inflation Target and the Great Moderation: An Empirical Investigation|This paper compares the empirical fit of a Taylor rule featuring constant versus time-varying inflation target by estimating a Generalized New Keynesian model under positive trend inflation while allowing for indeterminacy. The estimation is conducted over two different periods covering the Great Inflation and the Great Moderation. We find that the rule embedding time variation in target inflation turns out to be empirically superior and determinacy prevails in both sample periods. Counterfactual simulations point toward both `good policy' and `good luck' as drivers of the Great Moderation. We find that better monetary policy, both in terms of a more active response to inflation gap and a more anchored inflation target, has resulted in the decline in inflation gap volatility and predictability. In contrast, the reduction in output growth variability is mainly explained by reduced volatility of technology shocks.
C11|An automated prior robustness analysis in Bayesian model comparison|The marginal likelihood is the gold standard for Bayesian model comparison although it is well-known that the value of marginal likelihood could be sensitive to the choice of prior hyperparameters. Most models require computationally intense simulation-based methods to evaluate the typically high-dimensional integral of the marginal likelihood expression. Hence, despite the recognition that prior sensitivity analysis is important in this context, it is rarely done in practice. In this paper we develop efficient and feasible methods to compute the sensitivities of marginal likelihood, obtained via two common simulation-based methods, with respect to any prior hyperparameter alongside the MCMC estimation algorithm. Our approach builds on Automatic Differentiation (AD), which has only recently been introduced to the more computationally intensive setting of Markov chain Monte Carlo simulation. We illustrate our approach with two empirical applications in the context of widely used multivariate time series models.
C11|Efficient selection of hyperparameters in large Bayesian VARs using automatic differentiation|Large Bayesian VARs with the natural conjugate prior are now routinely used for forecasting and structural analysis. It has been shown that selecting the prior hyperparameters in a data-driven manner can often substantially improve forecast performance. We propose a computationally efficient method to obtain the optimal hyperparameters based on Automatic Differentiation, which is an efficient way to compute derivatives. Using a large US dataset, we show that using the optimal hyperparameter values leads to substantially better forecast performance. Moreover, the proposed method is much faster than the conventional grid-search approach, and is applicable in high-dimensional optimization problems. The new method thus provides a practical and systematic way to develop better shrinkage priors for forecasting in a data-rich environment.
C11|Asymmetric conjugate priors for large Bayesian VARs|Large Bayesian VARs are now widely used in empirical macroeconomics. One popular shrinkage prior in this setting is the natural conjugate prior as it facilitates posterior simulation and leads to a range of useful analytical results. This is, however, at the expense of modelling exibility, as it rules out cross-variable shrinkage – i.e. shrinking coefficients on lags of other variables more aggressively than those on own lags. We develop a prior that has the best of both worlds: it can accommodate cross-variable shrinkage, while maintaining many useful analytical results, such as a closed-form expression of the marginal likelihood. This new prior also leads to fast posterior simulation - for a BVAR with 100 variables and 4 lags, obtaining 10,000 posterior draws takes less than half a minute on a standard desktop. In a forecasting exercise, we show that a data-driven asymmetric prior outperforms two useful benchmarks: a data-driven symmetric prior and a subjective asymmetric prior.
C11|Minnesota-type adaptive hierarchical priors for large Bayesian VARs|Large Bayesian VARs with stochastic volatility are increasingly used in empirical macroeconomics. The key to make these highly parameterized VARs useful is the use of shrinkage priors. We develop a family of priors that captures the best features of two prominent classes of shrinkage priors: adaptive hierarchical priors and Minnesota priors. Like the adaptive hierarchical priors, these new priors ensure that only ‘small’ coefficients are strongly shrunk to zero, while ‘large’ coefficients remain intact. At the same time, these new priors can also incorporate many useful features of the Minnesota priors, such as cross-variable shrinkage and shrinking coefficients on higher lags more aggressively. We introduce a fast posterior sampler to estimate BVARs with this family of priors - for a BVAR with 25 variables and 4 lags, obtaining 10,000 posterior draws takes about 3 minutes on a standard desktop. In a forecasting exercise, we show that these new priors outperform both adaptive hierarchical priors and Minnesota priors.
C11|A non-linear Keynesian Goodwin-type endogenous model of the cycle: Bayesian evidence for the USA|This paper incorporates the so-called Bhaduri-Marglin accumulation function in Goodwin’s original growth cycle model and econometrically estimates the proposed model for the case of the US economy in the time period 1960–2012, using a modern Bayesian sequential Monte Carlo method. Based on our findings, the US economy follows an exhilarationist regime throughout our investigation period with the sole exception of an underconsumption regime for the time period 1974–1978. In general, the results suggest that the proposed approach is an appropriate vehicle for expanding and improving traditional Goodwin-type models.
C11|Bayesian Nonparametric Learning of How Skill Is Distributed across the Mutual Fund Industry|In this paper, we use Bayesian nonparametric learning to estimate the skill of actively managed mutual funds and also to estimate the population distribution for this skill. A nonparametric hierarchical prior, where the hyperprior distribution is unknown and modeled with a Dirichlet process prior, is used for the skill parameter, with its posterior predictive distribution being an estimate of the population distribution. Our nonparametric approach is equivalent to an infinitely ordered mixture of normals where we resolve the uncertainty in the mixture order by partitioning the funds into groups according to the group's average ability and variability. Applying our Bayesian nonparametric learning approach to a panel of actively managed, domestic equity funds, we find the population distribution of skill to be fat-tailed, skewed towards higher levels of performance. We also find that it has three distinct modes: a primary mode where the average ability covers the average fees charged by funds, a secondary mode at a performance level where a fund loses money for its investors, and lastly, a minor mode at an exceptionally high skill level.
C11|Assessing International Commonality in Macroeconomic Uncertainty and Its Effects|This paper uses a large vector autoregression to measure international macroeconomic uncertainty and its effects on major economies. We provide evidence of significant commonality in macroeconomic volatility, with one common factor driving strong comovement across economies and variables. We measure uncertainty and its effects with a large model in which the error volatilities feature a factor structure containing time-varying global components and idiosyncratic components. Global uncertainty contemporaneously affects both the levels and volatilities of the included variables. Our new estimates of international macroeconomic uncertainty indicate that surprise increases in uncertainty reduce output and stock prices, adversely affect labor market conditions, and in some economies lead to an easing of monetary policy.
C11|Monetary Policy and Macroeconomic Stability Revisited|A large literature with canonical New Keynesian models has established that the Fed's policy change from a passive to an active response to inflation led to U.S. macroeconomic stability after the Great Inflation of the 1970s. We revisit this view by estimating a staggered price model with trend inflation using a Bayesian method that allows for equilibrium indeterminacy and adopts a sequential Monte Carlo algorithm. {{p}} The model empirically outperforms a canonical New Keynesian model and demonstrates an active response to inflation even in the Great Inflation era, during which the U.S. economy was likely in the indeterminacy region of the model's parameter space. A more active response to inflation alone does not suffice for explaining the shift to determinacy after the Great Inflation, unless it is accompanied by a decline in trend inflation or a change in policy responses to the output gap and output growth.
C11|Ties That Bind: Estimating the Natural Rate of Interest for Small Open Economies|This paper estimates the natural interest rate for six small open economies (Australia, Canada, South Korea, Sweden, Switzerland and the U.K.) with a structural New Keynesian model using Bayesian techniques. Our empirical analysis establishes the following four novel findings: First, we show that the open-economy framework provides a better fit of the data than its closed-economy counterpart for the six countries we investigate. Second, we also show that, in all six countries, a monetary policy rule in which the domestic real policy rate tracks the Wicksellian domestic short-term natural rate fits the data better than an otherwise standard Taylor (1993) rule. Third, we show that over the past 35 years, the natural interest rates in all six countries have shifted downwards and strongly comoved with each other. Fourth, our findings illustrate that foreign output shocks (spillovers from the rest of the world) are a major contributor to the dynamics of the natural rate in these six small open economies, and that natural rates comove strongly with estimated U.S. natural rates.
C11|Uncertainty shocks, monetary policy and long-term interest rates|"We study the relationship between monetary policy and long-term rates in a structural, general equilibrium model estimated on both macro and yields data from the United States. Regime shifts in the conditional variance of productivity shocks, or ""uncertainty shocks"", are an important model ingredient. First, they account for countercyclical movements in risk premia. Second, they induce changes in the demand for precautionary saving, which affects expected future real rates. Through changes in both risk-premia and expected future real rates, uncertainty shocks account for about 1/2 of the variance of long-term nominal yields over long horizons. The remaining driver of long-term yields are changes in inflation expectations induced by conventional, autoregressive shocks. Long-term inflation expectations implied by our model are in line with those based on survey data over the 1980s and 1990s, but less strongly anchored in the 2000s. JEL Classification: C11, C34, E40, E43, E52"
C11|Business Cycles Across Space and Time|We study the comovement of international business cycles in a time series clustering model with regime-switching. We extend the framework of Hamilton and Owyang (2012) to include time-varying transition probabilities to determine what drives similarities in business cycle turning points. We find four groups, or “clusters”, of countries which experience idiosyncratic recessions relative to the global cycle. Additionally, we find the primary indicators of international recessions to be fluctuations in equity markets and geopolitical uncertainty. In out-of-sample forecasting exercises, we find that our model is an improvement over standard benchmark models for forecasting both aggregate output growth and country-level recessions.
C11|Online Estimation of DSGE Models|"This paper illustrates the usefulness of sequential Monte Carlo (SMC) methods in approximating DSGE model posterior distributions. We show how the tempering schedule can be chosen adaptively, explore the benefits of an SMC variant we call generalized tempering for \online"" estimation, and provide examples of multimodal posteriors that are well captured by SMC methods. We then use the online estimation of the DSGE model to compute pseudo-out-of-sample density forecasts of DSGE models with and without financial frictions and document the benefits of conditioning DSGE model forecasts on nowcasts of macroeconomic variables and interest rate expectations. We also study whether the predictive ability of DSGE models changes when we use priors that are substantially looser than those that are commonly adopted in the literature."
C11|What Do Sectoral Dynamics Tell Us About the Origins of Business Cycles?|"We use economic theory to rank the impact of structural shocks across sectors. This ranking helps us to identify the origins of U.S. business cycles. To do this, we introduce a Hierarchical Vector Auto-Regressive model, encompassing aggregate and sectoral variables. We find that shocks whose impact originate in the ""demand"" side (monetary, household, and government consumption) account for 43 percent more of the variance of U.S. GDP growth at business cycle frequencies than identified shocks originating in the ""supply"" side (technology and energy). Furthermore, corporate financial shocks, which theory suggests propagate to large extent through demand channels, account for an amount of the variance equal to an additional 82 percent of the fraction explained by these supply shocks."
C11|Estimating Stochastic Ray Production Frontiers|In this paper, we consider the stochastic ray production function that has been revived recently by Henningsen et al. (2017). We use a profit-maximizing framework to resolve endogeneity problems that are likely to arise, as in all distance functions, and we derive the system of equations after incorporating technical inefficiency. As technical inefficiency enters non-trivially into the system of equations and the Jacobian is highly complicated, we propose Monte Carlo methods of inference. We illustrate the new approach using US banking data and we also address the problems of missing prices and selection of ordering for outputs.
C11|Bayesian combination for inflation forecasts: The effects of a prior based on central banks’ estimates|Typically, central banks use a variety of individual models (or a combination of models) when forecasting inflation rates. Most of these require excessive amounts of data, time, and computational power, all of which are scarce when monetary authorities meet to decide over policy interventions. In this paper we use a rolling Bayesian combination technique that considers inflation estimates by the staff of the Central Bank of Colombia during 2002–2011 as prior information. Our results show that: (1) the accuracy of individual models is improved by using a Bayesian shrinkage methodology, and (2) priors consisting of staff estimates outperform all other priors that comprise equal or zero vector weights. Consequently, our model provides readily available forecasts that exceed all individual models in terms of forecasting accuracy at every evaluated horizon.
C11|Cadenas globales de valor, crecimiento y protección arancelaria en Colombia|Este documento realiza un análisis detallado de la dispersión del arancel en Colombia, evaluando su evolución desde la implementación de la apertura económica en la década de los noventa, y presenta un marco comparativo de los niveles de dispersión del arancel nacional frente al de los países de la región. El análisis de la dispersión del arancel en Colombia, junto con los niveles de este en cada uno de los sectores de la economía nacional, servirán de insumo para la estimación de un arancel menos disperso que permita la incursión de la economía colombiana en las cadenas globales de valor. **** ABSTRACT: The present paper makes a detailed analysis of the tariff dispersion in Colombia, considering its evolution since the implementation of the economic opening in the nineties. The document presents a comparative framework between dispersion levels of the national tariff against that of the countries in the region. The analysis of tariff dispersion in Colombia and the levels of this in each sector of the national economy contributes as input for the estimation of a less dispersed tariff that allows the incursion of the Colombian economy in the global value chains.
C11|High-dimensional macroeconomic forecasting using message passing algorithms|This paper proposes two distinct contributions to econometric analysis of large information sets and structural instabilities. First, it treats a regression model with time-varying coeﬃcients, stochastic volatility and exogenous predictors, as an equivalent high-dimensional static regression problem with thousands of covariates. Inference in this speciﬁcation proceeds using Bayesian hierarchical priors that shrink the high-dimensional vector of coeﬃcients either towards zero or time-invariance. Second, it introduces the frameworks of factor graphs and message passing as a means of designing eﬃcient Bayesian estimation algorithms. In particular, a Generalized Approximate Message Passing (GAMP) algorithm is derived that has low algorithmic complexity and is trivially parallelizable. The result is a comprehensive methodology that can be used to estimate time-varying parameter regressions with arbitrarily large number of exogenous predictors. In a forecasting exercise for U.S. price inﬂation this methodology is shown to work very well.
C11|Quarterly Forecasting Model for India’s Economic Growth: Bayesian Vector Autoregression Approach|This study develops a framework to forecast India’s gross domestic product growth on a quarterly frequency from 2004 to 2018. The models, which are based on real and monetary sector descriptions of the Indian economy, are estimated using Bayesian vector autoregression (BVAR) techniques. The real sector groups of variables include domestic aggregate demand indicators and foreign variables, while the monetary sector groups specify the underlying inflationary process in terms of the consumer price index (CPI) versus the wholesale price index given India’s recent monetary policy regime switch to CPI inflation targeting. The predictive ability of over 3,000 BVAR models is assessed through a set of forecast evaluation statistics and compared with the forecasting accuracy of alternate econometric models including unrestricted and structural VARs. Key findings include that capital flows to India and CPI inflation have high informational content for India’s GDP growth. The results of this study provide suggestive evidence that quarterly BVAR models of Indian growth have high predictive ability.
C11|Inducing Sparsity and Shrinkage in Time-Varying Parameter Models|Time-varying parameter (TVP) models have the potential to be over-parameterized, particularly when the number of variables in the model is large. Global-local priors are increasingly used to induce shrinkage in such models. But the estimates produced by these priors can still have appreciable uncertainty. Sparsification has the potential to remove this uncertainty and improve forecasts. In this paper, we develop computationally simple methods which both shrink and sparsify TVP models. In a simulated data exercise we show the benefits of our shrink-then-sparsify approach in a variety of sparse and dense TVP regressions. In a macroeconomic forecast exercise, we find our approach to substantially improve forecast performance relative to shrinkage alone.
C11|Measuring international uncertainty using global vector autoregressions with drifting parameters|This paper investigates the time-varying impacts of international macroeconomic uncertainty shocks. We use a global vector autoregressive (GVAR) specification with drifting coefficients and factor stochastic volatility in the errors to model six economies jointly. The measure of uncertainty is constructed endogenously by estimating a scalar driving the innovation variances of the latent factors, and is included also in the mean of the process. To achieve regularization, we use Bayesian techniques for estimation, and introduce a set of hierarchical global local shrinkage priors. The adopted priors center the model on a constant parameter specification with homoscedastic errors, but allow for time-variation if suggested by likelihood information. Moreover, we assume coefficients across economies to be similar, but provide sufficient flexibility via the hierarchical prior for country-specific idiosyncrasies. The results point towards pronounced real and financial effects of uncertainty shocks in all countries, with differences across economies and over time.
C11|Improved Marginal Likelihood Estimation via Power Posteriors and Importance Sampling|The power-posterior method of Friel and Pettitt (2008) has been used to estimate the marginal likelihoods of competing Bayesian models. In this paper it is shown that the Bernstein-von Mises (BvM) theorem holds for the power posteriors under regularity conditions. Due to the BvM theorem, the power posteriors, when adjusted by the square root of the corresponding grid points, converge to the same normal distribution as the original posterior distribution, facilitating the implementation of importance sampling for the purpose of estimating the marginal likelihood. Unlike the power-posterior method that requires repeated posterior sampling from the power posteriors, the new method only requires the posterior output from the original posterior. Hence, it is computationally more efficient to implement. Moreover, it completely avoids the coding efforts associated with drawing samples from the power posteriors. Numerical efficiency of the proposed method is illustrated using two models in economics and finance.
C11|Institutional determinants of export competitiveness among the EU countries: evidence from Bayesian model averaging|Although the impact of institutions has been broadly studied in the literature on economic growth, their impact on international trade is less well-established. We aim to fill this gap by creating an extended database that, apart from price and non-price factors traditionally analyzed as determinants of exports, also includes measures of institutional development. Next, we introduce the Bayesian Model Averaging to establish which factors play the most important role for the export performance. Our results show that institutions have two types of effects on exports: a direct positive effect on the overall export performance (e.g. regulation) as well as a transformational impact on the export structure (from less to more technologically advanced exports, e.g. freedom to trade internationally). Our results also confirm that technological factors (e.g. patents) have a much greater impact on export performance than price factors. Moreover, some technological factors only have a significant transformational impact on the export structure (e.g. R&D expenditure). Human capital also seems to have only a transformational, rather than direct, impact on exports.
C11|Global Spillover Effects of US Uncertainty|Spillover effects of US uncertainty shocks are studied in a panel VAR of ?fteen emerging market economies (EMEs). A US uncertainty shock negatively affects EME stock prices and exchange rates, raises EME country spreads, and decreases capital inflows into them. It decreases EME output and consumer prices while increasing net exports. Negative effects on output and asset prices are weaker, but effects on external balance stronger, for Latin American EMEs. We attribute such heterogeneity to di?erential EME monetary policy response to US uncertainty shocks. Analysis of central bank minutes shows Latin American EMEs pay less attention to smoothing capital ?ows.
C11|Financial and Fiscal Interaction in the Euro Area Crisis : This Time was Different|This paper highlights the anomalous characteristics of the Euro Area `twin crises' by contrasting the aggregate macroeconomic dynamics in the period 2009-2013 with the business cycle fluctuations of the previous decades. We report three stylised facts. First, the contraction in output was marked by an anomalous downfall in investment, while consumption, savings and unemployment followed their historical relation with GDP. Second, households and financial corporations debts, and house prices deviated from their pre-crisis trends. Third, the jump in the public deficit GDP ratio in 2008-2009 was unprecedented and so was the fiscal consolidation that followed. Our analysis points to the financial nature of the crisis as a likely explanation for these facts. Importantly, the `anomaly' in public deficit is in large part explained by extraordinary measures in support of the financial sector, which show up in the stockflow adjustments and reveal a key interaction between the fiscal and the financial sectors.
C11|The State of DSGE Modelling|This survey and assessment of the state of DSGE modelling is structured around six key criticisms levelled at the approach. The first is fundamental and common to macroeconomics and microeconomics alike - namely, problems with rationality and expected utility maximization. The second is that DSGE models examine fluctuations about an exogenous balanced growth path and there is no role for endogenous growth, either medium or long-term. The third consists of a number of concerns associated with systems estimation. The fourth is another fundamental problem with any microfounded macro-model - that of heterogeneity and aggregation. The fifth and sixth concerns focus on the rudimentary nature of earlier models that lacked unemployment and a banking sector.
C11|Global Spillover Effects of US Uncertainty|We study spillover effects of US uncertainty fluctuations using panel data from fifteen emerging market economies (EMEs). A US uncertainty shock negatively affects EME stock prices and exchange rates, raises EME country spreads, and leads to capital outflows from them. Moreover, it decreases EME output, while increasing their consumer prices and net exports. The negative effects on output, exchange rates, and stock prices are weaker, but the effects on capital and trade flows stronger, for South American countries compared to other EMEs. We present a model of a small open economy that faces an external shock to interpret our findings.
C11|Identifying Credit Supply Shocks in Turkey|This paper aims to identify credit supply shocks and analyse their macroeconomic effects in Turkey. For this purpose, we use a Bayesian Structural Vector Autoregression (SVAR) with sign and zero restrictions. We focus on the impact of credit supply shocks on real GDP growth and highlight how the size of this impact changes when we explicitly account for the effects of capital inflows on credit conditions. Hence, our results confirm the importance of external finance for credit supply in Turkey. Our main findings are robust to some alternative data choices, prior selections as well as some alternative identifying restrictions.
C11|Forecast Density Combinations with Dynamic Learning for Large Data Sets in Economics and Finance|A flexible forecast density combination approach is introduced that can deal with large data sets. It extends the mixture of experts approach by allowing for model set incompleteness and dynamic learning of combination weights. A dimension reduction step is introduced using a sequential clustering mechanism that allocates the large set of forecast densities into a small number of subsets and the combination weights of the large set of densities are modelled as a dynamic factor model with a number of factors equal to the number of subsets. The forecast density combination is represented as a large finite mixture in nonlinear state space form. An efficient simulation-based Bayesian inferential procedure is proposed using parallel sequential clustering and filtering, implemented on graphics processing units. The approach is applied to track the Standard & Poor 500 index combining more than 7000 forecast densities based on 1856 US individual stocks that are are clustered in a relatively small subset. Substantial forecast and economic gains are obtained, in particular, in the tails using Value-at-Risk. Using a large macroeconomic data set of 142 series, similar forecast gains, including probabilities of recession, are obtained from multivariate forecast density combinations of US real GDP, Inflation, Treasury Bill yield and Employment. Evidence obtained on the dynamic patterns in the financial as well as macroeconomic clusters provide valuable signals useful for improved modelling and more effective economic and financial policies.
C11|Dynamics in clickthrough and conversion probabilities of paid search advertisements|We develop a dynamic Bayesian model for clickthrough and conversion probabilities of paid search advertisements. These probabilities are subject to changes over time, due to e.g. changing consumer tastes or new product launches. Yet, there is little empirical research on these dynamics. Gaining insight into the dynamics is crucial for advertisers to develop effective search engine advertising (SEA) strategies. Our model deals with dynamic SEA environments for a large number of keywords: it allows for time-varying parameters, seasonality, data sparsity and position endogeneity. The model also discriminates between transitory and permanent dynamics. Especially for the latter case, dynamic SEA strategies are required for long-term profitability. We illustrate our model using a 2 year dataset of a Dutch laptop selling retailer. We find persistent time variation in clickthrough and conversion probabilities. The implications of our approach are threefold. First, advertisers can use it to obtain accurate daily estimates of clickthrough and conversion probabilities of individual ads to set bids and adjust text ads and landing pages. Second, advertisers can examine the extent of dynamics in their SEA environment, to determine how often their SEA strategy should be revised. Finally, advertisers can track ad performances to timely identify when keywords’ performances change.
C11|Partially Censored Posterior for Robust and Efficient Risk Evaluation|A novel approach to inference for a specific region of the predictive distribution is introduced. An important domain of application is accurate prediction of financial risk measures, where the area of interest is the left tail of the predictive density of logreturns. Our proposed approach originates from the Bayesian approach to parameter estimation and time series forecasting, however it is robust in the sense that it provides a more accurate estimation of the predictive density in the region of interest in case of misspecification. The first main contribution of the paper is the novel concept of the Partially Censored Posterior (PCP), where the set of model parameters is partitioned into two subsets: for the first subset of parameters we consider the standard marginal posterior, for the second subset of parameters (that are particularly related to the region of interest) we consider the conditional censored posterior. The censoring means that observations outside the region of interest are censored: for those observations only the probability of being outside the region of interest matters. This quasi-Bayesian approach yields more precise parameter estimation than a fully censored posterior for all parameters, and has more focus on the region of interest than a standard Bayesian approach. The second main contribution is that we introduce two novel methods for computationally efficient simulation: Conditional MitISEM, a Markov chain Monte Carlo method to simulate model parameters from the Partially Censored Posterior, and PCP-QERMit, an Importance Sampling method that is introduced to further decrease the numerical standard errors of the Value-at-Risk and Expected Shortfall estimators. The third main contribution is that we consider the effect of using a time-varying boundary of the region of interest, which may provide more information about the left tail of the distribution of the standardized innovations. Extensive simulation and empirical studies show the ability of the introduced method to outperform standard approaches.
C11|Copula Multivariate GARCH Model with Constrained Hamiltonian Monte Carlo|The Copula Multivariate GARCH (CMGARCH) model is based on a dynamic copula function with time-varying parameters. It is particularly suited for modelling dynamic dependence of non-elliptically distributed financial returns series. The model allows for capturing more flexible dependence patterns than a multivariate GARCH model and also generalizes static copula dependence models. Nonetheless, the model is subject to a number of parameter constraints that ensure positivity of variances and covariance stationarity of the modeled stochastic processes. As such, the resulting distribution of parameters of interest is highly irregular, characterized by skewness, asymmetry, and truncation, hindering the applicability and accuracy of asymptotic inference. In this paper, we propose Bayesian analysis of the CMGARCH model based on Constrained Hamiltonian Monte Carlo (CHMC), which has been shown in other contexts to yield efficient inference on complicated constrained dependence structures. In the CMGARCH context, we contrast CHMC with traditional random-walk sampling used in the previous literature and highlight the benefits of CHMC for applied researchers. We estimate the posterior mean, median and Bayesian confidence intervals for the coefficients of tail dependence. The analysis is performed in an application to a recent portfolio of S&P500 financial asset returns.
C11|The multivariate simultaneous unobserved components model and identification via heteroskedasticity|We propose a multivariate simultaneous unobserved components framework to determine the two-sided interactions between structural trend and cycle innovations. We relax the standard assumption in unobserved components models that trends are only driven by permanent shocks and cycles are only driven by transitory shocks by considering the possible spillover effects between structural innovations. The direction of spillover has a structural interpretation, whose identification is achieved via heteroskedasticity. We provide identifiability conditions and develop an efficient Bayesian MCMC procedure for estimation. Empirical implementations for both Okun’s law and the Phillips curve show evidence of significant spillovers between trend and cycle components.
C11|The Multivariate Simultaneous Unobserved Compenents Model and Identification via Heteroskedasticity|We propose a multivariate simultaneous unobserved components framework to determine the two-sided interactions between structural trend and cycle innovations. We relax the standard assumption in unobserved components models that trends are only driven by permanent shocks and cycles are only driven by transitory shocks by considering the possible spillover effects between structural innovations. The direction of spillover has a structural interpretation, whose identification is achieved via heteroskedasticity. We provide identifiability conditions and develop an efficient Bayesian MCMC procedure for estimation. Empirical implementations for both Okun's law and the Phillips curve show evidence of significant spillovers between trend and cycle components.
C11|Bayesian nonparametric clustering as a community detection problem|It is well known that a wide class of bayesian nonparametric priors lead to the representation of the distribution of the observable variables as a mixture density with an infinite number of components, and that such a representation induces a clustering structure in the observations. However, cluster identification is not straightforward a posteriori and some post-processing is usually required. In order to circumvent label switching, pairwise posterior similarity has been introduced, and it has been used in order to either apply classical clustering algorithms or estimate the underlying partition by minimising a suitable loss function. This paper proposes to map observations on a weighted undirected graph, where each node represents a sample item and edge weights are given by the posterior pairwise similarities. It will be shown how, after building a particular random walk on such a graph, it is possible to apply a community detection algorithm, known as map equation method, by optimising the description length of the partition. A relevant feature of this method is that it allows for both the quantification of the posterior uncertainty of the classification and the selection of variables to be used for classification purposes.
C11|A flexible state-space model with lagged states and lagged dependent variables: Simulation smoothing|We provide a simulation smoother to a exible state-space model with lagged states and lagged dependent variables. Qian (2014) has introduced this state-space model and proposes a fast Kalman filter with time-varying state dimension in the presence of missing observations in the data. In this paper, we derive the corresponding Kalman smoother moments and propose an efficient simulation smoother, which relies on mean corrections for unconditional vectors. When applied to a factor model, the proposed simulation smoother for the states is efficient compared to other state-space models without lagged states and/or lagged dependent variables in terms of computing time.
C11|Credit constraints and the propagation of the Great Depression in Germany|We evaluate the role played by loan supply shocks in the decline of investment and industrial production during the Great Depression in Germany from 1927 to 1932. We identify loan supply shocks in the context of a time varying parameter vector autoregression with stochastic volatility. Our results indicate that credit constraints were a significant driver of industrial production between 1927 and 1932, supporting the view that a structurally weak banking sector was an important contributor to the German Great Depression. We find further that loan supply shocks were an important driver of investment in the early phase of the depression, between 1927 and 1929, but not between 1930 and 1932. We suggest possible explanations for this puzzle and directions for future research.
C11|Uncertainty in the Black-Litterman model: A practical note|"Deriving an optimal asset allocation for institutional investors hinges crucially on the quality of inputs used in the optimization. If the mean vector and the covariance matrix are known with certainty, the classical mean-variance optimization of Markowitz (1952) produces optimal portfolios. If, however, both and are estimated with uncertainty, mean-variance optimization tends to maximize estimation error, as shown in Michaud (1989). The Black-Litterman model (Black and Litterman (1991, 1992)), a derivation of the Bayesian methods developed in academia, has particular practical appeal for institutional investors. It allows the specification of views and an uncertainty about these views, which are combined with equilibrium returns and incorporated consistently to arrive at and .These new parameters can then be used in the portfolio optimization process. In the Black-Litterman model, however, uncertainty about the equilibrium returns is specified with an overall scalar uncertainty parameter, which is difficult to set and introduces rigidity.We propose a slight modification of the Black-Litterman model to allow the specification of uncertainty in a flexible way not only in individual views, but also in the equilibrium returns of every asset entering the model. Our modification is an ""add-on"" to the traditional framework, which allows to adjust the uncertainty individually and is still permitting the Black-Litterman approach as a special case."
C11|Everyday econometricians: Selection neglect and overoptimism when learning from others|In this paper, we design an investment game which allows us to study the influence of selection when learning from others. Using the theoretical study of selection neglect in Jehiel (2018) as a guide, we test (i) for the presence of selection neglect in this investment context, and (ii) some comparative static predictions of the model. We find strong evidence for selection neglect—even though subjects are fully informed about the data generating process. As theoretically predicted, the degree of bias due to selection neglect increases when other decision makers become more informed, or become more rational. It decreases when signals are correlated.
C11|Thirty Years of Economic Growth in Africa|This paper examines the contribution of employment, capital accumulation and total factor productivity (TFP) to economic growth in African countries over the period 1986-2014. The methodology consists in the estimation of a translog dynamic stochastic production frontier for a set of 49 African economies, thus allowing for the breakdown of TFP along efficiency developments and technological progress. Although the heterogeneity amongst African countries poses a challenge to the estimation of a common production frontier, this is the best approach to perform cross-country comparisons. The results of our growth accounting exercise are more accurate for the contribution of input accumulation and TFP to GDP growth than for the separation between contributions of technological progress and efficiency. We conclude that economic growth patterns differ across African countries but they have been almost totally associated to input accumulation, notably in what concerns capital. The experience of Egypt, Nigeria and South Africa - the three largest African economies - confirms this pattern.
C11|Reducing Dimensions in a Large TVP-VAR|This paper proposes a new approach to estimating high dimensional time varying parameter structural vector autoregressive models (TVP-SVARs) by taking advantage of an empirical feature of TVP-(S)VARs. TVP-(S)VAR models are rarely used with more than 4-5 variables. However recent work has shown the advantages of modelling VARs with large numbers of variables and interest has naturally increased in modelling large dimensional TVP-VARs. A feature that has not yet been utilized is that the covariance matrix for the state equation, when estimated freely, is often near singular. We propose a specification that uses this singularity to develop a factor-like structure to estimate a TVP-SVAR for 15 variables. Using a generalization of the recentering approach, a rank reduced state covariance matrix and judicious parameter expansions, we obtain efficient and simple computation of a high dimensional TVP-SVAR. An advantage of our approach is that we retain a formal inferential framework such that we can propose formal inference on impulse responses, variance decompositions and, important for our model, the rank of the state equation covariance matrix. We show clear empirical evidence in favour of our model and improvements in estimates of impulse responses.
C11|Composite likelihood methods for large Bayesian VARs with stochastic volatility|Adding multivariate stochastic volatility of a flexible form to large Vector Autoregressions (VARs) involving over a hundred variables has proved challenging due to computational considerations and over-parameterization concerns. The existing literature either works with homoskedastic models or smaller models with restrictive forms for the stochastic volatility. In this paper, we develop composite likelihood methods for large VARs with multivariate stochastic volatility. These involve estimating large numbers of parsimonious models and then taking a weighted average across these models. We discuss various schemes for choosing the weights. In our empirical work involving VARs of up to 196 variables, we show that composite likelihood methods have similar properties to existing alternatives used with small data sets in that they estimate the multivariate stochastic volatility in a flexible and realistic manner and they forecast comparably. In very high dimensional VARs, they are computationally feasible where other approaches involving stochastic volatility are not and produce superior forecasts than natural conjugate prior homoscedastic VARs.
C11|Leverage, asymmetry and heavy tails in the high-dimensional factor stochastic volatility model|We develop a flexible modeling and estimation framework for a high-dimensional factor stochastic volatility (SV) model. Our specification allows for leverage effects, asymmetry and heavy tails across all systematic and idiosyncratic components of the model. This framework accounts for well-documented features of univariate financial time series, while introducing a flexible dependence structure that incorporates tail dependence and asymmetries such as stronger correlations following downturns. We develop an efficient Markov chain Monte Carlo (MCMC) algorithm for posterior simulation based on the particle Gibbs, ancestor sampling, and particle efficient importance sampling methods. We build computationally efficient model selection into our estimation framework to obtain parsimonious specifications in practice. We validate the performance of our proposed estimation method via extensive simulation studies for univariate and multivariate simulated datasets. An empirical study shows that the model outperforms other multivariate models in terms of value-at-risk evaluation and portfolio selection performance for a sample of US and Australian stocks.
C11|Sophisticated and small versus simple and sizeable: When does it pay off to introduce drifting coefficients in Bayesian VARs?|We assess the relationship between model size and complexity in the time-varying parameter VAR framework via thorough predictive exercises for the Euro Area, the United Kingdom and the United States. It turns out that sophisticated dynamics through drifting coefficients are important in small data sets while simpler models tend to perform better in sizeable data sets. To combine best of both worlds, novel shrinkage priors help to mitigate the curse of dimensionality, resulting in competitive forecasts for all scenarios considered. Furthermore, we discuss dynamic model selection to improve upon the best performing individual model for each point in time.
C11|The transmission of uncertainty shocks on income inequality: State-level evidence from the United States|In this paper, we explore the relationship between state-level household income inequality and macroeconomic uncertainty in the United States. Using a novel large-scale macroeconometric model, we shed light on regional disparities of inequality responses to a national uncertainty shock. The results suggest that income inequality decreases in most states, with a pronounced degree of heterogeneity in terms of shapes and magnitudes of the dynamic responses. By contrast, some few states, mostly located in the West and South census region, display increasing levels of income inequality over time. We find that this directional pattern in responses is mainly driven by the income composition and labor market fundamentals. In addition, forecast error variance decompositions allow for a quantitative assessment of the importance of uncertainty shocks in explaining income inequality. The findings highlight that volatility shocks account for a considerable fraction of forecast error variance for most states considered. Finally, a regression-based analysis sheds light on the driving forces behind differences in state-specific inequality responses.
C11|The dynamic impact of monetary policy on regional housing prices in the United States|This paper uses a factor-augmented vector autoregressive model to examine the impact of monetary policy shocks on housing prices across metropolitan and micropolitan regions. To simultaneously estimate the model parameters and unobserved factors we rely on Bayesian estimation and inference. Policy shocks are identified using high-frequency suprises around policy announcements as an external instrument. Impulse reponse functions reveal differences in regional housing price responses, which in some cases are substantial. The heterogeneity in policy responses is found to be significantly related to local regulatory environments and housing supply elasticities. Moreover, housing prices responses tend to be similar within states and adjacent regions in neighboring states.
C11|Financial and Fiscal Interaction in the Euro Area Crisis: This Time was Different|This paper highlights the anomalous characteristics of the Euro Area `twin crises' by contrasting the aggregate macroeconomic dynamics in the period 2009-2013 with the business cycle fluctuations of the previous decades. We report three stylised facts. First, the contraction in output was marked by an anomalous downfall in investment, while consumption, savings and unemployment followed their historical relation with GDP. Second, households' and financial corporations' debts, and house prices deviated from their pre-crisis trends. Third, the jump in the public deficit-GDP ratio in 2008-2009 was unprecedented and so was the fiscal consolidation that followed. Our analysis points to the financial nature of the crisis as a likely explanation for these facts. Importantly, the `anomaly' in public deficit is in large part explained by extraordinary measures in support of the financial sector, which show up in the stock-flow adjustments and reveal a key interaction between the fiscal and the financial sectors.
C11|On a quest for robustness: About model risk, randomness and discretion in credit risk stress tests|"In this paper we study the impact of model uncertainty, which occurs when linking a stress scenario to default probabilities, on reduced-form credit risk stress testing. This type of uncertainty is omnipresent in most macroeconomic stress testing applications due to short time series for banks' portfolio risk parameters and highly collinear macroeconomic covariates. We quantify the effect of model uncertainty on supervisory and bank stress tests in terms of predicted portfolio loss distributions and implied capital shortfalls by conducting a full-edged top-down credit risk stress test for over 1,500 German banks. Our results suggest that the impact of model uncertainty on predicted capital shortfalls can be huge, even among models with similar predictive power. This leaves both banks and supervisors with uncertainty when calculating stress impacts and implied capital requirements. To mitigate the impact of uncertainty, we suggest a modeling approach which filters the model space by combining the standard Bayesian model averaging (BMA) paradigm with a structural filter derived from the Merton/Vasicek credit risk model. Applying our stress testing framework, the dispersion decreases and the median stress effect is reduced from -5.0pp of CET1 ratio under the BMA model to -2.5pp under the structurally augmented BMA model, while the predicted capital shortfall is reduced by 70 %. The structural filter eliminates extreme outcomes on both sides of the stress forecast distribution, leading in our application to the German banking sector to a reduction in impact compared to the model without the ""stress testing plausibility"" filter."
C11|To sign or not to sign? On the response of prices to financial and uncertainty shocks|Based on SVAR models identified by sign restrictions, we estimate the macroeconomic effects of financial and uncertainty shocks in the euro area and the US, paying particular attention to their impact on prices. While our results confirm that such disturbances are important drivers of output fluctuations in both economies, we find the shock responses of consumer prices to be ambiguous. Moreover, restricting prices to co-moving with output can considerably attenuate the measured impact of financial and uncertainty shocks on real activity.
C11|Increasing taxes after a financial crisis: Not a bad idea after all ..|Based on OECD evidence, equity/housing-price busts and credit crunches are followed by substantial increases in public consumption. These increases in unproductive public spending lead to increases in distortionary marginal taxes, a policy in sharp contrast with presumably optimal Keynesian fiscal stimulus after a crisis. Here we claim that this seemingly adverse policy selection is optimal under rational learning about the frequency of rare capital-value busts. Bayesian updating after a bust implies massive belief jumps toward pessimism, with investors and policymakers believing that busts will be arriving more frequently in the future. Lowering taxes would be as if trying to kick a sick horse in order to stand up and run, since pessimistic markets would be unwilling to invest enough under any temporarily generous tax regime.
C11|Generalized Exogenous Processes in DSGE: A Bayesian Approach|The Reversible Jump Markov Chain Monte Carlo (RJMCMC) method can enhance Bayesian DSGE estimation by sampling from a posterior distribution spanning potentially nonnested models with parameter spaces of different dimensionality. We use the method to jointly sample from an ARMA process of unknown order along with the associated parameters. We apply the method to the technology process in a canonical neoclassical growth model using post war US GDP data and find that the posterior decisively rejects the standard AR(1) assumption in favor of higher order processes. While the posterior contains significant uncertainty regarding the exact order, it concentrates posterior density on hump-shaped impulse responses. A negative response of hours to a positive technology shock is within the posterior credible set when noninvertible MA representations are admitted.
C11|On the empirics of reserve requirements and economic growth|Reserve requirements, as a tool of macroprudential policy, have been increasingly employed since the outbreak of the great financial crisis. We conduct an analysis of the effect of reserve requirements in tranquil and crisis times on long-run growth rates of GDP per capita and credit (%GDP) making use of Bayesian model averaging methods. Regulation has on average a negative effect on GDP in tranquil times, which is only partly offset by a positive (but not robust effect) in crisis times. Credit over GDP is positively affected by higher requirements in the longer run.
C11|On the time-varying effects of economic policy uncertainty on the US economy|We study the time-varying impact of Economic Policy Uncertainty (EPU) on the US Economy by using a VAR with time-varying coefficients. The coefficients are allowed to evolve gradually over time which allows us to discover structural changes without imposing them a priori. We find three different regimes which match the three major business cycles of the US economy, namely the Great Inflation, the Great Moderation and the Great Recession. This finding is in contrast to previous literature which typically imposes two regimes a priori. Furthermore, we distinguish the effect of EPU on real economic activity and on financial markets.
C11|Exchange rate uncertainty and import prices in the euro area|This paper analyses the effects of exchange rate uncertainty on the pricing behaviour of import firms in the euro area. Uncertainty is measured via the volatility of the structural shocks to the exchange rate in a non-linear VAR framework and is an important determinant of import prices. An increase in exchange rate uncertainty is associated with a fall in prices on average, which suggests that the exchange rate risk is borne by the importers. The analysis utilizes a dataset on industrial import prices, disaggregated by origin of imports. Controlling for intra- and extra-euro area trade is important.
C11|Exchange rate predictability and dynamic Bayesian learning|This paper considers how an investor in foreign exchange markets might exploit predictive information in macroeconomic fundamentals by allowing for switching between multivariate time series regression models. These models are chosen to reflect a wide array of established empirical and theoretical stylized facts. In an application involving monthly exchange rates for seven countries, we find that an investor using our methods to dynamically allocate assets achieves significant gains relative to benchmark strategies. In particular, we find strong evidence for fast model switching, with most of the time only a small set of macroeconomic fundamentals being relevant for forecasting.
C11|Choosing Prior Hyperparameters: With Applications To Time-Varying Parameter Models|Time-varying parameter models with stochastic volatility are widely used to study macroeconomic and financial data. These models are almost exclusively estimated using Bayesian methods. A common practice is to focus on prior distributions that themselves depend on relatively few hyperparameters such as the scaling factor for the prior covariance matrix of the residuals governing time variation in the parameters. The choice of these hyperparameters is crucial because their influence is sizeable for standard sample sizes. In this paper we treat the hyperparameters as part of a hierarchical model and propose a fast, tractable, easy-to-implement, and fully Bayesian approach to estimate those hyperparameters jointly with all other parameters in the model. We show via Monte Carlo simulations that, in this class of models, our approach can drastically improve on using fixed hyperparameters previously proposed in the literature.
C11|Maximizing profit increase in manufacturing based on an Information Theory model|US Patent 10 054 929 B1
C11|Flexible dependence modeling using convex combinations of different types of connectivity structures|There is a great deal of literature regarding use of non-geographically based connectivity matrices or combinations of geographic and non-geographic structures in spatial econometrics models. We explore alternative approaches for constructing convex combinations of different types of dependence between observations. Pace and LeSage (2002) as well as Hazır et al. (2016) use convex combinations of different connectivity matrices to form a single weight matrix that can be used in conventional spatial regression estimation and inference. An example for the case of two weight matrices, W1,W2 reflecting different types of dependence between a cross-section of regions, firms, individuals etc., located in space would be: Wc=γ1W1+(1−γ1)W2,0≤γ1≤1. The matrix Wc reflects a convex combination of the two weight matrices, with the scalar parameter γ1 indicating the relative importance assigned to each type of dependence. We explore issues that arise in producing estimates and inferences from these more general cross-sectional regression relationships in a Bayesian framework. We propose two procedures to estimate such models and assess their finite sample properties through Monte Carlo experiments. We illustrate our methodology in an application to CEO salaries for a sample of nursing homes located in Texas. Two types of weights are considered, one reflecting spatial proximity of nursing homes and the other peer group proximity, which arise from the salary benchmarking literature.
C11|The Wall’s Impact in the Occupied West Bank: A Bayesian Approach to Poverty Dynamics Using Repeated Cross-Sections|In 2002, the Israeli government decided to build a wall inside the occupied West Bank. The wall had a marked effect on the access to land and water resources as well as to the Israeli labour market. It is difficult to include the effect of the wall in an econometric model explaining poverty dynamics as the wall was built in the richer region of the West Bank. So a diff-in-diff strategy is needed. Using a Bayesian approach, we treat our two-period repeated cross-section data set as an incomplete data problem, explaining the income-to-needs ratio as a function of time invariant exogenous variables. This allows us to provide inference results on poverty dynamics. We then build a conditional regression model including a wall variable and state dependence to see how the wall modified the initial results on poverty dynamics. We find that the wall has increased the probability of poverty persistence by 58 percentage points and the probability of poverty entry by 18 percentage points.
C11|Nonlinear impact estimation in spatial autoregressive models|This paper extends the literature on the calculation and interpretation of impacts for spatial autoregressive models. Using a Bayesian framework, we show how the individual direct and indirect impacts associated with an exogenous variable introduced in a nonlinear way in such models can be computed, theoretically and empirically. Rather than averaging the individual impacts, we suggest to graphically analyze them along with their confidence intervals calculated from Markov chain Monte Carlo (MCMC). We also explicitly derive the form of the gap between individual impacts in the spatial autoregressive model and the corresponding model without a spatial lag and show, in our application on the Boston dataset, that it is higher for spatially highly connected observations.
C11|Incertitude de classement final et affluence en Ligue 1 française de football : une nouvelle approche|This study proposes a new method for estimating supporters? attendance in considering uncertainty about the games results and the final ranking of teams. Outcome uncertainty is expressed by some probabilities, which is determined according to the gap between team qualities. In this turn, the qualities of teams are randomly estimated by average wages of their players. However, unlike previous studies, these probabilities are further adjusted according to the past results of these specific teams during the championship. Moreover, instead of relying on an aggregate indicator to estimate the uncertainty about the final ranking, we estimate the probability of each possible final ranking. This procedure allows us to make use of a more complete information statistics in estimation than usually done in this kind of studies. We provide three main results. First, the effect that uncertainty about the match outcome has on supporters? attendance is positive, significant and non-linear. Second, concerning the final ranking, we identify two opposite effects on supporter?s behaviour. While their attendance is inversely related to the gap between the position of their preferred team and its followers in the ongoing ranking, their attendance is also higher if the gap between each teams in the final ranking is sufficiently low. Third, supporters? attendance is not linearly decreasing with the position of the supported team in the championship, but this link presents a sinusoidal curve. The reason is that supporters? attendance also depends on the likelihood of their team to accede to Europea League during the next season, and also on the likelihood of their team being drawn out of ?Ligue?1? at the end of the season. We also show that supporters? attendance is directly proportional to the uncertainty level about the championship final victory. In particular, the attractiveness of the championship is negatively affected by the recent financial power of PSG.
C11|Exploring the effect of crisis on cooperatives: A Bayesian performance analysis of French craftsmen cooperatives| This paper aims at understanding the economic performance of craftsmen cooperatives during the crisis period. These cooperatives have the distinctive feature of being supply cooperatives. We use an exhaustive dataset for the French craftsmen cooperatives (2004-2014). We estimate Bayesian Translog econometric models in order to underline the impact of the 2008 crisis on these cooperatives. On the one hand, cooperatives’ turnover contracts during the crisis, the effect is lower for elder cooperatives and varies across sectors. On the other hand, there is convergence towards the mean for the various generations of cooperatives. Theses findings are robust to alternative econometric specifications.
C11|Sample statistics as convincing evidence: A tax fraud case|This report deals with the analysis of data used by tax officers to support their claim of tax fraud at a pizzeria. The possibilities of embezzlement under study are overreporting of take-away sales and underreporting of cash payments. Several modelling approaches are explored, ranging from simple well-known methods to presumably more precise tools. More specifically, we contrast common methods based on normal assumptions and models based on Gamma-assumptions. For the latter, both maximum likelihood and Bayesian approaches are covered. Several criteria for the choice of method in practice are discussed, among them, how easy the method is to understand, justify and communicate to the parties. Some dilemmas present itself: the choice of statistical method, its role in building the evidence, the choice of risk factor, the application of legal principles like “clear and convincing evidence” and “beyond reasonable doubt”. The insights gained may be useful for both tax officers and defenders of the taxpayer, as well as for expert witnesses.
C11|Is the US Phillips Curve Stable? Evidence from Bayesian VARs|Inflation did not fall as much as many economists expected as the Great Recession hit the US economy. One explanation suggested for this phenomenon is that the Phillips curve has become flatter. In this paper we investigate the stability of the US Phillips curve, employing Bayesian VARs to quarterly data from 1990Q1 to 2017Q3. We estimate bivariate models for PCE inflation and the unemployment rate under a number of different assumptions concerning the dynamics and covariance matrix. Specifically, we assess the importance of time-varying parameters and stochastic volatility. Using new tools for model selection, we find support for both time-varying parameters and stochastic volatility. Interpreting the Phillips curve as the inflation equation of our Bayesian VAR, we conclude that the US Phillips curve has been unstable. Our results also indicate that the Phillips curve may have been somewhat flatter between 2005 and 2013 than in the decade preceding that period. However, while the dynamic relations of the model appear to be subject to time variation, we note that the effect of a shock to the unemployment rate on inflation is not fundamentally different over time. Finally, a conditional forecasting exercise suggests that as far as the models are concerned, inflation may not have been unexpectedly high around the Great Recession.
C11|A Note on the Stability of the Swedish Philips Curve|We use Bayesian techniques to estimate bivariate VAR models for Swedish unemployment rate and inflation. Employing quarterly data from 1995Q1 to 2017Q3 and new tools for model selection, we compare a model with time-varying parameters and stochastic volatility to a specification with constant parameters and covar-iance matrix. We find strong evidence in favour of the specification with time-varying parameters and sto-chastic volatility. Our results indicate that the Swedish Phillips curve has not been stable over time. However, our findings do not suggest that the Phillips curve has been flatter in more recent years.
C11|A Model for Policy Interest Rates|This paper introduces a model that addresses the key worldwide features of modern monetary policy making: the discreteness of policy interest rates both in magnitude and in timing, the preponderance of status quo decisions, policy inertia and regime switching. We capture them by developing a new dynamic discrete-choice model with switching among three latent policy regimes (dovish, neutral and hawkish), estimated via the Gibbs sampler with data augmentation. The simulations and an application to federal funds rate target demonstrate that ignoring these features leads to biased estimates, worse in- and out-of-sample fit, and qualitatively different inference. Using all Federal Open Market Committee?s (FOMC) decisions made both at scheduled and unscheduled meetings as sample observations, we model the Federal Reserve?s response to real-time data available right before each meeting, and control for the endogeneity of monetary policy shocks. The new model, fitted for Greenspan?s tenure, correctly predicts the directions of about 90% of the next decisions on the target rate (hike, no change, or cut) out of sample during Bernanke?s term including the status quo decisions after reaching the zero lower bound, while the conventional linear model fails to adequately tackle the zero bound and wrongly predicts further cuts.
C11|Peers’ Parents and Educational Attainment|This paper contributes to the discussion on childhood exposure by investigating the extent to which the educational background of peers’ parents is related to a child’s future college attainment. I analyze the friendship networks of a nationally representative sample of high-school students in the US and find that the spillover from peers’ parents of the same gender operates independently of peer effects. The effects are robust to addressing friendship selection. The same gender pattern suggests either the transmission of gender-specific information or the presence of a role model effect. Furthermore, the same gender spillover is significant only for students from lower-educated families. A student whose father is absent or less caring also experiences significant influence from peers’ fathers. The heterogeneity by own family background indicates the influences from parental and non-parental adults are substitutes.
C11|Forecasting Inflation in Argentina|In 2016 the Central Bank of Argentina began to announce inflation targets. In this context, providing authorities with good estimates of relevant macroeconomic variables is crucial for making pertinent corrections in order to reach the desired policy goals. This paper develops a group of models to forecast inflation for Argentina, which includes autoregressive models and different scale Bayesian VARs (BVAR), and compares their relative accuracy. The results show that the BVAR model can improve the forecast ability of the univariate autoregressive benchmark’s model of inflation. The Giacomini-White test indicates that a BVAR performs better than the benchmark in all forecast horizons. Statistical differences between the two BVAR model specifications (small and large-scale) are not found. However, looking at the RMSEs, one can see that the larger model seems to perform better for longer forecast horizons.
C11|Modeling systemic risk with Markov Switching Graphical SUR models|We propose a Markov Switching Graphical Seemingly Unrelated Regression (MS-GSUR) model to investigate time-varying systemic risk based on a range of multi-factor asset pricing models. Methodologically, we develop a Markov Chain Monte Carlo (MCMC) scheme in which latent states are identified on the basis of a novel weighted eigenvector centrality measure. An empirical application to the constituents of the S&P100 index shows that cross-firm connectivity significantly increased over the period 1999–2003 and during the financial crisis in 2008–2009. Finally, we provide evidence that firm-level centrality does not correlate with market values and it is instead positively linked to realized financial losses.
C11|Adaptive Bayesian Estimation of Mixed Discrete-Continuous Distributions under Smoothness and Sparsity|We consider nonparametric estimation of a mixed discrete-continuous distribution under anisotropic smoothness conditions and possibly increasing number of support points for the discrete part of the distribution. For these settings, we derive lower bounds on the estimation rates in the total variation distance. Next, we consider a nonparametric mixture of normals model that uses continuous latent variables for the discrete part of the observations. We show that the posterior in this model contracts at rates that are equal to the derived lower bounds up to a log factor. Thus, Bayesian mixture of normals models can be used for optimal adaptive estimation of mixed discrete-continuous distributions.
C11|Lightning Prediction Using Model Output Statistics|A method to predict lightning by postprocessing numerical weather prediction (NWP) output is developed for the region of the European Eastern Alps. Cloud-to-ground flashes-detected by the ground-based ALDIS network-are counted on the 18x18 km^2 grid of the 51-member NWP ensemble of the European Centre of Medium-Range Weather Forecasts (ECMWF). These counts serve as target quantity in count data regression models for the occurrence and the intensity of lightning events. The probability whether lightning occurs or not is modelled by a binomial distribution. For the intensity a hurdle approach is employed, for which the binomial distribution is combined with a zero-truncated negative binomial to model the counts within a grid cell. In both statistical models the parameters of the distributions are described by additive predictors, which are assembled by potentially nonlinear terms of NWP covariates. Measures of location and spread of approx. 100 direct and derived NWP covariates provide a pool of candidates for the nonlinear terms. A combination of stability selection and gradient boosting selects influential terms. Markov chain Monte Carlo (MCMC) simulation estimates the final model to provide credible inference of effects, scores and predictions. The selection of terms and MCMC simulation are applied for data of the year 2016, and out-of-sample performance is evaluated for 2017. The occurrence model outperforms a reference climatology-based on seven years of data-up to a forecast horizon of 5 days. The intensity model is calibrated and also outperforms climatology for exceedance probabilities, quantiles, and full predictive distributions.
C11|The Determinants of Population Growth: Literature review and empirical analysis|We point out that the simple slice sampler generates chains that are correlation-free when the target distribution is centrally symmetric. This property explains several results in the literature about the relative performance of the simple and product slice samplers. We exploit it to improve two algorithms often used to circumvent the slice inversion problem, namely stepping out and multivariate sampling with hyperrectangles. In the general asymmetric case, we argue that symmetrizing the target distribution before simulating greatly enhances the efficiency of the simple slice sampler. To achieve symmetry we focus on the Box-Cox transformation with parameters chosen to minimize a measure of skewness. This strategy is illustrated with several sampling problems.
C11|A Bayesian Gamma Frailty Model Using the Sum of Independent Random Variables: Application of the Estimation of an Interpurchase Timing Model|In statistics, researchers have rigorously investigated the reproductive property, which maintains that the sum of independent random variables with the same distribution follows the same family of distributions. However, even if a distribution of the sum of random variables demonstrates the reproductive property, estimating parameters appropriately from only summed observations is difficult. This is because of identification problems when component random variables have different parameters. In this study, we develop a method to effectively estimate parameters from the sum of independent random variables with different parameters. In particular, we focus on the sum of Gamma random variables composed of two types of distributions. We generalize the result according to Moschopoulos(1985) to a proportional hazard model with covariates and a frailty model to capture individual heterogeneities. Additionally, to estimate each parameter from the sum of random variables, we incorporate auxiliary information using quasi-Bayesian methods, and we propose the estimation procedure by Markov chain Monte Carlo. We confirm the effectiveness of the proposed method through a simulation study and apply it to the interpurchase timing model in marketing.
C11|A century of gaps|This paper considers the role of financial information in the estimation and dynamics of the US output gap over more than a century. To this end, we extend the parsimonious approach of Borio, Disyatat, and Juselius (2016, 2014) to allow for time-varying effects of financial factors. This novel feature significantly improves real-time estimates of the output gap. It signals the peak and trough in economic activity related to both the Great Recession and the Great Depression. Two major insights follow. Credit dynamics are the primary drivers of the observed financial crisis, albeit with different conduits over the century: the stock market in 1929 and the housing market in 2008. Accounting for credit growth, the US potential growth has been stable at 2% since the beginning of 1980.
C11|Peer-Induced Beliefs Regarding College Participation| We study peer effects on the formation of beliefs regarding college participation. We present a structural model of learning in friendship networks. We show that the model is identified and we present a Bayesian estimation procedure. We estimate the model using data on teenagers’ beliefs regarding college participation, controlling for preferences and academic achievement. We find that, on average, friends’ beliefs account for about 12% of the updating process. We also find strong heterogeneity among schools and individuals. In particular, we find substantial unobserved individual heterogeneity, which casts doubt on the efficiency of network-targeted public policies.
C11|Understanding Discrimination against Same-Sex Couples in the United States: Evidence from an Email Correspondence Audit|As of 2017, no federal law protected Gay, Lesbian, Bisexual, Transgender, and Queer (LGBTQ) individuals from housing discrimination. However, 22 U.S. states and over 200 municipalities have passed laws prohibiting housing discrimination based on sexual orientation. In this paper, I present the results of a randomized pair-email correspondence audit of 6,490 property owners in 94 U.S. cities. I provide a nationally-representative estimate of the level of discrimination same-sex couples experience when inquiring about rental housing. I find that same-sex male couples, especially Black same-sex male couples, are less likely to receive a response to inquiries about rental units. Same-sex female couples receive preferential treatment compared to heterosexual couples. I then examine how state and local anti-discrimination laws covary with rates of housing discrimination against same-sex couples. Compare to localities without any housing protections, I find that response rates covary positively with statelevel protections but negatively with local-level protections. I conclude by testing several hypotheses about the causes of this discrimination. This preliminary evidence suggests that property owners are willfully discriminating against same-sex male couples.
C11|Contagious exporting and foreign ownership: Evidence from firms in Shanghai using a Bayesian spatial bivariate probit model|Whether a firm is able to attract foreign capital and whether it may participate at the export market depends on whether the fixed costs associated with doing so are at least covered by the incremental operating profits. This paper provides evidence that success for some firms in attracting foreign investors and in exporting appears to reduce the associated fixed costs with exporting or foreign ownership in other firms. Using data on 8,959 firms located in Shanghai, we find that contagion and spillovers in exporting and in foreign ownership decisions within an area of 10 miles in the city of Shanghai amplify fixed-cost reductions for both exporting as well as foreign ownership of neighboring firms. Contagion among exporters and among foreign-owned firms, respectively, amplify shocks to the profitability of these activities to a large extent. These findings are established through the estimation of a spatial bivariate probit model.
C11|Limited Asset Market Participation And The Euro Area Crisis: An Empirical Dsge Model|We estimate a medium‐scale dynamic stochastic general equilibrium model for the Euro area with limited asset market participation (LAMP). Our results suggest that in the recent European Monetary Union years LAMP is particularly sizable (39% during 1993–2012) and important to understand business cycle features. The Bayes factor and the forecasting performance show that the LAMP model is preferred to its representative household counterpart. In the representative agent model the risk premium shock is the main driver of output volatility in order to match consumption correlation with output. In the LAMP model this role is played by the investment‐specific shock, because non‐Ricardian households introduce a Keynesian multiplier effect and raise the correlation between consumption and investments. We also detect the contractionary role of monetary policy shocks during the post‐2007 years. In this period consumption of non‐Ricardian households fell dramatically, but this outcome might have been avoided by a more aggressive policy stance. (JEL C11, C13, C32, E21, E32, E37)
C11|Forecasting using Bayesian VARs: A Benchmark for STREAM|This study develops a suite of Bayesian Vector Autoregression (BVAR) models for the Maltese economy to benchmark the forecasting performance of STREAM, the traditional macro-econometric model used by the Central Bank of Malta for its regular forecasting exercises. Three different BVARs are proposed, containing an endogenous and exogenous block, and differ only in terms of the crosssectional size of the former. The small BVAR contains only three endogenous variables, the medium BVAR includes 17 variables, while the large BVAR includes 32 endogenous variables. The exogenous block remains consistent across the three models. By using a similar information set, the Bayesian VARs developed in this study are utilised to benchmark the forecast performance of STREAM. In general, for real GDP, the GDP deflator, and the unemployment rate, BVAR median projections for the period 2014-2016 improve the forecast performance at the one, two, and four-step ahead horizons when compared to STREAM. However, the latter does rather well at annual projections, but it is broadly outperformed by the medium and large BVARs.
C11|Forecasting external demand using BVAR models|As the impact of monetary policy decisions manifests itself with a lag, decision-makers also need economic forecasts when they make decisions. In this paper, we present a method that may facilitate the integration of incoming data in the external demand forecast faster than is currently possible. The external demand forecast helps to forecast exports and, through that, developments in GDP. In the current practice, for the imports of Hungary’s key trading partners we use the forecasts of international institutions as a starting point. Data received in the meantime can be included in the forecast using expert judgements. With the method described in this paper, we forecast the imports of Hungary’s key trading partners – and with the help thereof – their external demand, relying on BVAR models and using monthly time series (confidence indices, industrial production, orders). Based on the literature, we use the Kalman filter to eliminate the differences in the publication lags of the individual time series. The missing variable is then forecast using the other variables. The forecasts thus obtained perform better than the best ARMA models, and the model containing global imports and the oil price. With one exception, the forecast of the imports of the individual countries is more accurate when prepared on the whole sample, rather than on the rolling sample. The forecast of external demand is also more accurate if we use the whole sample. The most accurate BVAR model used to forecast external demand provides an unbiased forecast and also yields a better forecast of turning points than the models used for comparison. Compared to the forecasts of international institutions, the BVAR forecast performs better when actual import data from the respective year are already available. Thus, compared to previous practice, the novelty is represented by the BVAR methodology and the monthly time series, which can be integrated into the forecast in a formalised manner. Looking ahead, it may also be worthwhile to forecast GDP components using this method.
C11|Spending Policies of Italian Banking Foundations|Italian Banking Foundations are important institutional investors and not-forprofit institutions in Italy. Foundations finance their activities with the returns they get by investing their endowments on the financial markets. The financial management of Foundations has to meet two conflicting aims: maintaining a consitent and sufficient spending level in the short run, and preserving the real value of the endowment fund in the long run. During the financial crisis, the tension between these aims has strongly emerged. This paper is about the results of an analysis of the spending decisions of five main Foundations, from 2004 to 2016, based on balance sheet and market value data. We have estimated a bayesian model that allows to evaluate the relative importance of spending stability versus preserving the endowment value. We find that, from 2009 onwards, Foundations revised radically their spending decisions, enhancing short-term spending stability and reducing long-term endowment exploitation. This has made current spending less sensitive to negative market shocks. At the same time, these shocks may end up hitting the endowment value and are likely to persistently reduce future spending levels.
C11|Frekvensbaserede versus bayesianske metoder i empirisk økonomi|"Indenfor økonomi og samfundsvidenskab har den klassiske frekvens-baserede analysemetode traditionelt været fremherskende, men de senere år er flere samfundsforskere begyndt at anvende bayesianske metoder i empirisk modellering. I denne artikel beskrives og sammenlignes de to metoder. Der argumenteres for, at vi i højere grad bør anvende den bayesianske tilgang. Den klassiske metode giver sandsynligheden for data, givet modellen (nulhypotesen), mens den bayesianske metode giver sandsynligheden for modellen, givet data. Anvendelse af ""p-værdien""i det klassiske hypotesetest fører til for mange ""falsk positive"" resultater. Den bayesianske metode er mere velegnet end den klassiske til analyse af de hypoteser økonomer arbejder med, hvor en model ikke tilstræbes at være ""sand"", men i stedet opfattes som en grov approksimation til virkeligheden."
C11|Exploring the effect of crisis on cooperatives: A Bayesian performance analysis of French craftsmen cooperatives|This paper aims at understanding the economic performance of craftsmen cooperatives during the crisis period. These cooperatives have the distinctive feature of being supply cooperatives. We use an exhaustive dataset for the French craftsmen cooperatives (2004-2014). We estimate Bayesian Translog econometric models in order to underline the impact of the 2008 crisis on these cooperatives. On the one hand, cooperatives' turnover contracts during the crisis, the effect is lower for elder cooperatives and varies across sectors. On the other hand, there is convergence towards the mean for the various generations of cooperatives. Theses findings are robust to alternative econometric specifications.
C11|Bayesian Averaging of Classical Estimates (BACE) for gretl|This paper presents a software package that implements Bayesian Averaging of Classical Estimates BACE ver. 1.1 for gretl (the GNU regression, econometrics and time-series library).
C11|Density Forecasts in Panel Data Models : A Semiparametric Bayesian Perspective|This paper constructs individual-specific density forecasts for a panel of firms or households using a dynamic linear model with common and heterogeneous coefficients and cross-sectional heteroskedasticity. The panel considered in this paper features a large cross-sectional dimension N but short time series T. Due to the short T, traditional methods have difficulty in disentangling the heterogeneous parameters from the shocks, which contaminates the estimates of the heterogeneous parameters. To tackle this problem, I assume that there is an underlying distribution of heterogeneous parameters, model this distribution nonparametrically allowing for correlation between heterogeneous parameters and initial conditions as well as individual-specific regressors, and then estimate this distribution by pooling the information from the whole cross-section together. Theoretically, I prove that both the estimated common parameters and the estimated distribution of the heterogeneous parameters achieve posterior consistency, and that the density forecasts asymptotically converge to the oracle forecast. Methodologically, I develop a simulation-based posterior sampling algorithm specifically addressing the nonparametric density estimation of unobserved heterogeneous parameters. Monte Carlo simulations and an application to young firm dynamics demonstrate improvements in density forecasts relative to alternative approaches.
C11|The transmission of uncertainty shocks on income inequality: State-level evidence from the United States|In this paper, we explore the relationship between state-level household income inequality and macroeconomic uncertainty in the United States. Using a novel large-scale macroeconometric model, we shed light on regional disparities of inequality responses to a national uncertainty shock. The results suggest that income inequality decreases in most states, with a pronounced degree of heterogeneity in terms of shapes and magnitudes of the dynamic responses. By contrast, some few states, mostly located in the West and South census region, display increasing levels of income inequality over time. We find that this directional pattern in responses is mainly driven by the income composition and labor market fundamentals. In addition, forecast error variance decompositions allow for a quantitative assessment of the importance of uncertainty shocks in explaining income inequality. The findings highlight that volatility shocks account for a considerable fraction of forecast error variance for most states considered. Finally, a regression-based analysis sheds light on the driving forces behind differences in state-specific inequality responses.
C11|Stochastic model specification in Markov switching vector error correction models|This paper proposes a hierarchical modeling approach to perform stochastic model specification in Markov switching vector error correction models. We assume that a common distribution gives rise to the regime-specific regression coefficients. The mean as well as the variances of this distribution are treated as fully stochastic and suitable shrinkage priors are used. These shrinkage priors enable to assess which coefficients differ across regimes in a flexible manner. In the case of similar coefficients, our model pushes the respective regions of the parameter space towards the common distribution. This allows for selecting a parsimonious model while still maintaining sufficient flexibility to control for sudden shifts in the parameters, if necessary. In the empirical application, we apply our modeling approach to Euro area data and assume that transition probabilities between expansion and recession regimes are driven by the cointegration errors. Our findings suggest that lagged cointegration errors have predictive power for regime shifts and these movements between business cycle stages are mostly driven by differences in error variances.
C11|State Correlation and Forecasting: A Bayesian Approach Using Unobserved Components Models|Implications to signal extraction that arise from specifying unobserved components (UC) models with correlated or orthogonal innovations have been well-investigated. In contrast, an analogous statement for forecasting evaluation cannot be made. This paper attempts to fill this gap in light of the recent resurgence of studies adopting UC models for forecasting purposes. In particular, four correlation structures are entertained: orthogonal, correlated, perfectly correlated innovations as well as a novel approach which combines features from two contrasting cases, namely, orthogonal and perfectly correlated innovations. Parameter space restrictions associated with different correlation structures and their connection with forecasting are discussed within a Bayesian framework. Introducing perfectly correlated innovations, however, reduces the covariance matrix rank. To accommodate that, a Markov Chain Monte Carlo sampler which builds upon properties of Toeplitz matrices and recent advances in precision-based algorithms is developed. Our results for several measures of U.S. inflation indicate that the correlation structure between state variables has important implications for forecasting performance as well as estimates of trend inflation.
C11|Asymmetric Risks to the Economic Outlook Arising from Financial System Vulnerabilities|When financial system vulnerabilities are elevated, they can give rise to asymmetric risks to the economic outlook. To illustrate this, I consider the economic outlook presented in the Bank of Canada’s October 2017 Monetary Policy Report in the context of two key financial system vulnerabilities: high levels of household indebtedness and housing market imbalances. Uncertainty on the profile of consumption by indebted households—and, therefore, risks to growth in gross domestic product (GDP)—arises from higher interest rates and from recent changes to the Office of the Superintendent of Financial Institutions’ B-20 mortgage underwriting guideline. I use non-linear Bayesian techniques to capture the potential amplification of negative shocks in a vulnerable environment. I find that the materialization of larger-than-expected impacts on consumption from higher interest rates and/or the tighter mortgage qualifying criteria would imply asymmetric risks to GDP growth.
C11|Dynamic Interbank Network Analysis Using Latent Space Models|Longitudinal network data are increasingly available, allowing researchers to model how networks evolve over time and to make inference on their dependence structure. In this paper, a dynamic latent space approach is used to model directed networks of monthly interbank exposures. In this model, each node has an unobserved temporal trajectory in a low-dimensional Euclidean space. Model parameters and latent banks' positions are estimated within a Bayesian framework. We apply this methodology to analyze two different datasets: the unsecured and the secured (repo) interbank lending networks. We show that the model that incorporates a latent space performs much better than the model in which the probability of a tie depends only on observed characteristics; the latent space model is able to capture some features of the dyadic data such as transitivity that the model without a latent space is not able to.
C11|Private saving. New cross-country evidencebased on bayesian techniques|The existing literature exhibits high uncertainty over the theoretical and empirical determinants of private world saving. This paper reports new evidence on the drivers of private saving by applying Bayesian techniques, using data from the world’s 35 largest economies in the period 1980-2012. After reviewing the main theories of consumption and saving decisions, and discussing the potential effects of different determinants, we specify a general model that incorporates the most commonly used factors in the literature, considering the potential endogeneity of some of the regressors. The Bayesian Model Averaging (BMA) approach summarises the information embedded in all combinations of the explanatory variables considered by averaging each specification according to its likelihood. We find that in the medium term private credit to GDP ratio, the government surplus to GDP ratio, the terms of trade, life expectancy and the old-age dependency ratio are key determinants of cross-country private saving behaviour. Lastly, we assess the long-term effect of expected demographic changes in private saving globally.
C11|Exchange rate pass-through into euro area inflation. An estimated structural model|We evaluate the exchange rate pass-through (ERPT) into euro area (EA) inflation by estimating an open economy New Keynesian model with Bayesian methods. In the model ERPT is incomplete because of local currency pricing and distribution services, with the latter allowing to distinguish between ERPT at the border and ERPT at the consumer level. Our main results are the following ones. First, ERPT into EA prices is, in general, high. Second, it is particularly high in correspondence of exchange rate and monetary policy shocks. Third, the EA monetary stance is relevant for ERPT; in particular, ERPT is higher if the stance is accommodative in correspondence of expansionary demand shocks.
C11|Estimating Non-Linear DSGEs with the Approximate Bayesian Computation: an application to the Zero Lower Bound|Estimation of non-linear DSGE models is still very limited due to high computational costs and identification issues arising from the non-linear solution of the models. Besides, the use of small sample amplifies those issues. This paper advocates for the use of Approximate Bayesian Computation (ABC), a set of Bayesian techniques based on moments matching. First, through Monte Carlo exercises, I assess the small sample performance of ABC estimators and run a comparison with the Limited Information Method (Kim, 2002), the state-of-the-art Bayesian method of moments used in DSGE literature. I find that ABC has a better small sample performance, due to the more efficient way through which the information provided by the moments is used to update the prior distribution. Second, ABC is tested on the estimation of a new-Keynesian model with a zero lower bound, a real life application where the occasionally binding constraint complicates the use of traditional method of moments.
C11|Bayesian Forecasting of Electoral Outcomes with new Parties' Competition|We propose a new methodology for predicting electoral results that combines a fundamental model and national polls within an evidence synthesis framework. Although novel, the methodology builds upon basic statistical structures, largely modern analysis of variance type models, and it is carried out in open-source software. The methodology is largely motivated by the specific challenges of forecasting elections with the participation of new political parties, which is becoming increasingly common in the post-2008 European panorama. Our methodology is also particularly useful for the allocation of parliamentary seats, since the vast majority of available opinion polls predict at the national level whereas seats are allocated at local level. We illustrate the advantages of our approach relative to recent competing approaches using the 2015 Spanish Congressional Election. In general, the predictions of our model outperform the alternative specifications, including hybrid models that combine fundamental and polls' models. Our forecasts are, in relative terms, particularly accurate to predict the seats obtained by each political party.
C11|Inflation and Professional Forecast Dynamics: An Evaluation of Stickiness, Persistence, and Volatility|"This paper studies the joint dynamics of U.S. inflation and the average inflation predictions of the Survey of Professional Forecasters (SPF) on a sample running from 1968Q4 to 2014Q2. The joint data generating process (DGP) of these data consists of the unobserved components (UC) model of Stock and Watson (2007, ""Why has US inflation become harder to forecast?,"" Journal of Money, Credit and Banking 39(S1), 3-33) and the sticky information (SI) forecast updating equation of Mankiw and Reis (2002, ""Sticky information versus sticky prices: A proposal to replace the New Keynesian Phillips curve,"" Quarterly Journal of Economics 117, 1295-1328). We introduce timevarying inflation gap persistence into the Stock and Watson (SW)-UC model and a timevarying frequency of forecast updating into the SI forecast updating equating. These models combine to produce a nonlinear state space model. This model is estimated using Bayesian tools grounded in the particle filter, which is an implementation of sequential Monte Carlo methods. The estimates reveal the data prefer the joint DGP of time-varying frequency of SI forecast updating and a SW-UC model with time-varying persistence. The joint DGP produces estimates that indicate the inflation spike of 1974 was explained most by gap inflation, but trend inflation dominates the inflation peak of the early 1980s. We also find the stochastic volatility (SV) of trend inflation exhibits negative co-movement with the time-varying frequency of SI forecast updating while the SV and time-varying persistence of gap inflation often show positive co-movement. Thus, the average SPF respondent is most sensitive to the impact of permanent shocks on the conditional mean of inflation."
C11|The Likelihood of Effective Lower Bound Events|This paper provides estimates of the probability of an economy hitting its effective lower bound (ELB) on the nominal interest rate and of the expected duration of such an event for eight advanced economies. To that end, a mean-adjusted panel vector autoregression with static interdependencies and the possibility of regime change is estimated. The simulation procedure produces ELB risk estimates for both the short term, where the current phase of the business cycle plays an important role, and the medium term, where the occurrence of an ELB situation is determined mainly by the equilibrium values of macroeconomic variables. The paper also discusses the ELB event probability estimates with respect to previous approaches used in the literature.
C11|Forecast density combinations of dynamic models and data driven portfolio strategies|A dynamic asset-allocation model is specified in probabilistic terms as a combination of return distributions resulting from multiple pairs of dynamic models and portfolio strategies based on momentum patterns in US industry returns. The nonlinear state space representation of the model allows efficient and robust simulation-based Bayesian inference using a novel non-linear filter. Combination weights can be cross-correlated and correlated over time using feedback mechanisms. Diagnostic analysis gives insight into model and strategy misspecification. Empirical results show that a smaller flexible model-strategy combination performs better in terms of expected return and risk than a larger basic model-strategy combination. Dynamic patterns in combination weights and diagnostic learning provide useful signals for improved modeling and policy, in particular, from a risk-management perspective.
C11|Leverage effects and stochastic volatility in spot oil returns: A Bayesian approach with VaR and CVaR applications|Crude oil markets have been quite volatile and risky in the past few decades due to the large fluctuations of oil prices. We contribute to the current debate by testing for the existence of the leverage effect when considering daily spot returns in the WTI and Brent crude oil markets and by studying the direct impact of the leverage effect on measures of risk such as VaR and CVaR. More specifically, we model spot crude oil returns using Stochastic Volatility (SV) models with various distributions of the errors. We find that the introduction of the leverage effect in the traditional SV model with Normally distributed errors is capable of adequately estimating risk for speculative oil suppliers in both the WTI and Brent markets. Our results also show that financial regulators' model choice, both on the supply and on the demand side, would not be affected by the introduction of leverage. Focusing instead on firm's internal risk management, our results show that the introduction of leverage would be useful for firms who are on the demand side for oil, who use VaR for risk management and who are particularly worried about the magnitude of the losses exceeding VaR while wanting to minimize the opportunity cost of capital. Using the same logic, firms who are on the supply side would be better off not considering the leverage effect.
C11|DSGE-based priors for BVARs and quasi-Bayesian DSGE estimation|We present a new method for estimating Bayesian vector auto-regression (VAR) models using priors from a dynamic stochastic general equilibrium (DSGE) model. We use the DSGE model priors to determine the moments of an independent Normal-Wishart prior for the VAR parameters. Two hyper-parameters control the tightness of the DSGE-implied priors on the autoregressive coefficients and the residual covariance matrix respectively. Determining these hyper-parameters by selecting the values that maximize the marginal likelihood of the Bayesian VAR provides a method for isolating subsets of DSGE parameter priors that are at odds with the data. We illustrate the ability of our approach to correctly detect incorrect DSGE priors for the variance of structural shocks using a Monte Carlo experiment. We also demonstrate how posterior estimates of the DSGE parameter vector can be recovered from the BVAR posterior estimates: a new ‘quasi-Bayesian’ DSGE estimation. An empirical application on US data reveals economically meaningful differences in posterior parameter estimates when comparing our quasi-Bayesian estimator with Bayesian maximum likelihood. Our method also indicates that the DSGE prior implications for the residual covariance matrix are at odds with the data.
C11|Inference in structural vector autoregressions when the identifying assumptions are not fully believed: Re-evaluating the role of monetary policy in economic fluctuations|Point estimates and error bands for SVARs that are set identified are only justified if the researcher is persuaded that some parameter values are a priori more plausible than others. When such prior information exists, traditional approaches can be generalized to allow for doubts about the identifying assumptions. We use information about both structural coefficients and impacts of shocks and propose a new asymmetric t-distribution for incorporating information about signs in a nondogmatic way. We apply these methods to a three-variable macroeconomic model and conclude that monetary policy shocks are not the major driver of output, inflation, or interest rates.
C11|How do shocks to bank capital affect lending and growth?|We examine bank capital shocks using a recent new approach based on non-normal errors in vector autoregressive models. Using a sample of 14 European economies over January 2004 through March 2018 we identify two distinct classes of bank capital shocks, capital tightening shocks, and bank profitability shocks. We find that both bank capital shocks frequently lead to changes in lending volume and interest rates for new loans. In contrast to some recent similar studies, we find less evidence for impact on production. Bank capital shocks have further effects on the substitution between the bank and market-based financing and on credit allocation across different borrower sectors. Policymakers may find these results useful when considering counter-cyclical adjustments to the bank capital requirements.
C11|Central Bank Reputation and Inflation-Unemployment Performance: Empirical Evidence from an Executive Survey of 62 Countries|Although there is a well-established theoretical literature that links central bank (CB) reputation with inflation performance following Barro and Gordon, there is little empirical work testing the relationship rigorously. This paper empirically tests the impact of reputation on inflation-unemployment performance using a novel set of data on CB reputation--an annual local business manager survey on central bank policy covering 62 countries during 1995-2016. This paper finds that CB reputation is a significant determinant of inflation: the results of an FE panel and Arellano-Bond difference GMM model show that high-reputation CBs have achieved better inflation performances over the past 20 years with lower levels of inflation than others, holding the output gap and unemployment rate constant. This result remains robust to various control variables including money growth, past inflation levels, exchange rates, and financial crisis dummies. This paper also finds that high CB reputation is associated with a tight anchoring of inflation expectations to inflation targets in inflation-targeting countries. The effects of reputation on the volatility of inflation and unemployment rates are found to be not robust. This paper offers evidence of the opposite-direction causality as well that goes from high inflation to decreased CB reputation.
C11|The Effect of Investment-Specific Technology Shocks on the Gap of Wage and Employment by Workers¡Ç Skill or Tasks (in Korean)|This paper aims to assess the effect of investment-specific technology shocks(IST shocks) on the gap of wages and employment between different groups of workers in the Korean labor market. IST shocks can be identified by four long-run restrictions in a Structural Bayesian VAR framework. First, workers are categorized into skilled(college graduates and more) and unskilled(high-school graduates and less) ones by their educational attainment. After each shocks activates, including IST¡¤skill-biased technology¡¤other technology shocks, changes in the Skill-Unskill wage gap(skill premium) and employment differentials are examined. Second, workers are re-categorized into routine(tasked on repetitive and procedural jobs) and non-routine ones by their occupational tasks. By simulating IST¡¤routine-labor productivity¡¤other technology shocks respectively, it is originally checked how the Routine-Nonroutine wage gap and employment differentials will change. This paper shows that positive IST shocks can decrease the Skill-Unskill wage gap and increase the Routine-Nonroutine wage gap. It is also found, however, that positive IST shocks can not drive significant changes in employment differentials by any categorization.
C11|Optimal Asset Allocation with Multivariate Bayesian Dynamic Linear Models|We introduce a simulation-free method to model and forecast multiple asset returns and employ it to investigate the optimal ensemble of features to include when jointly predicting monthly stock and bond excess returns. Our approach builds on the Bayesian Dynamic Linear Models of West and Harrison (1997), and it can objectively determine, through a fully automated procedure, both the optimal set of regressors to include in the predictive system and the degree to which the model coefficients, volatilities, and covariances should vary over time. When applied to a portfolio of five stock and bond returns, we find that our method leads to large forecast gains, both in statistical and economic terms. In particular, we find that relative to a standard no-predictability benchmark, the optimal combination of predictors, stochastic volatility, and time-varying covariances increases the annualized certainty equivalent returns of a leverage-constrained power utility investor by more than 500 basis points.
C11|Dynamic Effects of Monetary Policy Shocks on Macroeconomic Volatility|We use a simple New Keynesian model, with firm specific capital, non-zero steady-state inflation, long-run risks and Epstein-Zin preferences to study the volatility implications of a monetary policy shock. An unexpected increase in the policy rate by 150 basis points causes output and inflation volatility to rise around 10% above their steady-state standard deviations. VAR based empirical results support the model implications that contractionary shocks increase volatility. The volatility effects of the shock are driven by agents' concern about the (in) ability of the monetary authority to reverse deviations from the policy rule and the results are re-enforced by the presence of non-zero trend inflation.
C11|DSGE-based Priors for BVARs & Quasi-Bayesian DSGE Estimation|We present a new method for estimating Bayesian vector autoregression (VAR) models using priors from a dynamic stochastic general equilibrium (DSGE) model. We use the DSGE model priors to determine the moments of an independent Normal-Wishart prior for the VAR parameters. Two hyper-parameters control the tightness of the DSGE-implied priors on the autoregressive coefficients and the residual covariance matrix respectively. Determining these hyper-parameters by selecting the values that maximize the marginal likelihood of the Bayesian VAR provides a method for isolating subsets of DSGE parameter priors that are at odds with the data. We illustrate the ability of our approach to correctly detect incorrect DSGE priors for the variance of structural shocks using a Monte Carlo experiment. We also demonstrate how posterior estimates of the DSGE parameter vector can be recovered from the BVAR posterior estimates: a new 'quasi-Bayesian' DSGE estimation. An empirical application on US data reveals economically meaningful differences in posterior parameter estimates when comparing our quasi-Bayesian estimator with Bayesian maximum likelihood. Our method also indicates that the DSGE prior implications for the residual covariance matrix are at odds with the data.
C11|Adolescents on the Road: A Case Study of Determinants of Risky Behaviors|The 2016 report of the European Transport Safety Council claims that EU safety progress has come to a standstill. This study aims at deepening the knowledge of factors that influence adolescents’ risky behavior on the road. Bayesian Networks offer a promising new way to looking at the issue. In the analysis of a dataset collected in Tuscany, Italy, called EDIT, we found evidence that the use of alcohol and illegal substances explain only part of the probability of having an accident, and that other observable variables, like the level of distress or the type of school attended are significantly related to the probability of incurring in a road crash. New and close attention should be given to a systemic approach and to a plethora of environmental and individual variables that may rise the probability of road accidents for very young drivers.
C11|Macroeconomic Uncertainty and Forecasting Macroeconomic Aggregates|Can information on macroeconomic uncertainty improve the forecast accuracy for key macroeconomic time series for the US? Since previous studies have demonstrated that the link between the real economy and uncertainty is subject to nonlinearities, I assess the predictive power of macroeconomic uncertainty in both linear and nonlinear Bayesian VARs. For the latter I use a threshold VAR that allows for regimedependent dynamics conditional on the level of the uncertainty measure. I find that the predictive power of macroeconomic uncertainty in the linear VAR is negligible. In contrast, using information on macroeconomic uncertainty in a threshold VAR can significantly improve the accuracy of short-term point and density forecasts, especially in the presence of high uncertainty.
C11|Forecasting using mixed-frequency VARs with time-varying parameters|We extend the literature on economic forecasting by constructing a mixed-frequency time-varying parameter vector autoregression with stochastic volatility (MF-TVP-SVVAR). The latter is able to cope with structural changes and can handle indicators sampled at different frequencies. We conduct a real-time forecast exercise to predict US key macroeconomic variables and compare the predictions of the MF-TVP-SV-VAR with several linear, nonlinear, mixed-frequency, and quarterly-frequency VARs. Our key finding is that the MF-TVPSV-VAR delivers very accurate forecasts and, on average, outperforms its competitors. In particular, inflation forecasts benefit from this new forecasting approach. Finally, we assess the models’ performance during the Great Recession and find that the combination of stochastic volatility, time-varying parameters, and mixed-frequencies generates very precise inflation forecasts.
C11|When Are Stocks Less Volatile in the Long Run?|Pastor and Stambaugh (2012) demonstrate that from a forward-looking perspective, stocks are more volatile in the long run than they are in the short run. We investigate how the economic constraint of non-negative equity premia aspects predictive variance. When investors expect non-negative returns in the market and thus impose the constraint on predictive regressions, they find that stocks are less volatile in the long run, even after taking account of estimation risk and uncertainties on current and future expected stock returns because the constraint provides additional parameter identification condition and prior information for future returns. Thus, it substantially reduces uncertainty on future stock returns. This fact, combined with the mean reversion property of stock return dynamics, leads to lower predictive variance in the long run.
C11|Distance-Based Metrics: A Bayesian Solution to the Power and Extreme-Error Problems in Asset-Pricing Tests|We propose a unified set of distance-based performance metrics that address the power and extreme-error problems inherent in traditional measures for asset-pricing tests. From a Bayesian perspective, the distance metrics coherently incorporate both pricing errors and their standard errors. Measured in units of return, they have an economic interpretation as the minimum cost of holding a dogmatic belief in a model. Our metrics identify Fama and French (2015) factor model (augmented with the momentum factor and/or without the value factor) as the best model and thus highlight the importance of the momentum factor. In contrast, the traditional alpha-based statistics often lead to inconsistent and counter-intuitive model rankings.
C11|Monetary policy and structural changes in Colombia, 1990-2016: A Markov Switching approach|This paper analyzes the Colombian economic context between 1990Q1-2014Q4, using a Markov-Switching Dynamic Stochastic General Equilibrium Model (MS-DSGE) to identify regime switches in the driven mechanisms of the economy. Bayesian methods are applied, allowing for changes in the monetary policy rule, nominal rigidities, and shock volatilities of the model. The results support evidence of shifts in the structural parameters of several equations that define the dynamics of the economy. The best fit specification of the MSDSGE model allows for independent switches in the Taylor rule, parameters, coefficients of the domestic Phillips curve, and changes in volatilities. Estimations from the DSGE model suggest a regime switching in monetary policy from a low to high inflation response, which coincides with the adoption of inflation targeting in 1999Q3, and with a lower Phillips curve slope in the Economy since 1998Q3. Historical decomposition analysis and counterfactual exercises show that regime switches were crucial when the Central Bank adopted an Inflation Targeting regime, and, by themselves, these regime changes could be interpreted as disinflationary shocks. These phenomena are not identifiable when using linear models.
C11|Inefficiency and Bank Failures: A Joint Bayesian Esti-mationof a Stochastic Frontier Model and a Hazards Model|In modeling bank failure, estimating inefficiency separately from the hazards modelresults in inefficient, biased, and inconsistent estimators. We develop a method to si-multaneously estimate a stochastic frontier model and a hazards model using Bayesiantechniques. This method overcomes issues related to two-stage estimation methods,allows for computing the marginal effects of the inefficiency over the probability offailure, and facilitates statistical inference of the functions of the parameters such aselasticities, returns to scale, and individual inefficiencies. Simulation exercises showthat our proposed method performs better than two-stage maximum likelihood, spe-cially in small samples. In addition we find that inefficiency plays a statistically and economically significant role in determining the time to failure of U.S. commercial banks during 2001 to 2010.<br><small>(This abstract was borrowed from another version of this item.)</small>
C11|Inefficiency and Bank Failures: A Joint Bayesian Estimationof a Stochastic Frontier Model and a Hazards Model|In modeling bank failure, estimating inefficiency separately from the hazards modelresults in inefficient, biased, and inconsistent estimators. We develop a method to si-multaneously estimate a stochastic frontier model and a hazards model using Bayesiantechniques. This method overcomes issues related to two-stage estimation methods,allows for computing the marginal effects of the inefficiency over the probability offailure, and facilitates statistical inference of the functions of the parameters such aselasticities, returns to scale, and individual inefficiencies. Simulation exercises showthat our proposed method performs better than two-stage maximum likelihood, spe-cially in small samples. In addition we find that inefficiency plays a statistically and economically significant role in determining the time to failure of U.S. commercial banks during 2001 to 2010.
C11|A Model of the Fed’s View on Inflation|A view often expressed by the Fed is that three components matter in inflation dynamics : a trend anchored by long run inflation expectations; a cycle connecting nominal and real variables; and oil prices. This paper proposes an econometric structural model of inflation formalising this view. Our findings point to a stable expectational trend, a sizeable and well identified Phillips curve and an oil cycle which, contrary to the standard rational expectation model, affects inflation via expectations without being reflected in the output gap. The latter often overpowers the Phillips curve. In fact, the joint dynamics of the Phillips curve cycle and the oil cycles explain the inflation puzzles of the last ten years.
C11|Network formation with local complements and global substitutes: the case of R&D networks|In this paper we introduce a stochastic network formation model where agents choose both actions and links. Neighbors in the network benefit from each other’s action levels through local complementarities and there exists a global interaction effect reflecting a strategic substitutability in actions. The tractability of the model allows us to provide a complete equilibrium characterization in the form of a Gibbs measure, and we show that the structural features of equilibrium networks are consistent with empirically observed networks. We then use our equilibrium characterization to show that the model can be conveniently estimated even for large networks. The policy relevance is demonstrated with examples of firm exit, mergers and acquisitions and subsidies in the context of R&D collaboration networks.
C11|Making Parametric Portfolio Policies Work|The implementation of parametric portfolio policies as introduced by Brandt, Santa Clara and Valkanov (RFS 2009) may run into empirical problems. For example, expected utility based on monthly returns of S&P-500 data from 1995-2013 turns non-monotonic for moderate levels of (constant) risk aversion. We establish that in the leading case of constant relative risk aversion (CRRA) strong assumptions on the properties of the returns, the variables used to implement the parametric portfolio policy and the parameter space are necessary to obtain a well defined optimization problem. Without such refinements an interior maximum of the expected utility functional may not exist. We provide economic conditions on the domain and/or the utility functions that overcome such empirical problems and that guarantee the effectiveness of the approach. We illustrate the implications of our improvements by applying parametric portfolio policies to a large universe of stocks.
C11|The Transmission of Monetary Policy Shocks|Despite years of research, there is still uncertainty around the effects of monetary policy shocks. We reassess the empirical evidence by combining a new identification that accounts for informational rigidities, with a flexible econometric method robust to misspecifications that bridges between VARs and Local Projections. We show that most of the lack of robustness of the results in the extant literature is due to compounding unrealistic assumptions of full information with the use of severely misspecified models. Using our novel methodology, we find that a monetary tightening is unequivocally contractionary, with no evidence of either price or output puzzles.
C11|An approach to increasing forecast-combination accuracy through VAR error modeling|We consider a situation in which the forecaster has available M individual forecasts of a univariate target variable. We propose a 3-step procedure designed to exploit the interrelationships among the M forecast-error series (estimated from a large time-varying parameter VAR model of the errors, using past observations) with the aim of obtaining more accurate predictions of future forecast errors. The refined future forecast-error predictions are then used to obtain M new individual forecasts that are adapted to the information from the estimated VAR. The adapted M individual forecasts are ultimately combined and any potential accuracy gains of the adapted combination forecasts analyzed. We evaluate our approach in an out-of-sample forecasting analysis, using a well-established 7-country data set on output growth. Our 3-step procedure yields substantial accuracy gains (in terms of loss reductions ranging between 6.2% up to 18%) for the simple average and three time-varying-parameter combination forecasts.
C11|Randomized Quasi Sequential Markov Chain Monte Carlo²|Sequential Monte Carlo and Markov Chain Monte Carlo methods are combined into a unifying framework for Bayesian parameter inference in non-linear, non-Gaussian state space models. A variety of tuning approaches are suggested to boost convergence: likelihood tempering, data tempering, adaptive proposals, random blocking, and randomized Quasi Monte Carlo numbers. The methods are illustrated and compared by running eight variants of the algorithm to estimate the parameters of a standard stochastic volatility model.
C11|Testing for time-varying stochastic volatility in Bitcoin returns|The study will be the first to offer empirical justification for time-varying stochastic volatility in Bitcoin returns. Specifically, it tests for time variation in both the trend and transitory components of the stochastic volatility using the unobserved components model that accounts for same. Thereafter, it calculates the Bayes factor using the approach of Chan (2018) which involves the Savage-Dickey density ratio in order to avoid the computation of the marginal likelihood. The results overwhelmingly support at least one time-varying stochastic volatility component in Bitcoin returns and the transitory component is favoured in this regard. These results are robust to different data frequencies.
C11|Does time-variation matter in the stochastic volatility components for G7 stock returns|This study empirically tests for time variation in the stochastic volatility (SV) components for the G7 stock returns. The time variation in both trend and transitory components of the SV is tested separately and jointly using the unobserved component model and following the approach developed by Chan (2018). Consequently, the computed Bayes factor obtained from the SavageDickey density ratio, which circumvents the computation of marginal likelihood, is used to adjudge the performance of each restricted time varying stochastic volatility model without the trend and transitory components against the unrestricted model that allows for same. The empirical evidence supports time variation in the transitory component of SV while the trend component is found to be relatively constant over time. These empirical estimates are not sensitive to data frequency.
C11|Revisiting supply and demand indexes in real estate|In this paper we disentangle reservation prices of buyers and sellers for commercial real estate at the city level. To do so, we further develop and extend the Fisher et al. (2003, 2007) methodology to a repeat sales indexing framework. This has the advantage that it takes care of all unobserved heterogeneity, which is an important consideration in commercial real estate. Furthermore, it allows for the construction of supply and demand indexes without the need for many property characteristics or assessed values. A key innovation in our methodology, which also enables granular index production, is our use of a Bayesian, structural time series model for index estimation. By introducing these new methodological developments, we are able to estimate reliable, robust supply and demand indexes for all major metropolitan areas in the US. Here we focus on two very different urban markets: New York and Phoenix. Consistent with the notion of pro-cyclical liquidity, we find that buyers' reservation prices move much more extremely and earlier than sellers' reservation prices. Our results show that the demand indexes in both New York and Phoenix went down a full year earlier than the supply indexes during the crisis.
C11|Does Housing Vintage Matter? Exploring the Historic City Center of Amsterdam|A home is typically thought of as a bundle of land and structure. Land is supplied inelastically and is non-reproducible. Land values are therefore affected by a number of demand factors. Conceptually, structures are easily produced, and thus are supplied elastically. Under elastic supply, it is reasonable to assume that replacement cost should be equivalent to the value of the structure for new properties. Here, we examine how particular structure characteristics may introduce heterogeneity into these demand and supply relationships. In other words, how do structure vintages influence price dynamics. Older vintages are not easily reproducible leading to the value of an older vintage to potentially diverge from its replacement cost. To test our hypotheses, we employ a nonlinear model in a Bayesian structural timeseries approach that explicitly disentangles structure and land values to identify vintage effects separately from physical deterioration and land values. We find large differences in price dynamics between four distinct vintages of Amsterdam old city center apartments. Between 1999 - 2016, new construction had an average return of 1.7%, with a standard deviation of 2.4%. On the other hand, properties build in the 19th century had an average return of 3.6% with a standard deviation of 6.1%, during the same period.
C11|Dynamic Effects of the Chilean Fiscal Policy|In Chile, the empirical literature studying the dynamic effects of fiscal policy and fiscal multipliers, using linear vector autoregression models, disagrees on the effects of government spending and taxes on output. In this paper, we bring new elements to this debate. We include the nonlinear dimension of vector autoregression models to answer if the state, “tight” or “normal”, of the Chilean economy, affects fiscal policy effectiveness. Last, based on the nonlinear framework we question if monetary policy has an influence on the size of fiscal multipliers. We find that: (i) once using the same quarterly data, the size of fiscal multipliers not only varies depending on the identification strategy and the linear vector autoregression model used but also on the definitions of government spending and taxes considered; (ii) the government spending multiplier from the nonlinear framework differs, being about the unit in the “tight” regime and around -0.5 in the “normal” regime; (iii) government spending and tax multipliers in the nonlinear framework are smaller when monetary policy is taken into account, which influences the effectiveness of fiscal policy.
C11|Priors for the Long Run| We propose a class of prior distributions that discipline the long-run behavior of vector autoregressions (VARs). These priors can be naturally elicited using economic theory, which provides guidance on the joint dynamics of macroeconomic time series in the long run. Our priors for the long run are conjugate, and can thus be easily implemented using dummy observations and combined with other popular priors. In VARs with standard macroeconomic variables, a prior based on the long-run predictions of a wide class of theoretical models yields substantial improvements in the forecasting performance. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.
C11|Euro area real-time density forecasting with financial or labor market frictions|We compare real-time density forecasts for the euro area using three DSGE models. The benchmark is the Smets and Wouters model, and its forecasts of real GDP growth and inflation are compared with those from two extensions. The first adds financial frictions and expands the observables to include a measure of the external finance premium. The second allows for the extensive labor-market margin and adds the unemployment rate to the observables. The main question that we address is whether these extensions improve the density forecasts of real GDP and inflation and their joint forecasts up to an eight-quarter horizon. We find that adding financial frictions leads to a deterioration in the forecasts, with the exception of longer-term inflation forecasts and the period around the Great Recession. The labor market extension improves the medium- to longer-term real GDP growth and shorter- to medium-term inflation forecasts weakly compared with the benchmark model.
C11|Selecting models with judgment|A statistical decision rule incorporating judgment does not perform worse than a judgmental decision with a given probability. Under model misspecification, this probability is unknown. The best model is the least misspecified, as it is the one whose probability of underperforming the judgmental decision is closest to the chosen probability. It is identified by the statistical decision rule incorporating judgment with lowest in sample loss. Averaging decision rules according to their asymptotic performance results in decisions which are weakly better than the best decision rule. The model selection criterion is applied to a vector autoregression model for euro area inflation. JEL Classification: C1, C11, C12, C13
C11|The New Area-Wide Model II: an extended version of the ECB's micro-founded model for forecasting and policy analysis with a financial sector|This paper provides a detailed description of an extended version of the ECB’s New Area-Wide Model (NAWM) of the euro area (cf. Christoffel, Coenen, and Warne 2008). The extended model—called NAWM II—incorporates a rich financial sector with the threefold aim of (i) accounting for a genuine role of financial frictions in the propagation of economic shocks and policies and for the presence of shocks originating in the financial sector itself, (ii) capturing the prominent role of bank lending rates and the gradual interest-rate pass-through in the transmission of monetary policy in the euro area, and (iii) providing a structural framework useable for assessing the macroeconomic impact of the ECB’s large-scale asset purchases conducted in recent years. In addition, NAWM II includes a number of other extensions of the original model reflecting its practical uses in the policy process over the past ten years. JEL Classification: C11, C52, E30, E37, E58
C11|A New Way to Quantify the Effect of Uncertainty|This paper develops a new way to quantify the effect of uncertainty and other higher-order moments. First, we estimate a nonlinear model using Bayesian methods with data on uncertainty, in addition to common macro time series. This key step allows us to decompose the exogenous and endogenous sources of uncertainty, calculate the effect of volatility following the cost of business cycles literature, and generate data-driven policy functions for any higherorder moment. Second, we use the Euler equation to analytically decompose consumption into several terms—expected consumption, the ex-ante real interest rate, and the ex-ante variance and skewness of future consumption, technology growth, and inflation—and then use the policy functions to filter the data and create a time series for the effect of each term. We apply our method to a familiar New Keynesian model with a zero lower bound constraint on the nominal interest rate and two stochastic volatility shocks, but it is adaptable to a broad class of models.
C11|Bayesian Parametric and Semiparametric Factor Models for Large Realized Covariance Matrices|This paper introduces a new factor structure suitable for modeling large realized covariance matrices with full likelihood based estimation. Parametric and nonparametric versions are introduced. Due to the computational advantages of our approach we can model the factor nonparametrically as a Dirichlet process mixture or as an infinite hidden Markov mixture which leads to an infinite mixture of inverse-Wishart distributions. Applications to 10 assets and 60 assets show the models perform well. By exploiting parallel computing the models can be estimated in a matter of a few minutes.
C11|Oil Price Shocks and Economic Growth: The Volatility Link|This paper shows that oil shocks primarily impact economic growth through the conditional variance of growth. We move beyond the literature that focuses on conditional mean point forecasts and compare models based on density forecasts. Over a range of dynamic models, oil shock measures and data we find a robust link between oil shocks and the volatility of economic growth. A new measure of oil shocks is developed and shown to be superior to existing measures and indicates that the conditional variance of growth increases in response to an indicator of local maximum oil price exceedance. The empirical results uncover a large pronounced asymmetric response of growth volatility to oil price changes. Uncertainty about future growth is considerably lower compared to a benchmark AR(1) model when no oil shocks are present.
C11|Bayesian inference and prediction of a multiple-change-point panel model with nonparametric priors|Change point models using hierarchical priors have been very successful estimating the parameter values of short-lived regimes. However, hierarchical priors have been parametric which leads to shrinkage in the estimates of extraordinary regime parameters. We overcome this by modeling the hierarchical priors nonparametrically. We also extend the change point to a panel of change point processes where the prior shares in the probabilities of changing regimes. When applied to the returns from a panel of actively managed, US equity, mutual funds our multiple-change-point panel model finds mutual fund skill is not persistent but changes over time.
C11|Forecasting With High Dimensional Panel VARs|In this paper, we develop econometric methods for estimating large Bayesian timevarying parameter panel vector autoregressions (TVP-PVARs) and use these methods to forecast inflation for euro area countries. Large TVP-PVARs contain huge numbers of parameters which can lead to over-parameterization and computational concerns. To overcome these concerns, we use hierarchical priors which reduce the dimension of the parameter vector and allow for dynamic model averaging or selection over TVP-PVARs of different dimension and different priors. We use forgetting factor methods which greatly reduce the computational burden. Our empirical application shows substantial forecast improvements over plausible alternatives.
C11|Adaptive Hierarchical Priors for High-Dimensional Vector Autoregessions|This paper proposes a scalable and simulation-free estimation algorithm for vector autoregressions (VARs) that allows fast approximate calculation of marginal posterior distributions. We apply the algorithm to derive analytical expressions for popular Bayesian shrinkage priors that admit a hierarchical representation and which would typically require computationally intensive posterior simulation methods. The proposed algorithm is modular, parallelizable, and scales linearly with the number of predictors, allowing fast and efficient estimation of large Bayesian VARs. The benefits of our approach are explored and computational gains of the proposed estimation algorithm and priors. Second, a forecasting exercise involving VARs estimated on macroeconomic data demonstrates the ability of hierarchical shrinkage priors to find useful parsimonious representations. Finally, we show that our approach can be used successfully for structural analysis and can replicate important features of structural shocks predicted by economic theory.
C11|Variational Bayes inference in high-dimensional time-varying parameter models|This paper proposes a mean field variational Bayes algorithm for efficient posterior and predictive inference in time-varying parameter models. Our approach involves: i) computationally trivial Kalman filter updates of regression coefficients, ii) a dynamic variables election prior that removes irrelevant variables in each time period, and iii) a fast approximate state-space estimator of the regression volatility parameter. In an exercise involving simulated data we evaluate the new algorithm numerically and establish its computational advantages. Using macroeconomic data for the US we find that regression models that combine time-varying parameters with the information in many predictors have the potential to improve forecasts over a number of alternatives.
C11|Reducing dimensions in a large TVP-VAR|This paper proposes a new approach to estimating high dimensional time varying parameter structural vector autoregressive models (TVP-SVARs) by taking advantage of an empirical feature of TVP-(S)VARs. TVP-(S)VAR models are rarely used with more than 4-5 variables. However recent work has shown the advantages of modelling VARs with large numbers of variables and interest has naturally increased in modelling large dimensional TVP-VARs. A feature that has not yet been utilized is that the covariance matrix for the state equation, when estimated freely, is often near singular. We propose a specification that uses this singularity to develop a factor-like structure to estimate a TVP-SVAR for 15 variables. Using a generalization of the re-centering approach, a rank reduced state covariance matrix and judicious parameter expansions, we obtain efficient and simple computation of a high dimensional TVP-SVAR. An advantage of our approach is that we retain a formal inferential framework such that we can propose formal inference on impulse responses, variance decompositions and, important for our model, the rank of the state equation covariance matrix. We show clear empirical evidence in favour of our model and improvements in estimates of impulse responses.
C11|Multivariate stochastic volatility with co-heteroscedasticity|This paper develops a new methodology that decomposes shocks into homoscedastic and heteroscedastic components. This specification implies there exist linear combinations of heteroscedastic variables that eliminate heteroscedasticity. That is, these linear combinations are homoscedastic; a property we call co-heteroscedasticity. The heteroscedastic part of the model uses a multivariate stochastic volatility inverse Wishart process. The resulting model is invariant to the ordering of the variables, which we show is important for impulse response analysis but is generally important for, e.g., volatility estimation and variance decompositions. The specification allows estimation in moderately high-dimensions. The computational strategy uses a novel particle filter algorithm, a reparameterization that substantially improves algorithmic convergence and an alternating-order particle Gibbs that reduces the amount of particles needed for accurate estimation. We provide two empirical applications; one to exchange rate data and another to a large Vector Autoregression (VAR) of US macroeconomic variables. We find strong evidence for co-heteroscedasticity and, in the second application, estimate the impact of monetary policy on the homoscedastic and heteroscedastic components of macroeconomic variables.
C11|Estimation bayésienne d’un modèle néo-keynésien pour l’économie marocaine|Ce travail porte sur l'estimation d'un modèle hybride néo-keynesien (HNKM) formé de trois équations structurelles caractérisant l'économie marocaine. Il s'agit de la courbe de demande, de la courbe d'o¤re et d'une règle Taylor augmentée des réserves de change. Le modèle est estimé par une approche bayésienne à partir des données trimestrielles couvrant la période 1998Q1-2016Q4. Parallèlement et s'inspirant des travaux de Del Negro et Schorfheide (2004), un modèle BVAR-DSGE a été estimé en exploitant les priors issus du modèle HNKM. Les fonctions de réponse impulsionnelles ont été comparées et les performances prédictives de ces deux modèles structurels ont été confrontées à des modèles statistiques alternatifs: le VAR classique et le BVAR. Il ressort des résultats des modèles HNKM et BVAR-DSGE que les réactions des variables aux di¤érents chocs sont globalement similaires et conformes aux prédictions de la théorie économique. L'étude de la qualité prévisionnelle des di¤érents modèles indique que le BVAR-DSGE et le HNKM présentent des avantages comparatifs mais sans dominer, en tous points, les modèles statistiques tels que le VAR classique et le VAR bayésien.
