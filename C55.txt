C55|In search of a job: Forecasting employment growth using Google Trends|We show that Google search activity on relevant terms is a strong out-of-sample predictor for future employment growth in the US over the period 2004-2018 at both short and long horizons. Using a subset of ten keywords associated with “jobs”, we construct a large panel of 173 variables using Google’s own algorithms to find related search queries. We find that the best Google Trends model achieves an out-of-sample R2 between 26% and 59% at horizons spanning from one month to a year ahead, strongly outperforming benchmarks based on a large set of macroeconomic and financial predictors. This strong predictability extends to US state-level employment growth, using state-level specific Google search activity. Encompassing tests indicate that when the Google Trends panel is exploited using a non-linear model it fully encompasses the macroeconomic forecasts and provides significant information in excess of those.
C55|Improving Forecast Accuracy of Financial Vulnerability: PLS Factor Model Approach|We present a factor augmented forecasting model for assessing the financial vulnerability in Korea. Dynamic factor models often extract latent common factors from a large panel of time series data via the method of the principal components (PC). Instead, we employ the partial least squares (PLS) method that estimates target specific common factors, utilizing covariances between predictors and the target variable. Applying PLS to 198 monthly frequency macroeconomic time series variables and the Bank of Korea's Financial Stress Index (KFSTI), our PLS factor augmented forecasting models consistently outperformed the random walk benchmark model in out-of-sample prediction exercises in all forecast horizons we considered. Our models also outperformed the autoregressive benchmark model in short-term forecast horizons. We expect our models would provide useful early warning signs of the emergence of systemic risks in Korea's financial markets.
C55|Do you Feel the Heat Around the Corner? The Effect of Weather on Crime|In this paper, we study the weather-crime relationship using a unique high-frequency, city-level data set for the United States with 2.4 mio. observations. In contrast to the existing literature using (often) daily data, we match hourly observations of weather and crime. Our results show that using daily observations overestimates the effect of temperature and underestimates the effect of precipitation on crime and leads to different conclusions about the significance of variables. We document evidence for a non-linear relationship between weather variables and crime. Again, results differ greatly between daily and hourly observations.
C55|Iassopack: Model Selection and Prediction with Regularized Regression in Stata|This article introduces lassopack, a suite of programs for regularized regression in Stata. lassopack implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso and post-estimation OLS. The methods are suitable for the high-dimensional setting where the number of predictors p may be large and possibly greater than the number of observations, n. We offer three different approaches for selecting the penalization ('tuning') parameters: information criteria (implemented in lasso2), K-fold cross-validation and h-step ahead rolling cross-validation for cross-section, panel and time-series data (cvlasso), and theory-driven ('rigorous') penalization for the lasso and square-root lasso for cross-section and panel data (rlasso). We discuss the theoretical framework and practical considerations for each approach. We also present Monte Carlo results to compare the performance of the penalization approaches.
C55|Boosting the Hodrick-Prescott Filter|The Hodrick-Prescott (HP) filter is one of the most widely used econometric methods in applied macroeconomic research. The technique is nonparametric and seeks to decompose a time series into a trend and a cyclical component unaided by economic theory or prior trend specification. Like all nonparametric methods, the HP filter depends critically on a tuning parameter that controls the degree of smoothing. Yet in contrast to modern nonparametric methods and applied work with these procedures, empirical practice with the HP filter almost universally relies on standard settings for the tuning parameter that have been suggested largely by experimentation with macroeconomic data and heuristic reasoning about the form of economic cycles and trends. As recent research has shown, standard settings may not be adequate in removing trends, particularly stochastic trends, in economic data. This paper proposes an easy-to-implement practical procedure of iterating the HP smoother that is intended to make the filter a smarter smoothing device for trend estimation and trend elimination. We call this iterated HP technique the boosted HP filter in view of its connection to L_2-boosting in machine learning. The paper develops limit theory to show that the boosted HP filter asymptotically recovers trend mechanisms that involve unit root processes, deterministic polynomial drifts, and polynomial drifts with structural breaks – the most common trends that appear in macroeconomic data and current modeling methodology. In doing so, the boosted filter provides a new mechanism for consistently estimating multiple structural breaks. A stopping criterion is used to automate the iterative HP algorithm, making it a data-determined method that is ready for modern data-rich environments in economic research. The methodology is illustrated using three real data examples that highlight the differences between simple HP filtering, the data-determined boosted filter, and an alternative autoregressive approach. These examples show that the boosted HP filter is helpful in analyzing a large collection of heterogeneous macroeconomic time series that manifest various degrees of persistence, trend behavior, and volatility.
C55|Battling Antibiotic Resistance: Can Machine Learning Improve Prescribing?|Antibiotic resistance constitutes a major health threat. Predicting bacterial causes of infections is key to reducing antibiotic misuse, a leading cause of antibiotic resistance. We combine administrative and microbiological laboratory data from Denmark to train a machine learning algorithm predicting bacterial causes of urinary tract infections. Based on predictions, we develop policies to improve prescribing in primary care, highlighting the relevance of physician expertise and time-variant patient distributions for policy implementation. The proposed policies delay prescriptions for some patients until test results are known and give them instantly to others. We find that machine learning can reduce antibiotic use by 7.42 percent without reducing the number of treated bacterial infections. As Denmark is one of the most conservative countries in terms of antibiotic use, targeting a 30 percent reduction in prescribing by 2020, this result is likely to be a lower bound of what can be achieved elsewhere.
C55|"MÃ©xico | La crisis por escasez de gasolina: un anÃ¡lisis de Big Data<BR>[Mexico | The gasoline shortage crisis: A Big Data analysis]"|Se analizaron las operaciones por dÃ­a y hora en TPVs de las gasolineras de la ZM del Valle de MÃ©xico. La crisis iniciÃ³ a las 12:00hrs del martes 8 de enero, durÃ³ 13 dÃ­as, concluyÃ³ el 20 de enero, se cargÃ³ 16% mÃ¡s gasolina por operaciÃ³n, y se incrementÃ³ hasta 400% la compra a altas horas de la noche y en la madrugada. An analysis of the POS operations in gas stations in the Valle de Mexico metro-area are analyzed by day and hour. The crisis began at noon on Tuesday January 8, lasted 13 days, ended on January 20, 16% more gasoline was loaded per operation, and the purchase was increased up to 400% at late night and in the early morning.
C55|Measuring retail trade using card transactional data|In this paper we present a high-dimensionality Retail Trade Index (RTI) constructed to nowcast the retail trade sector economic performance in Spain, using Big Data sources and techniques. The data are the footprints of BBVA clients from their credit or debit card transactions at Spanish point of sale (PoS) terminals. The resulting indexes have been found to be robust when compared with the Spanish RTI, regional RTI (Spain’s autonomous regions), and RTI by retailer type (distribution classes) published by the National Statistics Institute (INE). We also went one step further, computing the monthly indexes for the provinces and sectors of activity and the daily general index, by obtaining timely, detailed information on retail sales. Finally, we analyzed the high-frequency consumption dynamics using BBVA retailer behavior and a structural time series model.
C55|When are Google data useful to nowcast GDP? An approach via pre-selection and shrinkage|Nowcasting GDP growth is extremely useful for policy-makers to assess macroe-conomic conditions in real-time. In this paper, we aim at nowcasting euro area GDP with a large database of Google search data. Our objective is to check whether this specific type of information can be useful to increase GDP nowcasting accuracy, and when, once we control for official variables. In this respect, we estimate shrunk bridge regressions that integrate Google data optimally screened through a targeting method, and we empirically show that this approach provides some gain in pseudo-real-time nowcasting of euro area GDP quarterly growth. Especially, we get that Google data bring useful information for GDP nowcasting for the four first weeks of the quarter when macroeconomic information is lacking. However, as soon as official data become available, their relative nowcasting power vanishes. In addition, a true real-time anal-ysis confirms that Google data constitute a reliable alternative when official data are lacking.
C55|The Shale Oil Boom and the U.S. Economy: Spillovers and Time-Varying Effects|We analyze if the transmission of oil price shocks on the U.S. economy has changed as a result of the shale oil boom. To do so we allow for spillovers at the state level, as well as aggregate country level effects. We identify and quantify these spillovers using a factor-augmented vector autoregressive (VAR) model, allowing for time-varying changes. In contrast to previous results, we find considerable changes in the way oil price shocks are transmitted: there are now positive spillovers to non-oil investment, employment and production in many U.S. states from an increase in the oil price - effects that were not present before the shale oil boom.
C55|The Long-Run Information Effect of Central Bank Communication|Why do long-run interest rates respond to central bank communication? Whereas existing explanations imply a common set of signals drives short and long-run yields, we show that news on economic uncertainty can have increasingly large effects along the yield curve. To evaluate this channel, we use the publication of the Bank of England's Inflation Report, from which we measure a set of high-dimensional signals. The signals that drive long-run interest rates do not affect short-run rates and operate primarily through the term premium. This suggests communication plays an important role in shaping perceptions of long-run uncertainty.
C55|Heterogeneous beliefs and the Phillips curve|We establish a set of novel empirical facts concerning cross-section distributions of inflation expectations reported in surveys. Almost all the variation in expectations about their mean may be summarized via three factors we call disagreement, skew, and shape. We adopt a functional principal component regression approach to estimating forward-looking models of inflation that exploits the heterogeneity present in individual-level data. By using survey information more effectively, our approach reveals an enhanced role for expectations in inflation dynamics that is robust to lagged inflation, trend inflation, and supply factors. Our findings hold in similar form across two major economies.
C55|Machine learning explainability in finance: an application to default risk analysis|We propose a framework for addressing the ‘black box’ problem present in some Machine Learning (ML) applications. We implement our approach by using the Quantitative Input Influence (QII) method of Datta et al (2016) in a real‑world example: a ML model to predict mortgage defaults. This method investigates the inputs and outputs of the model, but not its inner workings. It measures feature influences by intervening on inputs and estimating their Shapley values, representing the features’ average marginal contributions over all possible feature combinations. This method estimates key drivers of mortgage defaults such as the loan‑to‑value ratio and current interest rate, which are in line with the findings of the economics and finance literature. However, given the non‑linearity of ML model, explanations vary significantly for different groups of loans. We use clustering methods to arrive at groups of explanations for different areas of the input space. Finally, we conduct simulations on data that the model has not been trained or tested on. Our main contribution is to develop a systematic analytical framework that could be used for approaching explainability questions in real world financial applications. We conclude though that notable model uncertainties do remain which stakeholders ought to be aware of.
C55|Understanding the Sources of Earnings Losses After Job Displacement: A Machine-Learning Approach|We document the sources behind earnings losses after job displacement adapting the generalized random forest due to Athey et al. (2019). Using administrative data from Austria over three decades, we show that displaced workers face large and persistent earnings losses. We identify substantial heterogeneity in losses across workers. A quarter of workers face cumulative 11-year losses higher than 2 times their pre-displacement annual income, while another quarter experiences losses less than 1.1 times their income. The most vulnerable are older high-income workers employed at well-paying firms in the manufacturing sector. Our methodology allows us to consider many competing theories of earnings losses prominently discussed in the literature. The two most important factors are the displacement firm's wage premia and the availability of well paying jobs in the local labor market. Our overall findings provide evidence that earnings losses can be understood by mean reversion in firm rents and losses in match quality, rather than by a destruction of firm-specific human capital.
C55|Fixed-effect regressions on network data| This paper studies inference on fixed eff ects in a linear regression model estimated from network data. We derive bounds on the variance of the fixed-e ffect estimator that uncover the importance of the smallest non-zero eigenvalue of the (normalized) Laplacian of the network and of the degree structure of the network. The eigenvalue is a measure of connectivity, with smaller values indicating less-connected networks. These bounds yield conditions for consistent estimation and convergence rates, and allow to evaluate the accuracy of first-order approximations to the variance of the fixed-eff ect estimator. Supplement for CWP32/16
C55|Dependent Microstructure Noise and Integrated Volatility: Estimation from High-Frequency Data|In this paper, we develop econometric tools to analyze the integrated volatility (IV) of the efficient price and the dynamic properties of microstructure noise in high-frequency data under general dependent noise. We first develop consistent estimators of the variance and autocovariances of noise using a variant of realized volatility. Next, we employ these estimators to adapt the pre-averaging method and derive consistent estimators of the IV, which converge stably to a mixed Gaussian distribution at the optimal rate n1/4. To improve the finite sample performance, we propose a multi-step approach that corrects the finite sample bias, which turns out to be crucial in applications. Our extensive simulation studies demonstrate the excellent performance of our multi-step estimators. In an empirical study, we analyze the dependence structures of microstructure noise and provide intuitive economic interpretations; we also illustrate the importance of accounting for both the serial dependence in noise and the finite sample bias when estimating IV.
C55|Innovative events|We take a fresh look at firms' innovation-productivity linkages, using novel data capturing new aspects of innovative activity. We combine UK administrative microdata, media and website content to develop experimental metrics - new product/service launches - for a large panel of SMEs. Extensive validation and descriptive exercises show that launches complement patents, trademarks and innovation surveys. We also establish connections between launches and previous innovative activity. We then link IP, launches and productivity, controlling for media exposure and firm heterogeneity. Launch activity is associated with higher SME productivity, especially in the service sector. High-quality launches and medium-size firms help drive this result.
C55|Business cycle narratives|This article quantifies the epidemiology of media narratives relevant to business cycles in the US, Japan, and Europe (euro area). We do so by first constructing daily business cycle indexes computed on the basis of the news topics the media writes about. At a broad level, the most in uential news narratives are shown to be associated with general macroeconomic developments, finance, and (geo-)politics. However, a large set of narratives contributes to our index estimates across time, especially in times of expansion. In times of trouble, narratives associated with economic uctuations become more sparse. Likewise, we show that narratives do go viral, but mostly so when growth is low. While narratives interact in complicated ways, we document that some are clearly associated with economic fundamentals. Other narratives, on the other hand, show no such relationship, and are likely better explained by classical work capturing the market's animal spirits.
C55|The long-run information effect of central bank communication|Why do long-run interest rates respond to central bank communication? Whereas existing explanations imply a common set of signals drives short and long-run yields, we show that news on economic uncertainty can have increasingly large effects along the yield curve. To evaluate this channel, we use the publication of the Bank of England’s Inflation Report, from which we measure a set of high-dimensional signals. The signals that drive long-run interest rates do not affect short-run rates and operate primarily through the term premium. This suggests communication plays an important role in shaping perceptions of long-run uncertainty.
C55|Marginal jobs and job surplus: a test of the efficiency of separations|"We present a sharp test for the efficiency of job separations. First, we document a dramatic increase in the separation rate – 11.2ppt (28%) over five years – in response to a quasi-experimental extension of UI benefit duration for older workers. Second, after the abolition of the policy, the ""job survivors"" in the formerly treated group exhibit exactly the same separation behavior as the control group. Juxtaposed, these facts reject the ""Coasean"" prediction of efficient separations, whereby the UI extensions should have extracted marginal (low-surplus) jobs and thereby rendered the remaining (high-surplus) jobs more resilient after its abolition. Third, we show that a formal model of predicted efficient separations implies a piece-wise linear function of the actual control group separations beyond the missing mass of marginal matches. A structural estimation reveals point estimates of the share of efficient separations below 4%, with confidence intervals rejecting shares above 13%. Fourth, to characterize the marginal jobs in the data, we extend complier analysis to difference-in-difference settings such as ours. The UI-indiced separators stemmed from declining firms, blue-collar jobs, with a high share of sick older workers, and firms more likely to have works councils – while their wages were similar to program survivors. The evidence is consistent with a ""non-Coasean"" framework building on wage frictions preventing efficient bargaining, and with formal or informal institutional constraints on selective separations."
C55|Big Data and Firm Dynamics|We study a model where firms accumulate data as a valuable intangible asset. Data accumulation affects firms' dynamics. It increases the skewness of the firm size distribution as large firms generate more data and invest more in active experimentation. On the other hand, small data-savvy firms can overtake more traditional incumbents, provided they can finance their initial money-losing growth. Our model can be used to estimate the market and social value of data.
C55|Migration and the Value of Social Networks|What is the value of a social network? Prior work suggests two distinct mechanisms that have historically been difficult to differentiate: as a conduit of information, and as a source of social and economic support. We use a rich 'digital trace' dataset to link the migration decisions of millions of individuals to the topological structure of their social networks. We find that migrants systematically prefer 'interconnected' networks (where friends have common friends) to 'expansive' networks (where friends are well connected). A micro-founded model of network-based social capital helps explain this preference: migrants derive more utility from networks that are structured to facilitate social support than from networks that efficiently transmit information.
C55|The transmission channels of unconventional monetary policy: Evidence from a change in collateral requirements in France|Using a bank-firm level credit registry combined with firm-level balance sheet data we establish the presence of heterogeneity in the effects of unconventional monetary policy transmission. We examine the consequences of a loosening in the collateral eligibility requirement for credit refinancing in France. The policy was designed to affect bank lending positively. We expect a linear increase in lending and an additional increase in loans to firms with newly acceptable rating. We find a large heterogeneity of the monetary policy transmission including the unexpected reduction of lending by the banks benefiting the most from the policy. These are small, risk-averse banks whose foremost concern after the recession was to strengthen their balance sheets. Banks least affected by the policy respond with a reduction in credit to low risk borrowers in reaction to the change in the market structure. Last we document heterogenous effects of the policy on firms depending on their size.
C55|Assessing International Commonality in Macroeconomic Uncertainty and Its Effects|This paper uses a large vector autoregression to measure international macroeconomic uncertainty and its effects on major economies. We provide evidence of signi cant commonality in macroeconomic volatility, with one common factor driving strong comovement across economies and variables. We measure uncertainty and its effects with a large model in which the error volatilities feature a factor structure containing time-varying global components and idiosyncratic components. Global uncertainty contemporaneously affects both the levels and volatilities of the included variables. Our new estimates of international macroeconomic uncertainty indicate that surprise increases in uncertainty reduce output and stock prices, adversely affect labor market conditions, and in some economies lead to an easing of monetary policy.
C55|When are Google data useful to nowcast GDP? An approach via pre-selection and shrinkage|Nowcasting GDP growth is extremely useful for policy-makers to assess macroeconomic conditions in real-time. In this paper, we aim at nowcasting euro area GDP with a large database of Google search data. Our objective is to check whether this specific type of information can be useful to increase GDP nowcasting accuracy, and when, once we control for official variables. In this respect, we estimate shrunk bridge regressions that integrate Google data optimally screened through a targeting method, and we empirically show that this approach provides some gain in pseudo-real-time nowcasting of euro area GDP quarterly growth. Especially, we get that Google data bring useful information for GDP nowcasting for the four first weeks of the quarter when macroeconomic information is lacking. However, as soon as official data become available, their relative nowcasting power vanishes. In addition, a true real-time analysis confirms that Google data constitute a reliable alternative when official data are lacking.
C55|Boosting the Hodrick-Prescott Filter|The Hodrick-Prescott (HP) filter is one of the most widely used econometric methods in applied macroeconomic research. The technique is nonparametric and seeks to decompose a time series into a trend and a cyclical component unaided by economic theory or prior trend specification. Like all nonparametric methods, the HP filter depends critically on a tuning parameter that controls the degree of smoothing. Yet in contrast to modern nonparametric methods and applied work with these procedures, empirical practice with the HP filter almost universally relies on standard settings for the tuning parameter that have been suggested largely by experimentation with macroeconomic data and heuristic reasoning about the form of economic cycles and trends. As recent research has shown, standard settings may not be adequate in removing trends, particularly stochastic trends, in economic data. This paper proposes an easy-to-implement practical procedure of iterating the HP smoother that is intended to make the filter a smarter smoothing device for trend estimation and trend elimination. We call this iterated HP technique the boosted HP filter in view of its connection to L2-boosting in machine learning. The paper develops limit theory to show that the boosted HP filter asymptotically recovers trend mechanisms that involve unit root processes, deterministic polynomial drifts, and polynomial drifts with structural breaks -- the most common trends that appear in macroeconomic data and current modeling methodology. A stopping criterion is used to automate the iterative HP algorithm, making it a data-determined method that is ready for modern data-rich environments in economic research. The methodology is illustrated using three real data examples that highlight the differences between simple HP filtering, the data-determined boosted filter, and an alternative autoregressive approach.
C55|Battling antibiotic resistance: can machine learning improve prescribing?|Antibiotic resistance constitutes a major health threat. Predicting bacterial causes of infections is key to reducing antibiotic misuse, a leading driver of antibiotic resistance. We train a machine learning algorithm on administrative and microbiological laboratory data from Denmark to predict diagnostic test outcomes for urinary tract infections. Based on predictions, we develop policies to improve prescribing in primary care, highlighting the relevance of physician expertise and policy implementation when patient distributions vary over time. The proposed policies delay antibiotic prescriptions for some patients until test results are known and give them instantly to others. We find that machine learning can reduce antibiotic use by 7.42 percent without reducing the number of treated bacterial infections. As Denmark is one of the most conservative countries in terms of antibiotic use, this result is likely to be a lower bound of what can be achieved elsewhere.
C55|Analyzing Credit Risk Transmission to the Non-Financial Sector in Europe: A Network Approach|A high-dimensional network of European CDS spreads is modeled to assess the transmission of credit risk to the non-financial corporate sector in Europe. We build on a network connectedness approach that uses variance decompositions in vector autoregressions (VARs) to characterize the dependence structure in the panel of CDS spreads. Our main findings suggest a sectoral clustering in the CDS network, where financial institutions are located in the center of the network and non-financial as well as sovereign CDS are grouped around the financial center. The network has a geographical component re flected in differences in the magnitude and direction of real-sector risk transmission across European countries. We identify an increase in the transmission of financial and sovereign credit risk to the non-financial sector during the global financial crisis and the European debt crisis. By contrast, we find that the transmission of risk within the non-financial sector remains largely unaffected by crisis events.
C55|Asymmetric conjugate priors for large Bayesian VARs|Large Bayesian VARs are now widely used in empirical macroeconomics. One popular shrinkage prior in this setting is the natural conjugate prior as it facilitates posterior simulation and leads to a range of useful analytical results. This is, however, at the expense of modelling exibility, as it rules out cross-variable shrinkage – i.e. shrinking coefficients on lags of other variables more aggressively than those on own lags. We develop a prior that has the best of both worlds: it can accommodate cross-variable shrinkage, while maintaining many useful analytical results, such as a closed-form expression of the marginal likelihood. This new prior also leads to fast posterior simulation - for a BVAR with 100 variables and 4 lags, obtaining 10,000 posterior draws takes less than half a minute on a standard desktop. In a forecasting exercise, we show that a data-driven asymmetric prior outperforms two useful benchmarks: a data-driven symmetric prior and a subjective asymmetric prior.
C55|Minnesota-type adaptive hierarchical priors for large Bayesian VARs|Large Bayesian VARs with stochastic volatility are increasingly used in empirical macroeconomics. The key to make these highly parameterized VARs useful is the use of shrinkage priors. We develop a family of priors that captures the best features of two prominent classes of shrinkage priors: adaptive hierarchical priors and Minnesota priors. Like the adaptive hierarchical priors, these new priors ensure that only ‘small’ coefficients are strongly shrunk to zero, while ‘large’ coefficients remain intact. At the same time, these new priors can also incorporate many useful features of the Minnesota priors, such as cross-variable shrinkage and shrinking coefficients on higher lags more aggressively. We introduce a fast posterior sampler to estimate BVARs with this family of priors - for a BVAR with 25 variables and 4 lags, obtaining 10,000 posterior draws takes about 3 minutes on a standard desktop. In a forecasting exercise, we show that these new priors outperform both adaptive hierarchical priors and Minnesota priors.
C55|Causal Tree Estimation of Heterogeneous Household Response to Time-Of-Use Electricity Pricing Schemes|No abstract is available for this item.
C55|Internal Migration in the United States: A Comprehensive Comparative Assessment of the Consumer Credit Panel|We introduce and provide the first comprehensive comparative assessment of the Federal Reserve Bank of New York/Equifax Consumer Credit Panel (CCP) as a valuable and underutilized data set for studying internal migration within the United States. Relative to other data sources on US internal migration, the CCP permits highly detailed cross-sectional and longitudinal analyses of migration, both temporally and geographically. We compare cross-sectional and longitudinal estimates of migration from the CCP to similar estimates derived from the American Community Survey, the Current Population Survey, Internal Revenue Service data, the National Longitudinal Survey of Youth, the Panel Study of Income Dynamics, and the Survey of Income and Program Participation. Our results establish the comparative utility and illustrate some of the unique advantages of the CCP relative to other data sources on US internal migration. We conclude by identifying some profitable directions for future research on US internal migration using the CCP, as well as reminding readers of the strengths and limitations of these data. More broadly, this paper contributes to discussions and debates on improving the availability, quality, and comparability of migration data.
C55|Improving the Accuracy of Economic Measurement with Multiple Data Sources: The Case of Payroll Employment Data|"This paper combines information from two sources of U.S. private payroll employment to increase the accuracy of real-time measurement of the labor market. The sources are the Current Employment Statistics (CES) from BLS and microdata from the payroll processing firm ADP. We briefly describe the ADP-derived data series, compare it to the BLS data, and describe an exercise that benchmarks the data series to an employment census. The CES and the ADP employment data are each derived from roughly equal-sized samples. We argue that combining CES and ADP data series reduces the measurement error inherent in both data sources. In particular, we infer ""true"" unobserved payroll employment growth using a state-space model and find that the optimal predictor of the unobserved state puts approximately equal weight on the CES and ADP-derived series. Moreover, the estimated state contains information about future readings of payroll employment."
C55|A Generalized Factor Model with Local Factors|I extend the theory on factor models by incorporating âlocalâ factors into the model. Local factors affect a decreasing fraction of the observed variables. This implies a continuum of eigenvalues of the covariance matrix, as is commonly observed in applications. I derive conditions under which local factors will be estimated consistently using the common Principal Component Estimator. I further propose a novel class of estimators for the number of factors. Unlike estimators that have been proposed in the past, my estimators use information in the eigenvectors as well as in the eigenvalues. Monte Carlo evidence suggests significant finite sample gains over existing estimators. Empirically I find evidence of local factors in a large panel of US macroeconomic indicators.
C55|Dynamic Factor Models|Dynamic factor models are parsimonious representations of relationships among time series variables. With the surge in data availability, they have proven to be indispensable in macroeconomic forecasting. This chapter surveys the evolution of these models from their pre-big-data origins to the large-scale models of recent years. We review the associated estimation theory, forecasting approaches, and several extensions of the basic framework.
C55|Subsampling Sequential Monte Carlo for Static Bayesian Models|We show how to speed up Sequential Monte Carlo (SMC) for Bayesian inference in large data problems by data subsampling. SMC sequentially updates a cloud of particles through a sequence of distributions, beginning with a distribution that is easy to sample from such as the prior and ending with the posterior distribution. Each update of the particle cloud consists of three steps: reweighting, resampling, and moving. In the move step, each particle is moved using a Markov kernel and this is typically the most computation- ally expensive part, particularly when the dataset is large. It is crucial to have an efficient move step to ensure particle diversity. Our article makes two important contributions. First, in order to speed up the SMC computation, we use an approximately unbiased and efficient annealed likelihood estimator based on data subsampling. The subsampling approach is more memory effi- cient than the corresponding full data SMC, which is an advantage for parallel computation. Second, we use a Metropolis within Gibbs kernel with two con- ditional updates. A Hamiltonian Monte Carlo update makes distant moves for the model parameters, and a block pseudo-marginal proposal is used for the particles corresponding to the auxiliary variables for the data subsampling. We demonstrate the usefulness of the methodology for estimating three gen- eralized linear models and a generalized additive model with large datasets.
C55|On the use of Hedonic Regression Models to Measure the Effect of Energy Efficiency on Residential Property Transaction Prices: Evidence for Portugal and Selected Data Issues|Using a unique dataset containing information of around 256 thousand residential property sales, this paper discloses a clear sales premium for most energy efficient dwellings, which is more pronounced for apartments (13%) than for houses (5 to 6%). Cross-country comparisons support the finding that energy efficiency price premiums are higher in the Portuguese residential market than in central and northern European markets. Results emphasize the relevance of data issues in hedonic regression models. They illustrate how the use of appraisal prices, explanatory variables with measurement errors, and the omission of variables associated with the quality of the properties, may seriously bias energy efficiency partial effect estimates. These findings provide valuable information not only to policy-makers, but also to researchers interested in this area.
C55|lassopack: Model selection and prediction with regularized regression in Stata|This article introduces lassopack, a suite of programs for regularized regression in Stata. lassopack implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso and post-estimation OLS. The methods are suitable for the high-dimensional setting where the number of predictors $p$ may be large and possibly greater than the number of observations, $n$. We offer three different approaches for selecting the penalization (`tuning') parameters: information criteria (implemented in lasso2), $K$-fold cross-validation and $h$-step ahead rolling cross-validation for cross-section, panel and time-series data (cvlasso), and theory-driven (`rigorous') penalization for the lasso and square-root lasso for cross-section and panel data (rlasso). We discuss the theoretical framework and practical considerations for each approach. We also present Monte Carlo results to compare the performance of the penalization approaches.
C55|Marginal Jobs and Job Surplus: A Test of the Efficiency of Separations|"We present a sharp test for the efficiency of job separations. First, we document a dramatic increase in the separation rate - 11.2ppt (28%) over five years - in response to a quasi-experimental extension of UI benefit duration for older workers. Second, after the abolition of the policy, the ""job survivors"" in the formerly treated group exhibit exactly the same separation behavior as the control group. Juxta-posed, these facts reject the ""Coasean"" prediction of efficient separations, whereby the UI extensions should have extracted marginal (low-surplus) jobs and thereby rendered the remaining (high-surplus) jobs more resilient after its abolition. Third, we show that a formal model of predicted efficient separations implies a piece-wise linear function of the actual control group separations beyond the missing mass of marginal matches. A structural estimation reveals point estimates of the share of efficient separations below 4%, with confidence intervals rejecting shares above 13%. Fourth, to characterize the marginal jobs in the data, we extend complier analysis to difference-indifference settings such as ours. The UI-indiced separators stemmed from declining firms, blue-collar jobs, with a high share of sick older workers, and firms more likely to have works councils - while their wages were similar to program survivors. The evidence is consistent with a ""non-Coasean"" framework building on wage frictions preventing efficient bargaining, and with formal or informal institutional constraints on selective separations."
C55|Opting out of Workers' Compensation: Non-Subscription in Texas and Its Effects|Texas is the only state that does not mandate that employers carry workers' compensation insurance (WC) coverage. We employ a quasi-experimental design paired with a novel machine learning approach to examine the effects of switching from traditional workers' compensation to a so-called non-subscription program in Texas. Specifically, we compare before and after effects of switching to non-subscription for employees in Texas to contemporaneously measured before and after differences for non-Texas-based employees. Importantly, we study large self-insured companies operating the same business in multiple states in the US; hence the non-Texas operations represent the control sites for the Texas treatment sites. The resulting difference-in-differences estimation technique allows us to control for any companywide factors that might be confounded with switching to non-subscription. Our empirical approach also controls for injury characteristics, employment characteristics, industry, and individual characteristics such as gender, age, number of dependents, and marital status. Outcomes include number of claims reported, medical expenditures, indemnity payments, time to return to work, likelihood of having permanent disability, likelihood of claim denial, and likelihood of litigation. The data include 25 switcher companies between the years 2004 and 2016, yielding 846,376 injury incidents. Regression findings suggest that indemnity, medical payments, and work-loss fall substantially. Claim denials increase and litigation falls.
C55|Immigration and Work-Related Injuries: Evidence from Italian Administrative Data|There is growing evidence that foreign-born workers are over represented in physically demanding and dangerous jobs with relatively higher injury hazard rates. Given this pattern, do increasing inflows of foreign-born workers alleviate native workers' exposure to injuries? This paper provides evidence of the effects of immigration on the incidence and severity of workrelated accidents. We combine administrative data on work-place accidents in Italy with the Labour Force Survey from 2009 to 2017. Our approach exploits spatial and temporal variation in the distribution of foreign-born residents across provinces. Using province fixed-effects and an instrumental variable specification based on historical settlements of immigrants, we show that inflows of foreign-born residents drive reductions in the injury rate, paid sick leave, and severity of impairment for natives. Next, we investigate potential underlying mechanisms that could drive this effect, such as increased unemployment and selection of the workforce, and the sorting of native workers into less physically demanding jobs. Our results rule out that decreased injuries are driven by higher native unemployment. We find that employment rates are positively associated with immigration, in particular for workers with higher education. While not statistically significant at conventional levels, we also find that average occupational physical intensity for natives is lower in provinces that receive larger foreign-born inflows.
C55|Determinants of Productivity Gap in the European Union: A Multilevel Perspective|The paper explores the determinants of productivity gap within the European Union in four industrial manufacturing sectors (computers, chemicals, basic metals and food) of strong macroeconomic significance and varied 'Research and Development' (R&D) intensity. Our analysis reveals that some of the most important factors determining productivity gap across the EU are related to technology gap variables - R&D intensity and R&D embedded in purchased equipment and machinery - and how they interact. While the signs for both R&D and embedded R&D are as expected and our results emphasise the relevance of technology for closing the productivity gap, this is not the case with the interaction between these two variables. The estimates for the interaction terms are indeed very significant and consistently negative in three out of four sectors. This negative relationship suggests that there is no complementarity between these two modes of technology acquisition - R&D and embedded R&D investments - which are however each separately crucial for catching up. In policy terms, this situation suggests that there is a lack of coordination between R&D policy and technology transfer (FDI, trade and industrial policy). Given that, our results also show a widening productivity gap between the countries of the EU periphery (South and East) and the rest of the sample.
C55|Robust measures of skewness and kurtosis for macroeconomic and financial time series|The sample skewness and kurtosis of macroeconomic and financial time series are routinely scrutinized in the early stages of model-building and are often the central topic of studies in economics and finance. Notwithstanding the availability of several robust estimators, most scholars in economics rely on method-of-moments estimation that is known to be very sensitive to outliers. We carry out an extensive Monte Carlo analysis to compare the bias and root mean squared error of twelve different estimators of skewness and kurtosis. We consider nine statistical distributions that approximate the range of data generating processes of many macroeconomic and financial time series. Both in independently and identically distributed samples and in data generating processes featuring serial correlation L-moments and trimmed L-moments estimators are particularly resistant to outliers and deliver the lowest root mean squared error. The application to 128 macroeconomic and financial time series sourced from a large, monthly frequency, database (i.e. the FRED-MD of McCracken and Ng, 2016) confirms the findings of the simulation study.
C55|Food inflation nowcasting with web scraped data|In this paper we evaluate the ability of web scraped data to improve nowcasts of Polish food inflation. The nowcasting performance of online price indices is compared with aggregated and disaggregated benchmark models in a pseudo realtime experiment. We also explore product selection and classification problems, their importance in constructing web price indices and other limitations of online datasets. Therefore, we experiment not only with raw indices, but also with several approaches to include them into model-based forecasts. Our findings indicate that the optimal way to incorporate web scraped data into regular forecasting is to include them in simple distributed-lag models at the lowest aggregation level, combine the forecasts and aggregate them using statistical office methodology. We find this approach superior to other benchmark models which do not take online information into account.
C55|Does Scientific Progress Affect Culture? A Digital Text Analysis|We study the interplay between scientific progress and culture through text analysis on a corpus of about eight million books, with the use of techniques and algorithms from machine learning. We focus on a specific scientific breakthrough, the theory of evolution through natural selection by Charles Darwin, and examine the diffusion of certain key concepts that characterized this theory in the broader cultural discourse and social imaginary. We find that some concepts in Darwin’s theory, such as Evolution, Survival, Natural Selection and Competition diffused in the cultural discourse immediately after the publication of On the Origins of Species. Other concepts such as Selection and Adaptation were already present in the cultural dialogue. Moreover, we document semantic changes for most of these concepts over time. Our findings thus show a complex relation between two key factors of long-term economic growth – science and culture. Considering the evolution of these two factors jointly can offer new insights to the study of the determinants of economic development, and machine learning is a promising tool to explore these relationships.
C55|Taming the Factor Zoo: A Test of New Factors|We propose a model-selection method to systematically evaluate the contribution to asset pricing of any new factor, above and beyond what a high-dimensional set of existing factors explains. Our methodology explicitly accounts for potential model-selection mistakes, unlike the standard approaches that assume perfect variable selection, which rarely occurs in practice and produces a bias due to the omitted variables. We apply our procedure to a set of factors recently discovered in the literature. While most of these new factors are found to be redundant relative to the existing factors, a few — such as profitability — have statistically significant explanatory power beyond the hundreds of factors proposed in the past. In addition, we show that our estimates and their significance are stable, whereas the model selected by simple LASSO is not.
C55|Transforming Naturally Occurring Text Data into Economic Statistics: The Case of Online Job Vacancy Postings|Using a dataset of 15 million UK job adverts from a recruitment website, we construct new economic statistics measuring labour market demand. These data are ‘naturally occurring’, having originally been posted online by firms. They offer information on two dimensions of vacancies—region and occupation—that firm-based surveys do not usually, and cannot easily, collect. These data do not come with official classification labels so we develop an algorithm which maps the free form text of job descriptions into standard occupational classification codes. The created vacancy statistics give a plausible, granular picture of UK labour demand and permit the analysis of Beveridge curves and mismatch unemployment at the occupational level.<br><small>(This abstract was borrowed from another version of this item.)</small>
C55|Simplicity Creates Inequity: Implications for Fairness, Stereotypes, and Interpretability|Algorithms are increasingly used to aid, or in some cases supplant, human decision-making, particularly for decisions that hinge on predictions. As a result, two additional features in addition to prediction quality have generated interest: (i) to facilitate human interaction and understanding with these algorithms, we desire prediction functions that are in some fashion simple or interpretable; and (ii) because they influence consequential decisions, we also want them to produce equitable allocations. We develop a formal model to explore the relationship between the demands of simplicity and equity. Although the two concepts appear to be motivated by qualitatively distinct goals, we show a fundamental inconsistency between them. Specifically, we formalize a general framework for producing simple prediction functions, and in this framework we establish two basic results. First, every simple prediction function is strictly improvable: there exists a more complex prediction function that is both strictly more efficient and also strictly more equitable. Put another way, using a simple prediction function both reduces utility for disadvantaged groups and reduces overall welfare relative to other options. Second, we show that simple prediction functions necessarily create incentives to use information about individuals' membership in a disadvantaged group—incentives that weren't present before simplification, and that work against these individuals. Thus, simplicity transforms disadvantage into bias against the disadvantaged group. Our results are not only about algorithms but about any process that produces simple models, and as such they connect to the psychology of stereotypes and to an earlier economics literature on statistical discrimination.
C55|A Machine Learning Analysis of Seasonal and Cyclical Sales in Weekly Scanner Data|This paper analyzes weekly scanner data collected for 108 groups at the county level between 2006 and 2014. The data display multi-dimensional weekly seasonal effects that are not exactly periodic but are cross-sectionally dependent. Existing univariate procedures are imperfect and yield adjusted series that continue to display strong seasonality upon aggregation. We suggest augmenting the univariate adjustments with a panel data step that pools information across counties. Machine learning tools are then used to remove the within-year seasonal variations. A demand analysis of the adjusted budget shares finds three factors: one that is trending, and two cyclical ones that are well aligned with the level and change in consumer confidence. The effects of the Great Recession vary across locations and product groups, with consumers substituting towards home cooking away from non-essential goods. The adjusted data also reveal changes in spending to unanticipated shocks at the local level. The data are thus informative about both local and aggregate economic conditions once the seasonal effects are removed. The two-step methodology can be adapted to remove other types of nuisance variations provided that these variations are cross-sectionally dependent.<br><small>(This abstract was borrowed from another version of this item.)</small>
C55|Improving the Accuracy of Economic Measurement with Multiple Data Sources: The Case of Payroll Employment Data|This paper combines information from two sources of U.S. private payroll employment to increase the accuracy of real-time measurement of the labor market. The sources are the Current Employment Statistics (CES) from BLS and microdata from the payroll processing firm ADP. We briefly describe the ADP-derived data series, compare it to the BLS data, and describe an exercise that benchmarks the data series to an employment census. The CES and the ADP employment data are each derived from roughly equal-sized samples. We argue that combining CES and ADP data series reduces the measurement error inherent in both data sources. In particular, we infer “true” unobserved payroll employment growth using a state-space model and find that the optimal predictor of the unobserved state puts approximately equal weight on the CES and ADP-derived series. Moreover, the estimated state contains information about future readings of payroll employment.<br><small>(This abstract was borrowed from another version of this item.)</small>
C55|Predicting Consumer Default: A Deep Learning Approach|We develop a model to predict consumer default based on deep learning. We show that the model consistently outperforms standard credit scoring models, even though it uses the same data. Our model is interpretable and is able to provide a score to a larger class of borrowers relative to standard credit scoring models while accurately tracking variations in systemic risk. We argue that these properties can provide valuable insights for the design of policies targeted at reducing consumer default and alleviating its burden on borrowers and lenders, as well as macroprudential regulation.
C55|Who is Tested for Heart Attack and Who Should Be: Predicting Patient Risk and Physician Error|In deciding whether to test for heart attack (acute coronary syndromes), physicians implicitly judge risk. To assess these decisions, we produce explicit risk predictions by applying machine learning to Medicare claims data. Comparing these on a patient-by-patient basis to physician decisions reveals more about low-value care than the usual approach of measuring average testing results. It more precisely quantifies over-use: while the average test is marginally cost-effective, tests at the bottom of the risk distribution are highly cost-ineffective. But it also reveals under- use: many patients at the top of the risk distribution go untested; and they go on to have frequent adverse cardiac events, including death, in the next 30 days. At standard clinical thresholds, these event rates suggest they should have been tested. In aggregate, 42.8% of the potential welfare gains of improving testing would come from addressing under-use. Existing policies though are too blunt: when testing is reduced, for example, both low-value and high-value tests fall. Finally, to understand physician error we build a separate algorithm of the physician and find evidence of bounded rationality as well as biases such as representativeness. We suggest models of physician moral hazard should be expanded to include ‘behavioral hazard’.
C55|Predicting Returns With Text Data|We introduce a new text-mining methodology that extracts sentiment information from news articles to predict asset returns. Unlike more common sentiment scores used for stock return prediction (e.g., those sold by commercial vendors or built with dictionary-based methods), our supervised learning framework constructs a sentiment score that is specifically adapted to the problem of return prediction. Our method proceeds in three steps: 1) isolating a list of sentiment terms via predictive screening, 2) assigning sentiment weights to these words via topic modeling, and 3) aggregating terms into an article-level sentiment score via penalized likelihood. We derive theoretical guarantees on the accuracy of estimates from our model with minimal assumptions. In our empirical analysis, we text-mine one of the most actively monitored streams of news articles in the financial system—the Dow Jones Newswires—and show that our supervised sentiment model excels at extracting return-predictive signals in this context.
C55|From Transactions Data to Economic Statistics: Constructing Real-Time, High-Frequency, Geographic Measures of Consumer Spending|Access to timely information on consumer spending is important to economic policymakers. The Census Bureau’s monthly retail trade survey is a primary source for monitoring consumer spending nationally, but it is not well suited to study localized or short-lived economic shocks. Moreover, lags in the publication of the Census estimates and subsequent, sometimes large, revisions diminish its usefulness for real-time analysis. Expanding the Census survey to include higher frequencies and subnational detail would be costly and would add substantially to respondent burden. We take an alternative approach to fill these information gaps. Using anonymized transactions data from a large electronic payments technology company, we create daily estimates of retail spending at detailed geographies. Our daily estimates are available only a few days after the transactions occur, and the historical time series are available from 2010 to the present. When aggregated to the national level, the pattern of monthly growth rates is similar to the official Census statistics. We discuss two applications of these new data for economic analysis: First, we describe how our monthly spending estimates are useful for real-time monitoring of aggregate spending, especially during the government shutdown in 2019, when Census data were delayed and concerns about the economy spiked. Second, we show how the geographic detail allowed us quantify in real time the spending effects of Hurricanes Harvey and Irma in 2017.<br><small>(This abstract was borrowed from another version of this item.)</small>
C55|Are small farms really more productive than large farms?|We revisit the long-standing empirical evidence of an inverse relationship between farm size and productivity using rich microdata from Uganda. We show that farm size is negatively related to yields (output per hectare), as commonly found in the literature, but positively related to farm productivity (a farm-specific component of total factor productivity). These conflicting results do not arise because of omitted variables such as land quality, measurement error in output or inputs, or specification issues. Instead, we reconcile the findings emphasizing the role of farm-specific distortions and returns to scale in traditional farm production. We exploit unique regional variation in land tenure regimes in Uganda in evaluating the role of farm-specific distortions. Our findings point to the limited value of yields (or land productivity) in establishing the farm size-productivity relationship. More generally, we demonstrate the limitation of using farm size in guiding policy applications.
C55|The potential of tax microdata for tax policy|This paper explores one distinctive form of the ‘big data’ of economics – individual tax record microdata – and its potential for tax policy analysis. The paper draws on OECD collaborations with Slovenia and Ireland in 2018 where tax microdata was used.Most empirical economics is based on survey data. However, the current trend of low and falling response rates has placed a question mark over the future value of survey practice generally. By contrast, this paper discusses the increasing use of tax microdata in economic research and the new types of policy analysis made possible by it. In the future, best-practice tax policy analysis is likely to combine tax microdata with survey and national account data. The advantages of these combined data will be important for policymakers to understand and address future policy challenges including protecting tax revenues in an era of population ageing and supporting fairness given the changing nature of economic mobility.
C55|Linking Aid to the Sustainable Development Goals – a machine learning approach|Official Development Assistance amounted USD 146.6 billions in 2017 but do we know how much of this aid contributed to the Sustainable Development Goals (SDGs)? And to what SDG in particular? This paper present a new methodology using machine learning designed to link project-based flows to the Sustainable Development Goals. It provide first estimates of DAC and non-DAC donors’ aid contribution for the goal and show that similar analysis can be done at the recipient level and for other type of textual database such as private sector reports; opening wide array for policy analysis.The methodology presented in this working paper uses semantic analysis of the text description of each project present in the Creditor Reporting System (CRS).
C55|Monotonic Estimation for the Survival Probability over a Risk-Rated Portfolio by Discrete-Time Hazard Rate Models|Monotonic estimation for the survival probability of a loan in a risk-rated portfolio is based on the observation arising, for example, from loan pricing that a loan with a lower credit risk rating is more likely to survive than a loan with a higher credit risk rating, given the same additional risk covariates. Two probit-type discrete-time hazard rate models that generate monotonic survival probabilities are proposed in this paper. The first model calculates the discrete-time hazard rate conditional on systematic risk factors. As for the Cox proportion hazard rate model, the model formulates the discrete-time hazard rate by including a baseline component. This baseline component can be estimated outside the model in the absence of model covariates using the long-run average discrete-time hazard rate. This results in a significant reduction in the number of parameters to be otherwise estimated inside the model. The second model is a general form model where loan level factors can be included. Parameter estimation algorithms are also proposed. The models and algorithms proposed in this paper can be used for loan pricing, stress testing, expected credit loss estimation, and modeling of the probability of default term structure.
C55|Monotonic Estimation for Probability Distribution and Multivariate Risk Scales by Constrained Minimum Generalized Cross-Entropy|Minimum cross-entropy estimation is an extension to the maximum likelihood estimation for multinomial probabilities. Given a probability distribution {r_i }_(i=1)^k, we show in this paper that the monotonic estimates {p_i }_(i=1)^k for the probability distribution by minimum cross-entropy are each given by the simple average of the given distribution values over some consecutive indexes. Results extend to the monotonic estimation for multivariate outcomes by generalized cross-entropy. These estimates are the exact solution for the corresponding constrained optimization and coincide with the monotonic estimates by least squares. A non-parametric algorithm for the exact solution is proposed. The algorithm is compared to the “pool adjacent violators” algorithm in least squares case for the isotonic regression problem. Applications to monotonic estimation of migration matrices and risk scales for multivariate outcomes are discussed.
C55|Short-term forecasting of the US unemployment rate|This paper aims to assess whether Google search data is useful when predicting the US unemployment rate among other more traditional predictor variables. A weekly Google index is derived from the keyword “unemployment” and is used in diffusion index variants along with the weekly number of initial claims and monthly estimated latent factors. The unemployment rate forecasts are generated using MIDAS regression models that take into account the actual frequencies of the predictor variables. The forecasts are made in real-time and the forecasts of the best forecasting models exceed, for the most part, the root mean squared forecast error of two benchmarks. However, as the forecasting horizon increases, the forecasting performance of the best diffusion index variants decreases over time, which suggests that the forecasting methods proposed in this paper are most useful in the short-term.
C55|Are attitudes towards immigration changing in Europe? An analysis based on bidimensional latent class IRT models|We analyse the changing attitudes towards immigration in EU host countries in the last few years (2010-2016) on the basis of the European Social Survey data. These data are collected by the administration of a questionnaire made of items concerning different aspects related to the immigration phenomenon. For this analysis we rely on a class of item response theory models that allow for: (i) multidimensionality; (ii) discreteness of the latent trait distribution; (iii) time-constant and time-varying covariates; and (iv) sample weights. Through these models we find latent classes of Europeans with similar levels of immigration acceptance, we study the effect of different socio-economic covariates on the probability of belonging to these classes, and we assess the item characteristics. In this way we show which countries tend to be more or less positive towards immigration and the temporal dynamics of the phenomenon under study.
C55|Nowcasting US GDP with artificial neural networks|We use a machine learning approach to forecast the US GDP value of the current quarter and several quarters ahead. Within each quarter, the contemporaneous value of GDP growth is unavailable but can be estimated using higher-frequency variables that are published in a more timely manner. Using the monthly FRED-MD database, we compare the feedforward artificial neural network forecasts of GDP growth to forecasts of state of the art dynamic factor models and the Survey of Professional Forecasters, and we evaluate the relative performance. The results indicate that the neural network outperforms the dynamic factor model in terms of now- and forecasting, while it generates at least as good now- and forecasts as the Survey of Professional Forecasters.
C55|From Twitter to GDP: Estimating Economic Activity From Social Media|Using all geo-located image tweets shared on Twitter in 2012-2013, I find that the volume of tweets is a valid proxy for estimating current GDP in USD at the country level. Residuals from my preferred model are negatively correlated to a data quality index, indicating that my estimates of GDP are more accurate for countries with more reliable GDP data. Comparing Twitter with more commonly-used proxy of night-light data, I find that variation in Twitter activity explains slightly more of the cross-country variance in GDP. I also exploit the continuous time and geographic granularity of social media posts to create monthly and weekly estimates of GDP for the US, as well as sub- national estimates, including those economic areas that span national borders. My findings suggest that Twitter can be used to measure economic activity in a more timely and more spatially disaggregate way than conventional data and that governments’ statistical agencies could incorporate social media data to complement and further reduce measurement error in their official GDP estimates.
C55|Forecasting with many predictors using message passing algorithms|Machine learning methods are becoming increasingly popular in economics, due to the increased availability of large datasets. In this paper I evaluate a recently proposed algorithm called Generalized Approximate Message Passing (GAMP) , which has been very popular in signal processing and compressive sensing. I show how this algorithm can be combined with Bayesian hierarchical shrinkage priors typically used in economic forecasting, resulting in computationally efficient schemes for estimating high-dimensional regression models. Using Monte Carlo simulations I establish that in certain scenarios GAMP can achieve estimation accuracy comparable to traditional Markov chain Monte Carlo methods, at a tiny fraction of the computing time. In a forecasting exercise involving a large set of orthogonal macroeconomic predictors, I show that Bayesian shrinkage estimators based on GAMP perform very well compared to a large set of alternatives.
C55|Local Currency Bond Risk Premia of Emerging Markets: The Role of Local and Global Factors|This paper investigates the sources of variation in emerging market (EM) local currency bond risk premium. Empirical results suggest that both global and local factors contain valuable information in explaining the local currency bond excess returns. We show that economic policy uncertainty causes the excess bond returns to increase while positive innovations in the term spread, CP factor and implied FX volatility have downward impacts on the excess returns. Besides, the high level of spillover from developed markets to EMs may confine the diversification benefits from holding EM local currency bonds.
C55|Forecasting Local Currency Bond Risk Premia of Emerging Markets: The Role of Cross-Country Macro-Financial Linkages|In this paper, we forecast local currency debt of five major emerging market countries (Brazil, Indonesia, Mexico, South Africa, and Turkey) over the period of January 2010 to January 2019 (with an in-sample: March 2005 to December 2018). We exploit information from a large set of economic and financial time series to assess the importance of not only “own-country” factors (derived from principal component and partial least squares approach), but also create “global” predictors by combining the country-specific variables across the five emerging economies. We find that while information on own-country factors can outperform the historical average model, global factors tend to produce not only greater statistical and economic gains, but also enhances market timing ability of investors, especially when we use the target-variable (bond premium) approach under the partial least squares method to extract our factors. Our results have important implications for not only fund managers, but also policymakers.
C55|High-dimensional macroeconomic forecasting using message passing algorithms|This paper proposes two distinct contributions to econometric analysis of large information sets and structural instabilities. First, it treats a regression model with time-varying coeﬃcients, stochastic volatility and exogenous predictors, as an equivalent high-dimensional static regression problem with thousands of covariates. Inference in this speciﬁcation proceeds using Bayesian hierarchical priors that shrink the high-dimensional vector of coeﬃcients either towards zero or time-invariance. Second, it introduces the frameworks of factor graphs and message passing as a means of designing eﬃcient Bayesian estimation algorithms. In particular, a Generalized Approximate Message Passing (GAMP) algorithm is derived that has low algorithmic complexity and is trivially parallelizable. The result is a comprehensive methodology that can be used to estimate time-varying parameter regressions with arbitrarily large number of exogenous predictors. In a forecasting exercise for U.S. price inﬂation this methodology is shown to work very well.
C55|Measuring international uncertainty using global vector autoregressions with drifting parameters|This paper investigates the time-varying impacts of international macroeconomic uncertainty shocks. We use a global vector autoregressive (GVAR) specification with drifting coefficients and factor stochastic volatility in the errors to model six economies jointly. The measure of uncertainty is constructed endogenously by estimating a scalar driving the innovation variances of the latent factors, and is included also in the mean of the process. To achieve regularization, we use Bayesian techniques for estimation, and introduce a set of hierarchical global local shrinkage priors. The adopted priors center the model on a constant parameter specification with homoscedastic errors, but allow for time-variation if suggested by likelihood information. Moreover, we assume coefficients across economies to be similar, but provide sufficient flexibility via the hierarchical prior for country-specific idiosyncrasies. The results point towards pronounced real and financial effects of uncertainty shocks in all countries, with differences across economies and over time.
C55|On Factor Models with Random Missing: EM Estimation, Inference, and Cross Validation|We consider the estimation and inference in approximate factor models with random missing values. We show that with the low rank structure of the common component, we can estimate the factors and factor loadings consistently with the missing values replaced by zeros. We establish the asymptotic distributions of the resulting estimators and those based on the EM algorithm. We also propose a cross validation-based method to determine the number of factors in factor models with or without missing values and justify its consistency. Simulations demonstrate that our cross validation method is robust to fat tails in the error distribution and significantly outperforms some existing popular methods in terms of correct percentage in determining the number of factors. An application to the factor-augmented regression models shows that a proper treatment of the missing values can improve the out-of-sample forecast of some macroeconomic variables.
C55|A Horse Race in High Dimensional Space|In this paper, we study the predictive power of dense and sparse estimators in a high dimensional space. We propose a new forecasting method, called Elastically Weighted Principal Components Analysis (EWPCA) that selects the variables, with respect to the target variable, taking into account the collinearity among the data using the Elastic Net soft thresholding. Then, we weight the selected predictors using the Elastic Net regression coefficient, and we finally apply the principal component analysis to the new “elastically” weighted data matrix. We compare this method to common benchmark and other methods to forecast macroeconomic variables in a data-rich environment, dived into dense representation, such as Dynamic Factor Models and Ridge regressions and sparse representations, such as LASSO regression. All these models are adapted to take into account the linear dependency of the macroeconomic time series. Moreover, to estimate the hyperparameters of these models, including the EWPCA, we propose a new procedure called “brute force”. This method allows us to treat all the hyperparameters of the model uniformly and to take the longitudinal feature of the time-series data into account. Our findings can be summarized as follows. First, the “brute force” method to estimate the hyperparameters is more stable and gives better forecasting performances, in terms of MSFE, than the traditional criteria used in the literature to tune the hyperparameters. This result holds for all samples sizes and forecasting horizons. Secondly, our two-step forecasting procedure enhances the forecasts’ interpretability. Lastly, the EWPCA leads to better forecasting performances, in terms of mean square forecast error (MSFE), than the other sparse and dense methods or naïve benchmark, at different forecasts horizons and sample sizes.
C55|Quantification of feedback effects in FX options markets|We model the feedback effect of delta hedging for the spot market volatility of the forex market (dollar-yen and dollar-euro) using an economy of two types of traders, an option market maker (OMM) and an option market taker (OMT), whose exposures reflect the total outstanding positions of all option traders in the market. A different hedge ratio of the OMM and OMT leads to a net delta hedge activity that introduces market friction and feedback effects. This friction is represented by a simple linear permanent impact model for the net delta hedge volumes that are executed in the spot market. This approach allows us to derive the dependence of the spot market volatility on the gamma exposure of the trader that hedges a larger share of her delta exposure and on the market impact of the delta hedge transactions. We reconstruct the aggregated OMM's gamma exposure by using publicly available DTCC trade repository data and find that it is negative, as expected: the OMT usually buys options with either a view on the spot price or with the desire to hedge other positions and, thus, is net long on options. As the OMM provides liquidity as a service to the market, their position is reversed compared with the OMT. Our regressions show a high goodness of fit, a highly significant parameter for the gamma exposure of the OMM and, as expected, that the volatility is increased by the OMM's short gamma exposure. Quantitatively, a negative gamma exposure of the OMM of approximately -1000 billion USD (which is around what we observe from our reconstructed OMM data) leads to an absolute increase in volatility of 0.7% in EURUSD and 0.9% in USDJPY. If we assume that the hedge ratios in the two markets are the same, the difference can be directly explained by the higher market impact of a transaction in the USDJPY spot market compared to the EURUSD spot market, as the liquidity of the EURUSD spot market is higher than that of the USDJPY spot market. Our results are in line with and empirically confirm previous theoretical work on the feedback effect of delta hedging strategies on spot market volatility.
C55|Effectiveness of policy and regulation in European sovereign credit risk markets: A network analysis|We study the impact of changes in regulations and policy interventions on systemic risk among European sovereigns measured as volatility spillovers in respective credit risk markets. Our unique intraday CDS dataset allows for precise measurement of the effectiveness of these events in a network setting. In particular, it allows discerning interventions which entail significant changes in network cross-effects with appropriate bootstrap confidence intervals. We show that it was mainly regulatory changes with the ban of trading naked sovereign CDS in 2012 as well as the new ISDA regulations in 2014 which were most effective in reducing systemic risk. In comparison, we find that the effect of policy interventions was minor and generally not sustainable. In particular, they only had a significant impact when implemented for the first time and when targeting more than one country. For the volatility spillover channels, we generally find balanced networks with no fragmentation over time.
C55|Factor augmented VAR revisited - A sparse dynamic factor model approach|We combine the factor augmented VAR framework with recently developed estimation and identification procedures for sparse dynamic factor models. Working with a sparse hierarchical prior distribution allows us to discriminate between zero and non-zero factor loadings. The non-zero loadings identify the unobserved factors and provide a meaningful economic interpretation for them. Given that we work with a general covariance matrix of factor innovations, we can implement different strategies for structural shock identification. Applying our methodology to US macroeconomic data (FRED QD) reveals indeed a high degree of sparsity in the data. The proposed identification procedure yields seven unobserved factors that account for about 52 percent of the variation in the data. We simultaneously identify a monetary policy, a productivity and a news shock by recursive ordering and by applying the method of maximizing the forecast error variance share in a specific variable. Factors and specific variables show sensible responses to the identified shocks.
C55|Data Science for Entrepreneurship Research : Studying Demand Dynamics for Entrepreneurial Skills in the Netherlands|The recent rise of big data and artificial intelligence (AI) is changing markets, politics, organizations, and societies. It also affects the domain of research. Supported by new statistical methods that rely on computational power and computer science --- data science methods --- we are now able to analyze data sets that can be huge, multidimensional, unstructured, and are diversely sourced. In this paper, we describe the most prominent data science methods suitable for entrepreneurship research and provide links to literature and Internet resources for self-starters. We survey how data science methods have been applied in the entrepreneurship research literature. As a showcase of data science techniques, based on a dataset of 95% of all job vacancies in the Netherlands over a 6-year period with 7.7 million data points, we provide an original analysis of the demand dynamics for entrepreneurial skills in the Netherlands. We show which entrepreneurial skills are particularly important for which type of profession. Moreover, we find that demand for both entrepreneurial and digital skills has increased for managerial positions, but not for others. We also find that entrepreneurial skills were significantly more demanded than digital skills over the entire period 2012-2017 and that the absolute importance of entrepreneurial skills has even increased more than digital skills for managers, despite the impact of datafication on the labor market. We conclude that further studies of entrepreneurial skills in the general population --- outside the domain of entrepreneurs --- is a rewarding subject for future research.
C55|Are Small Farms Really more Productive than Large Farms?|We revisit the long-standing empirical evidence of an inverse relationship between farm size and productivity using rich micro data from Uganda. We show that farm size is nega- tively related with yields (output per hectare), as commonly found in the literature, but positively related with farm productivity (a farm-specific component of total factor pro- ductivity). These conflicting results do not arise because of omitted variables such as land quality, measurement error in output or inputs, or specification issues. Instead, we reconcile the findings emphasizing decreasing returns to scale in farm production and farm-specific distortions. We exploit regional variation in land tenure regimes in Uganda in evaluating the role of farm-specific distortions. Our findings point to the limited value of yields (or land productivity) in establishing the size-productivity relationship. More generally, we highlight farm size as an ineffective instrument for policy implementation since size is deeply con- founded by distortions in developing countries.
C55|A flexible state-space model with lagged states and lagged dependent variables: Simulation smoothing|We provide a simulation smoother to a exible state-space model with lagged states and lagged dependent variables. Qian (2014) has introduced this state-space model and proposes a fast Kalman filter with time-varying state dimension in the presence of missing observations in the data. In this paper, we derive the corresponding Kalman smoother moments and propose an efficient simulation smoother, which relies on mean corrections for unconditional vectors. When applied to a factor model, the proposed simulation smoother for the states is efficient compared to other state-space models without lagged states and/or lagged dependent variables in terms of computing time.
C55|Dividend payout ratio follows a Tweedie distribution: International evidence|Dividend policy is still a largely discussed issue in corporate finance literature. One of the main indicators used in analysing the dividend policy is the dividend payout ratio. Using a database consisting of 12,085 companies operating in 73 countries, for the period 2008-2014, the authors found that the dividend payout ratio follows a Tweedie distribution, and not a normal one. This distribution is stable over time for the entire analysed period. In addition, it describes the case of almost all the countries included in the sample. Thus, a better estimation of the probability that dividend payout ratio is lower or higher than a benchmark can be provided. Also, an analysis of dividend policy, distinctly considering payer versus non-payer companies, can offer additional important information for both practitioners and academics.
C55|Persistent zeros: The extensive margin of trade|The extensive margin of bilateral trade exhibits a high level of persistence that cannot be explained by geography or trade policy. We combine a heterogeneous firms model of international trade with bounded productivity with features from the firm dynamics literature to derive expressions for an exporting country's participation in a specific destination market in a given period. The model framework asks for a dynamic binary choice estimator with two or three sets of high-dimensional fixed effects. To mitigate the incidental parameter problem associated with nonlinear fixed effects models, we characterize and implement suitable bias corrections. Extensive simulation experiments confirm the desirable statistical properties of the bias-corrected estimators. Empirically, taking two sources of persistence - true state dependence and unobserved heterogeneity - into account using a dynamic specification, along with appropriate fixed effects and bias corrections, changes the estimated effects considerably: out of the most commonly studied potential determinants (joint WTO membership, common regional trade agreement, and shared currency), only sharing a common currency retains a significant effect on whether two countries trade with each other at all in our preferred estimation.
C55|Effectiveness of policy and regulation in European sovereign credit risk markets: a network analysis|We study the impact of changes in regulations and policy interventions on systemic risk among European sovereigns measured as volatility spillovers in respective credit risk markets. Our unique intraday CDS dataset allows for precise measurement of the effectiveness of these events in a network setting. In particular, it allows discerning interventions which entail significant changes in network cross-effects with appropriate bootstrap confidence intervals. We show that it was mainly regulatory changes with the ban of trading naked sovereign CDS in 2012 as well as the new ISDA regulations in 2014 which were most effective in reducing systemic risk. In comparison, we find that the effect of policy interventions was minor and generally not sustainable. In particular, they only had a significant impact when implemented for the first time and when targeting more than one country. For the volatility spillover channels, we generally find balanced networks with no fragmentation over time. JEL Classification: G20, G01, G17, C32, C55, G28
C55|Generating univariate fractional integration within a large VAR(1)|This paper shows that a large dimensional vector autoregressive model (VAR) of finite order can generate fractional integration in the marginalized univariate series. We derive high-level assumptions under which the final equation representation of a VAR(1) leads to univariate fractional white noises and verify the validity of these assumptions for two specific models.
C55|“Tracking economic growth by evolving expectations via genetic programming: A two-step approach”|The main objective of this study is to present a two-step approach to generate estimates of economic growth based on agents’ expectations from tendency surveys. First, we design a genetic programming experiment to derive mathematical functional forms that approximate the target variable by combining survey data on expectations about different economic variables. We use evolutionary algorithms to estimate a symbolic regression that links survey-based expectations to a quantitative variable used as a yardstick (economic growth). In a second step, this set of empirically-generated proxies of economic growth are linearly combined to track the evolution of GDP. To evaluate the forecasting performance of the generated estimates of GDP, we use them to assess the impact of the 2008 financial crisis on the accuracy of agents' expectations about the evolution of the economic activity in 28 countries of the OECD. While in most economies we find an improvement in the capacity of agents' to anticipate the evolution of GDP after the crisis, predictive accuracy worsens in relation to the period prior to the crisis. The most accurate GDP forecasts are obtained for Sweden, Austria and Finland.
C55|“A geometric approach to proxy economic uncertainty by a metric of disagreement among qualitative expectations”|In this study we present a geometric approach to proxy economic uncertainty. We design a positional indicator of disagreement among survey-based agents' expectations about the state of the economy. Previous dispersion-based uncertainty indicators derived from business and consumer surveys exclusively make use of the two extreme pieces of information coming from the respondents expecting a variable to rise and to fall. With the aim of also incorporating the information coming from the share of respondents expecting a variable to remain constant, we propose a geometrical framework and use a barycentric coordinate system to generate a measure of disagreement, referred to as a discrepancy indicator. We assess its performance, both empirically and experimentally, by comparing it to the standard deviation of the share of positive and negative responses, which has been used by Bachman et al. (2013) as a proxy for economic uncertainty. When applied in sixteen European countries, we find that both time-varying metrics co-evolve in most countries for expectations about the country's overall economic situation in the present, but not in the future. Additionally, we obtain their simulated sampling distributions and we find that the proposed indicator gravitates uniformly towards the three vertices of the simplex representing the three answering categories, as opposed to the standard deviation, which tends to overestimate the level of uncertainty as a result of ignoring the no-change responses. Consequently, we find evidence that the information coming from agents expecting a variable to remain constant has an effect on the measurement of disagreement.
C55|A panel quantile approach to attrition bias in Big Data: Evidence from a randomized experiment|This paper introduces a quantile regression estimator for panel data models with individual heterogeneity and attrition. The method is motivated by the fact that attrition bias is often encountered in Big Data applications. For example, many users sign-up for the latest program but few remain active users several months later, making the evaluation of such interventions inherently very challenging. Building on earlier work by Hausman and Wise (1979), we provide a simple identification strategy that leads to a two-step estimation procedure. In the first step, the coefficients of interest in the selection equation are consistently estimated using parametric or nonparametric methods. In the second step, standard panel quantile methods are employed on a subset of weighted observations. The estimator is computationally easy to implement in Big Data applications with a large number of subjects. We investigate the conditions under which the parameter estimator is asymptotically Gaussian and we carry out a series of Monte Carlo simulations to investigate the finite sample properties of the estimator. Lastly, using a simulation exercise, we apply the method to the evaluation of a recent Time-of-Day electricity pricing experiment inspired by the work of Aigner and Hausman (1980).
C55|Big Data Econometrics: Now Casting and Early Estimates|This paper aims at providing a primer on the use of big data in macroeconomic nowcasting and early estimation. We discuss: (i) a typology of big data characteristics relevant for macroeconomic nowcasting and early estimates, (ii) methods for features extraction from unstructured big data to usable time series, (iii) econometric methods that could be used for nowcasting with big data, (iv) some empirical nowcasting results for key target variables for four EU countries, and (v) ways to evaluate nowcasts and ash estimates. We conclude by providing a set of recommendations to assess the pros and cons of the use of big data in a specific empirical nowcasting context.
C55|Measuring Retail Trade Using Card Transactional Data|In this paper we present a high-dimensionality Retail Trade Index (RTI) constructed to nowcast the retail trade sector economic performance in Spain, using Big Data sources and techniques. The data are the footprints of BBVA clients from their credit or debit card transactions at Spanish point of sale (PoS) terminals.
C55|Patterns of domestic and cross-border e-commerce in Spain: A gravitational model approach|This paper presents econometric evidence on the determinants of domestic and cross-border e-commerce in Spain based on BBVA anonymised data. The paper applies the gravity model of trade to explain online credit card payment flows, using all private customer transactions of BBVA for Spain.
C55|Can media and text analytics provide insights into labour market conditions in China?|The official Chinese labour market indicators have been seen as problematic, given their small cyclical movement and their only-partial capture of the labour force. In our paper, we build a monthly Chinese labour market conditions index (LMCI) using text analytics applied to mainland Chinese-language newspapers over the period from 2003 to 2017. We use a supervised machine learning approach by training a support vector machine classification model. The information content and the forecast ability of our LMCI are tested against official labour market activity measures in wage and credit growth estimations. Surprisingly, one of our findings is that the much-maligned official labour market indicators do contain information. However, their information content is not robust and, in many cases, our LMCI can provide forecasts that are significantly superior. Moreover, regional disaggregation of the LMCI illustrates that labour conditions in the export-oriented coastal region are sensitive to export growth, while those in inland regions are not. This suggests that text analytics can, indeed, be used to extract useful labour market information from Chinese newspaper articles.<br><small>(This abstract was borrowed from another version of this item.)</small>
C55|Can media and text analytics provide insights into labour market conditions in China?|The official Chinese labour market indicators have been seen as problematic given their small cyclical movement and their only partial capture of the labour force. In our paper, we build a monthly Chinese labour market conditions index (LMCI) using text analytics applied to Mainland Chinese-language newspapers over the period from 2003 to 2017. We use a supervised machine learning approach by training a support vector machine classification model. The information content and the forecast ability of our LMCI are tested against official labour market activity measures in wage and credit growth estimations. Surprisingly, one of our findings is that the much-maligned official labour market indicators do contain information. However, their information content is not robust and, in many cases, our LMCI can provide forecasts that are significantly superior. Moreover, regional disaggregation of the LMCI illustrates that labour conditions in the export-oriented coastal region are sensitive to export growth, while those in inland regions are not. This suggests that text analytics can, indeed, be used to extract useful labour market information from Chinese newspaper articles.
C55|What Drives Interbank Loans? Evidence from Canada|We identify the drivers of unsecured and collateralized loan volumes, rates and haircuts in Canada using the Bayesian model averaging approach to deal with model uncertainty. Our results suggest that the key friction driving behaviour in this market is the collateral reallocation cost faced by borrowers. Borrowers therefore adjust unsecured lending in response to changes in short-term cash needs, and use repos to finance persistent liquidity demand. We also find that lenders set rates and haircuts taking into account counterparty credit risk and collateral market price volatility.
C55|Multidimensional media slant: complementarities in news reporting by US newspapers|Are editors’ choices of front page news based on the potential complementarities between the news items? This paper studies front page choices made by editors of major newspapers in the US. I document that newspapers front pages are biased to certain combinations of news on top of biased to certain news. To identify my measures of bias, I exploit the variation in news relevance across different topics and days. To measure the news relevance I use lead news choices of other US mass media. As a consequence, my measures of bias are relative to the overall media bias. I also provide a reader-maximization model for front page decisions that I use to interpret the empirical biases of the newspaper as preferences of its population of target readers. From my estimation, I recover maps of complementarities among pairs of topics for each of the major US newspapers. I fi nd that complementarities between news contribute in a large portion to the probability that news on a topic appears in the front page.
C55|Business cycle narratives|This article quantifies the epidemiology of media narratives relevant to business cycles in the US, Japan, and Europe (euro area). We do so by first constructing daily business cycle indexes computed on the basis of the news topics the media writes about. At a broad level, the most in uential news narratives are shown to be associated with general macroeconomic developments, finance, and (geo-)politics. However, a large set of narratives contributes to our index estimates across time, especially in times of expansion. In times of trouble, narratives associated with economic uctuations become more sparse. Likewise, we show that narratives do go viral, but mostly so when growth is low. While narratives interact in complicated ways, we document that some are clearly associated with economic fundamentals. Other narratives, on the other hand, show no such relationship, and are likely better explained by classical work capturing the market's animal spirits.
C55|Measuring the Return to Online Advertising: Estimation and Inference of Endogenous Treatment Effects|In this paper we aim to conduct inference on the “lift” effect generated by an online advertisement display: specifically we want to analyze if the presence of the brand ad among the advertisements on the page increases the overall number of consumer clicks on that page. A distinctive feature of online advertising is that the ad displays are highly targeted- the advertising platform evaluates the (unconditional) probability of each consumer clicking on a given ad which leads to a higher probability of displaying the ads that have a higher a priori estimated probability of click. As a result, inferring the causal effect of the ad display on the page clicks by a given consumer from typical observational data is difficult. To address this we use the large scale of our dataset and propose a multi-step estimator that focuses on the tails of the consumer distribution to estimate the true causal effect of an ad display. This “identification at infinity” (Chamberlain (1986)) approach alleviates the need for independent experimental randomization but results in nonstandard asymptotics. To validate our estimates, we use a set of large scale randomized controlled experiments that Microsoft has run on its advertising platform. Our dataset has a large number of observations and a large number of variables and we employ LASSO to perform variable selection. Our non-experimental estimates turn out to be quite close to the results of the randomized controlled trials.
C55|Predictive regressions under asymmetric loss: Factor augmentation and model selection|This paper discusses the specifics of forecasting using factor-augmented predictive regressions under general loss functions. In line with the literature, we employ principal component analysis to extract factors from the set of predictors. In addition, we also extract information on the volatility of the series to be predicted, since the volatility is forecast-relevant under non-quadratic loss functions. We ensure asymptotic unbiasedness of the forecasts under the relevant loss by estimating the predictive regression through the minimization of the in-sample average loss. Finally, we select the most promising predictors for the series to be forecast by employing an information criterion that is tailored to the relevant loss. Using a large monthly data set for the US economy, we assess the proposed adjustments in a pseudo out-of-sample forecasting exercise for various variables. As expected, the use of estimation under the relevant loss is found to be effective. Using an additional volatility proxy as the predictor and conducting model selection that is tailored to the relevant loss function enhances the forecast performance significantly.
C55|Using job vacancies to understand the effects of labour market mismatch on UK output and productivity|Mismatch in the labour market has been implicated as a driver of the UK’s productivity ‘puzzle’, the phenomenon describing how the growth rate and level of UK productivity have fallen behind their respective pre-Great Financial Crisis trends. Using a new dataset of around 15 million job adverts originally posted online, we examine the extent to which eliminating occupational or regional mismatch would have boosted productivity and output growth in the UK in the post-crisis period. To show how aggregate labour market data hide important heterogeneity, we map the naturally occurring vacancy data into official occupational classifications using a novel application of text analysis. The effects of mismatch on aggregate UK productivity and output are driven by dispersion in regional or occupational productivity, tightness, and matching efficiency. We find, contrary to previous work, that unwinding occupational mismatch would have had a weak effect on growth in the post-crisis period. However, unwinding regional mismatch would have substantially boosted output and productivity relative to their realised paths, bringing them in line with their pre-crisis trends.
C55|Using online job vacancies to understand the UK labour market from the bottom-up|What type of disaggregation should be used to analyse heterogeneous labour markets? How granular should that disaggregation be? Economic theory does not currently tell us; perhaps data can. Analyses typically split labour markets according to top-down classification schema such as sector or occupation. But these may be slow-moving or inaccurate relative to the structure of the labour market as perceived by firms and workers. Using a dataset of 15 million job adverts posted online between 2008 and 2016, we create an empirically driven, ‘bottom-up’ segmentation of the labour market which cuts across wage, sector, and occupation. Our segmentation is based upon applying machine learning techniques to the demand expressed in the text of job descriptions. This segmentation automatically identifies traditional job roles but also surfaces sub-markets not apparent in current classifications. We show that the segmentation has explanatory power for offered wages. The methodology developed could be deployed to create data-driven taxonomies in conditions of rapidly changing labour markets and demonstrates the potential of unsupervised machine learning in economics.
C55|Determinants of distress in the UK owner-occupier and buy-to-let mortgage markets|The mortgage market has played a central role in the global financial crisis. One particularly pressing question surrounds the conditions under which mortgage borrowers enter distress, ie get into arrears or default. This paper develops a novel micro dataset from residential mortgage loans which UK banks and building societies have pre-positioned with the Bank of England for use as collateral in exchange for central bank funding. The dataset is used to investigate the determinants of borrower distress as a function of borrower and loan-level stock/flow characteristics over the loans’ lifetime in the buy-to-let (BTL) and owner-occupier (OO) mortgage markets. We find systematic differences between these two markets, controlling for a range of loan and borrower characteristics as well as macro variables. Our main result shows that, adjusting for affordability, the loan-to-value ratio is reliably more important for borrower distress in the OO market than for distress in the BTL market, contradicting McCann’s (2014) results.
C55|Uncertain Kingdom: Nowcasting GDP and its Revisions|We design a new econometric framework to nowcast macroeconomic data subject to revisions, and use it to predict UK GDP growth in real-time. To this aim, we assemble a novel dataset of monthly and quarterly indicators featuring over ten years of real-time data vintages. Successive monthly estimates of GDP growth for the same quarter are treated as correlated observables in a Dynamic Factor Model (DFM) that also includes a large number of mixed-frequency predictors, leading to the release-augmented DFM (RA-DFM). The framework allows for a simple characterisation of the stochastic process for the revisions as a function of the observables, and permits a detailed assessment of the contribution of the data flow in informing (i) forecasts of quarterly GDP growth; (ii) the evolution of forecast uncertainty; and (iii) forecasts of revisions to early released GDP data. By evaluating the real-time performance of the RA-DFM, we find that the model’s predictions have information about the latest GDP releases above and beyond that contained in the statistical office earlier estimates; predictive intervals are well-calibrated; and UK GDP growth real-time estimates are commensurate with professional nowcasters. We also provide evidence that statistical office data on production and labour markets, subject to large publication delays, account for most of the forecastability of the revisions.
C55|Estimating Interdependence Across Space, Time and Outcomes in Binary Choice Models Using Pseudo Maximum Likelihood Estimators|Binary outcome models are frequently used in Political Science. However, such models have proven particularly dicult in dealing with interdependent data structures, including spatial autocorrelation, temporal autocorrelation, as well as simultaneity arising from endogenous binary regressors. In each of these cases, the primary source of the estimation challenge is the fact that jointly determined error terms in the reduced-form specication are analytically intractable due to a high-dimensional integral. To deal with this problem, simulation approaches have been proposed, but these are computationally intensive and impractical for datasets with thousands of observations. As a way forward, in this paper we demonstrate how to reduce the computational burder signicantly by (i) introducing analytically tractable pseudo maximum likelihoodestimators for latent binary choice models that exhibit interdependence across space, time and/or outcomes, and by (ii) proposing an implementation strategy that increases computational eciency considerably. Monte-Carlo experiments demonstrate that our estimators perform similarly to existing alternatives in terms of error, but require only a fraction of the computational cost.
C55|Dynamic Spatial Autoregressive Models with Time-varying Spatial Weighting Matrices|We propose a new spatio-temporal model with time-varying spatial weighting matrices. We allow for a general parameterization of the spatial matrix, such as: (i) a function of the inverse distances among pairs of units to the power of an unknown time-varying distance decay parameter, and (ii) a negative exponential function of the time-varying parameter as in (i). The filtering procedure of the time-varying parameters is performed using the information in the score of the conditional distribution of the observables. An extensive Monte Carlo simulation study to investigate the finite sample properties of the ML estimator is reported. We analyze the association between eight European countries' perceived risk, suggesting that the economically strong countries have their perceived risk increased due to their spatial connection with the economically weaker countries, and we investigates the evolution of the spatial connection between the house prices in different areas of the UK, identifying periods when the usually adopted sparse weighting matrix is not sufficient to describe the underlying spatial process.
C55|A novel machine learning approach for identifying the drivers of domestic electricity users’ price responsiveness|Time-based pricing programs for domestic electricity users have been effective in reducing peak demand and facilitating renewables integration. Nevertheless, high cost, price non-responsiveness and adverse selection may create the possible challenges. To overcome these challenges, it can be fruitful to investigate the ‘high-potential’ users, which are more responsive to price changes and apply time-based pricing to these users. Few studies have investigated how to identify which users are more price-responsive. We aim to fill this gap by comprehensively identifying the drivers of domestic users’ price responsiveness, in order to facilitate the selection of the high-potential users. We adopt a novel data-driven approach, first by a feed forward neural network model to accurately determine the baseline monthly peak consumption of individual households, followed by an integrated machine-learning variable selection methodology to identify the drivers of price responsiveness applied to Irish smart meter data from 2009-10 as part of a national Time of Use trial. This methodology substantially outperforms traditional variable selection methods by combining three advanced machine-learning techniques. Our results show that the response of energy users to price change is affected by a number of factors, ranging from demographic and dwelling characteristics, psychological factors, historical electricity consumption, to appliance ownership. In particular, historical electricity consumption, income, the number of occupants, perceived behavioural control, and adoption of specific appliances, including immersion water heater and dishwasher, are found to be significant drivers of price responsiveness. We also observe that continual price increase within a moderate range does not drive additional peak demand reduction, and that there is an intention-behaviour gap, whereby stated intention does not lead to actual peak reduction behavior. Based on our findings, we have conducted scenario analysis to demonstrate the feasibility of selecting the high potential users to achieve significant peak reduction.
C55|Causal Tree Estimation of Heterogeneous Household Response to Time-Of-Use Electricity Pricing Schemes|We examine the distributional effects of the introduction of Time-of-Use (TOU) pricing schemes where the price per kWh of electricity usage depends on the time of consumption. These pricing schemes are enabled by smart meters, which can regularly (i.e. half-hourly) record consumption. Using causal trees, and an aggregation of causal tree estimates known as a causal forest (Athey & Imbens 2016, Wager & Athey 2017), we consider the association between the effect of TOU pricing schemes on household electricity demand and a range of variables that are observable before the introduction of the new pricing schemes. Causal trees provide an interpretable description of heterogeneity, while causal forests can be used to obtain individual-specific estimates of treatment effects. Given that policy makers are often interested in the factors underlying a given prediction, it is desirable to gain some insight to which variables in this large set are most often selected. A key challenge follows from that fact that partitions generated by tree-based methods are sensitive to subsampling, while the use of ensemble methods such as causal forests produce more stable, but less interpretable estimates. To address this problem we utilise variable importance measures to consider which variables are chosen most often by the causal forest algorithm. Given that a number of standard variable importance measures can be biased towards continuous variables, we address this issue by including permutation-based tests for our variable importance results.
C55|Estimation of a Multiplicative Correlation Structure in the Large Dimensional Case|We propose a Kronecker product model for correlation or covariance matrices in the large dimension case. The number of parameters of the model increases logarithmically with the dimension of the matrix. We propose a minimum distance (MD) estimator based on a log-linear property of the model, as well as a one-step estimator, which is a one-step approximation to the quasi-maximum likelihood estimator (QMLE).We establish the rate of convergence and a central limit theorem (CLT) for our estimators in the large dimensional case. A specification test and tools for Kronecker product model selection and inference are provided. In an Monte Carlo study where a Kronecker product model is correctly specified, our estimators exhibit superior performance. In an empirical application to portfolio choice for S&P500 daily returns, we demonstrate that certain Kronecker product models are good approximations to the general covariance matrix.
C55|A Residual-based Threshold Method for Detection of Units that are Too Big to Fail in Large Factor Models|The importance of units with pervasive impacts on a large number of other units in a network has become increasingly recognized in the literature. In this paper we propose a new method to detect such influential or dominant units by basing our analysis on unit-specific residual error variances in the context of a standard factor model, subject to suitable adjustments due to multiple testing. Our proposed method allows us to estimate and identify the dominant units without the a priori knowledge of the interconnections amongst the units, or using a short list of potential dominant units. It is applicable even if the cross section dimension exceeds the time dimension, and most importantly it could end up with none of the units selected as dominant when this is in fact the case. The sequential multiple testing procedure proposed exhibits satisfactory small-sample performance in Monte Carlo simulations and compares well relative to existing approaches. We apply the proposed detection method to sectoral indices of US industrial production, US house price changes by states, and the rates of change of real GDP and real equity prices across the world’s largest economies.
C55|Macroeconomic Uncertainty and Forecasting Macroeconomic Aggregates|Can information on macroeconomic uncertainty improve the forecast accuracy for key macroeconomic time series for the US? Since previous studies have demonstrated that the link between the real economy and uncertainty is subject to nonlinearities, I assess the predictive power of macroeconomic uncertainty in both linear and nonlinear Bayesian VARs. For the latter I use a threshold VAR that allows for regimedependent dynamics conditional on the level of the uncertainty measure. I find that the predictive power of macroeconomic uncertainty in the linear VAR is negligible. In contrast, using information on macroeconomic uncertainty in a threshold VAR can significantly improve the accuracy of short-term point and density forecasts, especially in the presence of high uncertainty.
C55|Forecasting using mixed-frequency VARs with time-varying parameters|We extend the literature on economic forecasting by constructing a mixed-frequency time-varying parameter vector autoregression with stochastic volatility (MF-TVP-SVVAR). The latter is able to cope with structural changes and can handle indicators sampled at different frequencies. We conduct a real-time forecast exercise to predict US key macroeconomic variables and compare the predictions of the MF-TVP-SV-VAR with several linear, nonlinear, mixed-frequency, and quarterly-frequency VARs. Our key finding is that the MF-TVPSV-VAR delivers very accurate forecasts and, on average, outperforms its competitors. In particular, inflation forecasts benefit from this new forecasting approach. Finally, we assess the models’ performance during the Great Recession and find that the combination of stochastic volatility, time-varying parameters, and mixed-frequencies generates very precise inflation forecasts.
C55|Uncertain Kingdom: nowcasting GDP and its revisions|We design a new econometric framework to nowcast macroeconomic data subject to revisions, and use it to predict UK GDP growth in real-time. To this end, we assemble a novel dataset of monthly and quarterly indicators featuring over ten years of real-time data vintages. In the Release-Augmented DFM (or RA-DFM) successive monthly estimates of GDP growth for the same quarter are treated as correlated observables in a Dynamic Factor Model (DFM) that also includes a large number of mixed-frequency predictors. The framework allows for a simple characterisation of the stochastic process for the revisions as a function of the observables, and permits a detailed assessment of the contribution of the data flow in informing (i) forecasts of quarterly GDP growth; (ii) the evolution of forecast uncertainty; and (iii) forecasts of revisions to early released GDP data. We find that the RA-DFM predictions have information about the latest GDP releases above and beyond that contained in the statistical office earlier estimates; predictive intervals are well-calibrated; and that real-time estimates of UK GDP growth are commensurate with those of professional forecasters. Data on production and labour markets, subject to large publication delays, account for most of the forecastability of the revisions.
C55|Using Supervised Learning to Select Audit Targets in Performance-Based Financing in Health: An Example from Zambia|Independent verification is a critical component of performance-based financing (PBF) in health care, in which facilities are offered incentives to increase the volume of specific services but the same incentives may lead them to over-report. We examine alternative strategies for targeted sampling of health clinics for independent verification. Specifically, we empirically compare several methods of random sampling and predictive modeling on data from a Zambian PBF pilot that contains reported and verified performance for quantity indicators of 140 clinics. Our results indicate that machine learning methods, particularly Random Forest, outperform other approaches and can increase the cost-effectiveness of verification activities.
C55|Dissection of Bitcoin's Multiscale Bubble History|We present a detailed bubble analysis of the Bitcoin to US Dollar price dynamics from January 2012 to February 2018. We introduce a robust automatic peak detection method that classifies price time series into periods of uninterrupted market growth (drawups) and regimes of uninterrupted market decrease (drawdowns). In combination with the Lagrange Regularisation Method for detecting the beginning of a new market regime, we identify 3 major peaks and 10 additional smaller peaks, that have punctuated the dynamics of Bitcoin price during the analyzed time period. We explain this classification of long and short bubbles by a number of quantitative metrics and graphs to understand the main socio-economic drivers behind the ascent of Bitcoin over this period. Then, a detailed analysis of the growing risks associated with the three long bubbles using the Log-Periodic Power Law Singularity (LPPLS) model is based on the LPPLS Confidence Indicators, defined as the fraction of qualified fits of the LPPLS model over multiple time windows. Furthermore, for various fictitious present analysis times t2, positioned in advance to bubble crashes, we employ a clustering method to group LPPLS fits over different time scales and the predicted critical times tc (the most probable time for the start of the crash ending the bubble). Each cluster is argued to provide a plausible scenario for the subsequent Bitcoin price evolution. We present these predictions for the three long bubbles and the four short bubbles that our time scale of analysis was able to resolve. Overall, our predictive scheme provides useful information to warn of an imminent crash risk.
C55|Measuring Venezuelan emigration with Twitter|Venezuela has seen an unprecedented exodus of people in recent months. In response to a dramatic economic downturn in which inflation is soaring, oil production tanking, and a humanitarian catastrophe unfolding, many Venezuelans are seeking refuge in neighboring countries. However, the lack of official numbers on emigration from the Venezuelan government, and receiving countries largely refusing to acknowledge a refugee status for affected people, it has been difficult to quantify the magnitude of this crisis. In this note we document how we use data from the social media service Twitter to measure the emigration of people from Venezuela. Using a simple statistical model that allows us to correct for a sampling bias in the data, we estimate that up to 2,9 million Venezuelans have left the country in the past year.
C55|What Influences Private Investment? The Case of the Czech Republic|What influences private investment in the Czech Republic? This paper arrives at a conclusion based on a survey of fixed-asset purchases in 30,000 non-financial corporations over the period 2008-2015. BVAR models are estimated on aggregates for 19 industries and the whole non-financial economy. As our results show, foreign demand is the most important factor for Czech business investment, especially in manufacturing and tourism. We also find an increased importance of expectations and uncertainty during the period under review. According to our findings, business investment is fostered by a devalued currency and is crowded out by public investment. The most profound crowding-out was seen in manufacturing and agriculture, whereas services, trade, and construction exhibit crowding-in. Finally, EU funds are found to be successful in providing occasional support to private investment.
C55|Preventing rather than Punishing: An Early Warning Model of Malfeasance in Public Procurement|Is it possible to predict corruption and public inefficiency in public procurement? With the proliferation of e-procurement in the public sector, anti-corruption agencies and watchdog organizations in many countries currently have access to powerful sources of information. These may help anticipate which transactions become faulty and why. In this paper, we discuss the promises and challenges of using machine learning models to predict inefficiency and corruption in public procurement, both from the perspective of researchers and practitioners. We exemplify this procedure using a unique dataset characterizing more than 2 million public contracts in Colombia, and training machine learning models to predict which of them face corruption investigations or implementation inefficiencies. We use different techniques to handle the problem of class imbalance typical of these applications, report the high accuracy of our models, simulate the trade-off between precision and recall in this context, and determine which features contribute the most to the prediction of malfeasance within contracts. Our approach is useful for governments interested in exploiting large administrative datasets to improve provision of public goods and highlights some of the tradeoffs and challenges that they might face throughout this process.
C55|Inflación y volatilidad cambiaria en México (1969-2017)|No abstract is available for this item.
C55|Clasificación por capitales de una muestra de microempresarios del Área Metropolitana de Bucaramanga a partir del Análisis de Correspondencia Múltiple|No abstract is available for this item.
C55|Macroeconomic Nowcasting and Forecasting with Big Data|Data, data, data…. Economists know their importance well, especially when it comes to monitoring macroeconomic conditions—the basis for making informed economic and policy decisions. Handling large and complex data sets was a challenge that macroeconomists engaged in real-time analysis faced long before so-called big data became pervasive in other disciplines. We review how methods for tracking economic conditions using big data have evolved over time and explain how econometric techniques have advanced to mimic and automate best practices of forecasters on trading desks, at central banks, and in other market-monitoring roles. We present in detail the methodology underlying the New York Fed Staff Nowcast, which employs these innovative techniques to produce early estimates of GDP growth, synthesizing a wide range of macroeconomic data as they become available.Data, data, data... Economists know their importance well, especially when it comes to monitoring macroeconomic conditions?the basis for making informed economic and policy decisions. Handling large and complex data sets was a challenge that macroeconomists engaged in real-time analysis faced long before so-called big data became pervasive in other disciplines. We review how methods for tracking economic conditions using big data have evolved over time and explain how econometric techniques have advanced to mimic and automate best practices of forecasters on trading desks, at central banks, and in other market-monitoring roles. We present in detail the methodology underlying the New York Fed Staff Nowcast, which employs these innovative techniques to produce early estimates of GDP growth, synthesizing a wide range of macroeconomic data as they become available.
C55|The effect of big data on recommendation quality: The example of internet search|Are there economies of scale to data in internet search? This paper is first to use real search engine query logs to empirically investigate how data drives the quality of internet search results. We find evidence that the quality of search results improve with more data on previous searches. Moreover, our results indicate that the type of data matters as well: personalized information is particularly valuable as it massively increases the speed of learning. We also provide some evidence that factors not directly related to data such as the general quality of the applied algorithms play an important role. The suggested methods to disentangle the effect of data from other factors driving the quality of search results can be applied to assess the returns to data in various recommendation systems in e-commerce, including product and information search. We also discuss the managerial, privacy, and competition policy implications of our findings.
C55|Nowcasting New Zealand GDP using machine learning algorithms|This paper analyses the real-time nowcasting performance of machine learning algorithms estimated on New Zealand data. Using a large set of real-time quarterly macroeconomic indicators, we train a range of popular machine learning algorithms and nowcast real GDP growth for each quarter over the 2009Q1-2018Q1 period. We compare the predictive accuracy of these nowcasts with that of other traditional univariate and multivariate statistical models. We find that the machine learning algorithms outperform the traditional statistical models. Moreover, combining the individual machine learning nowcasts further improves the performance than in the case of the individual nowcasts alone.<br><small>(This abstract was borrowed from another version of this item.)</small>
C55|Intertemporal Similarity of Economic Time Series|This paper adapts the non-parametric Dynamic Time Warping (DTW) technique in an application to examine the temporal alignment and similarity across economic time series. DTW has important advantages over existing measures in economics as it alleviates concerns regarding a pre-defined fixed temporal alignment of series. For example, in contrast to current methods, DTW can capture alternations between leading and lagging relationships of series. We illustrate DTW in a study of US states’ business cycles around the Great Recession, and find considerable evidence that temporal alignments across states dynamic. Trough cluster analysis, we further document state-varying recoveries from the recession.
C55|A novel machine learning approach for identifying the drivers of domestic electricity users' price responsiveness|No abstract is available for this item.
C55|Successful Crowdfunding Campaigns: The Role of Project Specifics, Competition and Founders’ Experience|We focus on reward-based crowdfunding and identify the basic determinants of successful crowdfunding campaigns including new determinants not analyzed in previous studies. Using a rich database of Kickstarter projects launched during the period from April 2009 to April 2017, we employ an empirical logit model to test the causalities and statistical significance of the selected factors. Our new empirical findings suggest that launching a project campaign during the weekend and during the month with the stronger competition in the form of other launched projects decreases the success rate of the campaign. On the other hand, a longer preparation period on Kickstarter and a higher projects’ density in the given state can increase the chances to succeed. We also conclude that the competition plays the most prominent role in the category of the smallest projects. Conversely, a negative effect of projects launched at weekends and a positive effect of a founder‘s experience is the strongest in the group of the largest projects.
C55|On the robustness of the principal volatility components|In this paper, we analyse the recent principal volatility components analysis procedure. The procedure overcomes several difficulties in modelling and forecasting the conditional covariance matrix in large dimensions arising from the curse of dimensionality. We show that outliers have a devastating effect on the construction of the principal volatility components and on the forecast of the conditional covariance matrix and consequently in economic and financial applications based on this forecast. We propose a robust procedure and analyse its finite sample properties by means of Monte Carlo experiments and also illustrate it using empirical data. The robust procedure outperforms the classical method in simulated and empirical data.
C55|Assessing International Commonality in Macroeconomic Uncertainty and Its Effects|This paper uses a large vector autoregression (VAR) to measure international macroeconomic uncertainty and its effects on major economies, using two datasets, one with GDP growth rates for 19 industrialized countries and the other with a larger set of macroeconomic indicators for the U.S., euro area, and U.K. Using basic factor model diagnostics, we first provide evidence of significant commonality in international macroeconomic volatility, with one common factor accounting for strong comovement across economies and variables. We then turn to measuring uncertainty and its effects with a large VAR in which the error volatilities evolve over time according to a factor structure. The volatility of each variable in the system reflects time-varying common (global) components and idiosyncratic components. In this model, global uncertainty is allowed to contemporaneously affect the macroeconomies of the included nations—both the levels and volatilities of the included variables. In this setup, uncertainty and its effects are estimated in a single step within the same model. Our estimates yield new measures of international macroeconomic uncertainty, and indicate that uncertainty shocks (surprise increases) lower GDP and many of its components, adversely affect labor market conditions, lower stock prices, and in some economies lead to an easing of monetary policy.
C55|Internal Migration in the United States: A Comparative Assessment of the Utility of the Consumer Credit Panel|This paper demonstrates that credit bureau data, such as the Federal Reserve Bank of New York Consumer Credit Panel/Equifax (CCP), can be used to study internal migration in the United States. It is comparable to, and in some ways superior to, the standard data used to study migration, including the American Community Survey (ACS), the Current Population Survey (CPS), and the Internal Revenue Service (IRS) county-to-county migration data. CCP-based estimates of migration intensity, connectivity, and spatial focusing are similar to estimates derived from the ACS, CPS, and IRS data. The CCP can measure block-to-block migration and it is available at quarterly rather than annual frequencies. Migrants’ precise origins are not available in public versions of the ACS, CPS, or IRS data. We report measures of migration from the CCP data at finer geographies and time intervals. Finally, we disaggregate migration flows into first-, second-, and higher-order moves. Individual-level panels in the CCP make this possible, giving the CCP an additional advantage over the ACS, CPS, or publicly available IRS data.
C55|Technological Innovation in Mortgage Underwriting and the Growth in Credit: 1985-2015|The application of information technology to finance, or “fintech,” is expected to revolutionize many aspects of borrowing and lending in the future, but technology has been reshaping consumer and mortgage lending for many years. During the 1990s computerization allowed mortgage lenders to reduce loan-processing times and largely replace human-based assessment of credit risk with default predictions generated by sophisticated empirical models. Debt-to-income ratios at origination add little to the predictive power of these models, so the new automated underwriting systems allowed higher debt-to-income ratios than previous underwriting guidelines would have typically accepted. In this way, technology brought about an exogenous change in lending standards, which helped raise the homeownership rate and encourage the conversion of rental properties to owner-occupied ones, but did not have large effects on housing prices. Technological innovation in mortgage underwriting may have allowed the 2000s housing boom to grow, however, because it enhanced the ability of both borrowers and lenders to act on optimistic beliefs about future house-price growth.
C55|Using Payroll Processor Microdata to Measure Aggregate Labor Market Activity|We show that high-frequency private payroll microdata can help forecast labor market conditions. Payroll employment is perhaps the most reliable real-time indicator of the business cycle and is therefore closely followed by policymakers, academia, and financial markets. Government statistical agencies have long served as the primary suppliers of information on the labor market and will continue to do so for the foreseeable future. That said, sources of “big data” are becoming increasingly available through collaborations with private businesses engaged in commercial activities that record economic activity on a granular, frequent, and timely basis. One such data source is generated by the firm ADP, which processes payrolls for about one fifth of the U.S. private sector workforce. We evaluate the efficacy of these data to create new statistics that complement existing measures. In particular, we develop a set of weekly aggregate employment indexes from 2000 to 2017, which allows us to measure employment at a higher frequency than is currently possible. The extensive coverage of the ADP data—similar in terms of private employment to the BLS CES sample—implies potentially high information value of these data, and our results confirm this conjecture. Indeed, the timeliness and frequency of the ADP payroll microdata substantially improves forecast accuracy for both current-month employment and revisions to the BLS CES data.
C55|The U.S. Syndicated Loan Market: Matching Data|We introduce a new software package for determining linkages between datasets without common identifiers. We apply these methods to three datasets commonly used in academic research on syndicated lending: Refinitiv LPC DealScan, the Shared National Credit Database, and S&P Global Market Intelligence Compustat. We benchmark the results of our match using results from the literature and previously matched files that are publicly available. We find that the company level matching is enhanced by careful cleaning of the data and considering hierarchical relationships. For loan level matching, a tailored approach based on a good understanding of the data can be better in certain dimensions than a more pure machine learning approach. The R package for the company level match can be found on Github at https://github.com/seunglee98/fedmatch.
C55|The U.S. Syndicated Loan Market : Matching Data|We introduce a new software package for determining linkages between datasets without common identifiers. We apply these methods to three datasets commonly used in academic research on syndicated lending: Refinitiv LPC DealScan, the Shared National Credit Database, and S&P Global Market Intelligence Compustat. We benchmark the results of our match using results from the literature and previously matched files that are publicly available. We find that the company level matching is enhanced by careful cleaning of the data and considering hierarchical relationships. For loan level matching, a tailored approach based on a good understanding of the data can be better in certain dimensions than a more pure machine learning approach. The R package for the company level match can be found on Github.
C55|Economic Predictions with Big Data: The Illusion Of Sparsity|"We compare sparse and dense representations of predictive models in macroeconomics, microeconomics and finance. To deal with a large number of possible predictors, we specify a ""spike-and-slab"" prior that allows for both variable selection and shrinkage. The posterior distribution does not typically concentrate on a single sparse or dense model but on a wide set of models. A clearer pattern of sparsity can only emerge when models of very low dimension are strongly favored a priori."
C55|Can pecuniary and environmental incentives via SMS messaging make households adjust their intra-day electricity demand to a fluctuating production?|The increasing deployment of renewables introduces substantial variability into the production of electricity, requiring demand to be more movable across time. We analyze data from a large Danish fi eld experiment (2015-2016) to investigate whether households can be prompted, via SMS messages, to move electricity consumption, and if so, whether these are motivated by pecuniary or environmental motives. To take heterogeneity fully into account we fi rst use general-to-speci c-based automatic model selection which allows for a different time-series regression for each of the 1488 households studied. From this we obtain a cross-section of estimated SMS effects which we then regress on the motive type. Since households can opt out there is a risk of self-selection. We therefore control for the size, income and average consumption of the household, and the age, educational- and labor market status of the SMS recipient. The results suggest that SMS messages can to some extent motivate households to move consumption. A stronger fi nancial motive seems more effective, whereas a purely environmental motive actually reduces the displaced amount. However, mixing financial and environmental motives seems the most effective. Finally, women and elderly people are more inclined to move consumption.
C55|Offshoring and Innovation Capabilities: Evidence from Swedish Manufacturing|This paper examines the impact of global value chains on rms' innovation capabilities. Using the United Nations Broad Economic Categories (BEC) system to identify offshoring-related intermediate imports, we study contracting out production over the period 2001-2014 from about 7,000 mainly small Swedish manufacturing frms to six different destinations and test hypotheses on improvements and outcomes of innovation capabilities. Our empirical fndings show that the strategy to participate in global value chains increases frms' innovative capability regardless of frms' technology intensity. The results are robust to a wide set of controls and in line with predictions in recent models of directed technical change.
C55|Placement Optimization in Refugee Resettlement|Every year thousands of refugees are resettled to dozens of host countries. While there is growing evidence that the initial placement of refugee families profoundly affects their lifetime outcomes, there have been few attempts to optimize resettlement destinations. We integrate machine learning and integer optimization technologies into an innovative software tool that assists a resettlement agency in the United States with matching refugees to their initial placements. Our software suggests optimal placements while giving substantial autonomy for the resettlement staff to fine-tune recommended matches. Initial back-testing indicates that Annie can improve short-run employment outcomes by 22%-37%. We discuss several directions for future work such as incorporating multiple objectives from additional integration outcomes, dealing with equity concerns, evaluating potential new locations for resettlement, managing quota in a dynamic fashion, and eliciting refugee preferences.
C55|Mapping the Radical Innovations in Food Industry: A Text Mining Study|The article presents the results of the study of radical innovations in the global food industry which were obtained through semantic analysis of heterogeneous unstructured text data sources by applying innovative big data text mining system. The approach used allows performing rapid, yet comprehensive aggregation of the whole polyphony of existing knowledge of the technology development in any sector for traditional foresight, future oriented technology analysis, and horizon scanning studies. The sources for the analysis include research papers, patent applications with both full-text data and additional structured metadata, analytical reports by main international organizations and national key players, various media and news resources, including all the major technology innovation, disruption and venture capital news websites. Their processing with an introduced approach for trend- and technology-mapping helps to identify ongoing and emerging technology-related trends, weak signals on possible scientific breakthroughs in the global food industry, including most promising startup strategies and food innovation controversies. This kind of analysis can be performed on a regular basis owing to constant accumulation of textual data and serve as a framework for constant science and technology (S&T) monitoring for early warning on changing technology landscape and its implications on agriculture and food markets
C55|Detecting and Validating Global Technology Trends Using Quantitative and Expert-Based Foresight Techniques|This paper contributes to the conceptualisation and operationalisation of the “technology trend” discussion in the scope of the Technology Foresight paradigm. It proposes a consistent logical approach to analysing technology trends and increase predictive potential of futures studies. The approach integrates Big Data analysis into the Foresight studies’ toolset by means of applying text mining, namely computerised analysis of large volumes of unstructured text-based industry-relevant analytics. It comprises methodological results such as analytical decomposition of the trend concept, including trend attributes (inherent characteristics) and various trend types and empirical results of detection and classification of global technology trends in the agricultural sector. The study makes a significant contribution to the development of a conceptual apparatus for trend analysis as a sub-area of Foresight methodology. The agricultural field is used to demonstrate the application the methodology. The empirical results can be applied by federal and regional authorities responsible for promoting development of the sectors to design relevant strategies and programmes, and by companies to set their long-term marketing and investment priorities.
C55|Stress Scenario Development: Global Challenges For The Russian Agricultural Sector|This paper defines a stress scenario as a global or national business development leading to the scrapping of established trends as a result of one or several technological breakthroughs, which can combine with a number of events and factors unfavorable for the global or national economy. The paper presents an analysis of technological shifts in the global agricultural sector focused the impact of these development on the Russian economy. Special attention is paid to scenarios involving deviation from conventional trends, when the imbalance between production and consumption becomes particularly acute while the situation in global food markets changes quickly and significantly with serious consequences for the Russian economy. This remains dependent on developed countries, which are major suppliers of vital resources required for the Russian agricultural sector. Six stress scenarios for the Russian agricultural sector, if certain drivers are triggered, were developed. In contrast to conventional forecasts based on the trends formed in recent years, stress scenarios consider the disruption of such trends, which today are recognized by most experts as the most realistic
C55|“A geometric approach to proxy economic uncertainty by a metric of disagreement among qualitative expectations”|In this study we present a geometric approach to proxy economic uncertainty. We design a positional indicator of disagreement among survey-based agents' expectations about the state of the economy. Previous dispersion-based uncertainty indicators derived from business and consumer surveys exclusively make use of the two extreme pieces of information coming from the respondents expecting a variable to rise and to fall. With the aim of also incorporating the information coming from the share of respondents expecting a variable to remain constant, we propose a geometrical framework and use a barycentric coordinate system to generate a measure of disagreement, referred to as a discrepancy indicator. We assess its performance, both empirically and experimentally, by comparing it to the standard deviation of the share of positive and negative responses, which has been used by Bachman et al. (2013) as a proxy for economic uncertainty. When applied in sixteen European countries, we find that both time-varying metrics co-evolve in most countries for expectations about the country's overall economic situation in the present, but not in the future. Additionally, we obtain their simulated sampling distributions and we find that the proposed indicator gravitates uniformly towards the three vertices of the simplex representing the three answering categories, as opposed to the standard deviation, which tends to overestimate the level of uncertainty as a result of ignoring the no-change responses. Consequently, we find evidence that the information coming from agents expecting a variable to remain constant has an effect on the measurement of disagreement.
C55|“Exposure to risk increases the excess of zero accident claims frequency in automobile insurance”|Most automobile insurance databases contain a large number of policy holders with zero claims. This high frequency of zeros may reflect the fact that some insureds make little use of their vehicle, or that they do not wish to make a claim for small accidents in order to avoid an increase in their premium, but it might also be because of good driving. We analyse information on exposure to risk and driving habits using telematics data from a Pay-as-you-Drive sample of insureds. We include distance travelled per year as part of an offset in a zero- inflated Poisson model to predict the excess of zeros. We show the existence of a learning effect for large values of distance travelled, so that longer driving should result in higher premium, but there should be a discount for drivers that accumulate longer distances over time due to the increased proportion of zero claims. We confirm that speed limit violations and driving in urban areas increase the expected number of accident claims. We discuss how telematics information can be used to design better insurance and to improve traffic safety.
C55|The network of inter-industry flows in a SAM framework|The networks of nominal flows between industries in a Social Accounting Matrix (SAM) framework are studied. The flows of the SAM submatrices of production (or output of goods and services) and intermediate consumption, are identified, which are constructed from the supply and use tables of the National Accounts. From these flows, the inter-industry networks are induced. The structure of these networks are analysed, as well as, the underlying generation of income. An application to Portugal illustrates the approach.
C55|Alternative Values-Based 'Recipes' for Life Satisfaction: German Results with an Australian Replication|In most research on Life Satisfaction (LS), it is assumed that the covariates of high and low LS are the same for everyone, or at least everyone in the West. In this paper, analysing data from the German Socio-Economic Panel, with a limited replication based on Australian panel data, we estimate models of alternative ‘recipes’ for LS. There appear to be at least four distinct ‘recipes’, which are primarily based on the values of different population sub-sets. These values are: altruistic values, family values, materialistic values and religious values. By a ‘recipe’ for LS we mean a linked set of values, behavioural choices and domain satisfactions, which appear to be held together by a person’s values, and which prove to have substantial effects on LS. Our German and Australian evidence indicates that individuals who follow recipes based on altruistic, family or religious values record above average long term LS, whereas the materialistic values ‘recipe’ is associated with below average LS.
C55|How Important Are Fixed Effects and Time Trends in Estimating Returns to Schooling? Evidence from a Replication of Jacobson, Lalonde and Sullivan, 2005|A substantial and rapidly growing literature has developed around estimating earnings gains from two-year college degrees using administrative data. These papers almost universally employ a person-level fixed effects strategy to estimate earnings premia net of fixed attributes. We note that the seminal piece on which these papers build, Jacobson, Lalonde and Sullivan (Journal of Econometrics, 2005), provides theoretical and empirical evidence for the importance of additionally differencing out individual time-trends. The subsequent literature has not followed suit. Through replication we ask whether this matters. We show that it does, and further that these person-level time-trends need not be computationally burdensome in large administrative data. We recommend them as a unifying econometric standard for future work.
C55|A Structural Model of The Demand For Telecare|In this paper, we formulate a structural model of the demand for telecare. We show how the Andersen's Behavioral Model of Health Services Use, the Almost Ideal Demand System and the Revealed Preference theory can be combined with microeconomic principles of health production to reason about individuals' utility maximizing behavior. We then estimate the model using a strategy that controls for the effects of both observable and unobservable factors, and later conduct a simulation exercise by way of a decomposition analysis.
C55|Sparse Approximate Factor Estimation for High-Dimensional Covariance Matrices|We propose a novel estimation approach for the covariance matrix based on the l1-regularized approximate factor model. Our sparse approximate factor (SAF) covariance estimator allows for the existence of weak factors and hence relaxes the pervasiveness assumption generally adopted for the standard approximate factor model. We prove consistency of the covariance matrix estimator under the Frobenius norm as well as the consistency of the factor loadings and the factors. Our Monte Carlo simulations reveal that the SAF covariance estimator has superior properties in finite samples for low and high dimensions and different designs of the covariance matrix. Moreover, in an out-of-sample portfolio forecasting application the estimator uniformly outperforms alternative portfolio strategies based on alternative covariance estimation approaches and modeling strategies including the 1/N-strategy.
C55|Directed Graphs and Variable Selection in Large Vector Autoregressive Models|We represent the dynamic relation among variables in vector autoregressive (VAR) models as directed graphs. Based on these graphs, we identify so-called strongly connected components (SCCs). Using this graphical representation, we consider the problem of variable selection. We use the relations among the strongly connected components to select variables that need to be included in a VAR if interest is in forecasting or impulse response analysis of a given set of variables. We show that the set of selected variables from the graphical method coincides with the set of variables that is multi-step causal for the variables of interest by relating the paths in the graph to the coecients of the `direct' VAR representation. Empirical applications illustrate the usefulness of the suggested approach: Including the selected variables into a small US monetary VAR is useful for impulse response analysis as it avoids the well-known `price-puzzle'. We also nd that including the selected variables into VARs typically improves forecasting accuracy at short horizons.
C55|Forecasting using Bayesian VARs: A Benchmark for STREAM|This study develops a suite of Bayesian Vector Autoregression (BVAR) models for the Maltese economy to benchmark the forecasting performance of STREAM, the traditional macro-econometric model used by the Central Bank of Malta for its regular forecasting exercises. Three different BVARs are proposed, containing an endogenous and exogenous block, and differ only in terms of the crosssectional size of the former. The small BVAR contains only three endogenous variables, the medium BVAR includes 17 variables, while the large BVAR includes 32 endogenous variables. The exogenous block remains consistent across the three models. By using a similar information set, the Bayesian VARs developed in this study are utilised to benchmark the forecast performance of STREAM. In general, for real GDP, the GDP deflator, and the unemployment rate, BVAR median projections for the period 2014-2016 improve the forecast performance at the one, two, and four-step ahead horizons when compared to STREAM. However, the latter does rather well at annual projections, but it is broadly outperformed by the medium and large BVARs.
C55|Anomaly detection in streaming nonstationary temporal data|This article proposes a framework that provides early detection of anomalous series within a large collection of non-stationary streaming time series data. We define an anomaly as an observation that is very unlikely given the recent distribution of a given system. The proposed framework first forecasts a boundary for the system's typical behavior using extreme value theory. Then a sliding window is used to test for anomalous series within a newly arrived collection of series. The model uses time series features as inputs, and a density-based comparison to detect any significant changes in the distribution of the features. Using various synthetic and real world datasets, we demonstrate the wide applicability and usefulness of our proposed framework. We show that the proposed algorithm can work well in the presence of noisy non-stationarity data within multiple classes of time series. This framework is implemented in the open source R package oddstream. R code and data are available in the supplementary materials.
C55|The Simple Empirics of Optimal Online Auctions|We study reserve prices computed to maximize the expected profit of the seller based on historical observations of incomplete bid data typically available to the auction designer in online auctions for advertising or e-commerce. This direct approach to computing reserve prices circumvents the need to fully recover distributions of bidder valuations. We derive asymptotic results and also provide a new bound, based on the empirical Rademacher complexity, for the number of historical auction observations needed in order for revenue under the estimated reserve price to approximate revenue under the optimal reserve arbitrarily closely. This simple approach to estimating reserves may be particularly useful for auction design in Big Data settings, where traditional empirical auctions methods may be costly to implement. We illustrate the approach with e-commerce auction data from eBay. We also demonstrate how this idea can be extended to estimate all objects necessary to implement the Myerson (1981) optimal auction.
C55|The Bigger Picture: Combining Econometrics with Analytics Improve Forecasts of Movie Success|There exists significant hype regarding how much machine learning and incorporating social media data can improve forecast accuracy in commercial applications. To assess if the hype is warranted, we use data from the film industry in simulation experiments that contrast econometric approaches with tools from the predictive analytics literature. Further, we propose new strategies that combine elements from each literature in a bid to capture richer patterns of heterogeneity in the underlying relationship governing revenue. Our results demonstrate the importance of social media data and value from hybrid strategies that combine econometrics and machine learning when conducting forecasts with new big data sources. Specifically, while recursive partitioning strategies greatly outperform dimension reduction strategies and traditional econometric approaches in forecast accuracy, there are further significant gains from using hybrid approaches. Further, Monte Carlo experiments demonstrate that these benefits arise from the significant heterogeneity in how social media measures and other film characteristics influence box office outcomes.
C55|Empirical Asset Pricing via Machine Learning|We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premia. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best performing methods (trees and neural networks) and trace their predictive gains to allowance of nonlinear predictor interactions that are missed by other methods. All methods agree on the same set of dominant predictive signals which includes variations on momentum, liquidity, and volatility. Improved risk premium measurement through machine learning simplifies the investigation into economic mechanisms of asset pricing and highlights the value of machine learning in financial innovation.
C55|Big Data & Macroeconomic Nowcasting: Methodological Review|This paper is concerned with an introduction to big data which can be potentially used in nowcasting the UK GDP and other key macroeconomic variables. We discuss various big data classifications and review some indicative studies in the big data and macroeconomic nowcasting literature. A detailed discussion of big data methodologies is also provided. In particular, we focus on sparse regressions, heuristic optimisation of information criteria, factor methods and textual-data methods.
C55|Temporal disaggregation of overlapping noisy quarterly data using state space models: Estimation of monthly business sector output from Value Added Tax data in the UK|This paper derives monthly estimates of turnover for small and medium size businesses in the UK from rolling quarterly VAT-based turnover data. We develop a state space approach for filtering and temporally disaggregating the VAT figures, which are noisy and exhibit dynamic unobserved components. We notably derive multivariate and nonlinear methods to make use of indicator series and data in logarithms respectively. After illustrating our temporal disaggregation method and estimation strategy using an example industry, we estimate monthly seasonally adjusted figures for the seventy-five industries for which the data are available. We thus produce an aggregate series representing approximately a quarter of gross value added in the economy. We compare our estimates with those derived from the Monthly Business Survey and find that the VAT-based estimates show a different time profile and are less volatile. In addition to this empirical work our contribution to the literature on temporal disaggregation is twofold. First, we provide a discussion of the effect that noise in aggregate figures has on the estimation of disaggregated model components. Secondly, we illustrate a new temporal aggregation strategy suited for overlapping data. The technique we adopt is more parsimonious than the seminal method of Harvey and Pierse (1984) and can easily be generalised to nonoverlapping data.
C55|Estimation of competing risks duration models with unobserved heterogeneity using hsmlogit|This article presents hsmlogit, a new Stata command that estimates multispells discrete time competing risks duration models with unobserved heterogeneity. hsmlogit allows for the estimation of one, two and up to three competing risks, as well as a maximum of ve points of support for the identication of unobserved heterogeneity distribution (Heckman and Singer, 1984). The main contribution of hsmlogit is that allows for exploiting the richness of large longitudinal micro datasets, by estimating competing risks duration models, instead of one-risk models (such as hshaz and hshaz2), as well as it takes into account the presence of unobserved heterogeneity affecting transition rates. In addition to this, and taking into account the larger size of longitudinal micro datasets used for the estimation of discrete time duration models, hsmlogit also provides the algebraic expressions of both rst and second order derivatives that, respectively, dene the gradient vector and Hessian matrix, which signicantly reduce time required to achieve model convergence.
C55|The Effects of Temporal Aggregation on Search Engine Data|Using structured machine learning, this paper examines the effect that temporal aggregation has on big data from Google Analytics and Google Trends. Specifically, daily and weekly data from the Charleston Area Convention and Visitors Bureau (CACVB) website from January 2008 to March 2009 via Google Analytics and weekly, monthly, and quarterly data from Google Trends for seven economic variables from 2004 to 2011 are examined. Taking into account the different levels of aggregation, the CDFs and the estimated regression results are examined. The Kolmogorov-Smirnov test rejects the null of equivalent data distributions in the vast majority of cases for the CACVB data, but this is not the case for the economic variable. Through data mining, this paper also finds that aggregation has the potential of affecting the level of integration and the regression results for both the CACVB data and the seven economic variables.
C55|Fiscal Policy, as the “Employer of Last Resort”: Impact of Direct fiscal transfer (MGNREGA) on Labour Force Participation Rates in India|We examine the impact of conditional fiscal transfers on public employment across gender in India taking the case of the Mahatma Gandhi National Rural Employment Guarantee Scheme (MGNREGS). The MGNREGS, as an “employer of last resort” fiscal policy, is a direct employment transfer, which guarantees to provide 100 days of paid work opportunities at a predetermined wage for public works in India through a self-selection criterion. Using unit record data of the latest 68th round of NSS Employment-Unemployment survey, we examined gender differential impacts of MGNREGS on labour force participation rates across States in India. The unit of analysis in our paper is not ‘household’, but is one step ahead to capture the intra-household level of participating behaviour in the economic activity. The results, based on the survey enumerating 2,80,763 individuals in rural areas, revealed that there is a striking heterogeneity in the gender impacts of job guarantee programme across States of India. The probit estimates showed that MGNREGS job card holder’s labour force participation rates were higher than the non-card holders and the result was more pronounced for women. The analysis of the time-use patterns and the unpaid care economy statistics of job guarantee card holders obtained from the unit records also shows that augmenting public investment in care economy infrastructure is significant for the job guarantee programme to function at its full potential in India.
C55|Economics of big data: review of best papers for January 2018|Hundreds of new papers on big data are released every month and at times it is difficult to distinguish between them in terms of quality and practical use. The purpose of this monthly review is to highlight the findings in the most relevant papers in Economics of big data to help readers identify the most important new developments in the field. The review for January 2018 includes a study of social networks in truancy, a paper on consumer privacy and data collection and three NBER papers on applications of Artificial Intelligence in Economics.
C55|A Levy Regime-Switching Temperature Dynamics Model for Weather Derivatives|Weather is a key production factor in agricultural crop production and at the same time the most significant and least controllable source of peril in agriculture. These effects of weather on agricultural crop production have triggered a widespread support for weather derivatives as a means of mitigating the risk associated with climate change on agriculture. However, these products are faced with basis risk as a result of poor design and modelling of the underlying weather variable (temperature). In order to circumvent these problems, a novel time-varying mean-reversion L´evy regime-switching model is used to model the dynamics of the deseasonalized temperature dynamics. Using plots and test statistics, it is observed that the residuals of the deseasonalized temperature data are not normally distributed. To model the nonnormality in the residuals, we propose using the hyperbolic distribution to capture the semiheavy tails and skewness in the empirical distributions of the residuals for the shifted regime. The proposed regime-switching model has a mean-reverting heteroskedastic process in the base regime and a Levy process in the shifted regime. By using the Expectation-Maximization algorithm, the parameters of the proposed model are estimated. The proposed model is flexible as it modelled the deseasonalized temperature data accurately.
C55|Economic Impact Analysis of Hospital Readmission Rate and Service Quality Using Machine Learning|The hospital readmission rate has been proposed as an important outcome indicator computable from routine statistics. The purpose of this research is to investigate the Economic Impact of service in hospitals and integrated delivery networks in the United States based on the readmission rates as the target variable. The data set includes information from 130 hospitals and integrated delivery networks in the United States from 1999 to 2008 to investigate significance of different factors in readmission rate. The dataset contains 101,766 patients’ encounters and 50 variables. The 30-day readmission rate is considered as an indicator of the quality of the health providers and is used as target variable in this project. Preliminary data analysis shows that age, admission type, discharge disposition etc. is correlated to the readmission rate and will be incorporated for further data analysis. Data analysis are performed on the diabetic patient dataset to develop a classification model to predict the likelihood for a discharged patient to be readmitted within 30 days. KNN, Naive Bayes and Logistic Regression algorithm were used to classify data and KNN appears to be the best approach to develop the model. Hospitalisations and drug prescriptions accounted for 50% and 20% of total readmission expenditure, respectively. Long term nursing home care after hospital admission cost an additional £46.4 million. With the ability to identify those patients who are more likely to be readmitted within 30 days, we can deploy the hospital resources more economically affordable while improving services. Based on the results it can be concluded that the direct cost of readmission rate for hospitals rose to £459 million in 2000 and nursing home costs rose to £111 million. Also, it can be perceived that a reduced length of hospital stay was associated with increased readmission rates for jaundice and dehydration.
C55|Agricultural Productivity, Fiscal and Trade Policies Nexus in Sub-Saharan Africa: A Panel Structural Vector Error Correction Model Analysis|Public expenditure on the agricultural sector targeted towards raising investments for increased agricultural productivity has been low in most countries in Sub-Saharan Africa (SSA). Also, existing empirical evidence on the impact of fiscal and trade policies on the improvement of agricultural systems remains mixed and inconclusive. In view of the above, this study employs a three-variable Panel Structural Vector Error Correction Model (PSVECM) in capturing the dynamic structure of the possible relationships among agricultural productivity, fiscal and trade policies in 37 selected countries within SSA, using annual data from 1990 to 2016. In imposing short- and long-run identifying restrictions, the cointegration structure of the PSVECM reveals an instantaneous impact of government expenditure and terms of trade on crop production in the transitory period. Likewise, terms of trade has a permanent significant effect on crop production and government expenditure within the reviewed period in SSA. The impulse response and variance decomposition analysis trace out a mixed result of both short and long run significant and fluctuating relationships among government expenditure, terms of trade and crop production in SSA. This finding implies that fiscal and trade policies are crucial in influencing agricultural productivity; and recommends that policymakers should adopt expansionary fiscal (in line with the Keynesian theory) and trade policies which stimulate both short and long run agricultural productivity growth in countries within SSA
C55|The Impact of LTV policy on Bank Lending: Evidence from Disaggregate Housing Loan Data|How did the Loan-to-Value (LTV) measures aimed at increasing resilience of the banking system affect banks' lending? This paper utilizes bank-level and contract-level data of housing credit in Thailand spanning from 2004 to 2017, and applies the panel data and probit approaches in evaluating the impact of LTV measures introduced in 2009, 2011 and 2013 on the housing loans. We find that the LTV measures had an impact on banks' risk-taking behavior in ways consistent with the policy's objectives. The effects manifest in a reshaping of LTV distribution of the targeted loan sector rather than a credit growth slowdown at the bank level. In addition, the size of adjustment varies across different types of banks, with stronger response from large and small banks compared with medium banks. Overall, our results suggest that certain macroprudential policies can achieve target-specific outcome, but with differential impact across banks. Nevertheless, questions remain regarding the channels through which LTV measures impact bank lending and factors underlying diverging response among banks.
C55|Nowcasting the Unemployment Rate in the EU with Seasonal BVAR and Google Search Data|Abstract In this paper a Bayesian vector autoregressive model for nowcasting the seasonally non-adjusted unemployment rate in EU-countries is developed. On top of the official statistical releases, the model utilizes Google search data and the effect of Google data on the forecasting performance of the model is assessed. The Google data is found to yield modest improvements in forecasting accuracy of the model. To the author’s knowledge, this is the first time the forecasting performance of the Google search data has been studied in the context of Bayesian vector autoregressive model. This paper also adds to the empirical literature on the hyperparameter choice with Bayesian vector autoregressive models. The hyperparameters are set according to the mode of the posterior distribution of the hyperparameters, and this is found to improve the out-of-sample forecasting accuracy of the model significantly, compared to the rule-of-thumb values often used in the literature.
C55|Capital humain au Maroc: Evaluation fondée sur le revenu de la vie entière|This working paper provides a measure of the human capital stock in Morocco in 1999 and 2012, by adopting the lifetime income approach (Jorgenson and Fraumeni, 1989, 1992a, 1992b). The results show that during the period 1999-2012, the stock of human capital rose at an annual average rate of 3% in real terms and remains the leading source of wealth in Morocco. This evolution is due, on the one hand, to the increase in the number of individuals in the working-age population and, on the other hand, to the relative improvement of their education level. However, the quality of the education and training system and the low employment rate remain the key challenges to be tackled in order to further increase human capital and maintain its position as the main source of wealth in Morocco. Finally, the estimations carried so far in this study have been enhanced by sensitivity analysis with respect to the data about salaries and the assumptions made about expected future income growth and its discount rate.
C55|Media based sentiment indices as an alternative measure of consumer confidence|The world is currently generating data at an uprecedented rate. Embracing the data revolution, case studies on the construction of alternative consumer confidence indices using large text datasets have started to make its way into the academic literature. These 'sentiment indices' are constructed using text-based analysis. A subfield within computational linguistics. In this paper we consider the feasibility of constructing online sentiment indices using large amounts of media data as an alternative for the conventional survey method in South Africa. A clustering framework is adopted to provide an indication of feasible cadidate sentiment indices that best reflect the traditional survey based confidence consumer index conducted by the BER. The results indicate that the best candidate indices are linked to a single data source with a focus on using specialised financial dictionaries. Finally, composite indices for consumer confidence is constructed using Principle Component Analysis. The resulting indices' high correlation with the traditional consumer confidence index provide motivation for using media data sources to track consumer confidence within an emerging market such as South Africa using sentiment based techniques
C55|Forecasting Industrial Production and Inflation in Turkey with Factor Models|In this paper, industrial production growth and core inflation are forecasted using a large number of domestic and international indicators. Two methods are employed, factor models and forecast combination, to deal with the curse of dimensionality problem stemming from the availability of ever growing data sets. A comprehensive analysis is carried out to understand the sensitivity of the forecast performance of factor models to various modelling choices. In this respect, effects of factor extraction method, number of factors, data aggregation level and forecast equation type on the forecasting performance are analyzed. Moreover, the effect of using certain data blocks such as European Union variables and interest rates on the forecasting performance is evaluated as well. Out-of-sample forecasting exercise is conducted for two consecutive periods to assess the stability of the forecasting performance. Results show that best performing specifications depend on the type of the variable that one wants to forecast, the forecast horizon and the sample period used to evaluate the out-of-sample forecasting performance. Factor models perform better than the combination of bi-variate forecasts.
C55|Forecasting with Bayesian Vector Autoregressions with Time Variation in the Mean|We develop a vector autoregressive model with time variation in the mean and the variance. The unobserved time-varying mean is assumed to follow a random walk and we also link it to long-term Consensus forecasts, similar in spirit to so called democratic priors. The changes in variance are modelled via stochastic volatility. The proposed Gibbs sampler allows the researcher to use a large cross-sectional dimension in a feasible amount of computational time. The slowly changing mean can account for a number of secular developments such as changing inflation expectations, slowing productivity growth or demographics. We show the good forecasting performance of the model relative to popular alternatives, including standard Bayesian VARs with Minnesota priors, VARs with democratic priors and standard time-varying parameter VARs for the euro area, the United States and Japan. In particular, incorporating survey forecast information helps to reduce the uncertainty about the unconditional mean and along with the time variation improves the long-run forecasting performance of the VAR models.
C55|Estimation Risk and Shrinkage in Vast-Dimensional Fundamental Factor Models|We investigate covariance matrix estimation in vast-dimensional spaces of 1,500 up to 2,000 stocks using fundamental factor models (FFMs). FFMs are the typical benchmark in the asset management industry and depart from the usual statistical factor models and the factor models with observed factors used in the statistical and finance literature. Little is known about estimation risk in FFMs in high dimensions. We investigate whether recent linear and non-linear shrinkage methods help to reduce the estimation risk in the asset return covariance matrix. Our findings indicate that modest improvements are possible using high-dimensional shrinkage techniques. The gains, however, are not realized using standard plug-in shrinkage parameters from the literature, but require sample dependent tuning.
C55|Classifying Firms with Text Mining|Statistics on the births, deaths and survival rates of firms are crucial pieces of information, as they enter as an input in the computation of GDP, the identification of each sectorâ€™s contribution to the economy, and the assessment of gross job creation and destruction rates. Official statistics on firm demography are made available only several months after data collection and storage, however. Furthermore, unprocessed and untimely administrative data can lead to a misrepresentation of the life-cycle stage of a firm. In this paper we implement an automated version of Eurostatâ€™s algorithm aimed at distinguishing true startup endeavors from the resurrection of pre-existing but apparently defunct firms. The potential gains from combining machine learning, natural language processing and econometric tools for pre- processing and analyzing granular data are exposed, and a machine learning method predicting reactivations of deceptively dead firms is proposed.
C55|Variable Selection in Sparse Semiparametric Single Index Models|"In this paper we consider the ""Regularization of Derivative Expectation Operator"" (Rodeo) of Lafferty and Wasserman (2008) and propose a modified Rodeo algorithm for semiparametric single index models in big data environment with many regressors. The method assumes sparsity that many of the regressors are irrelevant. It uses a greedy algorithm, in that, to estimate the semiparametric single index model (SIM) of Ichimura (1993), all coefficients of the regressors are initially set to start from near zero, then we test iteratively if the derivative of the regression function estimator with respect to each coefficient is significantly different from zero. The basic idea of the modified Rodeo algorithm for SIM (to be called SIM-Rodeo) is to view the local bandwidth selection as a variable selection scheme which amplifies the coefficients for relevant variables while keeping the coefficients of irrelevant variables relatively small or at the initial starting values near zero. For sparse semiparametric single index models, the SIM-Rodeo algorithm is shown to attain consistency in variable selection. In addition, the algorithm is fast to finish the greedy steps. We compare SIM-Rodeo with SIM-Lasso method in Zeng et al. (2012). Our simulation results demonstrate that the proposed SIM-Rodeo method is consistent for variable selection and show that it has smaller integrated mean squared errors than SIM-Lasso."
C55|Forecasting Using Supervised Factor Models|This paper examines the theoretical and empirical properties of a supervised factor model based on combining forecasts using principal components (CFPC), in comparison with two other supervised factor models (partial least squares regression, PLS, and principal covariate regression, PCovR) and with the unsupervised principal component regression, PCR. The supervision refers to training the predictors for a variable to forecast. We compare the performance of the three supervised factor models and the unsupervised factor model in forecasting of U.S. CPI inflation. The main finding is that the predictive ability of the supervised factor models is much better than the unsupervised factor model. The computation of the factors can be doubly supervised together with variable selection, which can further improve the forecasting performance of the supervised factor models. Among the three supervised factor models, the CFPC best performs and is also most stable. While PCovR also performs well and is stable, the performance of PLS is less stable over different out-of-sample forecasting periods. The effect of supervision gets even larger as forecast horizon increases. Supervision helps to reduce the number of factors and lags needed in modelling economic structure, achieving more parsimony.
C55|Predicting bond betas using macro-finance variables|We predict bond betas conditioning on a number of macro-finance variables. We explore differences across long-term government bonds, investment grade corporate bonds, and high yield corporate bonds. We conduct out-of-sample forecasting using the new approach of combining predictor variables through complete subset regressions (CSR). We consider the robustness of CSR forecasts across the 1-month, 3-month, and 12-month forecasting horizons. The CSR method performs well in predicting bond betas.
C55|Leverage, asymmetry and heavy tails in the high-dimensional factor stochastic volatility model|We develop a flexible modeling and estimation framework for a high-dimensional factor stochastic volatility (SV) model. Our specification allows for leverage effects, asymmetry and heavy tails across all systematic and idiosyncratic components of the model. This framework accounts for well-documented features of univariate financial time series, while introducing a flexible dependence structure that incorporates tail dependence and asymmetries such as stronger correlations following downturns. We develop an efficient Markov chain Monte Carlo (MCMC) algorithm for posterior simulation based on the particle Gibbs, ancestor sampling, and particle efficient importance sampling methods. We build computationally efficient model selection into our estimation framework to obtain parsimonious specifications in practice. We validate the performance of our proposed estimation method via extensive simulation studies for univariate and multivariate simulated datasets. An empirical study shows that the model outperforms other multivariate models in terms of value-at-risk evaluation and portfolio selection performance for a sample of US and Australian stocks.
C55|Credit Risk Analysis using Machine and Deep learning models|Due to the hyper technology associated to Big Data, data availability and computing power, most banks or lending financial institutions are renewing their business models. Credit risk predictions, monitoring, model reliability and effective loan processing are key to decision making and transparency. In this work, we build binary classifiers based on machine and deep learning models on real data in predicting loan default probability. The top 10 important features from these models are selected and then used in the modelling process to test the stability of binary classifiers by comparing performance on separate data. We observe that tree-based models are more stable than models based on multilayer artificial neural networks. This opens several questions relative to the intensive used of deep learning systems in the enterprises.
C55|Trade Policies and Integration of the Western Balkans|Based on a newly constructed multi-country input-output table including all European countries, we estimate the economic effects of the EU accession countries entering the ‘Stabilisation and Association Agreement’ (SAA) with the EU and the potential effects of joining the European Single Market applying a structural gravity framework. The results point towards strong positive effects on trade for the SAA countries, but only small effects for the EU Member States. Conducting a counterfactual analysis, the paper gives an indication of the magnitude of the positive impacts on GDP for these countries. In addition, a detailed industry breakdown of these effects is provided.
C55|Credit Conditions and the Effects of Economic Shocks: Amplifications and Asymmetries|In this paper we address three empirical questions related to credit conditions. Do they change the dynamic interactions of economic variables by characterizing different regimes? Do they amplify the effects of economic shocks? Do they generate asymmetries in the effects of economic shocks depending on the size and sign of the shock? To answer these questions, we introduce endogenous regime switching in the parameters of a large Multivariate Autoregressive Index (MAI) model, where all variables react to a set of observable common factors. We develop Bayesian estimation methods and show how to compute responses to common structural shocks. We find that credit conditions do act as a trigger variable for regime changes. Moreover, demand and supply shocks are amplified when they hit the economy during periods of credit stress. Finally, good shocks seem to have more positive effects during stress time, in particular on unemployment.
C55|“Tracking economic growth by evolving expectations via genetic programming: A two-step approach”|The main objective of this study is to present a two-step approach to generate estimates of economic growth based on agents’ expectations from tendency surveys. First, we design a genetic programming experiment to derive mathematical functional forms that approximate the target variable by combining survey data on expectations about different economic variables. We use evolutionary algorithms to estimate a symbolic regression that links survey-based expectations to a quantitative variable used as a yardstick (economic growth). In a second step, this set of empirically-generated proxies of economic growth are linearly combined to track the evolution of GDP. To evaluate the forecasting performance of the generated estimates of GDP, we use them to assess the impact of the 2008 financial crisis on the accuracy of agents' expectations about the evolution of the economic activity in 28 countries of the OECD. While in most economies we find an improvement in the capacity of agents' to anticipate the evolution of GDP after the crisis, predictive accuracy worsens in relation to the period prior to the crisis. The most accurate GDP forecasts are obtained for Sweden, Austria and Finland.
C55|A New Semiparametric Estimation Approach for Large Dynamic Covariance Matrices with Multiple Conditioning Variables|This paper studies the estimation of large dynamic covariance matrices with multiple conditioning variables. We introduce an easy-to-implement semiparametric method to estimate each entry of the covariance matrix via model averaging marginal regression, and then apply a shrinkage technique to obtain the dynamic covariance matrix estimation. Under some regularity conditions, we derive the asymptotic properties for the proposed estimators including the uniform consistency with general convergence rates. We further consider extending our methodology to deal with the scenarios: (i) the number of conditioning variables is divergent as the sample size increases, and (ii) the large covariance matrix is conditionally sparse relative to contemporaneous market factors. We provide a simulation study that illustrates the finite-sample performance of the developed methodology. We also provide an application to financial portfolio choice from daily stock returns.
C55|The Effect of Big Data on Recommendation Quality: The Example of Internet Search|Are there economies of scale to data in internet search? This paper is first to use real search engine query logs to empirically investigate how data drives the quality of internet search results. We find evidence that the quality of search results improve with more data on previous searches. Moreover, our results indicate that the type of data matters as well: personalized information is particularly valuable as it massively increases the speed of learning. We also provide some evidence that factors not directly related to data such as the general quality of the applied algorithms play an important role. The suggested methods to disentangle the effect of data from other factors driving the quality of search results can be applied to assess the returns to data in various recommendation systems in e-commerce, including product and information search. We also discuss the managerial, privacy, and competition policy implications of our findings.
C55|Measuring Venezuelan Emigration with Twitter|Venezuela has seen an unprecedented exodus of people in recent months. In response to a dramatic economic downturn in which inflation is soaring, oil production tanking, and a humanitarian catastrophe unfolding, many Venezuelans are seeking refuge in neighboring countries. However, the lack of official numbers on emigration from the Venezuelan government, and receiving countries largely refusing to acknowledge a refugee status for affected people, it has been difficult to quantify the magnitude of this crisis. In this note we document how we use data from the social media service Twitter to measure the emigration of people from Venezuela. Using a simple statistical model that allows us to correct for a sampling bias in the data, we estimate that up to 2.9 million Venezuelans have left the country in the past year.
C55|A New Measure of Intra-generational Redistribution within PAYG Pension Schemes and its Application to German Micro-data|This paper proposes a new Index for measuring intra-generational redistribution in PAYG pension schemes. This index solely requires information on contributions and pension benefits of retirees, eliminating the involvement of the contribution side in a PAYG pension scheme. As an application, we use contribution records of new German retirees to measure intra-generational redistribution in the German statutory pension scheme and the importance of certain additional benefits.
C55|Improving the forecasts of European regional banks' profitability with machine learning algorithms|Regional banks as savings and cooperative banks are widespread in continental Europe. In the aftermath of the financial crisis, however, they had problems keeping their profitability which is an important quantitative indicator for the health of a bank and the banking sector overall. We use a large data set of bank-level balance sheet items and regional economic variables to forecast protability for about 2,000 regional banks. Machine learning algorithms are able to beat traditional estimators as ordinary least squares as well as autoregressive models in forecasting performance.
C55|Let the data do the talking: Empirical modelling of survey-based expectations by means of genetic programming|In this study we use agents’ expectations about the state of the economy to generate indicators of economic activity in twenty-six European countries grouped in five regions (Western, Eastern, and Southern Europe, and Baltic and Scandinavian countries). We apply a data-driven procedure based on evolutionary computation to transform survey variables in economic growth rates. In a first step, we design five independent experiments to derive the optimal combination of expectations that best replicates the evolution of economic growth in each region by means of genetic programming, limiting the integration schemes to the main mathematical operations. We then rank survey variables according to their performance in tracking economic activity, finding that agents’ “perception about the overall economy compared to last year” is the survey variable with the highest predictive power. In a second step, we assess the out-of-sample forecast accuracy of the evolved indicators. Although we obtain different results across regions, Austria, Slovakia, Portugal, Lithuania and Sweden are the economies of each region that show the best forecast results. We also find evidence that the forecasting performance of the survey-based indicators improves during periods of higher growth.
C55|Cointegration in functional autoregressive processes| This paper derives a generalization of the Granger-Johansen Representation Theorem valid for H-valued autoregressive (AR) processes, where H is an infinite dimensional separable Hilbert space, under the assumption that 1 is an eigenvalue of finite type of the AR operator function and that no other non-zero eigenvalue lies within or on the unit circle. A necessary and sucient condition for integration of order d = 1, 2,... is given in terms of the decomposition of the space H into the direct sum of d+1 closed subspaces h, h = ,..,d, each one associated with components of the process integrated of order h. These results mirror the ones recently obtained in the nite dimensional case, with the only di erence that the number of cointegrating relations of order 0 is infinite.
C55|Applications For Businesses That Uses Relational Databases:|The paper presents a database production model designed as a warehouse star that contain dimensions like deposits, raw materials, stocks, products, producer, locations, time and a fact table with foreign keys and measures. This model optimize the activity of a business based on a production activity in the way that it can store large amount of data in a historical way that can be the base for future scenarios with key values changed by the decision maker. The decision maker analyses a large spectrum of reports and choose what indicators to observe and what measures to display and so it’s easy to decide based on large amount of data and trends. Database applications for business improve the efficiency in managing large quantity of data in the sense for storage, updates, queries, interaction with the users and also getting answers through reports. The schema specific to a database is very flexible and permits adding or removing columns and also adding and removing entities. This feature is very useful when the relational database schema is transformed in a data warehouse shaped as a star with dimensions and a fact table. This model permits advanced queries and the usage of rollup and drill down objects specific to the business intelligence tools that offer quick responses to the complex answers. To a production business the choice of a database application designed and implemented as data warehouse star model, bennefits from all the advantage of storage and also a superior and complex tool for building queries.
C55|Tracking chinese vulnerability in real time using Big Data|We develop an indicator to track vulnerability sentiment in China. In order to ensure robustness and depth, we use a combination of traditional macroeconomic and financial time series with textual analysis using Big Data techniques.The index is composed by the following dimensions: state owned enterprises; shadow banking; housing market bubble and exchange rate market.
C55|Large Time-Varying Parameter VARs: A Non-Parametric Approach|In this paper we introduce a nonparametric estimation method for a large Vector Autoregression (VAR) with time-varying parameters. The estimators and their asymptotic distributions are available in closed form. This makes the method computationally efficient and capable of handling information sets as large as those typically handled by factor models and Factor Augmented VARs (FAVAR). When applied to the problem of forecasting key macroeconomic variables, the method outperforms constant parameter benchmarks and large (parametric) Bayesian VARs with time-varying parameters. The tool can also be used for structural analysis. As an example, we study the time-varying effects of oil price innovations on sectoral U.S. industrial output. We find that the changing interaction between unexpected oil price increases and business cycle fluctuations is shaped by the durable materials sector, rather by the automotive sector on which a large part of the literature has typically focused.
C55|Beyond Early Warning Indicators: High School Dropout and Machine Learning|This paper provides an algorithm to predict which students are going to drop out of high schools relying only on information from 9th grade. It verifies that using a parsimonious early warning system - as implemented in many schools - leads to poor results. It shows that schools can obtain more precise predictions by exploiting the available high-dimensional data jointly with machine learning tools such as Support Vector Machine, Boosted Regression and Post-LASSO. It carefully selects goodness-of-fit criteria based on the context and the underlying theoretical framework: model parameters are calibrated by taking into account policy goals and budget constraints. Finally, it uses unsupervised machine learning to divide students at risk of dropping out into different clusters.
C55|Machine learning at central banks|We introduce machine learning in the context of central banking and policy analyses. Our aim is to give an overview broad enough to allow the reader to place machine learning within the wider range of statistical modelling and computational analyses, and provide an idea of its scope and limitations. We review the underlying technical sources and the nascent literature applying machine learning to economic and policy problems. We present popular modelling approaches, such as artificial neural networks, tree-based models, support vector machines, recommender systems and different clustering techniques. Important concepts like the bias-variance trade-off, optimal model complexity, regularisation and cross-validation are discussed to enrich the econometrics toolbox in their own right. We present three case studies relevant to central bank policy, financial regulation and economic modelling more widely. First, we model the detection of alerts on the balance sheets of financial institutions in the context of banking supervision. Second, we perform a projection exercise for UK CPI inflation on a medium-term horizon of two years. Here, we introduce a simple training-testing framework for time series analyses. Third, we investigate the funding patterns of technology start-ups with the aim to detect potentially disruptive innovators in financial technology. Machine learning models generally outperform traditional modelling approaches in prediction tasks, while open research questions remain with regard to their causal inference properties.
C55|Sending firm messages: text mining letters from PRA supervisors to banks and building societies they regulate|Our paper analyses confidential letters sent from the Bank of England’s Prudential Regulation Authority (PRA) to banks and building societies it supervises. These letters are a ‘report card’ written to firms annually, and are arguably the most important, regularly recurring written communication sent from the PRA to firms it supervises. Using a mix of methods, including a machine learning algorithm called random forests, we explore whether the letters vary depending on the riskiness of the firm to whom the PRA is writing. We find that they do. We also look across the letters as a whole to draw out key topical trends and confirm that topics important on the post-crisis regulatory agenda such as liquidity and resolution appear frequently. And we look at how PRA letters differ from the letters written by the PRA’s predecessor, the Financial Services Authority. We find evidence that PRA letters are different, with a greater abundance of forward-looking language and directiveness, reflecting the shift in supervisory approach that has occurred in the United Kingdom following the financial crisis of 2007–09.
C55|Improving Forecast Accuracy of Financial Vulnerability: Partial Least Squares Factor Model Approach|We present a factor augmented forecasting model for assessing the financial vulnerability in Korea. Dynamic factor models often extract latent common factors from a large panel of time series data via the method of the principal components (PC). Instead, we employ the partial least squares (PLS) method that estimates target specific common factors, utilizing covariances between predictors and the target variable. Applying PLS to 198 monthly frequency macroeconomic time series variables and the Bank of Korea's Financial Stress Index (KFSTI), our PLS factor augmented forecasting models consistently outperformed the random walk benchmark model in out-of-sample prediction exercises in all forecast horizons we considered. Our models also outperformed the autoregressive benchmark model in short-term forecast horizons. We expect our models would provide useful early warning signs of the emergence of systemic risks in Korea's financial markets.
C55|Toward understanding 17th century English culture: A structural topic model of Francis Bacon's ideas|We use machine-learning methods to study the features and origins of the ideas of Francis Bacon, a key figure who provided the intellectual roots of a cultural paradigm that spurred modern economic development. Bacon's works are the data in an estimation of a structural topic model, a recently developed methodology for analysis of text corpora. The estimates uncover sixteen topics prominent in Bacon's opus. Two are key elements of the ideas usually associated with Bacon—inductive epistemology and fact-seeking. The utilitarian promise of science and the centralized organization of the scientific quest, embraced by Bacon's followers, were not emphasized by him. Using strategic communication, Bacon facilitated reception of his scientific methodology, targeted influential groups, and finessed powerful opponents. We provide the first quantitative evidence that the genesis of Bacon's epistemology lies in his experience in the common-law. Combining our findings with accepted arguments in the existing literature, we suggest that the effects of common-law culture can help explain the coincidence of early political and economic development in England.
C55|Forecasting economic activity in data-rich environment|This paper compares the performance of five classes of forecasting models in an extensive out-of-sample exercise. The types of models considered are standard univariate models, factor-augmented regressions, dynamic factor models, other data-rich models and forecast combinations. These models are compared using four types of data: real series, nominal series, the stock market index and exchange rates. Our Findings can be summarized in a few points: (i) data-rich models and forecasts combination approaches are the best for predicting real series; (ii) ARMA(1,1) model predicts inflation change incredibly well and outperform data-rich models; (iii) the simple average of forecasts is the best approach to predict future SP500 returns; (iv) exchange rates can be predicted at short horizons mainly by univariate models but the random walk dominates at medium and long terms; (v) the optimal structure of forecasting equations changes much over time; and (vi) the dispersion of out-of-sample point forecasts is a good predictor of some macroeconomic and financial uncertainty measures as well as of the business cycle movements among real activity series.
C55|Risk Adjustment Revisited using Machine Learning Techniques|Risk adjustment is vital in health policy design. Risk adjustment defines the annual capitation payments to health insurers and is a key determinant of insolvency risk for health insurers. In this study we compare the current risk adjustment formula used by Colombia's Ministry of Health and Social Protection against alternative specifications that adjust for additional factors. We show that the current risk adjustment formula, which conditions on demographic factors and their interactions, can only predict 30% of total health expenditures in the upper quintile of the expenditure distribution. We also show the government's formula can improve significantly by conditioning ex ante on measures indicators of 29 long-term diseases. We contribute to the risk adjustment literature by estimating machine learning based models and showing non-parametric methodologies (e.g., boosted trees models) outperform linear regressions even when fitted in a smaller set of regressors.
C55|Consistent Pseudo-Maximum Likelihood Estimators|The development of the literature on the pseudo maximum likelihood (PML) estimators would not have been so efficient without the modern proof of consistency of extremum estimators introduced at the end of the sixties by E. Malinvaud and R. Jennrich. We discuss this proof and replace it in an historical perspective. In this paper we also provide a survey of the literature on consistent (PML) estimators. We emphasize the role of the white noise assumptions on the set of pseudo distributions leading to consistent estimators. The stronger these assumptions, the larger the set of consistent PML estimators. We also illustrate the importance of these PML approaches in big data environment.
C55|Pricing the quality of an innovative idea|This paper aims to analyze whether the quality of an innovative idea can spur the patent's price. From an economic perspective, we address the question of how the quality of an innovative idea increases the patent's price. We examine the problem for the case of a single innovation rather than patent´s families. Therefore, we follow the assumption that innovative ideas have patents. Nevertheless, the analysis is divided into two stages; first we estimated the quality of the innovation by quantifying information of the patent documents from the patent portfolios of firms of the ICT sector over the period 1996 to 2015. By providing new empirical evidence, we showed that the patent´s quality can be estimate with multiple observed patents' characteristics which are significant related to the utility of the patent in the market and its impact on the follow-on innovation. The analyses also estimate the patent's price in the market for technologies based on the quality index. In the same way, we used information of the patent´s transaction in the ICT sector over the period of 2012- 2015 and review the main costs of the American, European and the international patent system. Our finale results indicate the possibility to reduce the asymmetric information of the quality in the patent´s transactions by using public information.
C55|Model economic phenomena with CART and Random Forest algorithms|"The aim of this paper is to highlight the advantages of algorithmic methods for economic research with quantitative orientation. We describe four typical problems involved in econometric modeling, namely the choice of explanatory variables, a functional form, a probability distribution and the inclusion of interactions in a model. We detail how those problems can be solved by using ""CART"" and ""Random Forest"" algorithms in a context of massive increasing data availability. We base our analysis on two examples, the identification of growth drivers and the prediction of growth cycles. More generally, we also discuss the application fields of these methods that come from a machine-learning framework by underlining their potential for economic applications."
C55|Forecasting economic activity in data-rich environment|This paper compares the performance of five classes of forecasting models in an extensive out-of-sample exercise. The types of models considered are standard univariate models, factor-augmented regressions, dynamic factor models, other data-rich models and forecast combinations. These models are compared using four types of data: real series, nominal series, the stock market index and exchange rates. Our findings can be summarized in a few points: (i) data-rich models and forecasts combination approaches are the best for predicting real series; (ii) ARMA(1,1) model predicts inflation change incredibly well and outperform data-rich models; (iii) the simple average of forecasts is the best approach to predict future SP500 returns; (iv) exchange rates can be predicted at short horizons mainly by univariate models but the random walk dominates at medium and long terms; (v) the optimal structure of forecasting equations changes much over time; and (vi) the dispersion of out-of-sample point forecasts is a good predictor of some macroeconomic and financial uncertainty measures as well as of the business cycle movements among real activity series.
C55|Unconventional monetary policy and the anchoring of inflation expectations|The effects of the unconventional monetary policy (UMP) measures undertaken by the U.S. Federal Reserve (and other major central banks) remain a crucial topic for research. This paper investigates their effects on the anchoring of long-term inflation expectations, a key dimension of UMP that has been largely overlooked. Our analysis provides two key insights. First, the anchoring of inflation expectations deteriorated significantly since late 2008. Second, the expansion of the Fed JEL Classification: E43, E44, C52, C55
C55|What drives export market shares? It depends! An empirical analysis using Bayesian Model Averaging|"What drives external performance of countries? This is a recurring question in academia and policy. The factors underlying export growth are receiving great attention, as countries struggle to grow out of the crisis by increasing exports and as protectionist discourses take foot again. Despite decades of debates, it is still unclear what the drivers of external performance are and, importantly, which ones policy makers can influence. We use Bayesian Model Averaging in a panel setting to investigate the drivers of export market shares of 25 EU countries, considering a wide range of traditional indicators along with novel ones developed within the CompNet. We find that export market share growth is linked to different factors in the old and new EU Member States, with one exception: for both groups, competitive pressures from China have strongly affected export performance since the early 2000s. In the case of the old EU Member States, investment, the quality of institutions and liquidity available to firms also appear to play a role. For the new EU Member States, labour and total factor productivity are particularly important, while inward FDI matters more than domestic investment. Price competitiveness does not seem to play a very important role in either set of countries: relative export prices do show correlation with export performance for the new EU Member States, but only when they are adjusted for quality. Our results point to the importance of considering the ""exporting stage"" of a country when discussing export-enhancing policies."
C55|Does Governance Facilitate Foreign Direct Investment in Developing Countries?|Using panel data analysis, it is an attempt to estimates the significance of governance on foreign direct investment (FDI) for a sample of 80 developing countries from 1998 to 2014. For exploring the relationship, the paper has used the Kaufman et al. (2003) interpretation concerning the governance. Generalized least square (GLS), feasible GLS (FGLS), pooled ordinary least squares (OLS), random effect, fixed effect, poisson regression, praiswinsten, generalized method of movement and generalized estimating equation method are utilizing for estimates the importance of governance for facilitating FDI. According to the OLS method, for the governance variables the coefficient implies that a one standard deviation improvement in voice and accountability, political stability and absence of violence, government effectiveness, regulatory qualities, rules of law and control of corruption increases FDI by 29.4%, 29.2%, 28.6%, 20.5%, 23.1% and 23.6% respectively.
C55|Searching for empirical linkages between demographic structure and economic growth|Demographic structure could affect economic growth through many channels. However, little is known about how demographic structure affects economic growth since no study has examined an extensive collection of channels through which demographic structure could affect economic growth in a single context. This paper overcomes this limitation by examining 45 potential mediating variables between demographic structure and economic growth. A causal search algorithm is used to identify channels through which demographic structure affects economic growth. Our results suggest that demographic structure affects economic growth differently between developed and developing countries. For developed countries, we find that an increase in the share of middle-aged workers has a positive effect on economic growth through institutions, investment and education channels. On the other hand, an increase in the share of the senior population has a negative effect on economic growth through institutions and investment channels. For developing countries, we find (but with weak evidence) that an increase in the share of young workers has a negative effect on economic growth through investment, financial market development and trade channels.
C55|Forecasting the realized range-based volatility using dynamic model averaging approach|In this study, we forecast the realized range-based volatility (RRV) using the heterogeneous autoregressive realized range-based volatility (HAR-RRV) model and its various extensions, which are called HAR-RRV-type models. We first consider the time-varying property of those models’ parameters using the dynamic model averaging (DMA) approach and evaluate the forecasting performance of three types: individual HAR-RRV-type models, combined models with constant weights, and combined models with time-varying weights. Our out-of-sample empirical results show that combined models with time-varying weights can not only generate more accurate forecasts, but also beat individual models and combined models with constant weights.
C55|Functional time series forecasting with dynamic updating: An application to intraday particulate matter concentration|Environmental data often take the form of a collection of curves observed sequentially over time. An example of this includes daily pollution measurement curves describing the concentration of a particulate matter in ambient air. These curves can be viewed as a time series of functions observed at equally spaced intervals over a dense grid. The nature of high-dimensional data poses challenges from a statistical aspect, due to the so-called “curse of dimensionality”, but it also poses opportunities to analyze a rich source of information to better understand dynamic changes at short time intervals. Statistical methods are introduced and compared for forecasting one-day-ahead intraday concentrations of particulate matter; as new data are sequentially observed, dynamic updating methods are proposed to update point and interval forecasts to achieve better accuracy. These forecasting methods are validated through an empirical study of half-hourly concentrations of airborne particulate matter in Graz, Austria.
C55|Forecasting oil and stock returns with a Qual VAR using over 150years off data|The extant literature suggests that oil price, stock price and economic activity are all endogenous and the linkages between these variables are nonlinear. Against this backdrop, the objective of this paper is to use a Qualitative Vector Autoregressive (Qual VAR) to forecast (West Texas Intermediate) oil and (S&P500) stock returns over a monthly period of 1884:09 to 2015:08, using an in-sample period of 1859:10–1884:08. Given that there is no data on economic activity at monthly frequency dating as far back as 1859:09, we measure the same using the NBER recession dummies, which in turn, can be easily accommodated in a Qual VAR as an endogenous variable. In addition, the Qual VAR is inherently a nonlinear model as it allows the oil and stock returns to behave as nonlinear functions of their own past values around business cycle turning points. Our results show that, for both oil and stock returns, the Qual VAR model outperforms the random walk model (in a statistically significant way) at all the forecasting horizons considered, i.e., one- to twelve-months-ahead. In addition, the Qual VAR model, also outperforms the AR and VAR models (in a statistically significant manner) at long-run horizons for oil returns, and short- to medium-run horizons for stock returns.
