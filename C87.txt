C87|Dynamic Factor Models in gretl. The DFM package|This package deals with the estimation of dynamic factor models (DFM); for the moment, three factor extraction techniques are available, but we plan to add more in future versions. Further additions will include parameter restrictions.
C87|Iassopack: Model Selection and Prediction with Regularized Regression in Stata|This article introduces lassopack, a suite of programs for regularized regression in Stata. lassopack implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso and post-estimation OLS. The methods are suitable for the high-dimensional setting where the number of predictors p may be large and possibly greater than the number of observations, n. We offer three different approaches for selecting the penalization ('tuning') parameters: information criteria (implemented in lasso2), K-fold cross-validation and h-step ahead rolling cross-validation for cross-section, panel and time-series data (cvlasso), and theory-driven ('rigorous') penalization for the lasso and square-root lasso for cross-section and panel data (rlasso). We discuss the theoretical framework and practical considerations for each approach. We also present Monte Carlo results to compare the performance of the penalization approaches.
C87|xtserialpm: A portmanteau test for serial correlation in a linear panel model|We introduce the command xtserialpm to perform the portmanteau test developed in Jochmans (2019). The procedure tests for serial correlation in the errors of a linear panel model after estimation of the regression coefficients by the within-group estimator. The test is different from the test of Inoue and Solon (2006) that is performed by xtistest (Wursten 2018) in that it allows for heteroskedasticity. In simulations documented below, xtserialpm is found to provide a much more powerful test than xtistest. xtserialpm can deal with unbalanced panel data.
C87|twexp and twgravity: Estimating exponential regression models with two-way fixed effects|We introduce the commands twexp and twgravity that implement the estimators developed in Jochmans (2017) for exponential regression models with two-way fixed effects. twexp is applicable to generic n x m panel data. twgravity is written for the special case where the data is a cross-section on dyadic interactions between n agents. A prime example of the latter is cross-sectional bilateral trade data, where the model of interest is a gravity equation with importer and exporter effects. Both twexp and twgravity can deal with data where n and m are large, that is, the case of many fixed effects. They make use of Mata and are very fast to execute.
C87|Projections on the sustainability of the pension system in Romania|The social pension system is a matter of particular complexity for any country and national authorities. National and international statistical and forecasting institutions, as well as research institutions, assume the complexity of the pension system as a major scientific and professional challenge for identifying social protection phenomena and designing a sustainable pension system.In this context, the Institute of Financial Studies (ISF) in Bucharest takes over this rebellion of concerns, proposing a series of studies on the sustainability of the Romanian pension system. Fortunately, our intention is to support previous studies conducted by demographics and social protection teams, as well as projections freely provided by relevant bodies (Eurostat, US Census Bureau, INS, CNSP, CNPP, EFOR, etc.).In this first ISF study we summarize, in the first part, the specialized literature, especially Romanian, with approaches to the specificities of pensions in Romania compared to the European countries, in the second part, the demographic evolution and tendencies in Romania, in the third part and fourth, the projections on the number of pensioners versus the number of taxpayers and, respectively, the projection of the financial balance of the pension system in Romania. Our studyassumes, with appreciation, the updating of the many previous projections of the established institutions and the attempt to explain contextually demographic and social protection phenomena.
C87|Estimating long run e ects in models with cross-sectional dependence using xtdcce2|This paper describes how to estimate long run effects in a large heterogeneous panel data model with cross sectional dependence in Stata using the user written command xtdcce2. It builds on Chudik et al. (2016) and explains how to estimate models using the CS-DL and CS-ARDL estimator. In addition it includes a method how to estimate an error correction model.
C87|The transmission of unconventional monetary policy to bank credit supply: evidence from the TLTRO|We assess the transmission of the Targeted Longer-Term Refinancing Operations (TLTRO) to the bank credit supply for the Euro area (2014:05-2018:01) and for Portugal (2011:01-2018:01), using a panel data setup. For the Euro area, we find a positive relationship between the TLTRO and the amount of credit granted to the real economy. For the vulnerable countries, the effects of the TLTRO on the stock of credit increased from 2016 to 2017. Among the group of small banks, the effects are stronger in less vulnerable countries. We also find that competition has no statistically significant impact on the transmission of the TLTRO to the bank credit supply for the Euro area. For Portugal, using a difference-in-differences model, we find no statistically significant impact of the TLTRO on credit granted by banks. Finally, bidding banks set lower interest rates than non-bidding banks and the difference seems to be larger in 2017. In Portugal, the effects of the TLTRO on loan interest rates also increased from 2016 to 2017 and are stronger for small banks.
C87|lassopack: Model selection and prediction with regularized regression in Stata|This article introduces lassopack, a suite of programs for regularized regression in Stata. lassopack implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso and post-estimation OLS. The methods are suitable for the high-dimensional setting where the number of predictors $p$ may be large and possibly greater than the number of observations, $n$. We offer three different approaches for selecting the penalization (`tuning') parameters: information criteria (implemented in lasso2), $K$-fold cross-validation and $h$-step ahead rolling cross-validation for cross-section, panel and time-series data (cvlasso), and theory-driven (`rigorous') penalization for the lasso and square-root lasso for cross-section and panel data (rlasso). We discuss the theoretical framework and practical considerations for each approach. We also present Monte Carlo results to compare the performance of the penalization approaches.
C87|Estimating Selection Models without Instrument with Stata|This article presents the eqregsel command for implementing the estimation and bootstrap inference of sample selection models via extremal quantile regression. The command estimates a semiparametric sample selection model without instrument or large support regressor, and outputs the point estimates of the homogenous linear coefficients, their bootstrap standard errors, as well as the p-value for a specification test.
C87|Estimating Selection Models without Instrument with Stata|This article presents the eqregsel command for implementing the estimation and bootstrap inference of sample selection models via extremal quantile regression. The command estimates a semiparametric sample selection model without instrument or large support regressor, and outputs the point estimates of the homogenous linear coefficients, their bootstrap standard errors, as well as the p-value for a specification test.
C87|Microeconometric Dynamic Panel Data Methods: Model Specification and Selection Issues|A motivated strategy is presented to find step by step an adequate model specification and a matching set of instrumental variables by applying the programming tools provided by the Stata package Xtabond2. The aim is to implement generalized method of moment techniques such that useful and reasonably accurate inferences are extracted from an observational panel data set on a single microeconometric structural presumably dynamic behavioral relationship. In the suggested specification search three comprehensive heavily interconnected goals are pursued, namely: (i) to include all the relevant appropriately transformed possibly lagged regressors, as well as any interactions between these if it is required to relax the otherwise very strict homogeneity restrictions on the dynamic impacts of the explanatories in standard linear panel data models; (ii) to correctly classify all regressors as either endogenous, predetermined or exogenous, as well as being either effect-stationary or effect-nonstationary, implying which internal variables could represent valid and relatively strong instruments; (iii) to enhance the accuracy of inference in finite samples by omitting irrelevant regressors and by profitably reducing the space spanned by the full set of available internal instruments. For the various tests which trigger the decisions to be made in the sequential selection process the relevant considerations are spelled out to interpret the magnitude of p-values. Also the complexities to establish and interpret the ultimately established dynamic impacts are explained. Finally the developed strategy is applied to a classic data set and is shown to yield new insights.
C87|User-Specified General-to-Specific and Indicator Saturation Methods|General-to-Specific (GETS) modelling provides a comprehensive, systematic and cumulative approach to modelling that is ideally suited for conditional forecasting and counterfactual analysis, whereas Indicator Saturation (ISAT) is a powerful and flexible approach to the detection and estimation of structural breaks (e.g. changes in parameters), and to the detection of outliers. To these ends, multi-path backwards elimination, single and multiple hypothesis tests on the coefficients, diagnostics tests and goodness-of-fit measures are combined to produce a parsimonious final model. In many situations a specific model or estimator is needed, a specific set of diagnostics tests may be required, or a specific fit criterion is preferred. In these situations, if the combination of estimator/model, diagnostics tests and fit criterion is not offered by publicly available software, then the implementation of user-specified GETS and ISAT methods puts a large programming-burden on the user. Generic functions and procedures that facilitate the implementation of user-specified GETS and ISAT methods for specific problems can therefore be of great benefit. The R package gets, version 0.20 (September 2019), is the first software - both inside and outside the R universe - to provide a complete set of facilities for user-specified GETS and ISAT methods: User-specified model/estimator, user-specified diagnostics and user-specified goodness-of-fit criteria. The aim of this article is to illustrate how user-specified GETS and ISAT methods can be implemented.
C87|The Transmission of Unconventional Monetary Policy to Bank Credit Supply: Evidence from the TLTRO|We assess the transmission of the Targeted Longer-Term Refinancing Operations (TLTRO) to the bank credit supply for the Euro area (2014:05-2018:01) and for Portugal (2011:01-2018:01), using a panel data setup. For the Euro area, we find a positive relationship between the TLTRO and the amount of credit granted to the real economy. For the vulnerable countries, the effects of the TLTRO on the stock of credit increased from 2016 to 2017. Among the group of small banks, the effects are stronger in less vulnerable countries. We also find that competition has no statistically significant impact on the transmission of the TLTRO to the bank credit supply for the Euro area. For Portugal, using a difference-in-differences model, we find no statistically significant impact of the TLTRO on credit granted by banks. Finally, bidding banks set lower interest rates than non-bidding banks and the difference seems to be larger in 2017. In Portugal, the effects of the TLTRO on loan interest rates also increased from 2016 to 2017 and are stronger for small banks.
C87|Data Science for Entrepreneurship Research : Studying Demand Dynamics for Entrepreneurial Skills in the Netherlands|The recent rise of big data and artificial intelligence (AI) is changing markets, politics, organizations, and societies. It also affects the domain of research. Supported by new statistical methods that rely on computational power and computer science --- data science methods --- we are now able to analyze data sets that can be huge, multidimensional, unstructured, and are diversely sourced. In this paper, we describe the most prominent data science methods suitable for entrepreneurship research and provide links to literature and Internet resources for self-starters. We survey how data science methods have been applied in the entrepreneurship research literature. As a showcase of data science techniques, based on a dataset of 95% of all job vacancies in the Netherlands over a 6-year period with 7.7 million data points, we provide an original analysis of the demand dynamics for entrepreneurial skills in the Netherlands. We show which entrepreneurial skills are particularly important for which type of profession. Moreover, we find that demand for both entrepreneurial and digital skills has increased for managerial positions, but not for others. We also find that entrepreneurial skills were significantly more demanded than digital skills over the entire period 2012-2017 and that the absolute importance of entrepreneurial skills has even increased more than digital skills for managers, despite the impact of datafication on the labor market. We conclude that further studies of entrepreneurial skills in the general population --- outside the domain of entrepreneurs --- is a rewarding subject for future research.
C87|Estimating and Decomposing Conditional Average Treatment Effects: The Smoking Ban in England|We develop a practical method for estimating and decomposing conditional average treatment effects using locally-weighted regressions. We illustrate with an application to the smoking ban in England using a regression discontinuity design, based on Health Survey for England data. We estimate average treatment effects conditional on socioeconomic status and decompose these effects by smoking location. Results show, the ban had no effect on the level of active smoking, but significantly reduced average exposure to second-hand smoke among non-smokers by 1.38 hours per week. Our method reveals a complex relationship between socioeconomic status and the effect on passive smoking. Decomposition analysis shows that these effects stem primarily from exposure reductions in pubs, but also from workplace exposure reductions for high socioeconomic status individuals.
C87|The SVAR addon for gretl|The SVAR addon is a collection of gretl functions to estimate Structural Vector Autoregressions (SVARs) and to conduct inference on the resulting magnitudes such as the impulse response functions and short-run or long-run impact matrices. For the purpose of identifying the structural shocks short-run as well as long-run restrictions are supported, including those related to the cointegration properties in the case of non-stationary systems. For the stationary case a dialog-driven graphical interface is also offered. Inference can be based on the bootstrap, optionally using a bias correction as suggested in the literature. This documentation explains the addon's usage, capabilites and limitations, and provides some necessary econometric methodological background (version 1.32).
C87|Advice on using heteroscedasticity based identification|Lewbel (2012) provides a heteroscedasticity based estimator for linear regression models containing an endogenous regressor when no external instruments or other such information is available. The estimator is implemented in the Stata module ivreg2h by Baum and Schaffer (2012). This note gives some advice and instructions to researchers who want to use this estimator.
C87|Customizing Stata graphs made easy|The overall look of Stata's graphs is determined by so-called scheme files. Scheme files are system components, that is, they are part of the local Stata installation. In this note I argue that style settings deviating from default schemes should be part of the script producing the graphs, rather than being kept in separate scheme files, and I present a simple tool called -grstyle- that supports such practice.
C87|Customizing Stata graphs made even easier|In Jann (2017; University of Bern Social Sciences Working Paper No. 29) I presented a command called -grstyle- that simplifies changing the default look of Stata graphs. The command, however, still relies on idiosyncratic scheme file syntax, which may not be well known to many users. In this paper I therefore present an extension called -grstyle set- that automizes the creation of sets of scheme file entries for a number of potentially useful adjustments, without much typing and without requiring much knowledge about scheme file syntax. Covered topics include, for example, the rendering of the background and coordinate system, the placement and look of the legend, the assignment of colors, symbols, and line patterns, and the assignment of relative or absolute sizes.
C87|Color palettes for Stata graphics|In this article, I introduce the colorpalette command, which provides many color palettes and color generators for use in Stata graphics. It supports color palettes from official Stata’s graph schemes, a selection of palettes that have been proposed by users, standard collections such as the ColorBrewer or D3.js palettes, and HSV and HCL color generators. As a by-product, I also introduce commands for marker-symbol and line-pattern palettes.
C87|Intertemporal Similarity of Economic Time Series|This paper adapts the non-parametric Dynamic Time Warping (DTW) technique in an application to examine the temporal alignment and similarity across economic time series. DTW has important advantages over existing measures in economics as it alleviates concerns regarding a pre-defined fixed temporal alignment of series. For example, in contrast to current methods, DTW can capture alternations between leading and lagging relationships of series. We illustrate DTW in a study of US states’ business cycles around the Great Recession, and find considerable evidence that temporal alignments across states dynamic. Trough cluster analysis, we further document state-varying recoveries from the recession.
C87|Seasonal adjustment of time series and calendar influence on economic activity|This paper describes the process of the seasonal adjustment of data time series for Croatia, a process that involves cooperation between the Croatian National Bank and the Croatian Bureau of Statistics. The paper shows individual steps of the process, explains calendar effects, describes the revision policy for seasonally adjusted data and presents the seasonal adjustment of selected main monthly indicators of economic activity in the Republic of Croatia: industrial production, the volume of construction works and retail trade turnover. Working-day effect was identified for all indicators; leap year effect was identified for all but the volume of construction works, i.e. the Easter effect for retail trade turnover only. The described assumptions and limitations of the models applied are useful to end-users for the purpose of a better understanding of the published data and their use in further analysis.
C87|Anchor point selection: An approach for anchoring without anchor items|For detecting differential item functioning (DIF) between two groups of test takers, their item parameters need to be aligned in some way. Typically this is done by means of choosing a small number of so called anchor items. Here we propose an alternative strategy: the selection of an anchor point along the item parameter continuum, where the two groups best overlap. We illustrate how the anchor point is selected by means of maximizing an inequality criterion. It performs equally well or better than established approaches when treated as an anchoring technique, but also provides additional information about the DIF structure through its search path. Another distinct property of this new method is that no individual items are flagged as anchors. This is a major difference to traditional anchoring approaches, where flagging items as anchors implies - but does not guarantee - that they are DIF free, and may lull the user into a false sense of security. Our method can be viewed as a generalization of the search space of traditional anchor selection techniques and can shed new light on the practical usage as well as on the theoretical discussion on anchoring and DIF in general.
C87|Modelling Long Range Dependence and Non-linearity in the Infant Mortality Rates of Africa Countries|The Infant Mortality Rates in 34 sub-Saharan countries are examined in this paper by means of focusing on the degree of persistence and non-linearities. The results indicate that half of the countries examined display non-linearities and the orders of integration are extremely large in all cases, being around 2 in the majority of them. Looking at the growth rate series, we observe significant negative trends in three countries: Chad, Equatorial Guinea and Mozambique, and evidence of mean reversion, and thus, transitory shocks, in the cases of Lesotho, Rwanda, Botswana and Mozambique. As expected, time dynamics of IMR and its growth rates are expected to be persistent in order to ascertain the decline in mortality rates. Serious government interventions are therefore required in health management of infants in those listed countries.
C87|Data Science for Institutional and Organizational Economics|To which extent can data science methods – such as machine learning, text analysis, or sentiment analysis – push the research frontier in the social sciences? This essay briefly describes the most prominent data science techniques that lend themselves to analyses of institutional and organizational governance structures. We elaborate on several examples applying data science to analyze legal, political, and social institutions and sketch how specific data science techniques can be used to study important research questions that could not (to the same extent) be studied without these techniques. We conclude by comparing the main strengths and limitations of computational social science with traditional empirical research methods and its relation to theory.
C87|Data Science for Institutional and Organizational Economics|To which extent can data science methods – such as machine learning, text analysis, or sentiment analysis – push the research frontier in the social sciences? This essay briefly describes the most prominent data science techniques that lend themselves to analyses of institutional and organizational governance structures. We elaborate on several examples applying data science to analyze legal, political, and social institutions and sketch how specific data science techniques can be used to study important research questions that could not (to the same extent) be studied without these techniques. We conclude by comparing the main strengths and limitations of computational social science with traditional empirical research methods and its relation to theory.
C87|Basic Stata graphics for economics students|This paper provides an introduction to the main types of graph in Stata that economics students might need. It covers univariate discrete and continuous variables, bivariate distributions, some simple time plots and methods of visualising the output from estimating models. It shows a small number of the many options available and includes references to further resources.
C87|Basic Stata Graphics for Economics Students|This paper provides an introduction to the main types of graph in Stata that economics students might need. It covers univariate discrete and continuous variables, bivariate distributions, some simple time plots and methods of visualising the output from estimating models. It shows a small number of the many options available and includes references to further resources.
C87|distcomp: Comparing distributions|WP 18-17 has now been replaced by WP 19-08.
C87|Theory and Practice of Testing for a Single Structural Break in Stata|The major objective of this paper is to demonstrate, theoretically and empirically, the test of a single structural break/change. Failure to address a structural break can lead to forecasting errors and the general unreliability of a model. Three approaches of testing for structural change are discussed using data from Johnston et al. (1997, p.130) on Stata 14 software. The first approach assesses whether there is a structural break in parameters (slope and intercept) while the second and third assess whether there is a break in slope and intercept respectively. The Residual Sum of Squares (RSS) for the restricted and unrestricted models are established to necessitate the use of an F-test in making inferences. According to the first approach, a structural break exists at 5% level of significance. This result is confirmed by the Chow test. The second and third approaches establish that the structural break is from the intercept and not the slope. These results are also affirmed by the Chow test. Furthermore, all these results, from the first to the third approach, are confirmed by an alternative approach which relies on the knowledge that . Therefore, the dependent variable is not affected by the policy change on the explanatory variable but it is mainly affected by the basic unobserved qualitative characteristics of the two sub-periods. For further analysis, it is recommended that a unit root test be conducted using the Zivot-Andrews test. This test has been established as the panacea for the interplay between unit root and structural changes.
C87|Structural Change in (Economic) Time Series|Methods for detecting structural changes, or change points, in time series data are widely used in many fields of science and engineering. This chapter sketches some basic methods for the analysis of structural changes in time series data. The exposition is confined to retrospective methods for univariate time series. Several recent methods for dating structural changes are compared using a time series of oil prices spanning more than 60 years. The methods broadly agree for the first part of the series up to the mid-1980s, for which changes are associated with major historical events, but provide somewhat different solutions thereafter, reflecting a gradual increase in oil prices that is not well described by a step function. As a further illustration, 1990s data on the volatility of the Hang Seng stock market index are reanalyzed.
C87|Applications For Businesses That Uses Relational Databases:|The paper presents a database production model designed as a warehouse star that contain dimensions like deposits, raw materials, stocks, products, producer, locations, time and a fact table with foreign keys and measures. This model optimize the activity of a business based on a production activity in the way that it can store large amount of data in a historical way that can be the base for future scenarios with key values changed by the decision maker. The decision maker analyses a large spectrum of reports and choose what indicators to observe and what measures to display and so it’s easy to decide based on large amount of data and trends. Database applications for business improve the efficiency in managing large quantity of data in the sense for storage, updates, queries, interaction with the users and also getting answers through reports. The schema specific to a database is very flexible and permits adding or removing columns and also adding and removing entities. This feature is very useful when the relational database schema is transformed in a data warehouse shaped as a star with dimensions and a fact table. This model permits advanced queries and the usage of rollup and drill down objects specific to the business intelligence tools that offer quick responses to the complex answers. To a production business the choice of a database application designed and implemented as data warehouse star model, bennefits from all the advantage of storage and also a superior and complex tool for building queries.
C87|The R Package MitISEM: Efficient and Robust Simulation Procedures for Bayesian Inference| This paper presents the R package MitISEM (mixture of t by importance sampling weighted expectation maximization) which provides an automatic and flexible two-stage method to approximate a non-elliptical target density kernel - typically a posterior density kernel - using an adaptive mixture of Student t densities as approximating density. In the first stage a mixture of Student t densities is fitted to the target using an expectation maximization algorithm where each step of the optimization procedure is weighted using importance sampling. In the second stage this mixture density is a candidate density for efficient and robust application of importance sampling or the Metropolis-Hastings (MH) method to estimate properties of the target distribution. The package enables Bayesian inference and prediction on model parameters and probabilities, in particular, for models where densities have multi-modal or other non-elliptical shapes like curved ridges. These shapes occur in research topics in several scientific fields. For instance, analysis of DNA data in bio-informatics, obtaining loans in the banking sector by heterogeneous groups in financial economics and analysis of education's effect on earned income in labor economics. The package MitISEM provides also an extended algorithm, 'sequential MitISEM', which substantially decreases computation time when the target density has to be approximated for increasing data samples. This occurs when the posterior or predictive density is updated with new observations and/or when one computes model probabilities using predictive likelihoods. We illustrate the MitISEM algorithm using three canonical statistical and econometric models that are characterized by several types of non-elliptical posterior shapes and that describe well-known data patterns in econometrics and finance. We show that MH using the candidate density obtained by MitISEM outperforms, in terms of numerical efficiency, MH using a simpler candidate, as well as the Gibbs sampler. The MitISEM approach is also used for Bayesian model comparison using predictive likelihoods.
C87|Machine learning at central banks|We introduce machine learning in the context of central banking and policy analyses. Our aim is to give an overview broad enough to allow the reader to place machine learning within the wider range of statistical modelling and computational analyses, and provide an idea of its scope and limitations. We review the underlying technical sources and the nascent literature applying machine learning to economic and policy problems. We present popular modelling approaches, such as artificial neural networks, tree-based models, support vector machines, recommender systems and different clustering techniques. Important concepts like the bias-variance trade-off, optimal model complexity, regularisation and cross-validation are discussed to enrich the econometrics toolbox in their own right. We present three case studies relevant to central bank policy, financial regulation and economic modelling more widely. First, we model the detection of alerts on the balance sheets of financial institutions in the context of banking supervision. Second, we perform a projection exercise for UK CPI inflation on a medium-term horizon of two years. Here, we introduce a simple training-testing framework for time series analyses. Third, we investigate the funding patterns of technology start-ups with the aim to detect potentially disruptive innovators in financial technology. Machine learning models generally outperform traditional modelling approaches in prediction tasks, while open research questions remain with regard to their causal inference properties.
C87|Comparing cross‐country estimates of Lorenz curves using a Dirichlet distribution across estimators and datasets|Chotikapanich and Griffiths (Journal of Business and Economic Statistics, 2002, 20(2), 290–295) introduced the Dirichlet distribution to the estimation of Lorenz curves. This distribution naturally accommodates the proportional nature of income share data and the dependence structure between the shares. Chotikapanich and Griffiths fit a family of five Lorenz curves to one year of Swedish and Brazilian income share data using unconstrained maximum likelihood and unconstrained nonlinear least squares. We attempt to replicate the authors' results and extend their analyses using both constrained estimation techniques and five additional years of data. We successfully replicate a majority of the authors' results and find that some of their main qualitative conclusions also hold using our constrained estimators and additional data.
C87|Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R|"Clustered covariances or clustered standard errors are very widely used to account for correlated or clustered data, especially in economics, political sciences, or other social sciences. They are employed to adjust the inference following estimation of a standard least-squares regression or generalized linear model estimated by maximum likelihood. Although many publications just refer to ""the"" clustered standard errors, there is a surprisingly wide variety of clustered covariances particularly due to different flavors of bias corrections. Furthermore, while the linear regression model is certainly the most important application case, the same strategies can be employed in more general models (e.g. for zero-inflated, censored, or limited responses). In R, functions for covariances in clustered or panel models have been somewhat scattered or available only for certain modeling functions, notably the (generalized) linear regression model. In contrast, an object-oriented approach to ""robust"" covariance matrix estimation - applicable beyond lm() and glm() - is available in the sandwich package but has been limited to the case of cross-section or time series data. Now, this shortcoming has been corrected in sandwich (starting from version 2.4.0): Based on methods for two generic functions (estfun() and bread()), clustered and panel covariances are now provided in vcovCL(), vcovPL(), and vcovPC(). These are directly applicable to models from many packages, e.g., including MASS, pscl, countreg, betareg, among others. Some empirical illustrations are provided as well as an assessment of the methods' performance in a simulation study."
C87|A simple command to calculate travel distance and travel time|Obtaining the routing distance between two addresses should not be a hassle with current technology. Unfortunately, this is more complicated than it first seems. Recently, several commands have been implemented for this purpose (traveltime, traveltime3, mqtime, osrmtime), but most of them became obso- lete only a few months after their introduction or appear complicated to use. In this article, we introduce the community-contributed command georoute, which retrieves travel distance and travel time between two points defined either by their addresses or by their geographical coordinates. Compared with other existing com- mands, it is simple to use, efficient in terms of computational speed, and versatile regarding the information that can be provided as input. Copyright 2017 by StataCorp LP.
C87|Testing for Granger causality in panel data|With the development of large and long panel databases, the theory surrounding panel causality evolves quickly, and empirical researchers might find it difficult to run the most recent techniques developed in the literature. In this article, we present the community-contributed command xtgcause, which imple- ments a procedure proposed by Dumitrescu and Hurlin (2012, Economic Modelling 29: 1450–1460) for detecting Granger causality in panel datasets. Thus, it con- stitutes an effort to help practitioners understand and apply the test. xtgcause offers the possibility of selecting the number of lags to include in the model by minimizing the Akaike information criterion, Bayesian information criterion, or Hannan–Quinn information criterion, and it offers the possibility to implement a bootstrap procedure to compute p-values and critical values.
C87|Nowcasting real economic activity in the euro area : Assessing the impact of qualitative surveys|This paper analyses the contribution of survey data, in particular various sentiment indicators, to nowcasts of quarterly euro area GDP. It uses a genuine real-time dataset that is constructed from original press releases in order to transform the actual dataflow into an interpretable flow of news. The latter is defined as the difference between the released values and the prediction of a mixedfrequency dynamic factor model. Our purpose is twofold. First, we aim to quantify the specific value added for nowcasting GDP from a set of heterogeneous data releases including not only sentiment indicators constructed by Eurostat, Markit, the National Bank of Belgium, IFO, ZEW, GfK or Sentix, but also hard data regarding industrial production or retail sales in the aggregate euro area and individually in some of the largest euro area countries. Second, our quantitative analysis is used to draw up an overall ranking of the indicators, on the basis of their average contribution to updates of the nowcast. Among the survey indicators, we ??nd the strongest impact for the Markit Manufacturing PMI and the Business Climate Indicator in the euro area, and the IFO Business Climate and IFO Expectations in Germany. The widely monitored consumer con??dence indicators, on the other hand, typically do not lead to signi??cant revisions of the nowcast. In addition, even if euro area industrial production is a relevant predictor, hard data generally contribute less to the nowcasts: they may be more closely correlated with GDP but their relatively late availability implies that they can to a large extent be anticipated by nowcasting on the basis of survey data and, hence, their ‘news’ component is smaller. Finally, we also show that, in line with the previous literature, the NBB’s own business confidence indicator appears to be useful for predicting euro area GDP. The prevalence of survey data remains also under a counterfactual scenario in which hard data are released without any delay. This finding confirms that, in addition to being available in a more timely manner, survey data also contain relevant information that does not seem to be captured by hard data.
C87|Comparison of methods of data mining techniques for the predictive accuracy|This paper is based on the work of Yeh, Lien (2009). In the paper, authors used the payment data set from the important bank in Taiwan. To build a model, the whole sample was divided in two subsets - training and testing sets - so each model could be trained on the first one and then be evaluated on the second. Our motivation was to see whether the same result could be obtained if we repeatedly apply the models to the different data sets. To do so, Monte Carlo simulation was implemented to generate these sets.
C87|Eficiencia técnica en la producción de café en Nicaragua: Un análisis de fronteras estocásticas<BR>[Technical efficiency in coffee production: a stochastic frontier analysis for Nicaragua]|This article analyses the technical efficiency of coffee production in Nicaragua. We apply a stochastic frontier model to estimate the technical efficiency which reaches 60%; this means that Nicaraguan coffee producers have chances to improve the way they get things done. This level of efficiency prevents Nicaragua from capturing 340 million dollars for coffee exports. At the end of 2015, revenues from coffee exports represented 3.09 percent of gross domestic product (GDP), if 100 percent productive efficiency had been achieved, the relative importance of this item would have represented 5.77 percent of GDP. A counterfactual analysis shows the gains that would be derived from achieving technical efficiency.
C87|The Programmable Economy: Envisaging an Entire Planned Economic System as a Single Computer through Blockchain Networks|Since the dawn of the concept of nation-states, many nations have been planning their economies to increase people’s prosperity and standard of living. All economies have a centralized feature where decisions are taken. But data collection and plan implementation has been cumbersome because of the manual nature of economic planning. Centralized systems, even when digitized, are prone to single point failures. Controlled Blockchains allow for an economic system to be decentralized, yet having central supervision i.e. quasi-decentralization. This paper deals with shifting a national economy to a network of Blockchains and creating a Programmable Economy (P.E) where the whole economic system behaves like a single computer, taking in certain inputs and providing the desired outputs. We discuss the various factors that are needed to bring about such a transformation. We also analyse the impact this will have on various aspects of the economy and people’s lives. Finally the paper concludes by summarizing the purpose, methodology and impact of a Programmable Economy (P.E).
C87|Expert System and Heuristics Algorithm for Cloud Resource Scheduling|Rule-based scheduling algorithms have been widely used on cloud computing systems and there is still plenty of room to improve their performance. This paper proposes to develop an expert system to allocate resources in cloud by using Rule based Algorithm, thereby measuring the performance of the system by letting the system adapt new rules based on the feedback. Here performance of the action helps to make better allocation of the resources to improve quality of services, scalability and flexibility. The performance measure is based on how the allocation of the resources is dynamically optimized and how the resources are utilized properly. It aims to maximize the utilization of the resources. The data and resource are given to the algorithm which allocates the data to resources and an output is obtained based on the action occurred. Once the action is completed, the performance of every action is measured that contains how the resources are allocated and how efficiently it worked. In addition to performance, resource allocation in cloud environment is also considered.
C87|The Management of Businesses through Information Systems|The paper presents ways to manage businesses through information systems that can store data into databases and makes possible the creation of complex queries which will help managers to make an appropriate decision based on facts that express the business environment.Unlike conventional systems based on files for automatic data processing, information stored in databases so that data is not duplicated. However, sometimes, to achieve high performance in terms of response time is accepting some redundancy data. Sharing data refers not only to the aspect of ensuring multiple users access the same data, but also the opportunity to develop applications without modifying the database structure. Sharing problems arise at a higher level for DBMS enabling networking sites. An information system for business that is based on databases can resolve the issues of manipulation of large quantities of data that are provided from different departments such as supply, production, accounting, marketing and other areas. Based on these data can be built complex reports that allow the managers to create different scenarios with various inputs and multiple outputs on which decisions may be taken. Analysis and complex queries are provided by tools that extract certain amount of data in specific periods of time and so it is possible to create forecasts and scenarios that make the process of decision more efficient.
C87|Spatial panel-data models using Stata|xsmle is a new user-written command for spatial analysis. We consider the quasi–maximum likelihood estimation of a wide set of both fixed- and random-effects spatial models for balanced panel data. xsmle allows users to han- dle unbalanced panels using its full compatibility with the mi suite of commands, use spatial weight matrices in the form of both Stata matrices and spmat objects, compute direct, indirect, and total marginal effects and related standard errors for linear (in variables) specifications, and exploit a wide range of postestimation features, including the panel-data case predictors of Kelejian and Prucha (2007, Regional Science and Urban Economics 37: 363–374). Moreover, xsmle allows the use of margins to compute total marginal effects in the presence of nonlinear specifications obtained using factor variables. In this article, we describe the command and all of its functionalities using simulated and real data.
C87|Creating HTML or Markdown documents from within Stata using webdoc|In this article, I discuss the use of webdoc for creating HTML or Markdown documents from within Stata. The webdoc command provides a way to embed HTML or Markdown code directly in a do-file and automate the inte- gration of results from Stata in the final document. The command can be used, for example, to create a webpage documenting your data analysis, including all Stata output and graphs. More generally, the command can be used to create and maintain a website that contains results computed by Stata.
C87|Rolling window correlation procedure|"RollCorr RATS procedure calculates a rolling window correlation between two series and saves results in a series . It allows leads and lags in correlation, different window size, and it can plot rolling window correlation on a graph. Rolling window correlation can be computed as a backward, centered, or forward correlation. To use the procedure, it has to be saved in a procedure library, usually ""C:...\Estima\WinRATS version"" or it has to be called from the program using ""source rollcorr.src""."
C87|Small Sample Properties Of Panel Cointegration Tests In The Presence Of Structural Change|Panel tests for non-stationarity are increasingly popular in recent years, also for macroeconomic data. Given that panels used in practice are rather small, there is a need of exploring the small sample properties of the tests in various cases. For annual data the N dimension of the panels is limited to no more than 25, and spatial dimension is also limited, because of the nature of studied entities. So, the main concern of researchers remains the relatively small panels – theoretical critical values should be applied with caution, given that they are taken in limits. An additional feature of macroeconomic panels are cycles – with business cycles one can expect even more than one structural break in the series, because up to 3 major cycles can fit in a series with T=25. In the paper small sample properties for the three “group” statistics of Pedroni (1999) under presence of structural breaks are explored. A set of Monte Carlo experiments is applied to processes with a structural break for three possible break dates at 0.3T, 0.5T and 0.7T. Tested for power against the general alternative.
C87|European Union's Standards and Food Exports from Africa: Implications of the Comprehensive Africa Agricultural Development Program for Coffee and Fish|The preponderance and stringency of product standards have implications for global trade, especially for developing countries. Despite the importance of this issue to Africa, only a few empirical studies exist in the area. It is on this basis that this study draws its objective, which is to investigate the impact of EU standards on Africa's exports in relation to the Comprehensive Africa Agricultural Development Programme. A two-step Heckman model is adopted using mostly unexploited standards data from Perinorm International. A high-value commodity (fish) and traditional cash crop (coffee) are selected. The findings show that at the extensive margin of export, standards are trade-inhibiting for fish and coffee. At the intensive margin, the standards are trade-inhibiting in coffee exports while trade-enhancing in fish exports.
C87|Panel times series. A review of methodological developments|The document focuses on the econometric treatment of macro panels, known in literature as panel time series. This new approach rejects the assumption of slopes’ homogeneity and handles nonstationarity. It also recognizes that the presence of cross-section dependence (CSD), i.e. some correlation structure in the error term between units due to the presence of unobservable common factors, squanders efficiency gains by operating with a panel. This led to a new set of estimators known in literature as Common Correlated Effect (CCE), which essentially consists of increasing the model to be estimated by adding the averages of the individuals in each time t, of both the dependent variable and the specific regressors of each individual. Finally, two Stata codes developed for the evaluation and treatment of the cross-section dependence are presented.
C87|Estimating Lorenz and concentration curves in Stata|Lorenz and concentration curves are widely used tools in inequality research. In this paper I present a new Stata command called -lorenz- that estimates Lorenz and concentration curves from individual-level data and, optionally, displays the results in a graph. The -lorenz- command supports relative as well as generalized, absolute, unnormalized, or custom-normalized Lorenz or concentration curves, and provides tools for computing contrasts between different subpopulations or outcome variables. Variance estimation for complex samples is fully supported.
C87|Macroeconomic Performances Under Inflation Targeting. The Case Of Romania|This paper aims to analyze the macroeconomic performance of inflation targeting in terms of inflation, output, exchange rates and interest rates behaviour, as well as their volatilities. The study gives an overview of inflation targeting features with focus on Romanian experience. Using econometric models and statistical analysis we highlight several indicators before and after inflation targeting adoption and discuss the performance of the new regime. Another paper’s goal is to evaluate the performance of the monetary policy strategy in terms of transmission mechanism of the monetary policy impulses. With other words, as the monetary policy interest rate is the most important instrument used currently, we aim to measure the performance of inflation targeting regime on inflation by studying the characteristics of interest rate transmission mechanism on inflation. In order to do this, we use the VAR technique, assessing the monetary policy interest rate transmission mechanism. Our paper brings new empirical evidence.
C87|The BEAR toolbox|The Bayesian Estimation, Analysis and Regression toolbox (BEAR) is a comprehensive (Bayesian) (Panel) VAR toolbox for forecasting and policy analysis. BEAR is a MATLAB based toolbox which is easy for non-technical users to understand, augment and adapt. In particular, BEAR includes a user-friendly graphical interface which allows the tool to be used by country desk economists. Furthermore, BEAR is well documented, both within the code as well as including a detailed theoretical and user's guide. BEAR includes state-of-the art applications such as sign and magnitude restrictions, conditional forecasts, Bayesian forecast evaluation measures, Bayesian Panel VAR using different prior distributions (for example hierarchical priors), etc. BEAR is specifically developed for transparently supplying a tool for state-of-the-art research and is planned to be further developed to always be at the frontier of economic research. JEL Classification: C11, C30, C87, E00, F00
C87|The Relationship between Exchange Rate and Inflation: An Empirical Study of Turkey|This paper investigates the relationship between inflation and exchange rate in Turkey. Unlike many empirical studies which make use of the US and Turkey inflation data to test the relationship between inflation and exchange rate in Turkey, this paper employed inflation data of Turkey and the United Kingdom. An ordinary least square (OLS) regression and a simple generalized autoregressive conditional heteroskedasticity (GARCH) model were used to understand the relationship between inflation and exchange rate. The results obtained from OLS regression indicate purchasing power parity (PPP) does not exist in Turkey. However, the existence of ARCH and GARCH in the relationship indicates that the deviations from PPP are not random and follow a certain pattern. Therefore, we conclude that the deviation of PPP might be attributed to certain factors such as transaction cost, government restriction, product specialization or other related factors.
C87|Cost-Benefit Analysis of Renewable Power under Full Subsidy Targeting Law Enforcement Conditions in Iran|Fossil energy extracted resources limitation, on the one hand and fossil energies induced environment pollution, on the other hand, have made renewable energies more attractive, especially for developing countries. Thus international programs and policies such as the UN programs have been considered in line with global sustainable development playing a special role for renewable energy resources. Although, in practice various factors in particular high initial cost and marginal price, not sufficient investment for localization and the associated technologies efficiency enhancement, taking external costs for granted in the economic equations and lack of supportive policies at local, regional and international levels have made renewable energies penetration and development very slow and limited. It is emphasized that rich fossil resources (standing 4th rank in oil reserves and 2nd in gas reserves) existence in Iran has been another giant obstacle for renewable energies development in Iran. Thus executing subsidy targeting policy and Article 44 of Iranian Constitutional law can be a great opportunity for renewable energies development in Iran. Through economically evaluating the renewable power and comparing it with fossil electricity under full subsidy targeting law enforcement conditions, renewable electricity marginal cost sensitivity analysis and various guaranteed renewable energies resources electricity purchase tariff proposal to the government, the researcher in this study hopes that the private sector investor’s opportunity in this industry and subsequently, renewable electricity share increase in Iran energy basket will be provided.
C87|The Analysis of Regional Development on the Basis of Corporate Structures’ Activity|Modern conditions upgrade issues concerning the search of ways to develop and increase activity efficiency of large-scale industrial associations that possess a high concentration of material and scientific resources, and influence significantly both a certain economic sector or region and the country’s development in general. This paper aims to substantiate author’s techniques to define the impact of corporate entities on a regional social-economic sphere. The authors have highlighted main features of Russian corporate entities and possible forms of ownership. Types of regions and their features the consideration of which is necessary to evaluate regional development are presented on the basis of the author’s estimation procedure. The factor analysis made the foundation for a rating assessment of corporate entities impact on the region; it allowed to estimate quantitatively corporations’ activity and the level of regional development at a certain time period. The paper is intended for heads of regions, top-managers, researchers dealing with issues of corporate entities’ and regional economy development.
C87|Composite marginal likelihood estimation of spatial autoregressive probit models feasible in very large samples|Composite Marginal Likelihood (CML) has become a popular approach for estimating spatial probit models. However, for spatial autoregressive specifications the existing brute-force implementations are infeasible in large samples as they rely on inverting the high-dimensional precision matrix of the latent state variable. The contribution of this paper is to provide a CML implementation that circumvents inversion of that matrix and therefore can also be applied to very large sample sizes.
C87|Food safety regulations and fish trade: Evidence from European Union-Africa trade relations|The preponderance of food regulations in international trade and the stringency in standards application has trade effects for Africa. The proliferation of preferential tariffs among trading partners has brought to fore the importance and increasing use of technical regulations in global trade. These regulations have their pros and cons for different economic agents. To this end, this study investigated the export effects of the EU fish standard regulations for Africa in a two-step Helpman et al. (2008) model. The study covers 52 African countries from 1995 to 2012. It finds that, fish standards are trade enhancing at the extensive margins, but not the case at the intensive margins. Thus, the quality standards' institutions must be strengthened in Africa through adequate development of science and technology in order accelerate export intensity.
C87|Size distribution of Portuguese firms between 2006 and 2012|This study aims to describe the size distribution of Portuguese firms, as measured by annual sales and total assets, between 2006 and 2012, giving an economic interpretation for the evolution of the distribution along the time. Three distributions are fitted to data: the lognormal, the Pareto (and as a particular case Zipf) and the Simplified Canonical Law (SCL). We present the main arguments found in literature to justify the use of distributions and emphasize the interpretation of SCL coefficients. Methods of estimation include Maximum Likelihood, modified Ordinary Least Squares in log–log scale and Nonlinear Least Squares considering the Levenberg–Marquardt algorithm. When applying these approaches to Portuguese’s firms data, we analyze if the evolution of estimated parameters in both lognormal power and SCL is in accordance with the known existence of a recession period after 2008. This is confirmed for sales but not for assets, leading to the conclusion that the first variable is a best proxy for firm size.
C87|Q3-D3-Lsa|QuantNet 1 is an integrated web-based environment consisting of different types of statistics-related documents and program codes. Its goal is creating reproducibility and offering a platform for sharing validated knowledge native to the social web. To increase the information retrieval (IR) efficiency there is a need for incorporating semantic information. Three text mining models will be examined: vector space model (VSM), generalized VSM (GVSM) and latent semantic analysis (LSA). The LSA has been successfully used for IR purposes as a technique for capturing semantic relations between terms and inserting them into the similarity measure between documents. Our results show that different model configurations allow adapted similarity-based document clustering and knowledge discovery. In particular, different LSA configurations together with hierarchical clustering reveal good results under M3 evaluation. QuantNet and the corresponding Data-Driven Documents (D3) based visualization can be found and applied under http://quantlet.de. The driving technology behind it is Q3-D3-LSA, which is the combination of “GitHub API based QuantNet Mining infrastructure in R”, LSA and D3 implementation.
C87|xtdcce: Estimating Dynamic Common Correlated Effects in Stata|This article introduces a new Stata command, xtdcce, to estimate a dynamic common correlated effects model with heterogeneous coefficients. The estimation procedure mainly follows Chudik and Pesaran (2015b), in addition the common correlated effects estimator (Pesaran, 2006), as well as the mean group (Pesaran and Smith, 1995) and the pooled mean group estimator (Shin et al., 1999) are supported. Coefficients are allowed to be heterogeneous or homogeneous. In addition instrumental variable regressions and unbalanced panels are supported. The Cross Sectional Dependence Test (CD Test) is automatically calculated and presented in the estimation output. Small sample time series bias can be corrected by jackknife correction or recursive mean adjustment. Examples for empirical applications of all estimation methods mentioned above are given
C87|Score-Based Tests of Differential Item Functioning in the Two-Parameter Model|Measurement invariance is a fundamental assumption in item response theory models, where the relationship between a latent construct (ability) and observed item responses is of interest. Violation of this assumption would render the scale misinterpreted or cause systematic bias against certain groups of people. While a number of methods have been proposed to detect measurement invariance violations, they typically require advance definition of problematic item parameters and respondent grouping information. However, these pieces of information are typically unknown in practice. As an alternative, this paper focuses on a family of recently-proposed tests based on stochastic processes of casewise derivatives of the likelihood function (i.e., scores). These score-based tests only require estimation of the null model (when measurement invariance is assumed to hold), and they have been previously applied in factor-analytic, continuous data contexts as well as in models of the Rasch family. In this paper, we aim to extend these tests to two parameter item response models estimated via maximum likelihood. The tests' theoretical background and implementation are detailed, and the tests' abilities to identify problematic item parameters are studied via simulation. An empirical example illustrating the tests' use in practice is also provided.
C87|A Toolkit for Stability Assessment of Tree-Based Learners|Recursive partitioning techniques are established and frequently applied for exploring unknown structures in complex and possibly high-dimensional data sets. The methods can be used to detect interactions and nonlinear structures in a data-driven way by recursively splitting the predictor space to form homogeneous groups of observations. However, while the resulting trees are easy to interpret, they are also known to be potentially unstable. Altering the data slightly can change either the variables and/or the cutpoints selected for splitting. Moreover, the methods do not provide measures of confidence for the selected splits and therefore users cannot assess the uncertainty of a given fitted tree. We present a toolkit of descriptive measures and graphical illustrations based on resampling, that can be used to assess the stability of the variable and cutpoint selection in recursive partitioning. The summary measures and graphics available in the toolkit are illustrated using a real world data set and implemented in the R package stablelearner.
C87|On the Estimation of Standard Errors in Cognitive Diagnosis Models|Cognitive diagnosis models (CDMs) are an increasingly popular method to assess mastery or nonmastery of a set of fine-grained abilities in educational or psychological assessments. Several inference techniques are available to quantify the uncertainty of model parameter estimates, to compare different versions of CDMs, or to check model assumptions. However, they require a precise estimation of the standard errors (or the entire covariance matrix) of the model parameter estimates. In this article, it is shown analytically that the currently widely used form of calculation leads to underestimated standard errors because it only includes the item parameters but omits the parameters for the ability distribution. In a simulation study, we demonstrate that including those parameters in the computation of the covariance matrix consistently improves the quality of the standard errors. The practical importance of this finding is discussed and illustrated using a real data example.
C87|Using Recursive Partitioning to Account for Parameter Heterogeneity in Multinomial Processing Tree Models|In multinomial processing tree (MPT) models, individual differences between the participants in a study lead to heterogeneity of the model parameters. While subject covariates may explain these differences, it is often unknown in advance how the parameters depend on the available covariates, that is, which variables play a role at all, interact, or have a nonlinear influence, etc. Therefore, a new approach for capturing parameter heterogeneity in MPT models is proposed based on the machine learning method MOB for model-based recursive partitioning. This recursively partitions the covariate space, leading to an MPT tree with subgroups that are directly interpretable in terms of effects and interactions of the covariates. The pros and cons of MPT trees as a means of analyzing the effects of covariates in MPT model parameters are discussed based on a simulation experiment as well as on two empirical applications from memory research. Software that implements MPT trees is provided via the mpttree function in the psychotree package in R.
C87|Assessing Public Spending Efficiency in 20 OECD Countries|This study follows the framework of Afonso, Schuknecht, and Tanzi (2005), aiming to look at the public expenditure of 20 OECD countries for the period 2009-2013, from the perspective of efficiency and assess if these developed countries are performing efficiently compared to each other. Public Sector Performance (PSP) and Public Sector Efficiency (PSE) indicators were constructed and Data Envelopment Analysis was conducted. The results show that the only country that performed on the efficiency frontier is Switzerland. The average input-oriented efficiency score is equal to 0.732. That is, on average countries could have reduced the level of public expenditure by 26.8% and still achieved the same level of public performance. The average output-oriented efficiency score is 0.769 denoting that on average the sample countries could have increased their performance by 23.1% by employing the same level of public expenditure. Key Words : Public Spending, Technical Efficiency, Public Sector Performance (PSP), Data Envelopment Analysis (DEA)
C87|Macro-financial linkages in the Polish economy: combined impulse-response functions in SVAR models|We estimated a structural vector autoregressive (SVAR) model describing the links between a banking sector and a real economy. We proposed a new method to verify robustness of impulse-response functions in a SVAR model. This method applies permutations of the variable ordering in a structural model and uses the Cholesky decomposition of the error covariance matrix to identify parameters. Impulse response functions are computed for all permutations and are then combined. We explored the method in practice by analyzing the macro-financial linkages in the Polish economy. Our results indicate that the combined impulse response functions are more uncertain than those from a single specification ordering but some findings remain robust. It is evident that macroeconomic aggregate shocks and interest rate shocks have a significant impact on banking variables.
C87|Les modèles multiniveaux|"Multilevel models (also called hierarchical or mixed models) have been developed to answer issues raised by data structured by several levels, typically when some individuals share a common context that may affect the considered behaviour. This is for instance the case for pupils in one school, employees in one firm, patients in one hospital. . . The clas- sic questions that are adressed by multilevel models are to highlight the existence of these ""contextual effects"", to quantify in which measure they contribute to explain heterogeneity between individuals and/or simply obtain unbiased estimates of the impact of some individ- ual variables we are interested in. This document presents a first practical introduction of these models. It insists on the details of their concrete implementation by usual statistical softwares (Sas, R, Stata) and on the interpretation that can be done of the results obtained by these methods. It shows two concrete examples corresponding on a variable of interest respectively continous and binary."
C87|General-to-Specific (GETS) Modelling And Indicator Saturation With The R Package Gets|Abstract: This paper provides an overview of the R-package 'gets’, which contains facilities for General-to-Specific (GETS) modelling of the mean and variance of a regression, and Indicator Saturation (IS) methods for the detection and modelling of structural breaks and outliers. The mean can be specified as an autoregressive model with covariates (an 'AR-X' model), and the variance can be specified as an autoregressive log-variance model with covariates (a 'log-ARCH-X' model). The covariates in the two specifications need not be the same, and the classical regression model is obtained as a special case when there is no dynamics, and when there are no covariates in the variance equation. The four main functions of the package are arx, getsm, getsv and isat. The first function estimates an AR-X model with log-ARCH-X errors. The second function undertakes GETS model selection of the mean specification of an arx object. The third function undertakes GETS model selection of the log-variance specification of an arx object. The fourth function undertakes GETS model selection of an indicator saturated mean specification allowing for the detection of structural breaks and outliers. Examples of how LaTeX code of the estimation output can be generated is given, and the usage of two convenience functions for export of results to EViews and STATA are illustrated.
C87|Bias Correction Methods for Dynamic Panel Data Models with Fixed Effects|This paper considers the estimation methods for dynamic panel data (DPD) models with fixed effects which suggested in econometric literature, such as least squares (LS) and generalized method of moments (GMM). These methods obtain biased estimators for DPD models. The LS estimator is inconsistent when the time dimension (T) is short regardless of the cross sectional dimension (N). Although consistent estimates can be obtained by GMM procedures, the inconsistent LS estimator has a relatively low variance and hence can lead to an estimator with lower root mean square error after the bias is removed. Therefore, we discuss in this paper the different methods to correct the bias of LS and GMM estimations. The analytical expressions for the asymptotic biases of the LS and GMM estimators have been presented for large N and finite T. Finally, we display new estimators that presented by Youssef and Abonazel (2015) as more efficient estimators than the conventional estimators.
C87|Does economic growth really depend on the magnitude of debt? A threshold model approach|In recent economic literature it has been emphasized that across both advanced countries and emerging markets, high levels of debt-to-gross domestic product (GDP) ratio (90% and above) are associated with notably lower growth outcomes. On the other hand, much lower levels of external debt-to-GDP ratio (60% and below) are associated with adverse outcomes for emerging market growth. These findings have been broadly cited and used in practice. On the other hand, there is an opposite evidence, such that the initial level of debt-to-GDP ratio has no impact on economic growth rate. Taking both viewpoints into account, we propose to employ a time series-based nonlinear mechanism in the threshold autoregression form in order to examine the possible relationship between economic growth rate and its potential determinants included the mentioned debt-to GDP indicator. The originality of the study is that it employs threshold variables instead of exogenous variables and time-series data instead of panel data to reveal the economic instruments that have determined the business cycle in European countries for the last 2 decades -starting from 1995. The purpose of the study is to check the mechanism of growth (measured in terms of GDP growth rate and industrial production growth rate) depending on several important macroeconomic variables, such as public debt, rate of inflation, interest rate, and rate of unemployment with the level of growth itself serving as the threshold variable. We propose an efficient methodology for seeking the best specification of threshold autoregression model in terms of both goodness of fit and parsimony of parametrization. The data (quarterly and monthly) applied in the research cover the time period from the beginning of 1995 to the end of 2013. Such a long period is interesting because it allows investigation of the mechanism of growth under two different economic policy models. We identify that the exogenous monetary mechanism played an important role in diagnosing the phases of business cycle in most European economies which is in line with liberal economic policy dominating in the observed period. The initial level of debt-to-GDP ratio as its increase within the recession period was of no value for the economic growth pattern.
C87|Bias Correction Methods for Dynamic Panel Data Models with Fixed Effects|This paper considers the estimation methods for dynamic panel data (DPD) models with fixed effects which suggested in econometric literature, such as least squares (LS) and generalized method of moments (GMM). These methods obtain biased estimators for DPD models. The LS estimator is inconsistent when the time dimension (T) is short regardless of the cross sectional dimension (N). Although consistent estimates can be obtained by GMM procedures, the inconsistent LS estimator has a relatively low variance and hence can lead to an estimator with lower root mean square error after the bias is removed. Therefore, we discuss in this paper the different methods to correct the bias of LS and GMM estimations. The analytical expressions for the asymptotic biases of the LS and GMM estimators have been presented for large N and finite T. Finally, we display new estimators that presented by Youssef and Abonazel (2015) as more efficient estimators than the conventional estimators.
C87|Panel Data Analysis with Stata Part 1 Fixed Effects and Random Effects Models|The present work is a part of a larger study on panel data. Panel data or longitudinal data (the older terminology) refers to a data set containing observations on multiple phenomena over multiple time periods. Thus it has two dimensions: spatial (cross-sectional) and temporal (time series). The main advantage of panel data comes from its solution to the difficulties involved in interpreting the partial regression coefficients in the framework of a cross-section only or time series only multiple regression. Depending upon the assumptions about the error components of the panel data model, whether they are fixed or random, we have two types of models, fixed effects and random effects. In this paper we explain these models with regression results using a part of a data set from a famous study on investment theory by Yehuda Grunfeld (1958), who tried to analyse the effect of the (previous period) real value of the firm and the (previous period) real capital stock on real gross investment. We consider mainly three types of panel data analytic models: (1) constant coefficients (pooled regression) models, (2) fixed effects models, and (3) random effects models. The fixed effects model is discussed under two assumptions: (1) heterogeneous intercepts and homogeneous slope, and (2) heterogeneous intercepts and slopes. We discuss all the relevant statistical tests in the context of all these models.
C87|Estimation Of Star-Garch Models With Iteratively Weighted Least Squares|This study applies the Iteratively Weighted Least Squares (IWLS) algorithm to a Smooth Transition Autoregressive (STAR) model with conditional variance. Monte Carlo simulations are performed to measure the performance of the algorithm, to compare its performance with the performances of established methods in the literature, and to see the effect of initial value selection method. Simulation results show that low bias and mean squared error are received for the slope parameter estimator from the IWLS algorithm when the real value of the slope parameter is low. In an empirical illustration, STAR-GARCH model is used to forecast daily US Dollar/Australian Dollar and FTSE Small Cap index returns. 1-day ahead out-of-sample forecast results show that forecast performance of the STAR-GARCH model improves with the IWLS algorithm and the model performs better that the benchmark model.
C87|On some remarks about SEATS signal extraction|Abstract In seasonal adjustment a time series is considered as a juxtaposition of several components, the trend-cycle, and the seasonal and irregular components. The Bureau of the Census X-11 method, based on moving averages, correction of large errors and trading day adjustments, has long dominated. With the success of ARIMA modelling at the end of the 20th century, methods with better outlier detection and trading day corrections by regression with ARIMA errors have appeared, with the regARIMA module of Census X-12-ARIMA or Bank of Spain TRAMO-SEATS. SEATS consists of extracting the components by an ARIMA-model-based unobserved components approach. This means that models are used for each component such that the sum of the components is compatible with the ARIMA model for the corrected time series. The underlying theory of the SEATS program is studied in many papers but there is no complete and systematic description of its output. Our purpose is to examine SEATS text output and to explain the results in simple words and formulas. This is done on a simple example, a time series with a non-seasonal model so that the computations can be verified step by step. The principles behind SEATS are first described, including the admissible decompositions and the canonical decomposition, and the derivation of the Wiener-Kolmogorov filter. Then the example is introduced: the interest rates of US certificates of deposits. The text output from SEATS is presented in edited form in several tables. Finally, the main results are checked on the example by means of a Microsoft Excel workbook and direct computations. In particular, the forecasts and backcasts are obtained; the admissible and canonical decompositions with two components are discussed; the filters are first derived using autocorrelations of two auxiliary ARMA processes, then applied on the prolonged time series; and the characteristics of the estimates, the revisions and the growth rates are analyzed.
C87|Creating LaTeX documents from within Stata using texdoc|I discuss the use of texdoc for creating LaTeX documents from within Stata. Specifically, texdoc provides a way to embed LaTeX code directly in a do-file and to automate the integration of results from Stata in the final document. One can use the command, for example, to assemble automatic reports, write a Stata Journal article, prepare slides for classes, or put together solutions for homework assignments. Copyright 2016 by StataCorp LP.
C87|Assessing inequality using percentile shares|At least since Thomas Piketty’s best-selling Capital in the Twenty- First Century (2014, Cambridge, MA: The Belknap Press), percentile shares have become a popular approach for analyzing distributional inequalities. In their work on the development of top incomes, Piketty and collaborators typically report top-percentage shares, using varying percentages as thresholds (top 10%, top 1%, top 0.1%, etc.). However, analysis of percentile shares at other positions in the distribution may also be of interest. In this article, I present a new command, pshare, that estimates percentile shares from individual-level data and displays the results using histograms or stacked bar charts. Copyright 2016 by StataCorp LP.
C87|Panel time series: Review of the methodological evolution|In this article, we discuss the econometric treatment of macropanels, also known as panel time series. This new approach rejects the assumption of slope homogeneity and handles nonstationarity. It also recognizes that cross-section dependence (that is, some correlation structure in the error term between units due to unobservable common factors) squanders efficiency gains by operating with a panel. This approach uses a new set of estimators known in the literature as the common correlated effect, which essentially consists of increasing the model to be fit by adding the averages of the individuals in each time t, of both the dependent variable and the specific regressors of each individual. We present two commands developed for the evaluation and treatment of cross-section dependence.
C87|Numerical implementation of the QuEST function|Certain estimation problems involving the covariance matrix in large dimensions are considered. Due to the breakdown of finite-dimensional asymptotic theory when the dimension is not negligible with respect to the sample size, it is necessary to resort to an alternative framework known as large-dimensional asymptotics. Recently, an estimator of the eigenvalues of the population covariance matrix has been proposed that is consistent according to a mean-squared criterion under large-dimensional asymptotics. It requires numerical inversion of a multivariate nonrandom function called the QuEST function. The numerical implementation of this QuEST function in practice is explained through a series of six successive steps. An algorithm is provided in order to compute the Jacobian of the QuEST function analytically, which is necessary for numerical inversion via a nonlinear optimizer. Monte Carlo simulations document the effectiveness of the code.
C87|Mergers and acquisitions transactions strategies in diffusion - type financial systems in highly volatile global capital markets with nonlinearities|The M&A transactions represent a wide range of unique business optimization opportunities in the corporate transformation deals, which are usually characterized by the high level of total risk. The M&A transactions can be successfully implemented by taking to an account the size of investments, purchase price, direction of transaction, type of transaction, and using the modern comparable transactions analysis and the business valuation techniques in the diffusion – type financial systems in the finances. We analyzed the M&A transactions in Switzerland in 2012 in various industrial segments. We think that the globalization has a strong influence on the successful M&A deals completion in Switzerland. We believe that the fluctuating dependence of M&A transactions number over the certain time period is quasi-periodic. We think that there are many factors, which can generate the quasi periodic oscillations of the M&A transactions number in the time domain, for example: the stock market bubble effects. We performed the research of the nonlinearities in the M&A transactions number quasi-periodic oscillations in Matlab, including the ideal, linear, quadratic, and exponential dependences. We discovered that the average of a sum of random numbers in the M&A transactions time series represents a time series with the quasi periodic systematic oscillations, which can be finely approximated by the polynomial numbers. We think that, in the course of the M&A transaction implementation, the ability by the companies to absorb the newly acquired knowledge and to create the new innovative knowledge bases, is a key pre-determinant of the M&A deal completion success. In our opinion, the integrative collateral creative design thinking has a direct impact on the new innovative knowledge bases formation by companies in the highly competitive global markets. We would like to state that the winning virtuous mergers and acquisitions transactions strategies in the diffusion - type financial systems in the highly volatile global capital markets with the nonlinearities can only be selected through the decision making process on the various available M&A choices, applying the econophysical econometrical analysis with the use of the inductive, deductive and abductive logics.
C87|Estimation and Prediction of Shipping Trends with the Data-Driven Haar-Fisz Transform|We describe the implementation of a computer-based automatic procedure to estimate the trends associated with debit and credit transaction flows in Cyprus’s shipping industry. The procedure was also extended to forecasting. Transactions in the shipping industry do not always coincide with the time the service is provided. The transactions are usually completed gradually throughout the financial year and occasionally involve large amounts for balance settlements. In addition, the transactions are subject to several market risks such as the freight rate and exchange rate changes. Consequently, the transactions frequently exhibit large values and changes in variance, which makes trend estimation and forecasting difficult. A key component of the procedure we implemented is a variance stabilization method based on the Data-Driven Haar-Fisz Transform that enables accurate estimation of trends in volatile time series data. This method is sufficiently flexible to accommodate data characteristics such as cyclical changes, shifts in trend and spikes that are frequently encountered in transaction flow data.
C87|Research Ideas for the Journal of Informatics and Data Mining: Opinion|The purpose of this Opinion article is to discuss some ideas that might lead to papers that are suitable for publication in the Journal of Informatics and Data Mining. The suggestions include the analysis of citations databases, PI-BETA (Papers Ignored – By Even The Authors), model specification and testing, pre-test bias and data mining, international rankings of academic journals based on citations, international rankings of academic institutions based on citations and other factors, and case studies in numerous disciplines in the sciences and social sciences.
C87|Analysis Models for Territorial Variation of Demographic Phenomena. The Case of Romania|"Demographic phenomena that characterized the last decades have influenced its spatial distribution. Demographic aging of population has become lately an extremely sensitive and sometimes thorny issue, whose resolution requires a joint and coordinated effort of all decision makers in each country. Demographic aging has a profound impact on all generations and for most areas of economic activity and for the poverty level. As a member state of the European Union, Romania, ""enrols"", from the point of view of an aging population, in the European trend, sometimes was exceeding, in the negative, the levels of other Member States. Regional disparities, highlighted through different methods, recommend using multi-criteria analysis to determine the influence of various socio-economic factors on demographic factors. In the first part, the paper presents a brief analysis of the evolution of demographic phenomena in Romanian development regions. Econometric and stochastic methods for territorial forecasting of demographic phenomena are presented, which will be the basis for future research."
C87|The Capital Markets Research Based on the Financial Quantitative Models|In last period of time, progress in statistics has been marked by the increasing availability of software, such as the most known and open source R system. This has the potential to continue the transformation from a set of techniques used and developed by statisticians and computer scientists to an essential system of analysis tools for a much larger community. This paper aims to expose a small part of the capability of R to use mix-andmatch models and quantitative modelling in order to build an alternative way to analyze capital markets based on a scientific scope beyond commercial purposes. For those that wish to use R for making Trading decisions, this paper is a short introduction which one can pursue in order to make trade on capital market using different response variables, signal thresholds, technical indicators and classifiers. Applying testing methods should be used to assess the performance of each model which the trading strategy is based on. The paper also provides a description of various packages in R that includes all necessary functionality for generating signals, extracting precision/recall metrics of generated models, performing estimates and evaluating trading strategies.
C87|"Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say ""Usually Not"""|No abstract is available for this item.
C87|Long-term Stochastic Forecasting of the Nuclear Energy Global Market|The paper addresses the issue of long-term forecasting of the global nuclear power market and opportunities for studying its individual sections. Given the current state of the market and related industries, a scientific approach to strategic forecasting based on evaluating dispersions of forecasts acquires particular significance. The authors first developed and applied a probabilistic forecasting technique to a number of market indicators of the global nuclear power industry in real terms for the period up to 2035. In particular, the calculations concern the number and electric capacity of input-output nuclear power plants; and the demand for natural and enriched uranium and enrichment services. The forecasting is based on stochastic modeling of NPP lifecycles and exploitation parameters, uranium enrichment rate, and nuclear energy planes in the different regions. The proposed model, as opposed to scenario approaches, means that the probability distributions of mentioned values can be calculated. This is of crucial importance in assessing the economic risks for various economic agents operating in the world nuclear technology market. The results of modeling the main indicators of nuclear power markets (the dynamics of the net electric capacity of nuclear power plants worldwide and across the largest regions) are consistent with the scenario forecasts provided by WNA and IAEA (2013), which are based on data provided by the members of these organizations. This fact indicates the correct choice of the model for describing the frequency distributions of the key stages of reactor ‘arbor vitae’. The authors modeled the likely volumes of the market for constructing new NPP and taking the stopped ones off the road for the next 15 year period in the different regions, Russia and worldwide. Finally, the paper estimates the likely share of the new Russian designed NPP construction in the world market for the period up to 2030.
C87|Detecting Treatment-Subgroup Interactions in Clustered Data with Generalized Linear Mixed-Effects Model Trees|Identification of subgroups of patients for which treatment A is more effective than treatment B, and vice versa, is of key importance to the development of personalized medicine. Several tree-based algorithms have been developed for the detection of such treatment-subgroup interactions. In many instances, however, datasets may have a clustered structure, where observations are clustered within, for example, research centers, studies or persons. In the current paper we propose a new algorithm, generalized linear mixed-effects model (GLMM) trees, that allows for detection of treatment-subgroup interactions, as well as estimation of cluster-specific random effects. The algorithm uses model-based recursive partitioning (MOB) to detect treatment-subgroup interactions, and a GLMM for the estimation of random-effects parameters. In a simulation study, we evaluate the performance of GLMM tree and compare it with that of MOB without random-effects estimation. GLMM tree was found to have a much lower Type I error rate than MOB trees without random effects (4% and 33%, respectively). Furthermore, in datasets with treatment-subgroup interactions, GLMM tree recovered the true treatment subgroups much more often than MOB without random effects (in 90% and 61% of the datasets, respectively). Also, GLMM tree predicted treatment outcome differences more accurately than MOB without random effects (average predictive accuracy of .94 and .88, respectively). We illustrate the application of GLMM tree on a patient-level dataset of a meta-analysis on the effects of psycho- and pharmacotherapy for depression. We conclude that GLMM tree is a promising algorithm for the detection of treatment-subgroup interactions in clustered datasets.
C87|Structured Additive Regression Models: An R Interface to BayesX| Structured additive regression (STAR) models provide a flexible framework for modeling possible nonlinear effects of covariates: They contain the well established frameworks of generalized linear models and generalized additive models as special cases but also allow a wider class of effects, e.g., for geographical or spatio-temporal data, allowing for specification of complex and realistic models. BayesX is standalone software package providing software for fitting general class of STAR models. Based on a comprehensive open-source regression toolbox written in C++, BayesX uses Bayesian inference for estimating STAR models based on Markov chain Monte Carlo simulation techniques, a mixed model representation of STAR models, or stepwise regression techniques combining penalized least squares estimation with model selection. BayesX not only covers models for responses from univariate exponential families, but also models from less-standard regression situations such as models for multi-categorical responses with either ordered or unordered categories, continuous time survival data, or continuous time multi-state models. This paper presents a new fully interactive R interface to BayesX: the R package R2BayesX. With the new package, STAR models can be conveniently specified using R’s formula language (with some extended terms), fitted using the BayesX binary, represented in R with objects of suitable classes, and finally printed/summarized/plotted. This makes BayesX much more accessible to users familiar with R and adds extensive graphics capabilities for visualizing fitted STAR models. Furthermore, R2BayesX complements the already impressive capabilities for semiparametric regression in R by a comprehensive toolbox comprising in particular more complex response types and alternative inferential procedures such as simulation-based Bayesian inference.
C87|Bayesian Model Averaging and Jointness Measures for gretl| This paper presents a software package that implements Bayesian model averaging for gretl, the GNU regression, econometrics and time-series library. Bayesian model averaging is a model-building strategy that takes account of model uncertainty in conclusions about estimated parameters. It is an efficient tool for discovering the most probable models and obtaining estimates of their posterior characteristics. In recent years we have observed an increasing number of software packages devoted to Bayesian model averaging for different statistical and econometric software. In this paper, we propose the BMA package for gretl, which is an increasingly popular free, open-source software for econometric analysis with an easy-to-use graphical user interface. We introduce the BMA package for linear regression models with jointness measures proposed by Ley and Steel (2007) and Doppelhofer and Weeks (2009).
C87|Data Checking and Econometric Software Development: A Technique of Traceability by Fictive Data Encoding|In this paper, we have exposed the data checking problem in the context of the econometric software development. We have based our presentation on the development of our SIMUL multidimensional econometric software. We have shown that this problem is particularly important when one develops multi-dimensional econometric software. Then, we have briefly recalled the principle of the main arithmetical and managerial techniques available to deal with this problem. Finally, we have presented the data checking procedures we have conceived based on a fictive data encoding technique. Copyright Springer Science+Business Media New York 2015
C87|Unit-Linked Life Insurance Products Versus Other Alternative Investments|Unit-linked insurance is a life insurance policy with investment component. An important component of the activity carried out by the insurance companies is the investment of the premiums paid by policyholders in various types of assets, in order to obtain higher yields than those guaranteed by the insurance contracts, while providing the necessary liquidity for the payment of insurance claims in case of occurrence of the assumed risks. This research contributes to the existing literature regarding the study of investment alternatives, with an exclusive focus on the investment in unit-linked life insurance. A special place in this study is the presentation of investments in unit-linked insurance versus other types of financial investments: deposits, treasury bills, shares (BET), currency (EURO) and gold.
C87|Centralized vs. Distributed Databases. Case Study|Currently, in information technology domain and implicit in databases domain can be noticed two apparently contradictory approaches: centralization and distribution respectively. Although both aim to produce some benefits, it is a known fact that for any advantage a price must be paid. In addition, in this paper we have presented a case study, e-learning portal performance optimization by using distributed databases technology. In the stage of development in which institutions have branches distributed over a wide geographic area, distributed database systems become more appropriate to use, because they offer a higher degree of flexibility and adaptability then centralized ones.
C87|On the Ability to Disentangle the Two Errors in the Normal/Half-Normal Stochastic Frontier Model /Sobre la capacidad de separar los dos errores en el modelo de frontera estocástica normal/half-normal|In this paper, a simulation experiment is carried out in the framework of the normal/half-normal stochastic frontier model in order to analyse its ability to disentangle the two types of errors that form the composite error. According to the results obtained through the mean bias and the mean squared error of the parameters and efficiencies, and via Spearman rank correlation between actual and estimated efficiencies, a good performance of the model is only obtained when considering medium-sized or large samples and the variance of the inefficiencies highly contributes to that of the composite error. The problems of wrong skewness and absence of random error are also addressed. The influence on the results of selecting a wrong distribution for the inefficiency term is also analysed. En este artículo, se lleva a cabo un experimento de simulación en el contexto del modelo con frontera estocástica normal/half-normal para analizar su capacidad de separar los dos tipos de error que forman el error compuesto. Según los resultados obtenidos a través del sesgo medio y el error cuadrático medio de los parámetros y las eficiencias, y mediante el coeficiente de correlación por rangos de Spearman entre las eficiencias reales y las estimadas, se obtiene un buen comportamiento del modelo solo cuando se consideran muestras de tamaño mediano o grande y la varianza de las ineficiencias contribuye de forma muy importante a la del error compuesto. Los problemas de la asimetría errónea y de la ausencia de errores aleatorios también son abordados. La influencia en los resultados de seleccionar una distribución errónea para el término de ineficiencia también se analiza.
C87|Determinants of Corporate Dividend Payout in Nepal|There are several studies that investigated determinants of corporate dividend payout in developed and emerging stock markets. Such a study is scant in pre-emerging stock markets like that of Nepal. Therefore, the purpose of this paper is to investigate the determinants of corporate dividend payout in Nepal. This paper examines whether enterprises’ characteristics affect dividend payouts of the enterprises listed on Nepal Stock Exchange Ltd. A priori hypothesis between relationship of the dividends paid by the enterprises and enterprises’ characteristics- net profits, size, lagged dividends, liquidity, risk, investment opportunity set, and number of shareholders are set based on theoretical framework and other empirical studies, and tested on 22 listed enterprises covering a 5-year period, 2009 to 2013 by employing regression model. Purposive sampling technique is used to select the enterprises for the study. The relationships of variables firstly analysed for overall sector and further for sub-sectors of financial and non-financial sector. Overall sector analysis is performed through pooled cross-sectional data. Further to check sectoral differences, sector wise regression analysis is performed. The results, in overall, reveal that profitability, size, and liquidity are major determinants of corporate dividend payout in Nepal. This study also reveals that there is sector specific importance of the determinants of corporate dividend payout in Nepal.
C87|Object-Oriented Econometrics with Ox|This article reviews the object-oriented features of the Ox matrix programming language. We discuss object-oriented programming in general and give econometric examples coded in Ox. We also discuss some useful built-in classes that come with the Ox distribution.
C87|VARsignR: Estimating VARs using sign restrictions in R|VARsignR identifies structural shocks in Vector Autoregressions (VARs) using sign restrictions. It implements Uhlig’s (2005) rejection method, Uhlig’s (2005) penalty function approach, the Rubio-Ramirez et al. (2010) rejection method, and Fry and Pagan’s (2011) median target method. This vignette shows the usage and provides some technical information on the procedures that should help users to bridge the gap between VARsignR and the underlying technical papers.
C87|R-Codes to Calculate GMM Estimations for Dynamic Panel Data Models|These codes presented three functions for calculating three important estimators in dynamic panel data (DPD) models; these estimators are Arellano-Bond (1991), Arellano-Bover (1995), and Blundell-Bond (1998). All functions here need to the following variables: yit_1: dependent variable for DPD model; phi: the value of autoregressive coefficient; D.T_D.T: first-difference operator matrix of Arellano-Bond estimator; HD: instrumental variables of Arellano-Bond estimator; HL: instrumental variables of Arellano-Bover estimator; W: weighting matrix of Blundell-Bond estimator; HS: instrumental variables of Blundell-Bond estimator. Also, they need to the following R libraries: simex; plm; dlm. For more details about the theoretical bases and the developments of that estimators, see, e.g., Youssef et al. (2014a,b) and Youssef and Abonazel (2015). Moreover, these codes have been designed to enable the user to make a simulation study in this topic, such as the simulation study in Youssef et al. (2014b).
C87|Fast methods for jackknifing inequality indices|The jackknife is a resampling method that uses subsets of the original database by leaving out one observation at a time from the sample. The paper develops fast algorithms for jackknifing inequality indices with only a few passes through the data. The number of passes is independent of the number of observations. Hence, the method provides an efficient way to obtain standard errors of the estimators even if sample size is large. We apply our method using micro data on individual incomes for Germany and the US.
C87|Determinants of Labor Force Potential in Romania|In this research study there were applied Multinomial Logistic Regression models to examine the socio-economic factors that were responsible conducting individuals to be part of the employment or not. As a result of the multinomial regression model, the most significant factor to consider here is that each one tells the effect of the predictors of risk on the probability of success in that category, in comparison to the reference category. For computing the multinomial logistic regression model it was used the multinom function from the nnet package in R. This research will contribute to know the determinants of labor force potential in Romania. The data from a Romanian labor force survey 2013 is used for this study.
C87|Factors of life quality material dimension|This paper will be focused on Multinomial Logistic Regression models to examine the social and demographic factors that may influence the components of life quality material dimension in terms of income and durable goods. As statistical source for the regression model will be use the Household Budget Survey. Within the predictors of the model could be mentioned: gender, age, marital status, education level, residential area. Statistical software used for the analysis is R Project along with specific package for multinomial logistic regression. This research will contribute to know the determinants of life quality material dimension in Romania.
C87|Analysis of the labour market in Romania in relation with working time|In this research study there were applied Regression models to examine the socio-economic factors that could influence the working time on the labor market. As a result of the regression models applied, the most significant factors to consider here are the sex of the employed people, age, education level, residence areas where they live, the activity and the occupational status on the labour market. For computing the regression models it was used the glm function from the nlme package in R. This research will contribute to know the determinants of working time in Romania. The data from a Romanian labor force survey 2013 is used for this study.
C87|Matching and Winning? The Impact of Upper and Middle Managers on Team Performance|In this paper we investigate the impact of upper and middle level managers on firm performance by simultaneously estimating manager and match qualities for management pairings in Major League Baseball (MLB). We document the economic significance of managers at both organizational levels and illustrate the importance of accounting for match quality in evaluating manager impact. Our results suggest assortative matching as managerial quality is positively correlated across organizational levels. Higher match quality in a pairing is further associated with higher individual manager qualities and longer joint employment spells. Mismatches in educational attainment are linked to lower match quality.
C87|npbr: A Package for Nonparametric Boundary Regression in R| The package npbr is the first free specialized software for data edge and frontier analysis in the statistical literature. It provides a variety of functions for the best known and most innovative approaches to nonparametric boundary estimation. The selected methods are concerned with empirical, smoothed, unrestricted as well as constrained fits under both single and multiple shape constraints. They also cover data envelopment techniques as well as robust approaches to outliers. The routines included in npbr are user friendly and afford a large degree of flexibility in the estimation specifications. They provide smoothing parameter selection for the modern local linear and polynomial spline methods as well as for some promising extreme value techniques. Also, they seamlessly allow for Monte Carlo comparisons among the implemented estimation procedures. This package will be very useful for statisticians and applied researchers interested in employing nonparametric boundary regression models. Its use is illustrated with a number of empirical applications and simulated examples.
C87|Research Ideas for the Journal of Informatics and Data Mining: Opinion|The purpose of this Opinion article is to discuss some ideas that might lead to papers that are suitable for publication in the Journal of Informatics and Data Mining. The suggestions include the analysis of citations databases, PI-BETA (Papers Ignored – By Even The Authors), model specification and testing, pre-test bias and data mining, international rankings of academic journals based on citations, international rankings of academic institutions based on citations and other factors, and case studies in numerous disciplines in the sciences and social sciences.
C87|Assessment Of Approaches To Building An Analytical Crm System|The article assesses the advantages and disadvantages of the approaches to building analytical CRM systems using the author's own system of criteria. The results of the comparative analysis and the assessment indicate that there cannot be identified only one approach which can be classified as most suitable. Having in mind the advantages, the disadvantages and the limitations connected with the individual approaches, the author proposes building an analytical CRM system through combining different approaches and substantiates the feasibility of the combined approach through the use of service-oriented architecture.
C87|The Impact of Payments for Reforestation in the Mexican State Michoacán|As a means to adapt to climate change, the Mexican government grants, since 2003, payments as an incentive for landholders to conserve, maintain and increase the provision of environmental services through the promotion of a forestry strategy. This paper contributes to the literature with an empirical analysis of the impact of payments for reforestation in one of the states with the highest rates of deforestation, Michoac n. The impact is estimated by means of panel data regressions and propensity score matching. Our results suggest that the payments are not contributing to the overall reforestation in Michoac n because of the existence of leakage in areas that are not participating in the program. Our analysis underlines the challenges faced when implementing financial incentive based programs and provides policy makers with evidence for improving the design of such programs.
C87|A Fast Fractional Difference Algorithm|" type=""main"" xml:id=""jtsa12074-abs-0001""> We provide a fast algorithm for calculating the fractional difference of a time series. In standard implementations, the calculation speed (number of arithmetic operations) is of order T-super-2, where T is the length of the time series. Our algorithm allows calculation speed of order TlogT. For moderate and large sample sizes, the difference in computation time is substantial. Copyright © 2014 John Wiley & Sons, Ltd."
C87|CTREATREG: Stata module for estimating dose-response models under exogenous and endogenous treatment|This paper presents ctreatreg, a Stata module for estimating a dose-response function when: (i) treatment is continuous, (ii) individuals may react heterogeneously to observable confounders, and (iii) selection-into-treatment may be endogenous. Two estimation procedures are implemented: OLS under Conditional Mean Independence, and InstrumentalVariables (IV) under selection endogeneity. A Monte Carlo experiment to test the reliability of the proposed command is finally set out. Length: 25 pages
C87|Visualizing Count Data Regressions Using Rootograms| The rootogram is a graphical tool associated with the work of J. W. Tukey that was originally used for assessing goodness of fit of univariate distributions. Here, we extend the rootogram to regression models and show that this is particularly useful for diagnosing and treating issues such as overdispersion and/or excess zeros in count data models. We also introduce a weighted version of the rootogram that can be applied out of sample or to (weighted) subsets of the data, for example, in finite mixture models. An empirical illustration revisiting a well-known dataset from ethology is included, for which a negative binomial hurdle model is employed. Supplementary materials providing two further illustrations are available online: the first, using data from public health, employs a two-component finite mixture of negative binomial models; the second, using data from finance, involves underdispersion. An R implementation of our tools is available in the R package countreg. It also contains the data and replication code.
C87|LinRegInteractive: An R Package for the Interactive Interpretation of Linear Regression Models|The package provides the generic function fxInteractive() to facilitate the interpretation of various kinds of regression models. It allows to observe the effects of variations of metric covariates in an interactive manner by means of termplots for different model classes. Currently linear regression models, generalized linear models, generalized additive models and linear mixed-effects models are supported. Due to the interactive approach the function provides an intuitive understanding of the mechanics of a particular model and is therefore especially useful for educational purposes. Technically the package is based on the package rpanel and the only mandatory argument for the main function is an appropriate fitted-model object. Given this, the linear predictors, the marginal effects and, for generalized linear models, the responses are calculated automatically. For the marginal effects a numerical approach is used to handle non-constant marginal effects automatically. If there are two or more categorical covariates the corresponding effects are presented in a novel way. For publication purposes the user can customize the appearance of the termplots to a large extent. Tables of the effects and marginal effects can be printed to the R Console, optionally as copy-and-paste-ready LaTeX-code.
C87|Structural breaks and the time-varying levels of weak-form efficiency in crude oil markets: Evidence from the Hurst exponent and Shannon entropy methods|This paper examines the time-varying levels of weak-form efficiency and the presence of structural breaks for two worldwide crude oil benchmarks over the period spanning from January 2, 1990, through September 18, 2012. We use two different econophysics approaches for comparison purposes. The Hurst exponent is provided by the scaled range R/S analysis to measure the degree of long-range dependency exhibited by the West Texas Intermediate (WTI) and European Brent crude oil indices. The Shannon entropy approach, which is based on a symbolic time series analysis (STSA), allows a ranking of market-level efficiency. The empirical results show that the European Brent index is less inefficient than the WTI index for both methods. Moreover, we find that the Hurst exponent displays better performance than the Shannon entropy method. The Hurst exponent is also more effective than the Shannon entropy in detecting financial crashes and crises as well as extreme events, such as wars and terrorist attacks. These findings have several implications for commodity portfolio hedgers and risk managers.
C87|Business Intelligence Adoption In Large Romanian Companies|The economic conditions and market competition create pressures on companies to adopt new technologies that can provide more efficient information and can support decision-making better. The purpose of the research is to investigate the decision support information systems in order to apprise and enhance the capacity of the entities to apply the new knowledge that BI produces for organizational success and competitiveness. The importance of the conducted research consists in identifying solutions to improve reporting and stimulate the entities to start using business intelligence (BI) technologies, which facilitate obtaining new information, in order to ensure flexibility, resilience and provide answers to questions that go beyond what the pre-defined reports can do to support decision-making. The estimated result is a technical and operational overview of the large companies in Romania, drawing future directions for an improved competitive behaviour and strategic awareness, and identifying the significant factors for optimizing the decision-making process.
C87|Identification and Estimation of Treatment Effects in the Presence of Neighbourhood Interactions|This paper presents a parametric counter-factual model identifying Average Treatment Effects (ATEs) by Conditional Mean Independence when externality (or neighbourhood) effects are incorporated within the traditional Rubin’s potential outcome model. As such, it tries to generalize the usual control-function regression, widely used in program evaluation and epidemiology, when SUTVA (i.e. Stable Unit Treatment Value Assumption) is relaxed. As by-product, the paper presents also ntreatreg, an author-written Stata routine for estimating ATEs when social interaction may be present. Finally, an instructional application of the model and of its Stata implementation through two examples (the first on the effect of housing location on crime; the second on the effect of education on fertility), are showed and results compared with a no-interaction setting.
C87|Uluslararasý Portföy Yönetiminde Rejim Geçiþken Karar Destek Modelleri: Geliþmekte Olan Menkul Kýymet Piyasalarý Üzerine Bir Uygulama|Bu makale, portföy yatýrýmlarýnda bir karar destek sistemi olarak rejim geçiþken modellerin ne þekilde kullanýlabileceðini geliþmekte olan hisse senedi piyasalarýna ait zaman serilerini ve Gauss yazýlým programýný kullanarak incelemektedir. Yönetim biliþim sistemlerinde, model riskinin minimize edilmesi, karar destek siteminin uygulanacaðý problemin net olarak tanýmlanmasý ve bu problemin çözümünde kullanýlacak modelin doðru seçilmesi ile mümkündür. Ekonometrik testlerin sonuçlarý, Ukrayna hariç, geliþmekte olan ekonomilerde hisse senetleri piyasalarýnda 09/01/2004 - 13/09/2007 tarihleri arasýnda, ABD hisse senedi piyasalarý ile karþýlaþtýrýldýðýnda kalýcý bir volatilitenin gözlemlendiðini ortaya koymaktadýr. Bu kapsamda, Türkiye, Rusya, Ukrayna, Brezilya,Lübnan, ABD (Dow Jones Industrial Average) ve MSCI (Morgan Stanley Composite Index) hisse senedi piyasalarýnda rejim geçiþkenliði ekonometrik olarak karþýlaþtýrmalý incelenmiþtir.
C87|Pattern Of Financial Savings In A Romanian Bank – A Statistical Analysis|This study aims to expose the profile of the pattern of saving for the financial consumers of a Romanian bank. The methodology of analysis is based on a logistic regression, using the relationship between variables which expose relevant characteristics of persons who save money. The analysis was conducted in R software, which represents a powerful tool for statistical modelling.
C87|Stochastic Volatility Estimation with GPU Computing|In this paper, we show how to estimate the parameters of stochastic volatility models using Bayesian estimation and Markov chain Monte Carlo (MCMC) simulations through the approximation of the a-posteriori distribution of parameters. Simulated independent draws are made possible by using Graphics Processing Units (GPUs) to compute several Markov chains in parallel. We show that the higher computational power of GPUs can be harnessed and put to good use by addressing two challenges. Bayesian estimation using MCMC simulations benefit from powerful processors since it is a complex numerical problem. Moreover, sequential approaches are characterized for drawing highly correlated samples which reduces the Effective Sample Size (ESS) associated with the simulated values obtained from the posterior distribution under a Bayesian analysis. However, under the proposed parallel expression of the algorithm, we show that a faster convergence rate is possible by running independent Markov chains, drawing lower correlations and therefore increase the ESS. The results obtained with this approach are presented for the Stochastic Volatility (SV) model, basic and with leverage.
C87|Portfolio Choice under Parameter Uncertainty: Bayesian Analysis and Robust Optimization Comparison|Parameter uncertainty has been a recurrent subject treated in the financial literature. The normative portfolio selection approach considers two main kinds of decision rules: expected expected utility maximization and mean-variance criterion. Assuming that the mean-variance criterion is a good approximation to the expected utility maximization paradigm, a major factor of concern is parameter uncertainty which, when it is not taken into account, can lead to meaningless portfolios. A statistical approach, based on a Bayesian analysis, can be applied to parameter uncertainty. This can be compared with a robust optimization approach where it is assumed that the value of the unknown parameters can change within a given region. Comparisons over these two approaches are performed in this paper. We consider two measures to quantify the effects of the estimation risk, one of the measures is new and extends an existing one. The results allows us to distinguish the approaches and select the one that implies lower mean losses.
C87|Practical Correlation Bias Correction in Two-way Fixed Effects Linear Regression|When doing two-way fixed effects OLS estimations, both the variances and covariance of the fixed effects are biased. A formula for a bias correction is known, but in large datasets it involves inverses of impractically large matrices. We detail how to compute the bias correction in this case.
C87|partykit: A Modular Toolkit for Recursive Partytioning in R|The R package partykit provides a flexible toolkit for learning, representing, summarizing, and visualizing a wide range of tree-structured regression and classification models. The functionality encompasses: (a) basic infrastructure for representing trees (inferred by any algorithm) so that unified print/plot/predict methods are available; (b) dedicated methods for trees with constant fits in the leaves (or terminal nodes) along with suitable coercion functions to create such trees (e.g., by rpart, RWeka, PMML); (c) a reimplementation of conditional inference trees (ctree, originally provided in the party package); (d) an extended reimplementation of model-based recursive partitioning (mob, also originally in party) along with dedicated methods for trees with parametric models in the leaves. Here, a brief overview of the package and its design is given while more detailed discussions of items (a)--(d) are available in vignettes accompanying the package.
C87|Reweight: a stata module to reweight survey data to external totals|This paper describes reweight, a Stata module to reweight survey data to external aggregate totals.
C87|evtree: Evolutionary Learning of Globally Optimal Classification and Regression Trees in R| Commonly used classification and regression tree methods like the CART algorithm are recursive partitioning methods that build the model in a forward stepwise search. Although this approach is known to be an efficient heuristic, the results of recursive tree methods are only locally optimal, as splits are chosen to maximize homogeneity at the next step only. An alternative way to search over the parameter space of trees is to use global optimization methods like evolutionary algorithms. This paper describes the evtree package, which implements an evolutionary algorithm for learning globally optimal classification and regression trees in R. Computationally intensive tasks are fully computed in C++ while the partykit package is leveraged for representing the resulting trees in R, providing unified infrastructure for summaries, visualizations, and predictions. evtree is compared to the open-source CART implementation rpart, conditional inference trees (ctree), and the open-source C4.5 implementation J48. A benchmark study of predictive accuracy and complexity is carried out in which evtree achieved at least similar and most of the time better results compared to rpart, ctree, and J48. Furthermore, the usefulness of evtree in practice is illustrated in a textbook customer classification task.
C87|Pricing And Assessing Unit-Linked Insurance Contracts With Investment Guarantees|One of the most interesting life insurance products to have emerged in recent years in the Romanian insurance market has been the unit-linked contract. Unit-linked insurance products are life insurance policies with investment component. A unit-linked life insurance has two important components: protection and investment. The protection component refers to the insured sum in case of the occurrence of insured risks and the investment component refers to the policyholders' account that represents the present value of the units from the chosen investment funds. Due to the financial instability caused by the Global Crisis and the amplification of market competitiveness, insurers from international markets have started to incorporate guarantees in unit-linked products. So a unit- linked life insurance policy with an asset value guarantee is an insurance policy whose benefit payable on death or at maturity consists of the greater of some guaranteed amount and the value of the units from the investment funds. One of the most challenging issues concerns the pricing of minimum death benefit and maturity benefit guarantees and the establishing of proper reserves for these guarantees. Insurers granting guarantees of this type must estimate the cost and include the cost in the premium. An important component of the activity carried out by the insurance companies is the investment of the premiums paid by policyholders in various types of assets, in order to obtain higher yields than those guaranteed by the insurance contracts, while providing the necessary liquidity for the payment of insurance claims in case of occurrence of the assumed risks. So the guaranteed benefits can be broadly matched or immunized with various types of financial assets, especially with fixed-interest instruments. According to Romanian legislation which regulates the unit-linked life insurance market, unit-linked life insurance contracts pass most of the investment risk to the policyholder and involve no investment risk for the insurer. Although the Romanian legislation authorizes the Romanian insurers to offer unit-linked contracts without investment guarantees, this research provides a proposal of a theoretical and empirical basis for pricing the unit-linked insurance contracts with incorporated investment guarantees.
C87|Strategies on initial public offering of company equity at stock exchanges in imperfect highly volatile global capital markets with induced nonlinearities|This research considers the strategies on the initial public offering of company equity at the stock exchanges in the imperfect highly volatile global capital markets with the nonlinearities. We provide the IPO definition and compare the initial listing requirements on the various markets. We analyze the IPO techniques: the fixed-price offerings, auctions, book-building. We focus on the IPO initial underpricing, long-run performance and after market liquidity problems. 1. We propose that the information absorption by the investors occurs in the evolving learning process about the company’s value, taking to the consideration the fundamental purpose of investing and the responsibilities of investors. 2. We think that the information absorption capacity by the investors on the IPOs impacts the investor’s investment decisions and serves as a pre-determinant for the successful IPO deal completion. We propose the Ledenyov theory on the origins of the IPO underpricing and long term underperformance effects, which states that the IPO underpricing and long term underperformance can be explained by the changing information absorption capacity by the investors on the IPO value. 3. We think that the IPO winning virtuous investment strategies can only be selected by the investors with the highest information absorption capacity through the decision making process on the IPO investment choices at the selected stock exchange in the imperfect highly volatile global capital markets with the nonlinearities; applying the econophysical econometrical analysis with the use of the inductive, deductive and abductive logics in the frames of the strategic choice structuring process, that is the winning through the distinctive choices process.
C87|Robust standard error estimators for panel models: a unifying approach|The different robust estimators for the standard errors of panel models used in applied econometric practice can all be written and computed as combinations of the same simple building blocks. A framework based on high-level wrapper functions for most common usage and basic computational elements to be combined at will, coupling user-friendliness with flexibility, is integrated in the 'plm' package for panel data econometrics in R. Statistical motivation and computational approach are reviewed, and applied examples are provided.
C87|What happens if in the principal component analysis the Pearsonian is replaced by the Brownian coefficient of correlation?|The Brownian correlation has been recently introduced by Székely et al. (2007; 2009), which has an attractive property that when it is zero, it guarantees independence. This paper investigates into the effects and advantages, if any, of replacement of the Pearsonian coefficient of correlation (r) by the Brownian coefficient of correlation (say, ρ), other things remaining the same. Such a replacement and analysis of its effects have been made by the Host-Parasite Co-evolutionary algorithm of global optimization applied on six datasets.
C87|A Comparison Between Direct and Indirect Seasonal Adjustment of the Chilean GDP 1986–2009 with X-12-ARIMA|Abstract It is well known among practitioners that the seasonal adjustment applied to economic time series involves several decisions to be made by the econometrician. As such, it would always be desirable to have an informed opinion on the risks taken by each of those decisions. In this paper, I assess which disaggregation strategy delivers the best results for the case of the Chilean 1986–2009 GDP quarterly dataset (base year: 2003). This is done by performing an aggregate-by-disaggregate analysis under different schemes, as the fixed base year dataset allows this fair comparison. The analysis is based on seasonal adjustment diagnostics contained in the X-12-ARIMA program plus some statistical tests for robustness. This exercise is relevant for conjunctural economic assessment, as it concerns signal extraction from seasonal, noisy series, direction of change detection, and econometric applications based on reliable and accurate unobserved variables. The results show that it is preferable, in terms of stability, to use the first block of supply-side disaggregation, while demand-side disaggregation tends to be less reliable. This result carries important implications for policymakers aiming to evaluate its short-term effectiveness in both households and firms.
C87|Institutions and Economic Growth in the MENA Countries: An Empirical Investigation by Using Panel data model|"This paper will investigate the impact of institution on economic growth rates in MENA nations, Using panel data model over the period 1995-2012. Within the framework of the neoclassical growth model, this study integrates a broad set of institutional variables such. Security of property rights, governance, political freedom and size of government are the indicators used in the study, facilitating identification of the most important institutions that account for the observed variations in economic growth rates among nations. We find that, The sign and significance of all of the variables are qualitatively similar to the results obtained by MRW (1992). We also find The human capital is highly significant at 99% with initial income and Investment Share in MENA countries. The Results indicate that the dummy variable for oil exporters is positive and significant, indicating that other things being equal, oil exporters would be expected to have higher economic growth rates in MENA Countries. Basic OLS results, as well as a variety of additional evidence, suggest that (a) security of property rights, is the most significant institutions that explain the variations in economic growth rates, (b) The significant and negative sign on the government consumption, indicating that smaller governments are ""better"" in MENA countries."
C87|Macro Stress-Testing Credit Risk in Romanian Banking System|This report presents an application of a macro stress testing procedure on credit risk in the Romanian banking system. Macro stress testing, i.e. assessing the vulnerability of financial systems to exceptional but plausible macroeconomic scenarios, maintains a central role in macro-prudential and crisis management frameworks of central banks and international institutions around the globe. Credit risk remains the dominant risk challenging financial stability in the Romanian financial system, and thus this report analyses the potential impact of macroeconomic shocks scenarios on default rates in the corporate and household loan portfolios in the domestic banking system. A well-established reduced form model is proposed and tested as the core component of the modelling approach. The resulting models generally confirm the influence of macroeconomic factors on credit risk as documented in previous research including applications for Romania, but convey also specific and novel findings, such as inclusion of leading variables and construction activity level for corporate credit risk. Using the estimated model, a stress testing simulation procedure is undertaken. The simulation shows that under adverse shock scenarios, corporate default rates can increase substantially more than the expected evolution under the baseline scenario, especially in case of GDP shock, construction activity shock or interest rate shocks. Under the assumptions of these adverse scenarios, given also the large share of corporate loans in the banks’ balance sheet, the default rates evolution could have a substantial impact on banks’ loan losses. The households sector stress testing simulation show that this sector is more resilient to macroeconomic adverse evolutions, with stressed default rates higher than expected values under baseline scenario, but with substantially lower deviations. The proposed macro-perspective model and its findings can be incorporated by private banks in their micro-level portfolio risk management tools. Additionally, supplementing the authorities’ stress tests with independent approaches can enhance credibility of such financial stability assessment.
C87|ivporbit:An R package to estimate the probit model with continuous endogenous regressors|One of the most important problem of misspeciﬁcation in the probit model is the correlation between regressors and error term. To deal with this problem, some commercial software gives a solution such as Stata. For the famous R language the ivprobit gives the users the way to estimate the instrumental probit model.
C87|Synthetic data: an endogeneity simulation|This paper uses synthetic data and different scenarios to test treatments for endogeneity problems under different parameter settings. The model uses initial conditions and provides the solution for a hypothetical equation system with an embedded endogeneity problem. The behavioral and statistical assumptions are underlined as they are used through this research. A methodology is proposed for constructing and computing simulation scenarios. The econometric modeling of the scenarios is developed accordingly with the feedback obtained from previous scenarios. The inputs for these scenarios are synthetic data, which are constructed using random number machines and/or Monte Carlo simulations. The outputs of the scenarios are the model estimators. The research results demonstrated that a treatment for endogeneity can be developed as the sample size increases.
C87|Modelling the Confidence in Industry in Romania and other European Member Countries Using the Ordered Logit Model|The application of qualitative choice models is usually made by neglecting the analysis of autocorrelated and heteroscedastic errors. In the current paper, we aim to evaluate and mitigate the effects of violation of such a hypothesis using as example the modeling of confidence in industry in relation to the macroeconomic indicators for six countries of the European Union. The ordered Logit model identified in the paper revealed the common macroeconomic factors which explain the formation of confidence in industry for the countries considered in the analysis. By mitigating the heteroscedasticity problems and specifying in the model the functional form of the error dispersion, the statistically significant improvement of the model performance was obtained.
C87|The European Union Sanitary and Phytosanitary Measures and Africa’s Exports|Changes in tastes and preferences in importing countries as well as the need to keep the environment safe, especially in developed markets, has contributed to a rising trend in the demand for sanitary and phytosanitary measures for quality products. However, the stringency and the preponderance of these measures have effects on trade, particularly for the developing and least developed countries in Africa. The effects often influence the attainment of the development aspirations of these Africa countries, especially employment, poverty reduction and sustainable growth. To this end, this study investigates the export effects of the EU standards for Africa. It uses the two-step Helpman et al. (2008) extensive and intensive trade margins model for two high-value foods and two traditional products. The EU standard requirements for each product are called the ‘hurdle to pass’ before the product can gain access to the EU market. In all, 52 African countries are considered in an empirical analysis covering the period 1995 to 2012. The study finds that product standards for fish and cocoa are trade-enhancing at the extensive margins, but this is not the case at the intensive margins. However, the standards are trade-inhibiting at both the extensive and intensive margins of exports for vegetables, while the standards are trade-restrictive at the extensive margins and trade-enhancing at the intensive margins for coffee. Thus, the findings suggest that the impacts of standards on exports are commodity-specific due to the significant differences in the costs of compliance, the size of the exporting firms or countries, access to development assistance and the commodity-specific interests of countries. The study recommends development partnerships and alliance policies on the part of Africa, with the development of institutions that can improve the level of standard-compliance in all African exporting markets.
C87|Multilevel model analysis using R|The complex datasets cannot be analyzed using only simple regressions. Multilevel models (also known as hierarchical linear models, nested models, mixed models, random coefficient, random-effects models, random parameter models or split-plot designs) are statistical models of parameters that vary at more than one level. Multilevel models can be used on data with many levels, although 2-level models are the most common.Multilevel models, or mixed effects models, can be estimated in R. There are several packages available in CRAN. In this paper we are presenting some common methods to analyze these models.
C87|On heterogeneous latent class models with applications to the analysis of rating scores|Discovering the preferences and the behaviour of consumers is a key challenge in marketing. Information about such topics can be gathered through surveys in which the respondents must assign a score to a number of items. A strategy based on different latent class models can be used to analyze such data and achieve this objective: it consists in identifying groups of consumers whose response patterns are similar and characterizing them in terms of preferences and covariates. The basic latent class model can be extended by including covariates to model differences in (1) latent class probabilities and (2) conditional probabilities. A strategy for fitting and choosing a suitable model among them is proposed taking into account identifiability issues, the identification of potential covariates and the checking of goodness-of-fit. The tools to perform this analysis are implemented in the R package covLCA available from CRAN. We illustrate and explain the application of this strategy using data about the preferences of Belgian households for supermarkets. Copyright Springer-Verlag Berlin Heidelberg 2014
C87|The Relationship between Inflation Targeting and Exchange Rate Pass-Through in Turkey with a Model Averaging Approach|Turkey, as an emerging economy, has a unique experience regarding to the relationship between the rate of inflation and the exchange rate. As opposed to developed countries, the effects of exchange rate fluctuations are felt significantly on inflation dynamics and these fluctuations also influence many other macroeconomic variables via different channels with different magnitudes in developing countries. Therefore, the main concern of the paper, which is to evaluate the exchange rate pass-through (ERPT), has an important role in the success of inflation targeting regime. Using correlation coefficients between exchange rates and inflation differentials, single equation regressions, vector auto-regressions (VAR) and Markov switching regression methods; the determinants of ERPT to producer and consumer prices are quantitatively analyzed between January 1986 and August 2013. Error correction models are used to estimate the exchange rate pass-through. According to the estimation results, it is found that, similar to other developing countries, there is a substantial degree of ERPT for Turkey the greater part of which is realized almost instantaneously. Comparing to the studies on industrial countries, it is found that ERPT is higher but there are additional transmission channels just like the other emerging economies. The higher degree of ERPT in Turkey is found in those studies conducted for industrialized countries implies that there are additional transmission channels for Turkey. ERPT for producer price-index-based inflation is found to be higher than for consumer-price-index-based inflation. We also found that the degree of ERPT increases as the data frequency falls. We also determined an asymmetry in pricing behavior : while exchange rates increase, this increase is passed on to prices, yet decreases in exchange rates. Estimation results also indicate that the main factors contributing to high pass-through are past currency crises and the high degree of openness of the economy. These factors are the basis for the indexation behavior of agents. Although, the aforementioned factors are the main determinants of the degree of exchange rate pass-through, the persistency and the volatility of exchange rates can significantly affect the short run dynamics of the pass-through. The results also imply that, even if the pass-through slows down due to changing pattern of exchange rates, in order to achieve a low and stable inflation in the long run, fundamental factors that exacerbate the link between exchange rates and prices should change. Another crucial point is that according to Markov switching regression results of ERPT coefficients of domestic prices, the exchange rate pass-through coefficients vary significantly between different states.
C87|MATLAB for Economics and Econometrics A Beginners Guide|This beginners' guide to MATLAB for economics and econometrics is an updated and extended version of Frain (2010). The examples and illustrations here are based on Matlab version 8.3 (R2014a). It describes the new MATLAB Desktop, contains an introductory MATLAB session showing elementary MATLAB operations, gives details of data input/output, decision and loop structures, elementary plots, describes the LeSage econometrics toolbox and shows how to do maximum likelihood estimation. Various worked examples of the use of MATLAB in economics and econometrics are also given. I see MATLAB not only as a tool for doing economics/econometrics but as an aid to learning economics/econometrics and understanding the use of linear algebra there. This document can also be seen as an introduction to the MATLAB on-line help, manuals and various specialist MATLAB books.
C87|Note on Lilien and modified Lilien index| This article is a companion to the Lilien (lilien) and modified Lilien commands for computing relative indices in Stata. In this article, we illustrate the main features of the commands with an application to the structural determinants of regional unemployment.
C87|Note on Lilien and modified Lilien index|This article is a companion to the Lilien (lilien) and modified Lilien commands for computing relative indices in Stata. In this article, we illustrate the main features of the commands with an application to the structural determinants of regional unemployment. Copyright 2014 by StataCorp LP.
C87|The Impact of Public Support Intensity on Business R&D: Evidence from a Dose-Response Approach|This paper presents an original econometric model for estimating a dose-response function through a regression approach when: (i) treatment is continuous, (ii) individuals may react heterogeneously to observable confounders, and (iii) selection-into-treatment may be potentially endogenous. After describing the model, two estimation procedures are suggested: one based on OLS under Conditional Mean Independence (or CMI), and one based on Instrumental-Variables (IV) under selection endogeneity. The paper goes on by presenting ctreatreg, a user-written Stata routine for an easy implementation of such a model, thereby performing a Monte Carlo experiment to test the reliability of the model and of its software implementation. Finally, an application to real data for assessing the effect of public R&D support on companies' R&D expenditure is presented and results briefly commented. The usefulness of such a model for program evaluation is clearly stressed.
C87|Evaluating locally-based policies in the presence of neighbourhood effects: The case of touristic accommodation in the Garda district of Trentino|This paper presents a counter-factual model identifying Average Treatment Effects (ATEs) by Conditional Mean Independence when externality (or neighbourhood) effects are incorporated within the traditional potential outcome model. As such, it tries to generalize the usual approach, widely used in program evaluation, when SUTVA (i.e. Stable Unit Treatment Value Assumption) is relaxed. This new approach is applied to a locally-based policy. More specifically, we focus on the Garda lake area that is one of the 14 touristic districts in Trentino ? an Alpine province in north-east Italy. In the time window under scrutiny ? 2002-2006 ? Trentino had in place a subsidy policy for hotels ? 3-digit sector 55.1: hotels and similar accommodation, as defined in the NACE Rev.2 ? within the Provincial Law 6/99. There is no confounding effects coming from other policy measures given that Trentino hotels can only have access to subsidies related to Provincial Law 6/99. We rely on a database built relying on different sources and contains administrative information, structural characteristics of hotels and exhaustive information about the subsidies they received during the period. The sample size is 415 and consists of the Garda lake hotels active from 2002 to 2006.
C87|Estimating Liquidity Risk Using The Exposure‐Based Cash‐Flow‐At‐Risk Approach: An Application To The Uk Banking Sector| ABSTRACT This paper uses a relatively new quantitative model for estimating UK banks' liquidity risk. The model is called the exposure‐based cash‐flow‐at‐risk (CFaR) model, which not only measures a bank's liquidity risk tolerance but also helps to improve liquidity risk management through the provision of additional risk exposure information. Using data for the period 1997–2010, we provide evidence that there is variable funding pressure across the UK banking industry, which is forecasted to be slightly illiquid with a small amount of expected cash outflow (i.e. £0.06 billion) in 2011. In our sample of the six biggest UK banks, only the HSBC maintains positive CFaR with 95% confidence, which means that there is only a 5% chance that HSBC's cash flow will drop below £0.67 billion by the end of 2011. RBS is expected to face the largest liquidity risk with a 5% chance that the bank will face a cash outflow that year in excess of £40.29 billion. Our estimates also suggest Lloyds TSB's cash flow is the most volatile of the six biggest UK banks, because it has the biggest deviation between its downside cash flow (i.e. CFaR) and expected cash flow. Copyright © 2014 John Wiley & Sons, Ltd.
C87|Statistical Analysis Of Romanian Insurance Market. A Gross Written Premiums Perspective|The paper aims to analyze the Romanian insurance market in 2011 from the perspective of gross written premiums on non-life insurance classes, using principal component analysis technique (PCA) and cluster analysis in order to classify the 30 insurance companies by the most important components obtained by PCA.The empirical results showed that aircraft liability insurance, accident and sickness insurance, liability insurance for ships, insurance of legal expenses and the general liability insurance and surety ship insurance explain best the evolution of the insurance market. Grouping companies after the first two principal components, aircraft liability insurance, accident and sickness insurance, which recovers about 53% of the total variance of the original variables, can highlight three classes of companies: Astra, Allianz-Tiriac and Omniasig. The cluster analysis indicates the existence of four classes of companies: City Insurance, Astra, Groupama Insurances and the rest
C87|Localización espacial de la actividad económica en Medellín, 2005-2010 Un enfoque de economía urbana|Este artículo se propone estudiar la configuración espacial de la actividad económica para la ciudad de Medellín, Colombia, entre los años 2005-2010. Con este propósito se lleva a cabo la caracterización de siete actividades económicas con referencia en la estrategia de desarrollo denominada “Medellín ciudad clúster”. Se hace uso del marco teórico proporcionado por la economía urbana, herramientas de estadística y econometría espacial (análisis exploratorio de datos espaciales (AEDE) y el análisis de clúster) e información suministrada por la Subsecretaría de Catastro. El análisis concluye la existencia de estructuras espaciales definidas para los renglones económicos analizados, esto es, una estructura policéntrica para el sector servicios con dos nodos especializados y evidencia para el proceso de conformación de un nuevo nodo de desarrollo en el norte de la ciudad. Además de una distribución espacial diferenciada para la industria.
C87|Localización espacial de la actividad económica en Medellín, 2005-2010 Un enfoque de economía urbana|Este artículo se propone estudiar la configuración espacial de la actividad económica para la ciudad de Medellín, Colombia, entre los años 2005-2010. Con este propósito se lleva a cabo la caracterización de siete actividades económicas con referencia en la estrategia de desarrollo denominada “Medellín ciudad clúster”. Se hace uso del marco teórico proporcionado por la economía urbana, herramientas de estadística y econometría espacial (análisis exploratorio de datos espaciales (AEDE) y el análisis de clúster) e información suministrada por la Subsecretaría de Catastro. El análisis concluye la existencia de estructuras espaciales definidas para los renglones económicos analizados, esto es, una estructura policéntrica para el sector servicios con dos nodos especializados y evidencia para el proceso de conformación de un nuevo nodo de desarrollo en el norte de la ciudad. Además de una distribución espacial diferenciada para la industria.
C87|Understanding and teaching unequal probability of selection|This paper focuses on econometrics pedagogy. It demonstrates the importance of including probability weights in regression analysis using data from surveys that do not use simple random samples (SRS). We use concrete, numerical examples and simulation to show how to effectively teach this difficult material to a student audience. We relax the assumption of simple random sampling and show how unequal probability of selection can lead to biased, inconsistent OLS slope estimates. We then explain and apply probability weighted least squares, showing how weighting the observations by the reciprocal of the probability of inclusion in the sample improves performance. The exposition is non-mathematical and relies heavily on intuitive, visual displays to make the content accessible to students. This paper will enable professors to incorporate unequal probability of selection into their courses and allow students to use best practice techniques in analyzing data from complex surveys. The primary delivery vehicle is Microsoft Excel®. Two user-defined array functions, SAMPLE and LINESTW, are included in a prepared Excel workbook. We replicate all results in Stata® and offer a do-file for easy analysis in Stata. Documented code in Excel and Stata allows users to see each step in the sampling and probability weighted least squares algorithms. All files and code are available at www.depauw.edu/learn/stata.
C87|Understanding and Teaching Within-Cluster Correlation in Complex Surveys|This econometrics pedagogy paper demonstrates the importance of using cluster standard errors with data generated from complex surveys. Simulation is used to show that both classic ordinary least squares and robust standard errors perform poorly in the presence of within-cluster correlated errors, while cluster standard errors are much better. We take advantage of Excel’s spreadsheet interface to produce clear, strong visuals of the data generation process and intuitively explain key results. Stata and R implementations are also provided. We conclude with suggestions for how to use these files in the classroom.
C87|Seasonal Stability Tests in gretl. An Application to International Tourism Data| The seasonal stability tests of Canova & Hansen (1995) (CH) provide a method complementary to that of Hylleberg et al. (1990) for testing for seasonal unit roots. But the distribution of the CH tests are unknown in small samples. We present a method to numerically compute critical values and P-values for the CH tests for any sample size and any seasonal periodicity. In fact this method is applicable to the types of seasonality which are commonly in use, but also to any other.
C87|"Profile Of Migrants In Romania – A Statistical Analysis Using ""R"""|This paper exposes the methodology of creating migrant’s profile before crisis and after crisis using logistic regression in R. The profile was created based on some social and demographic characteristics provided by the 2007 and 2011 Labour Force Survey, assuring the representativeness of results at national and regional level. In this sense, the logistic regression was used to model the relationship between migrants and some independent variables.
C87|"Manipulation Of Large Databases With ""R"""|Nowadays knowledge is power. In the informational era, the ability to manipulate large datasets is essential for making long term strategies. More and more companies and official statistics offices need to use large databases but also by increasing the volume of data it increases the complexity of manipulating it. A basic economy rule is that the supply follows the demand; hereby, much software for manipulating big data was created in order to follow the necessity. The aim of this paper is to underline the performance of R in manipulating large databases. Regarding to this, the paper is primarily intended for people already familiar with common databases and statistical concepts. The paper illustrates as its best how simply R - both open-source and commercial versions – is used to handle large databases manipulation.
C87|Simultaneous-equations Analysis in Regional Science and Economic Geography|This paper provides an overview over simultaneous equation models (SEM) in the context of analyses based on regional data. We describe various modelling approaches and highlight close link of SEMs to theory and also comment on the advantages and disadvantages of SEMs.We present selected empirical works using simultaneous-equations analysis in regional science and economic geography in or-der to show the wide scope for applications. We thereby classify the empirical contributions as either being structural model presentations or vector autoregressive (VAR) models. Finally, we provide the reader with some details on how the various models can be estimated with available software packages such as STATA, LIMDEP or Gauss.
C87|Merger Simulation with Nested Logit Demand - Implementation using Stata|In this article we show how to implement merger simulation in Stata after estimating an aggregate nested logit demand system with a linear regression model. We also show how to implement merger simulation when the demand parameters are not estimated, but instead calibrated to be consistent with outside information on average price elasticities and profit margins.
C87|Score-Based Tests of Measurement Invariance: Use in Practice|In this paper, we consider a family of recently-proposed measurement invariance tests that are based on the scores of a fitted model. This family can be used to test for measurement invariance w.r.t. a continuous auxiliary variable, without pre-specification of subgroups. Moreover, the family can be used when one wishes to test for measurement invariance w.r.t. an ordinal auxiliary variable, yielding test statistics that are sensitive to violations that are monotonically related to the ordinal variable (and less sensitive to non-monotonic violations). The paper is specifically aimed at potential users of the tests who may wish to know (i) how the tests can be employed for their data, and (ii) whether the tests can accurately identify specific models parameters that violate measurement invariance (possibly in the presence of model misspecification). After providing an overview of the tests, we illustrate their general use via the R packages lavaan and strucchange. We then describe two novel simulations that provide evidence of the tests' practical abilities. As a whole, the paper provides researchers with the tools and knowledge needed to apply these tests to general measurement invariance scenarios.
C87|Rasch Mixture Models for DIF Detection: A Comparison of Old and New Score Specifications|Rasch mixture models can be a useful tool when checking the assumption of measurement invariance for a single Rasch model. They provide advantages compared to manifest DIF tests when the DIF groups are only weakly correlated with the manifest covariates available. Unlike in single Rasch models, estimation of Rasch mixture models is sensitive to the specification of the ability distribution even when the conditional maximum likelihood approach is used. It is demonstrated in a simulation study how differences in ability can influence the latent classes of a Rasch mixture model. If the aim is only DIF detection, it is not of interest to uncover such ability differences as one is only interested in a latent group structure regarding the item difficulties. To avoid any confounding effect of ability differences (or impact), a score distribution for the Rasch mixture model is introduced here which is restricted to be equal across latent classes. This causes the estimation of the Rasch mixture model to be independent of the ability distribution and thus restricts the mixture to be sensitive to latent structure in the item difficulties only. Its usefulness is demonstrated in a simulation study and its application is illustrated in a study of verbal aggression.
C87|SIMUL 3.2: An Econometric Tool for Multidimensional Modelling|Initially developed in the context of $${\tt REGILINK}$$ project, $${\tt SIMUL 3.2}$$ econometric software is able to estimate and to run large-scale dynamic multi-regional, multi-sectoral models. The package includes a data bank management module, $${\tt GEBANK}$$ which performs the usual data import/export functions, and transformations (especially the RAS and the aggregation one), a graphic module, $${\tt GRAPHE}$$ , a cartographic module, $${\tt GEOGRA}$$ for a “typical use”. For an “atypical use” the package includes $${\tt CHRONO}$$ to help for the WDC (Working Days Correction) estimation and $${\tt GNOMBR}$$ to replace the floating point arithmetic by a multi-precision one in a program. Although the current package includes a basic estimation’s (OLS) and solving’s (Gauss–Seidel) algorithms, it allows user to implement the equations in their reduced form $${Y_{r,b}=X_{r,b} + \varepsilon}$$ and to use alternative econometric equations. $${\tt SIMUL}$$ provides results and reports documentation in ASCII and $${\hbox{\LaTeX}}$$ formats. The next releases of $${\tt SIMUL}$$ should improve the OLS procedure according to the Wilkinson’s criteria, include Hildreth–Lu’s algorithm and comparative statics option. Later, the package should allow other models implementations (Input–Output, VAR etc.). Even if it’s probably outclassed by the major softwares in terms of design and statistic tests sets, $${\tt SIMUL}$$ provides freely basic evolutive tools to estimate and run easily and safety some large scale multi-sectoral, multi-regional, econometric models. Copyright Springer Science+Business Media, LLC. 2013
C87|Gateveys|This paper describes how to use the R package gateveys to establish a transparent and reproducible aggregation work flow for longitudinal data stemming from business tendency surveys (BTS). Business tendency survey researchers are addressed in particular though the suggested work flow could also be applied to other processes that generate categorical data. The package has two main features: First, it provides functions to build an aggregation process that re-calculates all periods when a new survey wave is added and hence can be fully reproduced at any later stage. Second, the package can be used to dynamically add localized meta information to the resulting time series object during the aggregation process. Besides, the paper suggests a software architecture for use of the package in a scenario with regular, periodical survey waves.
C87|Fast Methods for Jackknifing Inequality Indices|The jackknife is a resampling method that uses subsets of the original database by leaving out one observation at a time from the sample. The paper outlines a procedure to obtain jackknife estimates for several inequality indices with only a few passes through the data. The number of passes is independent of the number of observations. Hence, the method provides an efficient way to obtain standard errors of the estimators even if sample size is large. We apply our method using micro data on individual incomes for Germany and the US.
C87|Interbank Market Structure and Accurate Estimation of an Aggregate Liquidity Shock|It's customary among money market analysts to blame interest rate deviations from the Bank of Russia's target band on the market structure imperfections or segmentation. We isolate one form of such market imperfection and provide an illustration of its potential impact on central bank's open market operations efficiency in the current monetary policy framework. We then hypothesize that naive (market) structure-agnostic liquidity gap aggregation will lead to market demand underestimation in some conditions and provide an empirical backing for it.
C87|R versus Other Statistical Software|In this paper we intend to present an overview of the advantages of using R – the most powerful statistical software. The statements on R are exposed versus SAS and SPSS – actually the most used statistical software in Romania. The study also focuses on comparing strengths and weaknesses of SAS, SPSS and R. As an example on how R is much more flexible that the other software, we used a statistical analysis. We will see how dominant R is already in the academia and how is predicted to be in the commercial. It is time now for opensource powerful software for official statistics as well as for companies and business.
C87|Statistical Analysis of International Migration Using R Software|The aim of this paper is to expose the results of my research concerning the migrant’s profile built up by means of logit regression model based on social and demographic characteristics. Within these characteristics could be mentioned: the age group, the gender, the education level, the marital status, the activity, the residence area. The statistical software used is R which represents the most popular and powerful open source programming technology among statisticians during the last years. An application on logistic model and its performance is presented based on 2011 Labour Force Survey (LFS) referring to the international migration in Romania
C87|Some thoughts on accurate characterization of stock market indexes trends in conditions of nonlinear capital flows during electronic trading at stock exchanges in global capital markets|This research represents some thoughts on the accurate characterization of the stock market indexes trends in the conditions of the nonlinear capital flows at the stock exchanges in the global capital markets. We make our original research proposal that the nonlinear capital flows in the process of the electronic trading can originate the nonlinear changes of the stock market indexes at the stock exchanges in the global capital markets. We suggest that the econophysics techniques can be used to precisely characterize the nonlinearities in the finances. We performed the research of the nonlinearities in Matlab, researching: 1) the ideal dependence of the stock market index over the time, 2) the linear dependence of the stock market index over the time, 3) the quadratic dependence of the stock market index over the time, 2) the exponential dependence of the stock market index over the time. We researched the following indexes: 1) The Dow Jones Industrial Average (DJIA) index; 2) The Standard and Poor’s 500 (S&P 500) index; 3) The NYSE Composite index; 4) The Hong Kong Hang Seng index; 5) The Shanghai Composite index; 6) The Financial Times Securities Exchange (FTSE100) index; 7) The Deutscher Aktienindex (DAX) index; 8) The Nikkei 225 Stock Average index over the certain time periods. The selected time periods were: 6 months; 12 months; 24 months. We assumed that, in every considered case, there are the complex changes of the company valuation, foreign exchange rates, interest rates, prices of strategic commodities over the specified time period. We found that there are the nonlinearities in the characteristic dependences of the stock exchanges indexes on the time. Our research results are in a good agreement with the research findings in Abhyankar, Copeland, Wong (1995, 1997), however the multiple evidences of quantum chaos were found in the researched stock market indexes dependences for the first time.
C87|On the tracking and replication of hedge fund optimal investment portfolio strategies in global capital markets in presence of nonlinearities, applying Bayesian filters: 1. Stratanovich – Kalman – Bucy filters for Gaussian linear investment returns distribution and 2. Particle filters for non-Gaussian non-linear investment returns distribution|The hedge fund represents a unique investment opportunity for the institutional and private investors in the diffusion-type financial systems. The main objective of this condensed article is to research the hedge fund’s optimal investment portfolio strategies selection in the global capital markets with the nonlinearities. We provide a definition for the hedge fund, describe the hedge fund’s organization structures and characteristics, discuss the hedge fund’s optimal investment portfolio strategies and review the appropriate hedge fund’s risk assessment models for investing in the global capital markets in time of high volatilities. We analyze the advanced techniques for the hedge fund’s optimal investment portfolio strategies replication, based on both the Stratonovich – Kalman - Bucy filtering algorithm and the particle filtering algorithm. We developed the software program with the embedded Stratonovich – Kalman - Bucy filtering algorithm and the particle filtering algorithm, aiming to track and replicate the hedge funds optimal investment portfolio strategies in the practical cases of the non-Gaussian non-linear chaotic distributions.
C87|Temporal Disaggregation of Time Series|Temporal disaggregation methods are used to disaggregate low frequency time series to higher frequency series, where either the sum, the average, the ﬁrst or the last value of the resulting high frequency series is consistent with the low frequency series. Temporal disaggregation can be performed with or without one or more high frequency indicator series. The package tempdisagg is a collection of several methods for temporal disaggregation.
C87|Determinantes de la Pobreza y Vulnerabilidad Social en República Dominicana. 2000-2012<BR>[Determinants of Poverty and Social Vulnerability in the Dominican Republic. 2000-2012]|The paper examines the macro and microeconomic determinants of monetary poverty in the Dominican Republic for the period 2000-2006, using the National Labor Force Survey (ENFT) and the official monetary poverty methodology as the main inputs. Macroeconomic determinants are analyzed from the effect of household income growth, inflation and inequality, which show evidence for pro-poor growth and a positive effect of reducing inequality, despite persisting levels of Poverty rates higher than those observed before the 2003 crisis, partly as a result of the deterioration of real per capita household income (inflation) around the crisis. The microeconomic determinants are analyzed from the profiles of monetary poverty that allow to decompose the changes in the incidence within and between population groups and show evidence in favor of the multidimensionality of poverty (Alkire-Foster) and its explanatory variables. These variables are used in a probabilistic model (Probit) that shows that the greatest marginal effects on the probabilities of being poor are derived from the conditions of the head of the household, especially those related to the labor market.
C87|Stress indicator construction for internal money market|In this article we propose a modification of time-series segmentation algorithm which allows to identify homogenous periods of money market history by clustering multidimensional probability distributions of relevant variables. We provide step-by-step instructions to systematically choose how many distinct states of the nominal variable is sufficient for precise description of the money market historical conditions and hint at variables which might be suitable for monitoring money market form a central bank’s point of view
C87|Some Conceptual Aspects of the Multilevel Modeling for the Study of Social-Economic Phenomena|Multilevel modeling has been mainly used in education research, health studies, psychology and sociology, its application to the study of social-economic phenomena and especially to business studies is rather scarce. Hence, its application to business studies might produce interesting new insights on business performance, especially due to the micro-macro interactions, for all the stakeholders. Therefore, the research is focused on business, namely to the analysis of the Romanian ICT sector. The current paper aims at illustrating a new assessment tool for business analysis, namely the assessment of company performance through a new approach called multilevel modeling. Given the nested structured of the database comprising the Romanian companies operating in ICT industry, a multilevel random coefficient model is suggested and how it can be fitted in R software. The conclusions of the research can be used by various stakeholders, including policy makers.
C87|Stochastic frontier analysis using Stata|" This article describes sfcross and sfpanel, two new Stata commands for the estimation of cross-sectional and panel-data stochastic frontier models. sfcross extends the capabilities of the frontier command by including additional models (Greene, 2003, Journal of Productivity Analysis 19: 179–190; Wang, 2002, Journal of Productivity Analysis 18: 241–253) and command functionality, such as the possibility of managing complex survey data characteristics. Similarly, sfpanel allows one to fit a much wider range of time-varying inefficiency models compared with the xtfrontier command, including the model of Cornwell, Schmidt, and Sickles (1990, Journal of Econometrics 46: 185–200); the model of Lee and Schmidt (1993, in The Measurement of Productive Efficiency: Techniques and Applications), a production frontier model with flexible temporal variation in technical efficiency; the flexible model of Kumbhakar (1990, Journal of Econometrics 46: 201–211); the inefficiency effects model of Battese and Coelli (1995 Empirical Economics 20: 325–332); and the ""true"" fixed- and random-effects models of Greene (2005a, Journal of Econometrics 126: 269–303). A brief overview of the stochastic frontier literature, a description of the two commands and their options, and examples using simulated and real data are provided."
C87|Stochastic frontier analysis using Stata|"This article describes sfcross and sfpanel, two new Stata commands for the estimation of cross-sectional and panel-data stochastic frontier models. sfcross extends the capabilities of the frontier command by including additional models (Greene, 2003, Journal of Productivity Analysis 19: 179–190; Wang, 2002, Journal of Productivity Analysis 18: 241–253) and command functionalities, such as the possibility of managing complex survey data characteristics. Similarly, sfpanel allows one to fit a much wider range of time-varying inefficiency models compared with the xtfrontier command, including the model of Cornwell, Schmidt, and Sickles (1990, Journal of Econometrics 46: 185–200); the model of Lee and Schmidt (1993, in The Measurement of Productive Efficiency: Techniques and Applications), a production frontier model with flexible temporal variation in technical efficiency; the flexible model of Kumbhakar (1990, Journal of Econometrics 46: 201–211); the inefficiency effects model of Battese and Coelli (1995 Empirical Economics 20: 325–332); and the ""true"" fixed- and random-effects models of Greene (2005a, Journal of Econometrics 126: 269–303). A brief overview of the stochastic frontier literature, a description of the two commands and their options, and examples using simulated and real data are provided. Copyright 2013 by StataCorp LP."
C87|Productivity Premia for German Manufacturing Firms Exporting to the Euro-Area and Beyond: First Evidence from Robust Fixed Effects Estimations|This paper makes three contributions. (1) It summarizes in tabular form a recent literature made of 36 micro-econometric studies for 16 different countries on the relationship between export destination and firm performance. (2) It reports estimates of the productivity premium of German firms exporting to the Euro-zone and beyond, controlling for unobserved time invariant firm specific effects, and tests for self-selection of more productive firms into exporting beyond the Euro-zone. (3) It corrects a serious flaw in hitherto published studies that ignore the potentially disastrous consequences of extreme observations, or outliers. The paper shows that estimates of the exporter productivity premium by destination are driven by a small share of outliers. Using a “clean” sample without outliers the estimated productivity premium of firms that export to the Euro-zone only is no longer much smaller that the premium of firms that export beyond the Euro-zone, too, and the premium itself over firms that serve the German market only is tiny. Furthermore, an ex-ante differential that is statistically significant and large only shows up for enterprises that exported to the Euro-zone already and start to export to countries outside the Euro-zone. These conclusions differ considerably from those based on non-robust standard regression analyses.<br><small>(This abstract was borrowed from another version of this item.)</small>
C87|Economic loss of chronic kidney disease in Chun district, Phayao province|This study is an analysis of the economic loss and loss of income, and a holistic approach of life expectancy of outpatient who has chronic kidney disease in Chun district, Phayao province. The sample is an outpatient with chronic kidney disease and admitted to the Chun hospital, Chun district, Phayao, for 300 people in a total. The cost of hospital data is used for analysis the cost of providing care for patients with chronic kidney disease, and allocated by using simultaneous equation method the descriptive statistics, percentage and average are used in the analysis of economic loss. The holistic health of the population measurement model (Disability-Adjusted Life Years-DALYs) is used to find the economic loss thoughtout a holistic life expectancy. The results found that the total economic loss cost was 14,624.98 baht per person in annual. The cost of providing care for patients showed a maximum value for 47.24 percent. The total economic loss throughout a holistic life expectancy was 368,605.38 baht per person, which in terms of the opportunity cost of premature death showed a maximum value of 97.79 percent. Obviously, the economic loss in terms of the opportunity cost of premature death was the highest cost. Therefore, if a patient was likely to increase or death with chronic kidney disease, this would result in economic loss up.
C87|Fiscal asymmetries and the survival of the euro zone|A model of a dependent central bank that internalizes the government’s budget constraint is used to examine the optimal composition of the euro zone. The model embodies the desire to stimulate output and to provide monetary financing to governments. Unable to pre-commit to first-best policies, the central bank produces excess inflation — a tendency partially reduced in a monetary union. On the basis of this framework, calibrated to euro zone data, the current membership is shown not to be optimal: other members would benefit from the expulsion of several countries, notably Greece, Italy, and France. A narrow monetary union centered around Germany might be able to guarantee central bank independence, but simulation results suggest that such a narrow monetary union would not be in Germany’s interest relative to a return to the deutsche mark.
C87|Crude oil market efficiency: An empirical investigation via the Shannon entropy|This paper evaluates the time-varying degrees of weak-form efficiency of the crude oil markets using the Modified Shannon Entropy (MSE) and the Symbolic Time Series Analysis (STSA) approach. Using daily data from May 20, 1987 to March 6, 2012 for two worldwide crude oil benchmarks (West Texas Intermediate and Europe Brent), our findings reveal that the weak-form market efficiency of two oil markets evolves through time, but with different time trends. Moreover, the WTI market appears to be less efficient than the Europe Brent. These results have several implications for commodity portfolio hedgers and policymakers.
C87|Ivtreatreg: a new STATA routine for estimating binary treatment models with heterogeneous response to treatment under observable and unobservable selection|This paper presents a new user-written STATA command called ivtreatreg for the estimation of five different (binary) treatment models with and without idiosyncratic (or heterogeneous) average treatment effect. Depending on the model specified by the user, ivtreatreg provides consistent estimation of average treatment effects both under the hypothesis of “selection on observables” and “selection on unobservables” by using Ordinary Least Squares (OLS) regression in the first case, and Intrumental-Variables (IV) and Selection-model (à la Heckman) in the second one. Conditional on a pre-specified subset of exogenous variables x – thought of as driving the heterogeneous response to treatment – ivtreatreg calculates for each model the Average Treatment Effect (ATE), the Average Treatment Effect on Treated (ATET) and the Average Treatment Effect on Non-Treated (ATENT), as well as the estimates of these parameters conditional on the observable factors x, i.e., ATE(x), ATET(x) and ATENT(x). The five models estimated by ivtreatreg are: Cf-ols (Control-function regression estimated by OLS), Direct-2sls (IV regression estimated by direct two-stage least squares), Probit-2sls (IV regression estimated by Probit and two-stage least squares), Probit-ols (IV two-step regression estimated by Probit and ordinary least squares), and Heckit (Heckman two-step selection model). An extensive treatment of the conditions under which previous methods provide consistent estimation of ATE, ATET and ATENT can be found, for instance, in Wooldgrige (2002, Chapter 18). The value added of this new STATA command is that it allows for a generalization of the regression approach typically employed in standard program evaluation, by assuming heterogeneous response to treatment.
C87|A continuous treatment model for estimating a Dose Response Function under endogeneity and heterogeneous response to observable confounders: Description and implementation via the Stata module “ctreatreg”|This paper presents an original econometric model for estimating a Dose Response Function though a regression approach when treatment is continuous, individuals may react heterogeneously to observable confounders and selection-into-treatment may be (potentially) endogenous. After the description of the model, two estimation procedures are set out: one based on OLS under conditional mean independence, and one based on IV under selection endogeneity. The paper goes on by presenting ctreatreg, a author’s user-written Stata routine for an easy implementation of such a model. The paper proceeds by performing a Monte Carlo experiment to test the reliability of the model and of its associated Stata routine. Results show that the model and the Stata routine ctreatreg are both reliable as estimates consistently fit the expected results.
C87|Wavelet multiple correlation and cross-correlation: A multiscale analysis of Eurozone stock markets|Statistical studies that consider multiscale relationships among several variables use wavelet correlations and cross-correlations between pairs of variables. This procedure needs to calculate and compare a large number of wavelet statistics. The analysis can then be rather confusing and even frustrating since it may fail to indicate clearly the multiscale overall relationship that might exist among the variables. This paper presents two new statistical tools that help to determine the overall correlation for the whole multivariate set on a scale-by-scale basis. This is illustrated in the analysis of a multivariate set of daily Eurozone stock market returns during a recent period. Wavelet multiple correlation analysis reveals the existence of a nearly exact linear relationship for periods longer than the year, which can be interpreted as perfect integration of these Euro stock markets at the longest time scales. It also shows that small inconsistencies between Euro markets seem to be just short within-year discrepancies possibly due to the interaction of different agents with different trading horizons.
C87|Estimation of Discount Factor and Coefficient of Relative Risk Aversion in Selected Countries|We estimate the long-run discount factor for a group of developed and developing countries through standard methodology incorporating adaptive expectations of inflation. We find that the discount factor of developing countries is relatively nearer to unity as compared to that of the developed countries. In the second part, while considering a standard Euler equation for household's inter-temporal consumption, we estimate the parameter of constant relative risk aversion (CRRA) for Pakistan by using the Generalized Method of Moments (GMM) approach. The resulting parameter value of CRRA conforms to the empirical range for developing countries (as given in, Cardenas and Carpenter, 2008) The GMM estimator for the discount factor reinforces its result from the first part of the paper. Consequently we show that different combination values for both the parameters result in different (in terms of magnitude) impulse response functions, in response to tight monetary policy shocks in a simple New Keynesian macroeconomic model.
C87|Food Responsibility - A National Challenge. Case Study - Implementation Of A Social Campaign In Bucharest Schools|Those who think they have no time for healthy eating will sooner or later have to find time for illness. (Edward Stanley) Our food should be our medicine and our medicine should be our food. (Hippocrates) Eating healthy does not mean giving up favorite food dishes or replace them with others, who have not heard and to be cooked by complicated methods. Your favorite dishes - steak, hamburgers, fries, delicious sauces - can be cooked to be not only tasty but also healthier. It is important to know the principles of healthy eating. The proportion of food consumed is very important. Healthy body needs five parts of carbohydrates (sugar and starch) in a part of a protein and fat. Excess or absence of a component can damage balance, favoring disease.
C87|Hierarchical Archimedean Copulae: The HAC Package|This paper aims at explanation of the R-package HAC, which provides user friendly methods for dealing with high-dimensional hierarchical Archimedean copulae (HAC). A computationally ecient estimation procedure allows to recover the structure and the parameters of HACs from data. In addition, arbitrary HACs can be constructed to sample random vectors and to compute the values of the corresponding cumulative distribution as well as density functions. Accurate graphics of the important characteristics of the package's object hac can be produced by the generic plot function.
C87|Flexible Rasch Mixture Models with Package psychomix| Measurement invariance is an important assumption in the Rasch model and mixture models constitute a flexible way of checking for a violation of this assumption by detecting unobserved heterogeneity in item response data. Here, a general class of Rasch mixture models is established and implemented in R, using conditional maximum likelihood estimation of the item parameters (given the raw scores) along with flexible specification of two model building blocks: (1) Mixture weights for the unobserved classes can be treated as model parameters or based on covariates in a concomitant variable model. (2) The distribution of raw score probabilities can be parametrized in two possible ways, either using a saturated model or a specification through mean and variance. The function raschmix() in the R package psychomix provides these models, leveraging the general infrastructure for fitting mixture models in the flexmix package. Usage of the function and its associated methods is illustrated on artificial data as well as empirical data from a study of verbally aggressive behavior.
C87|Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned| Beta regression – an increasingly popular approach for modeling rates and proportions – is extended in various directions: (a) bias correction/reduction of the maximum likelihood estimator, (b) beta regression tree models by means of recursive partitioning, (c) latent class beta regression by means of finite mixture models. All three extensions may be of importance for enhancing the beta regression toolbox in practice to provide more reliable inference and capture both observed and unobserved/latent heterogeneity in the data. Using the analogy of Smithson and Verkuilen (2006), these extensions make beta regression not only “a better lemon squeezer” (compared to classical least squares regression) but a full-fledged modern juicer offering lemon-based drinks: shaken and stirred (bias correction and reduction), mixed (finite mixture model), or partitioned (tree model). All three extensions are provided in the R package betareg (at least 2.4-0), building on generic algorithms and implementations for bias correction/reduction, model-based recursive partioning, and finite mixture models, respectively. Specifically, the new functions betatree() and betamix() reuse the object-oriented flexible implementation from the R packages party and flexmix, respectively.
C87|Global Financial Crisis And Unit-Linked Insurance Markets Efficiency: Empirical Evidence From Central And Eastern European Countries|This paper empirically investigates the impact of the Global financial crisis on the efficiency of four Central and Eastern European emerging unit-linked insurance markets, applying the automatic variance ratio (AVR) test of Kim (2009) and variance ratio tests using ranks and signs by Wright (2000) for entire, pre-crisis and crisis periods. This study contributes to the existing literature on efficient market hypothesis with several distinct features: it provides a systematic review of the weak-form market efficiency literature that examines return predictability of the daily ING unit-linked funds prices; also the article aims at monitoring any improvement in the degree of efficiency in time and also examines the relative efficiency of unit-linked insurance markets in pre-crisis and crisis periods. Unit linked insurance are life insurance policies with investment component. In the literature there are few studies investigating the effects of a financial crisis on the potential of predictability and implicitly on the degree of efficiency of financial markets. The occurrence of a market crash or financial crisis is a possible contributing factor of market inefficiency. Most of the studies are focused on the Asian crisis in 1997: Holden et al. (2005) examined the weak-form efficiency of eight emerging Asian stock markets using VR tests before, during and after the Asian crisis; Kim and Shamsuddin (2008) used three different types of multiple VR tests for nine Asian stock markets; the findings reported by Lim et al. (2008) are consistent with those reported by Cheong et al. (2007), in which the highest inefficiency occurs during the crisis period. Todea and Lazar (2010) investigated the effects of the Global crisis on the relative efficiency of ten CEE stock markets, using Generalized Spectral test of Escanciano and Velasco (2006). Wright (2000) proposes the alternative non-parametric variance ratio tests using ranks and signs of return and demonstrates that they may have better power properties than other variance ratio tests. Kim (2009) found that the wild bootstrap AVR significantly improves the size and power properties of the AVR test. Using the bootstrapped automatic VR test developed by Kim (2009) and Wright'(tm)s test, the statistical findings show that the degree of the markets'(tm) inefficiency varies through time and surprisingly the empirical results suggest that the Global crisis led to a decrease of predictability and hence to an improvement of relative efficiency for five of the eight ING funds.
C87|Comparing performance of statistical models for individual’s ability index and ranking|Efficient allocation of resources is the basic problem in economics. Firms, educational institutions, universities are faces problem of estimating true abilities and ranking of individuals to be selected for job, admissions and scholarship awards etc. This study will provide a guide line what technique should to be used for estimating true ability indices and ranking that reveals ability with maximum efficiency as well as it clearly has the advantage of differentiating among individuals having equal raw score. Two major theories Classical Testing Theory and Item Response Theory have been using in the literature. We design two different Monte Carlo studies to investigate which theory is better and which model perform more efficiently. By discussing the weaknesses of CTT this study proved that IRT is superior to CTT. Different IRT models have been used in literature; we measured the performance of these models and found that Logistic P2 model is best model. By using this best model we estimate the ability indices of the students on the basis of their entry test scores and then compare with their abilities obtained from final board examination result (used as proxy of true abilities). This is a reasonable because the final exam consists of various papers and chance variation in ability Index is a minimum. With real life application this study also proved that IRT estimate the true abilities more efficiently as compared to classical methodology.
C87|On whether foreign direct investment catalyzes economic development in Nigeria|This paper investigated the impact of Foreign Direct Investment on some selected macro-economic variables such as real GDP, gross fixed capital formation and unemployment. Data for the variables were sourced from the Central Bank of Nigeria’s Statistical Bulletin. For the assessment of this impact, the author used co-integration and error correction model to arrive at a parsimonious result which revealed that foreign direct investment though impacts positively and significantly on the gross fixed capital formation and the real GDP, has not made any positive and significant impact on the reduction of unemployment in Nigeria. The researcher therefore calls for putting in order the Country’s social, political and economic environment for foreign investment to yield dividends that will be enormous enough as to significantly reduce unemployment and engender instant growth on real GDP.
C87|Numerical accuracy of Ox 6.2|This article evaluates the numerical accuracy of Ox object-oriented matrix programming language. It uses the Standard Reference Datasets (StRD) for estimation accuracy, the ELV DOS program for statistical distribution accuracy, and the DIEHARD battery of randomness tests for random number generation accuracy. The main finding of the paper is that Ox is quite accurate in almost every area it is tested. However, it should also be stressed that there is room for improvement in its optimization library.
C87|Indirect estimation of GARCH models with alpha-stable innovations|Several studies have highlighted the fact that heavy-tailedness of asset returns can be the consequence of conditional heteroskedasticity. GARCH models have thus become very popular, given their ability to account for volatility clustering and, implicitly, heavy tails. However, these models encounter some difficulties in handling financial time series, as they respond equally to positive and negative shocks and their tail behavior remains too short even with Student-t error terms. To overcome these weaknesses we apply GARCH-type models with alpha-stable innovations. The stable family of distributions constitutes a generalization of the Gaussian distribution that has intriguing theoretical and practical properties. Indeed it is stable under addiction and, having four parameters, it allows for asymmetry and heavy tails. Unfortunately stable models do not have closed likelihood function, but since simulated values from α-stable distributions can be straightforwardly obtained, the indirect inference approach is particularly suited to the situation at hand. In this work we provide a description of how to estimate a GARCH(1,1) and a TGARCH(1,1) with symmetric stable shocks using as auxiliary model a GARCH(1,1) with skew-t innovations. Monte Carlo simulations, conducted using GAUSS, are presented and finally the proposed models are used to estimate the IBM weekly return series as an illustration of how they perform on real data.
C87|Global optimization of some difficult benchmark functions by cuckoo-hostco-evolution meta-heuristics|This paper proposes a novel method of global optimization based on cuckoo-host co-evaluation. It also develops a Fortran-77 code for the algorithm. The algorithm has been tested on 96 benchmark functions (of which the results of 30 relatively harder problems have been reported). The proposed method is comparable to the Differential Evolution method of global optimization.
C87|Stock Market Integration and International Portfolio Diversification between U.S. and ASEAN Equity Markets|The paper empirically analyzes stock market integration and the benefit possibilities of international portfolio diversification across the Southeast Asia (ASEAN) and U.S. equity markets. It employs daily sample of 6 ASEAN equity market indices and S&P 500 index as a proxy of U.S. market index from years 2001 to 2010. The paper examines the stock market return interdependence from three different perspectives which are ‘long-term’, ‘short-term’ and ‘dynamic’ perspectives. In order to investigate the long-run interdependencies, the Johansen-Juselius multivariate co-integration test and the bivariate Engle-Granger 2-step method were used. In respect to the short-run interdependencies, the Generalized Impulse Response Function (GIRF) and the Generalized Forecast Error Variance Decomposition (GFEVD) are employed. Finally, to assess the dynamic structure of equity market co-movements, the Dynamic Conditional Correlation (DCC) model is engaged. Results suggest that in the long-run, there are no potential benefits in diversifying investment portfolios across the ASEAN and U.S. market since there are evidences of cointegration among them. However, the potential benefits of international portfolio diversification can be seen throughout the short-run-period. Subsequently, the DCC findings suggest an overall proposition that by the end of 2010, most of the ASEAN markets do not share the U.S. stock price movement.
C87|Simplifying the estimation of difference in differences treatment effects with Stata|This paper explains the insights of the Stata's user written command diff for the estimation of Difference in Differences treatment effects (DID). The options and the formulas are detailed for the single DID, Kernel Propensity Score DID, Quantile DID and the balancing properties . An example of the features of diff is presented by using the dataset from Card and Krueger (1994).
C87|Bringing New Opportunities to Develop Statistical Software and Data Analysis Tools in Romania|In the last decade, open source programming technology is widely used among statisticians for developing a new statistical software and data analysis. This is R software environment and the main objective of this paper is to underline the importance of R for statistical computations, data analysis, visualization and applications in various fields. Regarding to this, the paper is primarily intended for people already familiar with common statistical concepts. Thus the statistical methods used to illustrate the R performance are not explained in detail. The main intention is to offer an overview to get started, to motivate beginners by illustrating the flexibility of R, and to show how simply it enables the user to carry out statistical computations.
C87|Estimation of Discount Factor ÃŸ and Coefficient of Relative Risk Aversion ? in Selected Countries|The long-run discount factor for a group of developed and developing countries is estimated through standard methodology incorporating adaptive expectations of inflation. In the second part, while considering a standard Euler equation for household's intertemporal consumption, the parameter of constant relative risk aversion (CRRA) for Pakistan is estimated by using the Generalized Method of Moments (GMM) approach. The resulting parameter value of CRRA conforms to the empirical range for developing countries (as given in, Cardenas and Carpenter, 2008) The GMM estimator for the discount factor reinforces its result from the first part of the paper. [SBP WP no. 53]. URL:[http://www.sbp.org.pk/publications/wpapers/2012/wp53.pdf].
C87|Instrumental variable estimation of a nonlinear Taylor rule|This paper studies nonlinear, threshold, models in which some of the regressors can be endogenous. An estimation strategy based on instrumental variables was originally developed for dynamic panel models and we extend it to time series models. We apply this methodology to a forward-looking Taylor rule where nonlinearity is introduced via inflation thresholds.<br><small>(This abstract was borrowed from another version of this item.)</small>
C87|Using analytics for understanding the consumer online|To stay competitive companies need to understand the present consumer behavior and anticipate it for the future. As online businesses continue to grow and people spend more time on Internet, analytics services became essential in this new world of communication, globalization and localization. Web analytics provide invaluable insights into the behavior of visitors and consumers. The purpose of this paper is to analyze how the analytics data can be effectively used for understanding the present consumer behavior. The result shows that through the use of analytics a company can predict the future consumer behavior and tailor specific messages and promotions sent to each individual. Patterns and anomalies identified at this step can help improve overall understanding of business processes and web site content and design.
C87|Technical Efficiency of Thai Manufacturing SMEs: a Comparative Study of North-eastern Provinces|A major motivation of this study is to examine factors that are most important in contributing to the relatively poor efficiency performance of Thai manufacturing SMEs. The results obtained will be significant in devising effective policies aimed at tackling this poor performance. This paper uses data on manufacturing SMEs in the North-eastern region of Thailand in 2007 as a case study, by applying a stochastic frontier analysis (SFA) and technical inefficiency effects model. The empirical results obtained indicate that the mean technical efficiency of all categories of manufacturing SMEs in the North-eastern region is 43 percent, implying that manufacturing SMEs have high levels of technical inefficiency in their production process. Manufacturing SMEs in the North-eastern region are particularly labour intensive. The empirical results of the technical inefficiency effects model suggest that skilled labour, municipal area and ownership characteristics are important firm-specific factors affecting technical efficiency. The paper argues that the government should play a more substantive role in developing manufacturing SMEs in the North-eastern provinces through: providing training programs for employees and employers, encouraging greater usage of capital and technology in the production process of SMEs, enhancing the efficiency of state owned enterprises, encouraging a wide range of ownership forms and improving information and communications infrastructure.
C87|Zróżnicowanie odpowiedzi respondentów testu koniunktury w świetle miar entropii [Differentiation of Business Tendency Survey Responses: Application of Measures of Entropy]|Opracowanie prezentuje wyniki zastosowania empirycznej miary entropii rozkładu prawdopodobieństwa w celu oceny zawartości informacyjnej danych pochodzących z testu koniunktury Instytutu Rozwoju Gospodarczego SGH. Miary entropii wyznaczane są dla realizacji i oczekiwań wyrażanych w teście koniunktury, dla wszystkich pytań kwestionariusza kierowanego do przedsiębiorstw przemysłowych, w podziale na sektory własnościowe, klasy wielkości oraz sektor działalności wg klasyfikacji PKD. Z przeprowadzonej analizy empirycznej wynika, że zastosowanie miar entropii statystycznej pozwala zróżnicować odpowiedzi respondentów w przekroju badanych zmiennych ekonomicznych (pytań testu koniunktury) oraz wielkości i sektora działalności przedsiębiorstwa. Szczególnie wysoka niepewność związana jest z pytaniami o wielkość produkcji, portfel zamówień ogółem i zamówień eksportowych, a najmniejsza – z pytaniem o ceny. Przedsiębiorstwa małe cechuje szczególnie wysoka niepewność związana z prognozowaniem i oceną bieżącej sytuacji finansowej, a przedsiębiorstwa duże – wysoka zmienność entropii, odzwierciedlająca znaczące wahania rozkładu odpowiedzi z miesiąca na miesiąc. [This paper presents results of application of statistical entropy to evaluate information content of business tendency surveys administered by the Research Institute for Economic Development, Warsaw School of Economics. Measures of entropy, corresponding to changes observed and predicted by the survey respondents, are calculated for all questions included in the monthly industrial survey, taking into account ownership structure, size, and industrial sector in which an enterprise operates. Empirical results lead to conclusion that measures of statistical entropy allow to differentiate responses of industrial enterprises from the point of view of economic variables included in the questionnaire, size and industrial sector. Questions concerning size of production and number of domestic and export orders are associated with the highest uncertainty, and those pertaining to prices – with the lowest uncertainty. High uncertainty of forecasting and evaluating current financial situation is typical for small enterprises; variable entropy, reflecting significant changes in month-to-month distribution of survey answers, is typical for large firms.]
C87|SPECTRAN, a set of Matlab programs for Spectral analysis|Spectral analysis is one of the most important areas of time series econometrics. The use of spectral measures is widespread in different science fields such as economics, physics, engineering, geology. The SPECTRAN toolbox has been developed to facilitate the application of spectral concepts to univariate as well as to multivariate series. It offers a variety of frequency-domain techniques and supports the statistical inference. It also provides convenient tools for the examination of the results, e.g.functions for writing the output to a file or functions specially designed for plotting the estimated spectral measures. The key feature of SPECTRAN is the user-friendliness embodied in, e.g., the central function spectran which performs the whole analysis with default settings, but also gives the user the possibility to adjust them. This document sets out the most relevant spectral concepts and their implementation in SPECTRAN. Finally, three examples shall illustrate the application of different toolbox function to macroeconomic data.
C87|Creativity In Conscience Society|Creativity is a result of brain activity which differentiates individuals and could ensure an important competitive advantage for persons, for companies, and for Society in general. Very innovative branches – like software industry, computer industry, car industry – consider creativity as the key of business success. Natural Intelligence Creativity can develop basic creative activities, but Artificial Intelligence Creativity, and, especially, Conscience Intelligence Creativity should be developed and they could be enhanced over the level of Natural Intelligence. Providing only neurological research still does not offer a scientific basis for understanding creativity but thousand years of creative natural intelligence behavior observations offer some algorithms, models, methods, guidelines and procedures which could be used successfully in Conscience Society Creativity. Present Essay discusses the evolution of the notion of Creativity (what it is, why it is important, where it is used), analyzes creativity from basic point of view (Creativity as a Brain Activity; Mastering Daily Life; Creativity and Profession; Piirto’s six Steps; When and where Creativity Occurs; How Creative People are looked upon), and also manages Individual Creativity and Company Goals (Individual Creativity; Teams, Creativity and Product Development; Company’s Product Development Goals; Entrepreneur’s and Small Companies’ Product Development).
C87|Applying and interpreting model-based seasonal adjustment. The euro-area industrial production series|The recent economic crisis has altered the dynamics of economic series and, as a consequence, introduced uncertainty in seasonal adjustment of recent years. This problem was discussed in recent workshops at the European Central Bank and at Eurostat in the context of adjustment of the Euro Area Industrial Production (EPI) series. Because a seasonal component is unobserved and undefi ned, it is diffi cult to compare results from different adjustment methods. Within the regARIMA model-based approach, however, a framework for systematic analysis and comparison of results is indeed present. The EPI series is analyzed under the TRAMO-SEATS framework. The purpose of the analysis is not to compare alternative methods, but to show how the results of the model-based analysis can be exploited at the identifi cation, diagnostics, and inference stages of modeling, and in the selection of an appropiate seasonal adjustment (and underlying model). Despite the uncertainty induced by the crisis (and the revisions to the unadjusted data), the automatic procedure, with ramps to capture the spectacular 2008 drop in the series, provides excellent and stable results.
C87|Using Stata For Applied Research: Reviewing Its Capabilities|We review the Stata statistical package and evaluate its suitability for applied research.<br><small>(This abstract was borrowed from another version of this item.)</small>
C87|Econometría para la evaluación de políticas públicas con Stata: introducción y análisis de datos|Es el primero de una serie de documentos de carácter académico sobre el uso en econometría del software Stata. En econometría es necesario el uso continuo de software especializado tanto en la docencia como en la investigación aplicada. El objetivo de este documento es introducir al lector en el manejo de Stata, posiblemente el software econométrico más popular y con las herramientas predefinidas más adecuadas del cálculo automatizado para la docencia y la investigación en economía.
C87|Factores y Mapas de Riesgo Electoral. Alcaldía de Cali 2003 y 2007|Este documento busca identificar anomalías e irregularidades electorales en el Municipio de Santiago de Cali,a través del análisis descriptivo de datos de las elecciones a la Alcaldía de Cali 2003 y 2007, con las elecciones a Cámara de Representantes 2010 como variable de control, por medio del cálculo de factores de riesgo y elaboración de mapas de riesgo electoral, tanto para la zona rural como urbana del Municipio, utilizando la metodología de la Misión de Observación Electoral.
