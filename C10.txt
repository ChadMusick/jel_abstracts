C10|Edgeworth expansion for Euler approximation of continuous diffusion processes|In this paper we present the Edgeworth expansion for the Euler approximation scheme of a continuous diffusion process driven by a Brownian motion. Our methodology is based upon a recent work [22], which establishes Edgeworth expansions associated with asymptotic mixed normality using elements of Malliavin calculus. Potential applications of our theoretical results include higher order expansions for weak and strong approximation errors associated to the Euler scheme, and for studentized version of the error process.
C10|Cross-sectional noise reduction and more efficient estimation of Integrated Variance|In this paper we propose a straightforward approach to obtain a more efficient estimate of the integrated variance of an asset through a cross-sectional combination with a futures contract written on it. Our method constructs a variance-preserving series with reduced noise size as a linear combination of the underlying asset and the futures and base measurement of the integrated variance on this new series. We first illustrate how a theoretically but infeasible optimal series can be obtained and then suggest a feasible procedure to attain noise reduction. In a simulation study we verify how prevalent estimators of integrated variance applied to such noise-reduced series outperform estimators applied directly to the asset price. Finally, we apply the method to an empirical data set and, through the stabilized signature plot, we show how the noise reduced series provides consistent integrated variance estimates using naive realized measures at very high frequencies.
C10|An Iterative Approach to Ill-Conditioned Optimal Portfolio Selection|Covariance matrix of the asset returns plays an important role in the portfolio selection. A number of papers is focused on the case when the covariance matrix is positive definite. In this paper, we consider portfolio selection with a singular covariance matrix. We describe an iterative method based on a second order damped dynamical systems that solves the linear rank-deficient problem approximately. Since the solution is not unique, we suggest one numerical solution that can be chosen from the iterates that balances the size of portfolio and the risk. The numerical study confirms that the method has good convergence properties and gives a solution as good as or better than the constrained least norm Moore-Penrose solution. Finally, we complement our result with an empirical study where we analyze a portfolio with actual returns listed in S&P 500 index.
C10|Editorial: Enhancing Quantitative Exploratory Entrepreneurship Research|The purpose of this editorial is to discuss ways to enhance exploratory quantitative studies in entrepreneurship. We use examples from entrepreneurship research and other scientific fields to illustrate the advantages of graphical data display for both exploratory purposes and post hoc tests. We provide suggestions for authors, reviewers, and editors on ways to enhance the transparency, accuracy, and pedagogical presentation of quantitative data in papers with the explicit purpose of illuminating emerging and important entrepreneurship phenomena. Our hope is that we spark a conversation among entrepreneurship scholars about the state of our empirical work and the possibilities that lie ahead to enhance exploratory entrepreneurship research.
C10|2 Editorial: Enhancing Quantitative Theory-Testing Entrepreneurship Research|The purpose of this editorial is to discuss methodological advancements to enhance quantitative theory-testing entrepreneurship research. As the impact of entrepreneurship scholarship accelerates and deepens, our methods must keep pace to continue shaping theory, policy, and practice. Like our sister fields in business, entrepreneurship is coming to terms with the replication and credibility crisis in the social sciences, forcing the field to revisit commonly-held assumptions that limit the promise and prospect of our scholarship. Thus, we provide suggestions for reviewers and editors to identify concerns in empirical work, and to guide authors in improving their analyses and research designs. We hope that our editorial provides useful and actionable guidance for entrepreneurship researchers submitting theory-testing papers to Journal of Business Venturing.
C10|The Perry Preschoolers at Late Midlife: A Study in Design-Specific Inference|This paper presents the first analysis of the life course outcomes through late midlife (around age 55) for the participants of the iconic Perry Preschool Project, an experimental high-quality preschool program for disadvantaged African-American children in the 1960s. We discuss the design of the experiment, compromises in and adjustments to the randomization protocol, and the extent of knowledge about departures from the initial random assignment. We account for these factors in developing conservative small-sample hypothesis tests that use approximate worst-case (least favorable) randomization null distributions. We examine how our new methods compare with standard inferential methods, which ignore essential features of the experimental setup. Widely used procedures produce misleading inferences about treatment effects. Our design-specific inferential approach can be applied to analyze a variety of compromised social and economic experiments, including those using re-randomization designs. Despite the conservative nature of our statistical tests, we find long-term treatment effects on crime, employment, health, cognitive and non-cognitive skills, and other outcomes of the Perry participants. Treatment effects are especially strong for males. Improvements in childhood home environments and parental attachment appear to be an important source of the long-term benefits of the program.
C10|Market Timing with Option-Implied Distributions in an Exponentially Tempered Stable Lévy Market|This paper explores the empirical implementation of a dynamic asset allocation strategy using option-implied distributions when the underlying risky asset price is modeled by an exponential Lévy process. One month risk-neutral densities are extracted from option prices and are subsequently transformed to the risk-adjusted, or real-world densities. Optimal portfolios consisting of a risky and risk-free asset rebalanced on a monthly basis are then constructed and their performance analyzed. It is found that the portfolios formed using option-implied expectations under the Lévy market assumption, which are flexible enough to capture the higher moments of the implied distribution, are far more robust to left-tail market risks and offer statistically significant improvements to risk-adjusted performance when investor risk aversion is low, however this diminishes as risk aversion increases.
C10|Let the Data Speak? On the Importance of Theory-Based Instrumental Variable Estimations|In absence of randomized controlled experiments, identification is often aimed via instrumental variable (IV) strategies, typically two-stage least squares estimations. According to Bayes’ rule, however, under a low ex ante probability that a hypothesis is true (e.g. that an excluded instrument is partially correlated with an endogenous regressor), the interpretation of the estimation results may be fundamentally flawed. This paper argues that rigorous theoretical reasoning is key to design credible identification strategies, aforemost finding candidates for valid instruments. We discuss prominent IV analyses from the macro-development literature to illustrate the potential benefit of structurally derived IV approaches.
C10|Modelling the Dynamic Effects of Elective Hospital Admissions on Emergency Levels in England|In England as elsewhere, policy makers are trying to reduce the pressure on costs due to rising hospital admissions by encouraging GPs to refer fewer patients to hospital specialists. This could have an impact on elective treatment levels, particularly procedures for conditions which are not life-threatening and can be delayed or perhaps withheld entirely. This study attempts to determine whether cost savings in one area of publicly funded health care may lead to cost increases in another and therefore have unintended consequences by offsetting the cost-saving benefits anticipated by policy makers. Using administrative data from Hospital Episode Statistics (HES) in England we estimate dynamic fixed effects panel data models for emergency admissions at Primary Care Trust and Hospital Trust levels for the years 2004–13, controlling for a group of area-specific characteristics and other secondary care variables. We find a negative link between current levels of elective care and future levels of emergency treatment. This observation comes from a time of growing admissions and there is no guarantee that the link between emergency and elective activity will persist if policy is effective in reducing levels of elective treatment, but our results suggest that the cost-saving benefits to the NHS from reducing elective treatment are reduced by between 5.6 per cent and 15.5 per cent in aggregate as a consequence of increased emergency activity.
C10|Practical Significance, Meta-Analysis and the Credibility of Economics|Recently, there has been much discussion about replicability and credibility. By integrating the full research record, increasing statistical power, reducing bias and enhancing credibility, meta-analysis is widely regarded as 'best evidence'. Through Monte Carlo simulation, closely calibrated on the typical conditions found among 6,700 economics research papers, we find that large biases and high rates of false positives will often be found by conventional meta-analysis methods. Nonetheless, the routine application of meta-regression analysis and considerations of practical significance largely restore research credibility.
C10|A Simulation Study for Monotonic Dependence in the Presence of Outliers|This paper aims at examining the performance of a recently proposed measure of dependence – the Monotonic Dependence Coefficient – with respect to classical correlation measures like the Pearson’s product-moment and the Spearman’s rank-order correlation coefficients, using simulated outlier contaminated and non-contaminated datasets as well as a real dataset. The comparison aims at checking how and when these coefficients detect dependence relationships between two variables when outliers are present. Several scenarios are created, contemplating in particular multiple values for the coefficients, multiple outlier contamination percentages, various simulation data patterns, or a combination of these. The basic simulation dataset is generated from a bivariate standard normal distribution. Then, the contaminated data are generated from exponential, power-transformed and lognormal distributions. The main findings tend to favour the Spearman’s rank-order correlation coefficient for most of the scenarios, especially when the contamination is taken into account, whereas MDC performs better than the Spearman’s rank-order correlation coefficient in non-contaminated data.
C10|A Feature-Based Framework for Detecting Technical Outliers in Water-Quality Data from In Situ Sensors|Outliers due to technical errors in water-quality data from in situ sensors can reduce data quality and have a direct impact on inference drawn from subsequent data analysis. However, outlier detection through manual monitoring is unfeasible given the volume and velocity of data the sensors produce. Here, we proposed an automated framework that provides early detection of outliers in water-quality data from in situ sensors caused by technical issues.The framework was used first to identify the data features that differentiate outlying instances from typical behaviours. Then statistical transformations were applied to make the outlying instances stand out in transformed data space. Unsupervised outlier scoring techniques were then applied to the transformed data space and an approach based on extreme value theory was used to calculate a threshold for each potential outlier. Using two data sets obtained from in situ sensors in rivers flowing into the Great Barrier Reef lagoon, Australia, we showed that the proposed framework successfully identified outliers involving abrupt changes in turbidity, conductivity and river level, including sudden spikes, sudden isolated drops and level shifts, while maintaining very low false detection rates. We implemented this framework in the open source R package oddwater.
C10|Battling antibiotic resistance: can machine learning improve prescribing?|Antibiotic resistance constitutes a major health threat. Predicting bacterial causes of infections is key to reducing antibiotic misuse, a leading driver of antibiotic resistance. We train a machine learning algorithm on administrative and microbiological laboratory data from Denmark to predict diagnostic test outcomes for urinary tract infections. Based on predictions, we develop policies to improve prescribing in primary care, highlighting the relevance of physician expertise and policy implementation when patient distributions vary over time. The proposed policies delay antibiotic prescriptions for some patients until test results are known and give them instantly to others. We find that machine learning can reduce antibiotic use by 7.42 percent without reducing the number of treated bacterial infections. As Denmark is one of the most conservative countries in terms of antibiotic use, this result is likely to be a lower bound of what can be achieved elsewhere.
C10|Risk analysis of energy in Vietnam|The purpose of the paper is to estimate market risk for the ten major industries in Vietnam. The focus is on the Energy sector, which has been designated as one of the four key industries, together with Services, Food, and Telecommunications, targeted for economic development by the Vietnam Government through to 2020. Oil and Gas is a separate energy-related major industry. The data set is from 2009 to 2017, which is decomposed into two distinct sub-periods after the Global Financial Crisis (GFC), namely the immediate post-GFC (2009-2011) period and the normal (2012-2017) period, in order to identify the behaviour of market risk for Vietnam major industries. Two widely-used approaches to measure and analyze risk are used in the empirical analysis, namely Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR). The empirical findings indicate that Energy and Pharmaceuticals are the least risky industries, whereas Oil and Gas and Securities have the greatest risk. In general, there is strong empirical evidence that the four key industries display relatively low risk. For public policy, the Vietnam Government’s pro-active emphasis on the targeted industries, including Energy, to achieve sustainable economic growth and national economic development, seems to be working effectively.
C10|Jury Theorems|We give a review of jury theorems, including Condorcet's (1785) classic theorem and several later refinements and departures. The review comes with a critique of jury theorems from a social-epistemology perspective. We assess the plausibility of the theorems' conclusions and premises and the potential of jury theorems to serve as formal arguments for the 'wisdom of crowds'. In particular, we argue (i) that there is a fundamental tension between voters' independence and voters' competence, hence between the two premises of typical jury theorems; (ii) that the (asymptotic) conclusion that 'huge groups are infallible', reached by many jury theorems, is an artifact of unjustified premises; and (iii) that the (non-asymptotic) conclusion that 'larger groups are more reliable', also reached by many jury theorems, is not an artifact and should be regarded as the more adequate formal rendition of the 'wisdom of crowds'.
C10|The potential economic impact of Brexit on Denmark|This paper provides estimates of the potential trade effects on exports and production at the sectoral level as well as GDP in Denmark of the exit of the United Kingdom (UK) from the European Union (EU). Owing to the high uncertainty regarding the final Brexit deal between the EU and the UK, this paper assumes a worst case outcome where trade relations are governed by World Trade Organization (WTO) most favoured nation (MFN) rules. In doing so, it provides something close to an upper bound estimate of the potential negative economic impact. Any trade agreement that would result in a closer relationship between the United Kingdom and the EU than WTO rules reduces the negative impact.Under the worst case illustrative scenario assumed in this paper, Danish exports to the UK fall by 17%, total exports and GDP decline by 1.3% in the medium term. This effect is from the trade channel absent any change in foreign direct investment (FDI) or productivity. The fall in exports is concentrated in the Danish agri-food and machinery and equipment sectors, which account for half of the export reduction. Exports to the UK of agri-food and machinery and equipment fall by 24% and 17% respectively. Smaller manufacturing sectors such as wood and leather products, metals and textiles see falls of over 20% in their exports to the UK. The chemicals sector, which includes pharmaceuticals, comprises 9.5% of Danish exports to the UK and would experience an 18% reduction in its exports to the UK.Seven Danish sectors experience production declines of over 2.5% in the scenario. The largest decline is in the meat products sectors (7%), metals (3%), material manufacturing (2.3%) and other agri-food sectors (2.2%). These sectors would also see the largest declines in labour demand.
C10|Resolutions to flip-over credit risk and beyond|Abstract Given a risk outcome y over a rating system {R_i }_(i=1)^k for a portfolio, we show in this paper that the maximum likelihood estimates with monotonic constraints, when y is binary (the Bernoulli likelihood) or takes values in the interval 0≤y≤1 (the quasi-Bernoulli likelihood), are each given by the average of the observed outcomes for some consecutive rating indexes. These estimates are in average equal to the sample average risk over the portfolio and coincide with the estimates by least squares with the same monotonic constraints. These results are the exact solution of the corresponding constrained optimization. A non-parametric algorithm for the exact solution is proposed. For the least squares estimates, this algorithm is compared with “pool adjacent violators” algorithm for isotonic regression. The proposed approaches provide a resolution to flip-over credit risk and a tool to determine the fair risk scales over a rating system.
C10|New Approach to Estimating Gravity Models with Heteroscedasticity and Zero Trade Values|This paper proposes new estimation techniques for gravity models with zero trade values and heteroscedasticity. We revisit the standard PPML estimator and we propose an improved version. We also propose various Heckman estimators with different distributions of the residuals, nonlinear forms of both selection and measure equations, and various process of the variance. We add to the existent literature alternative estimation methods taking into account the non-linearity of both the variance and the selection equation. Moreover, because of the unavailability of pre-set package in the econometrics software (Stata, Eviews, Matlab, etc.) to perform the estimation of the above-mentioned Heckman versions, we had to code it in Matlab using a combination of fminsearch and fminunc functions. Using numerical gradient matrix G, we report standard errors based on the BHHH technique. The proposed new Heckman version could be used in other applications. Our results suggest that previous empirical studies might be overestimating the contribution of the GDP of both import and export countries in determining the bilateral trade.
C10|IFRS9 Expected Credit Loss Estimation: Advanced Models for Estimating Portfolio Loss and Weighting Scenario Losses|Estimation of portfolio expected credit loss is required for IFRS9 regulatory purposes. It starts with the estimation of scenario loss at loan level, and then aggregated and summed up by scenario probability weights to obtain portfolio expected loss. This estimated loss can vary significantly, depending on the levels of loss severity generated by the IFSR9 models, and the probability weights chosen. There is a need for a quantitative approach for determining the weights for scenario losses. In this paper, we propose a model to estimate the expected portfolio losses brought by recession risk, and a quantitative approach for determining the scenario weights. The model and approach are validated by an empirical example, where we stress portfolio expected loss by recession risk, and calculate the scenario weights accordingly.
C10|Adaptive Analytical Approach to Lean and Green Operations|Recent problems faced by industrial players commonly relates to global warming and depletion of resources. This situation highlights the importance of improvement solutions for industrial operations and environmental performances. Based on interviews and literature studies, manpower, machine, material, money and environment are known as the foundation resources to fulfil the facility's operation. The most critical and common challenge that is being faced by the industrialists is to perform continuous improvement effectively. The needs to develop a systematic framework to assist and guide the industrialist to achieve lean and green is growing rapidly. In this paper, a novel development of an adaptive analytic model for lean and green operation and processing is presented. The development of lean and green index will act as a benchmarking tool for the industrialist. This work uses the analytic hierarchy process to obtain experts opinion in determining the priority of the lean and green components and indicators. The application of backpropagation optimisation method will further enhance the lean and green model in guiding the industrialist for continuous improvement. An actual industry case study (combine heat and power plant) will be presented with the proposed lean and green model. The model is expected to enhance processing plant performance in a systematic lean and green manner.
C10|The Well-meaning Economist|Economists usually inform policymakers with conclusions that come from studying the conditional expectation, i.e. arithmetic mean, of some potential outcome. But there are other means to study, from the same 'quasilinear' family. And they can support very different conclusions. In trade research, for instance, studying other means can transform the perceived roles of colonial history, geography, and trade wars. In wages research, studying other means can reverse perceived earnings differentials between groups. Similar scenarios will be common in other tasks of policy evaluation and forecasting. To choose means well I propose selection criteria, which also consider options that are outside of the quasilinear family, such as quantiles. Optimal choices are application-specific and ideally accommodate the preferences of the relevant policymaker. In the wages case, policymaker aversion to inequality makes it sensible to reject the arithmetic mean for another quasilinear one.
C10|Economic Uncertainty and Subjective Inflation Expectations|Measuring economic uncertainty is crucial for understanding investment decisions by individuals and firms. Macroeconomists increasingly rely on survey data on subjective expectations. An innovative approach to measure aggregate uncertainty exploits the rounding patterns in individuals\' responses to survey questions on inflation expectations (Binder, 2017). This paper uses the panel dimension of household surveys to study individual-level heterogeneity in this measure of individual uncertainty. The results provide evidence for the existence of considerable heterogeneity in individuals\' response behavior and inflation expectations.
C10|Specification Tests for Temporal Heterogeneity in Spatial Panel Models with Fixed Effects|We propose score type tests for testing the existence of temporal heterogeneity in slope and spatial parameters in spatial panel data (SPD) models, allowing for the presence of individual-specific and/or time-specific fixed effects (or in general intercept heterogeneity). The SPD model with spatial lag effect is treated in detail by first considering the model with individual-specific effects only, and then extending it to the model with both individual and time specific effects. Two types of tests (naive and robust) are proposed, and their asymptotic properties are presented. These tests are then fully extended to an SPD model with both spatial lag and spatial error effects. Monte Carlo results show that the robust tests have much superior finite and large sample properties than the naive tests. Thus, the proposed robust tests provide reliable tools for identifying possible existence of temporal heterogeneity in regression and spatial coefficients. Empirical illustrations of the proposed tests are given.
C10|Nearest Comoment Estimation With Unobserved Factors|We propose a minimum distance estimator for the higher-order comoments of a multivariate distribution exhibiting a lower dimensional latent factor structure. We derive the in uence function of the proposed estimator and prove its consistency and asymptotic normality. The simulation study confirms the large gains in accuracy compared to the traditional sample comoments. The empirical usefulness of the novel framework is shown in applications to portfolio allocation under non-Gaussian objective functions and to the extraction of factor loadings in a dataset with mental ability scores.
C10|Production Factor Coefficients Transition through the Lens of State Space Model|"Economic growth can be considered as an important element of countries? development process. For developing countries, like Thailand, to ensure the continuous growth of the economy, the Thai government usually implements various policies to stimulate economic growth. This study, therefore, investigates explanatory variables for economic growth in Thailand from 2005 to 2017 with the total of 52 quarters. The investigation is estimated throughout the production function with non-linear Cobb-Douglas equation. The relevant factors included in the estimation cover three traditional means of production and implicit effects with the internal and external instabilities. According to empirical results, the AR(|2|) equation with the inclusion of seven significant variables presents the most suitable model. However, this is not the case of the recursive coefficient model from the state space model that allows the transition of coefficients. With the powerful state space model, it provides the productivity or effect of each significant factor more in detail. The state coefficients are estimated based on the AR(|2|) with the exception of the one previous GDP and the 2009 world financial crisis dummy. The findings shed the light that those factors seem to be stable through time since the occurrence of the world financial crisis together with the political situation in Thailand. These two events could lower the confidence in the Thai economy. Moreover, state coefficients highlight the sluggish rate of machinery replacement and the low level of technology of capital goods imported from abroad. The Thai government should apply the proactive policies via taxation and specific credit policy to improve technological advance, for instance. Another interesting evidence is the issue of trade openness which shows the negative transition effect along the sample period. This could be explained by the loss of price competitiveness to imported goods, especially under the widespread implementation of free trade agreement."
C10|Dana Desa on clean water and sanitation access in Indonesia: Does Cash-for-work (PKT) matter?|This paper estimates the impact of Dana Desa as a form of the community-driven development program (CDD) on clean water and sanitation improvement in Indonesia. The data used is Indonesian National Socioeconomic Survey (SUSENAS) from Statistics Indonesia (BPS) and the amount of Dana Desa?s money transfer in districts level from Ministry of Village, Development of Disadvantaged Regions And Transmigration (KEMENDESA) in 2015. The baseline data used is SUSENAS in 2014, and SUSENAS in 2016 as the data of post-intervention. The study used is quantitative analysis named difference-in-difference estimation (DID) which compare the outcome before (2014) and after (2016) the program using fixed-effect regressions. The analysis involves 405 districts and 810 observations of rural area. The study aims to assess the impact of Dana Desa on clean water and sanitation. The findings show that Dana Desa gives a positive and significant impact on sanitation access and clean water access in the districts where more people are working in informal sectors. Because they have more time to participate on supporting the program by joining cash-for-work (Padat Karya Tunai). This research is important to evaluate Dana Desa program as the biggest CDD program under President Joko Widodo?s era.
C10|Parental Leave and Life Satisfaction: The Dutch case|There is extensive literature on ambiguous effects of having children on life satisfaction. Although parenthood can provide a meaning of life, parenting may increase the amount of obligations and decrease leisure time, which in turn reduce life satisfaction. In the Netherlands, parental leave is a part-time work arrangement which allows parents with young children to reconcile better work and family commitments. Using data from the Dutch Longitudinal Internet Studies for the Social Sciences (LISS), we analyzed the impact of taking parental leave on the life satisfaction of parents with young children. We found that the legal framework of Dutch parental leave offering job protected leave and fiscal benefits is crucial to enhance parents’ life satisfaction. Further, we estimated that short parental leave schemes are more conducive to life satisfaction than long parental leave schemes.
C10|The identification problem for linear rational expectations models|We consider the problem of the identification of stationary solutions to linear rational expectations models from the second moments of observable data. Observational equivalence is characterized and necessary and sufficient conditions are provided for: (i) identification under affine restrictions, (ii) generic identification under affine restrictions of analytically parametrized models, and (iii) local identification under non-linear restrictions. The results strongly resemble the classical theory for VARMA models although significant points of departure are also documented.
C10|Benchmarked Risk Minimizing Hedging Strategies for Life Insurance Policies|Traditional life insurance policies offer no equity investment opportunities for the premium paid, and suffer from low returns over the long insurance terms. Modern equity-linked insurance policies offer equity investment opportunities exposed to equity market risk. To combine the low-risk of traditional policies with the high returns offered by equity-linked policies, we consider insurance policies under the benchmark approach (BA), where the policyholders’ funds are invested in the growth-optimal portfolio and the locally risk-free savings account. Under the BA, life insurance policies can be delivered at their minimal costs, lower than the classical actuarial theory predicts. Due to unhedgeable mortality risk, life insurance policies cannot be fully hedged. In this case benchmarked risk-minimization can be applied to obtain hedging strategies with minimally fluctuating profit and loss processes, where the fluctuations can further be reduced through diversification.
C10|Lévy processes on the cryptocurrency market|Lévy processes are very often used in financial modelling since they address various characteristics of financial data. One of those characteristics is the heavy-tailedness of probability density functions - a very common empirical stylized fact on the cryptocurrency market. The aim of this study was to determine which type of Lévy motion fits the data of cryptocurrencies better, namely Alpha-Stable distribution or one of distributions from the family of generalized hyperbolic motions. The log-returns of 227 cryptocurrencies, standardized by the realized volatility estimated with the GARCH (1,1), were fitted to 11 types of distributions. The results show that the generalized hyperbolic motions fit the cryptocurrency data much more accurately than the Alpha-Stable distribution, similarly as in the case of TOP100 NASDAQ stocks. In the further stage of the analysis, it is shown how the distribution of cryptocurrency data varies over time, i.e. before, during, and after the ‘boom-period’ of 2017/2018.
C10|Big Data, Data Mining, Machine Learning und Predictive Analytics: Ein konzeptioneller Überblick|Mit der fortschreitenden Digitalisierung von Wirtschaft und Gesellschaft wächst die Bedeutung von Big Data Analytics, maschinellem Lernen und Künstlicher Intelligenz für die Analyse und Pognose ökonomischer Trends. Allerdings werden in wirtschaftspolitischen Diskussionen diese Begriffe häufig verwendet, ohne dass jeweils klar zwischen den einzelnen Methoden und Disziplinen differenziert würde. Daher soll nachfolgend ein konzeptioneller Überblick über die Gemeinsamkeiten, Unterschiede und Interdependenzen der vielfältigen Begrifflichkeiten im Bereich Data Science gegeben werden. Denn gerade für Entscheidungsträger aus Wirtschaft und Politik kann eine grundlegende Einordnung der Konzepte eine sachgerechte Diskussion über politische Weichenstellungen erleichtern.
C10|Takeaways from the special issue on The Practice of Replication|In July 2017, Economics: The Open Access, Open Assessment E-Journal issued a call for papers for a special issue on 'The Practice of Replication.' In that call, the journal explained that there was no generally accepted procedure for how to do a replication. Likewise, there was no generally accepted standard for determining whether a replication 'confirms or disconfirms' an original study. Accordingly, the journal called for papers to identify principles for how to do a replication and how to interpret its results; and to apply those principles in crafting a replication plan for a study of the author's choosing. The hope was that this exercise would produce some progress on 'the practice of replication.' The special issue is now complete with a total of eight journal articles. This commentary places the respective articles within a common framework and identifies observations and lessons learned from the respective studies.
C10|Estimating the Price Markup in the New Keynesian Model|This paper shows that the price demand elasticity can be estimated reliably in a standard log-linearized version of the New Keynesian model when including firm profit as an observable in the estimation. Using this identification strategy for the post-war US economy, we find an estimated price demand elasticity of 2.58 with a tight standard error of 0.31. This corresponds to an average price markup of 63% with a 95% confidence interval of [39%, 88%]. We also show that a calibrated markup of 20%, as commonly used in the literature, is rejected by the data, because it generates too much variability in firm profit.
C10|Battling Antibiotic Resistance: Can Machine Learning Improve Prescribing?|Antibiotic resistance constitutes a major health threat. Predicting bacterial causes of infections is key to reducing antibiotic misuse, a leading cause of antibiotic resistance. We combine administrative and microbiological laboratory data from Denmark to train a machine learning algorithm predicting bacterial causes of urinary tract infections. Based on predictions, we develop policies to improve prescribing in primary care, highlighting the relevance of physician expertise and time-variant patient distributions for policy implementation. The proposed policies delay prescriptions for some patients until test results are known and give them instantly to others. We find that machine learning can reduce antibiotic use by 7.42 percent without reducing the number of treated bacterial infections. As Denmark is one of the most conservative countries in terms of antibiotic use, targeting a 30 percent reduction in prescribing by 2020, this result is likely to be a lower bound of what can be achieved elsewhere.
C10|The Identification Problem for Linear Rational Expectations Models|We consider the problem of the identification of stationary solutions to linear rational expectations models from the second moments of observable data. Observational equivalence is characterized and necessary and sufficient conditions are provided for: (i) identification under affine restrictions, (ii) generic identification under affine restrictions of analytically parametrized models, and (iii) local identification under non-linear restrictions. The results strongly resemble the classical theory for VARMA models although significant points of departure are also documented.
C10|Predicción de precios de vivienda: Aprendizaje estadístico con datos de oferta y transacciones para la ciudad de Montevideo|En este trabajo se presentan modelos predictivos para el precio de un activo de difícil valuación como la vivienda. Se utilizan dos fuentes de datos para la ciudad de Montevideo: una proveniente de sitios web (a través de web scraping) y otra de registros administrativos de transacciones. Se implementan tres modelos fácilmente replicables: modelo lineal, árbol de regresión y bosques aleatorios. Los resultados arrojan una mejor performance del modelo de bosques aleatorios respecto al modelo lineal hedónico, ampliamente difundido en la literatura. Se busca incorporar al análisis de predicción de precios una metodología aún escasamente difundida a nivel nacional, implementada en el software R y poner a disposición una nueva base de datos.
C10|Mitigating misleading implications for policy: Treatment of outliers in a difference-indifferences framework|Applications of the difference-in-differences estimator in economics, banking and finance, and management commonly treat outliers using the winsorize method. However, failure to winsorize outliers in both the treatment and controls groups introduces volatility in estimated coefficients, significance levels, and standard errors. A faulty process can lead to an exogenous event realising a significant effect that proper process would fail to detect. In demonstration, we randomly generate placebo interventions in bank-level data and discuss how to detect and limit the problem.
C10|Density Forecasting|This paper reviews different methods to construct density forecasts and to aggregate forecasts from many sources. Density evaluation tools to measure the accuracy of density forecasts are reviewed and calibration methods for improving the accuracy of forecasts are presented. The manuscript provides some numerical simulation tools to approximate predictive densities with a focus on parallel computing on graphical process units. Some simple examples are proposed to illustrate the methods.
C10|Does FDI Promote Entrepreneurial Activities? A Meta-Analysis|This study uses meta-analysis to analyze 557 estimates from 35 studies that estimate the effect of inward FDI on entrepreneurial activity. We address two questions: (i) Does FDI lead to greater entrepreneurial activity in host countries? (ii) What factors are responsible for the different estimates across studies? In addressing these questions, we make two methodological contributions. We extend the new Andrews-Kasy meta-analysis estimators (Andrews & Kasy, 2019) to allow for explanatory variables, and we develop a nested framework of multiple meta-analysis models that allows for testing between models and model selection. We estimate that, across all studies, the average estimated effect of FDI on entrepreneurship is positive but small in size, and statistically insignificant. In contrast, the average effect from studies that control for endogeneity is negative and statistically significant.
C10|Let the Data Speak? On the Importance of Theory-Based Instrumental Variable Estimations|In absence of randomized controlled experiments, identification is often aimed via instrumental variable (IV) strategies, typically two-stage least squares estimations. According to Bayes' rule, however, under a low ex ante probability that a hypothesis is true (e.g. that an excluded instrument is partially correlated with an endogenous regressor), the interpretation of the estimation results may be fundamentally flawed. This paper argues that rigorous theoretical reasoning is key to design credible identification strategies, aforemost finding candidates for valid instruments. We discuss prominent IV analyses from the macro-development literature to illustrate the potential benefit of structurally derived IV approaches.
C10|Changing Current Net Nutrition with Weight as a Measure of Net Nutritional Change with the Transition from Bound to Free Labor: A Difference-in-Decompositions Approach|A population’s weight conditioned on height reflects its current net nutrition and demonstrates health variation during economic development. This study builds on the use of weight as a measure for current net nutrition and uses a difference-in-decompositions technique to illustrate how black and white current net nutrition varied with the transition to free-labor. Adult black age-related weight gain was greater with the transition to free-labor yet was not as large as the adult white age related weight gain. Agricultural worker’s current net nutrition was better than workers in other occupations, and agricultural workers’ net nutrition was better than workers in other occupations but was worse-off with the transition to free labor. Nativity had the greatest effect with weight changes and the transition to free-labor. Within-group weight variation was greater than across-group variation.
C10|Inference in Moment Inequality Models That Is Robust to Spurious Precision under Model Misspecification|Standard tests and confidence sets in the moment inequality literature are not robust to model misspecification in the sense that they exhibit spurious precision when the identified set is empty. This paper introduces tests and confidence sets that provide correct asymptotic inference for a pseudo-true parameter in such scenarios, and hence, do not suffer from spurious precision.
C10|Generating univariate fractional integration within a large VAR(1)|This paper shows that a large dimensional vector autoregressive model (VAR) of finite order can generate fractional integration in the marginalized univariate series. We derive high-level assumptions under which the final equation representation of a VAR(1) leads to univariate fractional white noises and verify the validity of these assumptions for two specific models.
C10|Stock and Flows in the Countegration Context|The paper describes relationships between stocks and flows in the context of cointegration analysis. Long-run and medium-term equilibrium relationships between stocks and flows are considered where stocks are described as cumulative flows. Polynomial cointegration is described as a tool for the analysis of equilibrium relationships and adjustment mechanisms for stocks and flows.
C10|Continuous Record Asymptotics for Structural Change Models|For a partial structural change in a linear regression model with a single break, we develop a continuous record asymptotic framework to build interference methods for the break date. We have T observations with a sampling frequency h over a fixed time horizon [0, N], and let T â†’ âˆž with h â†“ 0 wile keeping the time span N fixed. We impose very mild regularity conditions on an underlying continuous-time model assumed to generate the data. We consider the least-squares estimate of the break date and establish consistency and convergence rate. We providea limit theory for shrinking magnitudes of shifts and locally increasing variances. The asymptotic distribution corresponds to the location of the extremum of a function of the quadratic variation of the regressors and of a Gaussian centered martingale process over a certain time interval. We can account for the asymmetric informational content provided by the pre- and post-break regimes and show how the location of the break and shift magnitude are key ingredients in shaping the distribution. We consider a feasible version based on plug-in estimates, which provides a very good approximation to the finite sample distribution. We use the concept of Highest Density Region to construct confidence sets. Overall, our method is reliable and delivers accurate coverage probabilities and relatively short average length of the confidence sets. Importantly, it does so irrespective of the size of the break.
C10|What Determines the Neutral Rate of Interest in an Emerging Economy?|Evidence suggests that potential growth and the neutral rate co-move in advanced economies. In contrast, this co-movement is not observed in emerging economies. We argue that capital flows may explain this behavior. We focus on Mexico, a benchmark emerging economy, and find that capital inflows may account for a temporary reduction in the Mexican neutral rate after the global financial crisis. These inflows surged during the implementation of unconventional monetary policies in advanced economies. In turn, low-frequency changes in the neutral rate may be attributed to increasing domestic savings, demographics, and a decreasing global long-run real interest rate. These results are largely consistent with other studies showing that the neutral rate has decreased in the last 25 years in advanced and emerging economies.
C10|Forecast density combinations of dynamic models and data driven portfolio strategies|A dynamic asset-allocation model is specified in probabilistic terms as a combination of return distributions resulting from multiple pairs of dynamic models and portfolio strategies based on momentum patterns in US industry returns. The nonlinear state space representation of the model allows efficient and robust simulation-based Bayesian inference using a novel non-linear filter. Combination weights can be cross-correlated and correlated over time using feedback mechanisms. Diagnostic analysis gives insight into model and strategy misspecification. Empirical results show that a smaller flexible model-strategy combination performs better in terms of expected return and risk than a larger basic model-strategy combination. Dynamic patterns in combination weights and diagnostic learning provide useful signals for improved modeling and policy, in particular, from a risk-management perspective.
C10|The Identification Zoo - Meanings of Identification in Econometrics|Over two dozen different terms for identification appear in the econometrics literature, including set identification, causal identification, local identification, generic identification, weak identification, identification at infinity, and many more. This survey: 1. gives a new framework unifying existing definitions of point identification, 2. summarizes and compares the zooful of different terms associated with identification that appear in the literature, and 3. discusses concepts closely related to identification, such as normalizations and the differences in identification between structural models and causal, reduced form models.
C10|Estimating Interdependence Across Space, Time and Outcomes in Binary Choice Models Using Pseudo Maximum Likelihood Estimators|Binary outcome models are frequently used in Political Science. However, such models have proven particularly dicult in dealing with interdependent data structures, including spatial autocorrelation, temporal autocorrelation, as well as simultaneity arising from endogenous binary regressors. In each of these cases, the primary source of the estimation challenge is the fact that jointly determined error terms in the reduced-form specication are analytically intractable due to a high-dimensional integral. To deal with this problem, simulation approaches have been proposed, but these are computationally intensive and impractical for datasets with thousands of observations. As a way forward, in this paper we demonstrate how to reduce the computational burder signicantly by (i) introducing analytically tractable pseudo maximum likelihoodestimators for latent binary choice models that exhibit interdependence across space, time and/or outcomes, and by (ii) proposing an implementation strategy that increases computational eciency considerably. Monte-Carlo experiments demonstrate that our estimators perform similarly to existing alternatives in terms of error, but require only a fraction of the computational cost.
C10|Joint and conditional dependence modeling of peak district heating demand and outdoor temperature: a copula-based approach|This paper examines the complex dependence between the peak district heating demand and the outdoor temperature. The final aim is to provide the probability law of the heat demand given extreme weather conditions and derive useful implications for the management and the production of thermal energy. We propose a copula-based approach and consider the case of the district of the city of Bozen-Bolzano. The analysed data concerns daily maxima of heat demand observed from January 2014 to November 2017 and the corresponding outdoor temperature. We find that the marginal behavior of the univariate time series of the district heating demand and the temperature is well-described by autoregressive integrated moving average models. Moreover, the selected copula model exhibits a symmetric dependence between the two investigated phenomena that tend to comove closely together during the whole heating season. Taking into account the conditional behaviour of the heat demand given the temperature leads to find that the demand is strongly affected by the temperature and, in case of extreme climatic events, the demand of thermal energy reach a peak with high probability. These findings motivate for improving the production schedule, the system design, and the operational strategies.
C10|A New Semiparametric Estimation Approach for Large Dynamic Covariance Matrices with Multiple Conditioning Variables|This paper studies the estimation of large dynamic covariance matrices with multiple condition- ing variables. We introduce an easy-to-implement semiparametric method to estimate each entry of the covariance matrix via model averaging marginal regression, and then apply a shrinkage technique to obtain the dynamic covariance matrix estimation. Under some regularity conditions, we derive the asymptotic properties for the proposed estimators including the uniform consistency with general convergence rates. We further consider extending our methodology to deal with the scenarios: (i) the number of conditioning variables is divergent as the sample size increases, and (ii) the large covariance matrix is conditionally sparse relative to contemporaneous market factors. We provide a simulation study that illustrates the finite-sample performance of the developed methodology. We also provide an application to financial portfolio choice from daily stock returns.
C10|Nonparametric estimation of infinite order regression and its application to the risk-return tradeoff|This paper studies nonparametric estimation of the infinite order regression [see code in paper] with stationary and weakly dependent data. We propose a Nadaraya-Watson type estimator that operates with an infinite number of conditioning variables. The established theories are applied to examine the intertemporal risk-return relation for the aggregate stock market, and some new empirical evidence is reported. With a bandwidth sequence that shrinks the effects of long lags, the influence of all conditioning information is modelled in a natural and flexible way, and the issues of omitted information bias and specification error are effectively handled. Asymptotic properties of the estimator are shown under a wide range of static and dynamic regressions frameworks, thereby allowing various kinds of conditioning variables to be used. We establish pointwise/uniform consistency and CLTs. It is shown that the convergence rates are at best logarithmic, and depend on the smoothness of the regression, the distribution of the marginal regressors and their dependence structure in a non-trivial way via the Lambert W function. The empirical studies on S&P 500 daily data from 1950-2016 using our estimator report an overall positive risk-return relation. We also .find evidence of strong time variation and counter- cyclical behaviour in risk aversion. These conclusions are attributable to the inclusion of otherwise neglected information in our method.
C10|Fiscal and Education Spillovers from Charter School Expansion|The fiscal and educational consequences of charter expansion for non-charter students are central issues in the debate over charter schools. Do charter schools drain resources and high-achieving peers from non-charter schools? This paper answers these questions using an empirical strategy that exploits a 2011 reform that lifted caps on charter schools for underperforming districts in Massachusetts. We use complementary synthetic control instrumental variables (IV-SC) and differences-in-differences instrumental variables (IV-DiD) estimators. The results suggest greater charter attendance increases per-pupil expenditures in traditional public schools and induces them to shift expenditure from support services to instruction and salaries. At the same time, charter expansion has a small positive effect on non-charter students' achievement.
C10|The 19th Centure Net Nutrition Transition from Free to Bound Labor: A Difference-in-Decompositions Approach|The body mass index (BMI) reflects current net nutrition and health during economic development. This study introduces a difference-in-decompositions approach to show that although 19th century African-American current net nutrition was comparable to working class whites, it was made worse-off with the transition to free-labor. BMI reflects net nutrition over the life-course, and like stature, slave children’s BMIs increased more than whites as they approached entry into the adult slave labor force. Agricultural worker’s net nutrition was better than workers in other occupations but was worse-off under free-labor and industrialization. Within-group BMI variation was greater than across-group variation, and white within-group variation associated with socioeconomic status was greater than African-Americans.
C10|Partially Adaptive Econometric Methods and the Modern Obesity Epidemic|Assumptions about explanatory variables and errors are central in regression analysis. For example, the well-known method of ordinary least squares yields consistent and efficient estimators if the underlying error terms are independently, identically, and normally distributed. Additionally, the conditional distribution of the dependent variable is symmetric. The modern obesity epidemic is a well-known health dilemma where the BMI distribution was initially positively skewed but has become more symmetric, which may affect inferences about health and public resource allocation. This study applies partially adaptive estimation methods with flexible error distributions to account for possible skewness and leptokurtosis in the distribution of BMI.
C10|Linking Tax Morale and Personal Income Tax in Spain|The paper presents a study of the relationship between the tax morale and the individual payments of personal income tax using the statistical matching of opinion polls with a representative sample of the personal income tax returns in Spain. As an initial step, the method selected to execute the match -imputations using Bayesian Networks- is described. The relationship between a proxy variable of the individual tax morale and other variables in the declared income tax file is later analyzed using the matched files. A first result is that tax morale increases with the level of declared wages, salaries and capital gains, while it has no link with declared business income.
C10|Instrumental Variables in the Long Run|In the field of long-run economic growth, it is common to use historical or geographical variables as instruments for contemporary endogenous regressors. We study the interpretation of these conventional instrumental variable (IV) regressions in a simple, but general, framework. We are interested in estimating the long-run causal effect of changes in historical conditions. For this purpose, we develop an augmented IV estimator that accounts for the degree of persistence in the endogenous regressor. We apply our results to estimate the long-run effect of institutions on economic performance. Using panel data, we find that institutional characteristics are imperfectly persistent, implying that conventional IV regressions overestimate the long-run causal effect of institutions. When applying our augmented estimator, we find that increasing constraints on executive power from the lowest to the highest level on the standard index increases national income per capita three centuries later by 1.2 standard deviations.
C10|Revealed Preference Analysis with Framing Effects|In many settings, decision-makers' behavior is observed to vary based on seemingly arbitrary factors. Such framing effects cast doubt on the welfare conclusions drawn from revealed preference analysis. We relax the assumptions underlying that approach to accommodate settings in which framing effects are present. Plausible restrictions of varying strength permit either partial- or point-identification of preferences for the decision-makers who choose consistently across frames. Recovering population preferences requires understand- ing the empirical relationship between decision-makers' preferences and their sensitivity to the frame. We develop tools for studying this relationship and illustrate them with data on automatic enrollment into pension plans.
C10|A composite likelihood approach for dynamic structural models|We describe how to use the composite likelihood to ameliorate estimation, computational, and inferential problems in dynamic stochastic general equilibrium models. We present a number of situations where the methodology has the potential to resolve well-known problems and formally justifies existing practices. In each case we consider, we provide an example to illustrate how the approach works and its properties in practice.
C10|Consistent Pseudo‐Maximum Likelihood Estimators and Groups of Transformations|In a transformation model yt=c[a(xt,β),ut], where the errors ut are i.i.d. and independent of the explanatory variables xt, the parameters can be estimated by a pseudo‐maximum likelihood (PML) method, that is, by using a misspecified distribution of the errors, but the PML estimator of β is in general not consistent. We explain in this paper how to nest the initial model in an identified augmented model with more parameters in order to derive consistent PML estimators of appropriate functions of parameter β. The usefulness of the consistency result is illustrated by examples of systems of nonlinear equations, conditionally heteroscedastic models, stochastic volatility, or models with spatial interactions.
C10|Modelling Structural Zeros in Compositional Data|Inspired by Butler and Glasbey (2008) we propose a model that treats the zero values for compositional data in a different manner.
C10|Sensitivity Analysis using Approximate Moment Condition Models|We consider inference in models defined by approximate moment conditions. We show that near-optimal confidence intervals (CIs) can be formed by taking a generalized method of moments (GMM) estimator, and adding and subtracting the standard error times a critical value that takes into account the potential bias from misspecification of the moment conditions. In order to optimize performance under potential misspecification, the weighting matrix for this GMM estimator takes into account this potential bias, and therefore differs from the one that is optimal under correct specification. To formally show the near-optimality of these CIs, we develop asymptotic efficiency bounds for inference in the locally misspecified GMM setting. These bounds may be of independent interest, due to their implications for the possibility of using moment selection procedures when conducting inference in moment condition models. We apply our methods in an empirical application to automobile demand, and show that adjusting the weighting matrix can shrink the CIs by a factor of 3 or more.
C10|Sensitivity Analysis using Approximate Moment Condition Models|We consider inference in models defined by approximate moment conditions. We show that near-optimal confidence intervals (CIs) can be formed by taking a generalized method of moments (GMM) estimator, and adding and subtracting the standard error times a critical value that takes into account the potential bias from misspecification of the moment conditions. In order to optimize performance under potential misspecification, the weighting matrix for this GMM estimator takes into account this potential bias, and therefore differs from the one that is optimal under correct specification. To formally show the near-optimality of these CIs, we develop asymptotic efficiency bounds for inference in the locally misspecified GMM setting. These bounds may be of independent interest, due to their implications for the possibility of using moment selection procedures when conducting inference in moment condition models. We apply our methods in an empirical application to automobile demand, and show that adjusting the weighting matrix can shrink the CIs by a factor of 3 or more.
C10|Technology Adoption in a Hierarchical Network| This paper studies the effect of network structure on technology adoption, in the setting of the Python programming language. A major release of Python, Python 3, provides more advanced but backward-incompatible features to Python 2. We model the dynamics of Python 3 adoption made by package developers. Python packages form a hierarchical network through dependency requirements. The adoption decision involves not only updating one's own source code, but also dealing with dependency packages lacking Python 3 support. We build a dynamic model of technology adoption where each package makes an irreversible decision to provide support for Python 3. The optimal timing of adoption requires a prediction of all future states, for the package itself as well as each of its dependencies. With a complete dataset of package characteristics for all historical releases, we are able to draw the complete hierarchical structure of the network, and simplify the estimation by grouping packages into different layers based on the dependency relationship. We study how individual adoption decisions can propagate along the links in such a hierarchical network. We also test the effectiveness of various counterfactual policies that can promote a faster adoption process.
C10|An Information-Constrained Model for Ultimatum Bargaining|We argue for the use of the principle of maximum entropy to carry out inference in experimental eco- nomics. In particular we take the ultimatum game as a case study. We derive the Logit equilibrium by maximizing Shannon's informational entropy subject to behavioral constraints. This provides an e ective way to translate behavioral hypotheses into theoretical distributions that are candidates to characterize em- pirical frequencies when performing experiments. Based on this approach we present two maximum entropy models applied to the ultimatum game. The rst one assumes that the payo functions of agents playing the game depend only on the portion of the money prize they obtain at the end of the game. The second one introduces an additional fairness constraint to represent the behavioral hypothesis that players also follow altruistic motivations. Each model suggests a particular distribution of o ers that we can compare to empirical distributions from data gathered from experimental results. We build a database containing observed interactions of simple ultimatum game experiments conducted by Henrich et al. (2004), Ensminger & Henrich (2014), and Andreoni & Blanchard (2006).The data consists of 1,016 observations of demands made by proposers in the standard ultimatum game interaction. Out of these demands, a total of 636 report whether the demand was accepted or rejected, allowing us to derive the joint probability distribution of demands and acceptance/rejection. The experiments conducted by by Henrich et al., and Ensminger & Henrich consists on ultimatum experiments performed around the world on small scale societies. On the other hand, the experiments conducted by Andreoni & Blanchard were implemented to individuals from the University of Wisconsin-Milwaukee. The information distinguishability index shows that the fairness constrained model recovers 90% of the information in the marginal distribution of demands, in contrast with the 60% recovered by the non-fairness constrained model. We also estimate the fairness constrained model on the joint distribution of demands and quantal responses, recovering 87% of the information contained in the data, in contrast with the 52% recovered by the non-fairness constrained model.
C10|Replication and reconciling of the CICE evaluations on employment (2013-2014)|We reproduce the main estimates of the effects of the CICE (Tax Credit for Competitiveness and Employment) on employment presented by the TEPP and LIEPP teams for France Stratégie based on the methodological indications described in their first research reports (september 2016 and march 2017). After the reconstitution of the samples, globally validated by comparisons in descriptive statistics, we reproduce their different empirical strategies. The effects on employment reproduced are consistent with the results presented by TEPP: positive, significant and of the same order of magnitude for the firms most exposed to the CICE (fourth quartile of exposure to the CICE). On the other hand, the effects on employment by socio-professional category do not reproduce the results of TEPP. For LIEPP, the replication of the methodology leads to results similar to those presented in their report on aggregate employment in 2013 and 2014 (no significant and positive effect), and as well as on the breakdown by socio-professional category. We then compare the specifications of the two teams. The differences in results persist on a common sample, and are robust to the linear or non-linear specification of the treatment. However, we observe a particular sensitivity of the results to the presence or absence of past productivity of firms, in level, among the explanatory variables of employment growth. We present two reconciled specifications that lead to consistent results, shedding light on origins of the divergence. We do not conclude on employment effects of the CICE, as the work of the two teams is still ongoing.
C10|The Potential Macroeconomic and Sectoral Consequences of Brexit on Ireland|This paper provides estimates of the potential effects on exports, imports, production, factor demand and GDP in Ireland of an exit of the United Kingdom (UK) from the European Union (EU), focusing on trade and FDI channels. Owing to the high uncertainty regarding the final trade agreement between the negotiating parties, the choice has been made to assume a worst-case outcome where trade relations between the United Kingdom and EU are governed by World Trade Organization (WTO) most favoured nation (MFN) rules. In doing so, it provides something close to an upper bound estimate of the negative economic impact taking into account the potential for some firms to relocate to Ireland. Any final trade agreement that would result in closer relationships between the United Kingdom and the EU could reduce this negative impact. The simulations use two large-scale models: a global macroeconomic model (NiGEM) and a general equilibrium trade model (METRO). These models are used to quantify, both at the macroeconomic and the sectoral level, two key channels through which Ireland would be affected: trade and foreign direct investment. The simulation results highlight that the negative effect on trade could result in Ireland's GDP falling by 1½ per cent in the medium-term and around 2½ per cent in the long-term. The impacts are highly heterogeneous across sectors. Agriculture, food, and some smaller manufacturing sectors experience the largest declines in total gross exports at over 15%. By contrast, financial services exports increase slightly. The modelling suggests that any positive offsetting impact to the trade shock from increased inward FDI to Ireland is likely to be modest.
C10|The potential economic impact of Brexit on the Netherlands|This paper provides estimates of the potential trade effects of an exit of the United Kingdom (UK) from the European Union (EU) on exports and production at the sectoral level as well as GDP in the Netherlands. Owing to the high uncertainty regarding the final trade agreement between the negotiating parties, the choice has been made to assume a worst case outcome where trade relations between the United Kingdom and EU are governed by World Trade Organization (WTO) most favoured nation (MFN) rules. In doing so, it provides an upper bound estimate of the potential negative economic impact stemming from disruptions in trade. Any final trade agreement that would result in closer relationships between the United Kingdom and the EU could reduce this negative impact. Simulations using the METRO model suggest that from an increase in tariff and non-tariff measures (NTM’s) Dutch exports to the UK would fall by 17% and GDP declines by 0.7% in the medium term compared to baseline. This effect is from the trade channel absent any change in foreign direct investment (FDI) or productivity. The Dutch agri-food sector would experience a 22% fall in its UK exports. There are some sectors that gain from the export opportunities provided by Brexit, notably financial services and transport.
C10|Child well-being and the Sustainable Development Goals: How far are OECD countries from reaching the targets for children and young people?|This paper summarises available evidence on the distance that OECD countries need to travel in order to reach the Sustainable Development Goal (SDG) targets for children and young people. More than 50 indicators are included in this analysis, covering 43 of the 169 targets, and 11 of the 17 Goals. The analysis finds that, on average, OECD countries are still far from reaching the targets pertaining to Goals 4 “Quality education”, and 8 “Decent work and economic growth”. Goals 1 “No poverty”, 2 “Zero hunger” and 16 “Peace, justice and strong institutions” are also highlighted as priority areas. However, the results vary widely across OECD countries, and among specific targets within each of the goals. Yet, all of these findings need to be considered in light of what it is not currently possible to measure. In particular, there are large data gaps for Goals 1 (“No poverty”), 5 (“Gender equality”), 11 (“Sustainable cities and communities”), and 16 (“Peace, justice and strong institutions”).
C10|White heteroscedasticty testing after outlier removal|Abstract Given the effect that outliers can have on regression and specification testing, a vastly used robustification strategy by practitioners consists in: (i) starting the empirical analysis with an outlier detection procedure to deselect atypical data values; then (ii) continuing the analysis with the selected non-outlying observations. The repercussions of such robustifying procedure on the asymptotic properties of subsequent specification tests are, however, underexplored. We study the effects of such a strategy on the White test for heteroscedasticity. Using weighted and marked empirical processes of residuals theory, we show that the White test implemented after the outlier detection and removal is asymptotically chi-square if the underlying errors are symmetric. Under asymmetric errors, the standard chi-square distribution will not always be asymptotically valid. In a simulation study, we show that - depending on the type of data contamination - the standard White test can be either severely undersized or oversized, as well as have trivial power. The statistic applied after deselecting outliers has good finite sample properties under symmetry but can suffer from size distortions under asymmetric errors.
C10|An Evaluation of Singular Spectrum Analysis-Based Seasonal Adjustment|In this paper, the Singular Spectrum Analysis (SSA) is presented and applied in the US air traffic emplacements for the period Jan. 1954 – Sept. 2011. I decompose the US air traffic emplacements in trend, cycle, seasonal and noise components. In turn, I apply several spectral criteria in order to evaluate the SSA as a seasonal adjustment filter. SSA detects, beyond trend, strong cycles and seasonal components and leaves as a residual a GARCH process. SSA performs quite well as a seasonal adjustment mechanism in the case of the GARCH process but it performs even better in the case of a simulated white noise process. SSA is a serious candidate in economics in dealing with filtering, denoising, smoothing and seasonal adjustment.
C10|La Ley Petty-Clark en el Área Metropolitana del Valle de Aburrá en Colombia, en el periodo 2000-2016<BR>[The Petty-Clark law in the Metropolitan Area of The Aburrá Valley in Colombia, in the period 2000-2016]|The topics of interest of the economists have been common since the emergence of the economic discipline like science in the beginnings of the Modernity; the theoretical contributions of William Petty are an example of this. The articles wants to trace the origin of the Petty Clark law from pre-classical thinking; verify compliance with the so-called Petty-Clark Law in the Metropolitan Area of the Aburrá Valley (amva) in Colombia; and to observe the effect of the definition of the Agreement of the City Council of the central city, which establishes the Territorial Planning Plan (pot) and, with respect to the rest of the amva, particularly to the north, show a dynamic, non-homogenous effect in relation to Medellín. Basic econometrics of time series is used between the year 2000 and the year 2016, paying particular attention to the year 2006 when the pot was approved with the use of dichotomous variables, as well as a very small exploration, with some data of the Public Employment Office for the northern municipalities.
C10|Trade misinvoicing in OECD countries: what can we learn from bilateral trade intensity indices?|This paper aims to explore the extend of trade misinvoicing among OECD countries over the period 2006-2016. Following the standard approach developed by Morgenstern (1950), four categories of misreported bilateral transactions are estimated to highlight two channels used to shift illicit financial flows. The study is reinforced by an analysis in terms of bilateral intensity indices proposed by Kojima (1964) and extended by Kunimoto (1977) to determine trends and patterns in trade misinvoicing among bilateral relations for selected OECD countries. Some interesting findings can be pointed out: (i) the assessment of intra-OECD misinvoicing trade shows that the accumulated amount reaches more than 12 trillion US dollars over the period and is characterised by illicit inflows, although outflows tend to increase during the last years; (ii) significant amounts of illicit financial flows occur in the most advanced countries despite the quality of their statistical recording services; (iii) arguing against explanation based on tax evasion and capital flight, it is shown that countries with high GDP per capita are senders and recipients of illicit financial flows, while lower GDP per capita countries are also receivers of illicit inflows; (iv) the share of misreported imports in countries´ total imports is larger than for total exports, which seems to indicate that imports are the principal vehicle sustaining bilateral misinvoicing trade; and (v) geographical proximity appears to be an important factor in determining the channel used and the direction of illicit financial flows as well as in describing intense relations relative to bilateral misinvoicing trade.
C10|Return level applied to portfolio analysis|In this paper, we estimated return levels of a portfolio of two assets using extreme value theory.
C10|Does corruption hampers inward FDI in South Africa from other African countries? a gravity model analysis|The purpose of this paper is to investigate the relationship between corruption and FDI inflows from other African countries to South Africa. The study uses gravity model and employs panel data econometric technique such as pooled, fixed and random effects model. The results indicate that there is a significant negative relationship between corruption and FDI inflows from other African countries to South Africa. This implies that policy makers in South Africa should implement measures to curb corruption. This will help in attracting FDI inflows from other African countries and encourage the creation of job opportunities.
C10|The predictive relationship between exchange rate expectations and base metal prices|In this paper we show that survey-based-expectations about the future evolution of the Chilean exchange rate have the ability to predict the returns of the six primary non-ferrous metals: aluminum, copper, lead, nickel, tin and zinc. Predictability is also found for returns of the London Metal Exchange Index. Previous studies have shown that the Chilean exchange rate has the ability to predict copper returns, a world commodity index and base metal prices. Nevertheless, our results indicate that expectations about the Chilean peso have stronger predictive ability relative to the Chilean currency. This is shown both in-sample and out-of-sample. By focusing on expectations of a commodity currency, and not on the currency itself, our paper provides indirect but new and strong evidence of the ability that commodity currencies have to forecast commodity prices. Our results are also consistent with the present-value-model for exchange rate determination.
C10|Can we beat the Random Walk? The case of survey-based exchange rate forecasts in Chile|We examine the accuracy of survey-based expectations of the Chilean exchange rate relative to the US dollar. Our out-of-sample analysis reveals that survey-based forecasts outperform the Driftless Random Walk (DRW) in terms of Mean Squared Prediction Error at several forecasting horizons. This result holds true even when comparing the survey to a more competitive benchmark based on a refined information set. A similar result is found when precision is measured in terms of Directional Accuracy: survey-based forecasts outperform a “pure luck” benchmark at several forecasting horizons. Differing from the traditional “no predictability” result reported in the literature for many exchange rates, our findings suggest that the Chilean peso is indeed predictable.
C10|Analyse spatiale de la localisation de la production agricole des plantes irriguées et non irriguées : Cas de la Tunisie<BR>[Spatial analysis of the location of the agricultural production of irrigated and non-irrigated plants: Evidence from Tunisia]|This study analyzes the spatial location of agricultural production of both irrigated and non-irrigated plants in Tunisia in 2012. At the micro-spatial level, the results show that the location movements of agricultural production of irrigated and non-irrigated plants confirm the existence of a spatial dynamic between Tunisian governorates. Moreover, the findings of the global autocorrelation test provide evidence for the presence of a positive spatial autocorrelation between the considered variables.
C10|"Comment on ""Model Confidence Bounds for Variable Selection"" by Yang Li, Yuetian Luo, Davide Ferrari, Xiaonan Hu, and Yichen Qin"|This is a comment on the article mentioned in the title
C10|Financial Portfolios based on Tsallis Relative Entropy as the Risk Measure|Tsallis relative entropy, which is the generalization of Kullback-Leibler relative entropy to non-extensive systems, is investigated as a possible risk measure in constructing risk optimal portfolios whose returns beat market returns. The results are compared with those from three other risk measures: 1) the commonly used ‘beta’ of the Capital Asset Pricing Model (CAPM), 2) Kullback-Leibler relative entropy, and 3) the relative standard deviation. Portfolios are constructed by binning the securities according to their risk values. The mean risk value and the mean return in excess of market returns for each bin is calculated to get the risk-return patterns of the portfolios. The investigations have been carried out for both long (~18 years) and shorter (~9 years) terms that include the dot-com bubble and the 2008 crash periods. In all cases, a linear fit can be obtained for the risk and excess return profiles, both for long and shorter periods. For longer periods, the linear fits have a positive slope, with Tsallis relative entropy giving the best goodness of fit. For shorter periods, the risk-return profiles from Tsallis relative entropy show a more consistent behavior in terms of goodness of fit than the other three risk measures.
C10|Future developments in cyber risk assessment for the internet of things|This article is focused on the economic impact assessment of Internet of Things (IoT) and its associated cyber risks vectors and vertices – a reinterpretation of IoT verticals. We adapt to IoT both the Cyber Value at Risk model, a well-established model for measuring the maximum possible loss over a given time period, and the MicroMort model, a widely used model for predicting uncertainty through units of mortality risk. The resulting new IoT MicroMort for calculating IoT risk is tested and validated with real data from the BullGuard's IoT Scanner (over 310,000 scans) and the Garner report on IoT connected devices. Two calculations are developed, the current state of IoT cyber risk and the future forecasts of IoT cyber risk. Our work therefore advances the efforts of integrating cyber risk impact assessments and offer a better understanding of economic impact assessment for IoT cyber risk.
C10|Does global economic uncertainty matter for the volatility and hedging effectiveness of Bitcoin?|We assess whether the long-run volatilities of Bitcoin, global equities, commodities, and bonds are affected by global economic policy uncertainty. Empirical results provide evidence supporting this hypothesis, except in the case of bonds. For Bitcoin investors, the results imply the ability to use information about the state of global economic uncertainty to enhance the predictions of Bitcoin volatility. We further examine whether the correlation between Bitcoin and global equities, commodities, and bonds are affected by global economic policy uncertainty. Empirical results reveal that global economic policy uncertainty has a negative significant impact on the Bitcoin-bonds correlation and a positive impact on both Bitcoin-equities and Bitcoin-commodities correlations, suggesting the possibility of Bitcoin acting as a hedge under specific economic uncertainty conditions. Interestingly, the hedging effectiveness of Bitcoin for both global equities and global bonds enhances slightly after considering the level of global economic policy uncertainty. Such a weak effect of the state of global economic uncertainty on the hedging ability of Bitcoin implies that investors cannot substantially enhance the hedging performance of Bitcoin under different economic uncertainty conditions.
C10|Stochastic Frontier Analysis: Foundations and Advances|This chapter reviews some of the most important developments in the econometric estimation of productivity and efficiency surrounding the stochastic frontier model. We highlight endogeneity issues, recent advances in generalized panel data stochastic frontier models, nonparametric estimation of the frontier, quantile estimation and distribution free methods. An emphasis is placed on highlighting recent research and providing broad coverage, while details are left for further reading in the abundant (although not limited to) list of references provided.
C10|Improving Finite Sample Approximation by Central Limit Theorems for DEA and FDH efficiency scores|We propose an improvement of the finite sample approximation of the central limit theorems (CLTs) that were recently derived for statistics involving production efficiency scores estimated via Data Envelopment Analysis (DEA) or Free Disposal Hull (FDH) approaches. The improvement is very easy to implement since it involves a simple correction of the already employed statistics without any additional computational burden and preserves the original asymptotic results such as consistency and asymptotic normality. The proposed approach persistently showed improvement in all the scenarios that we tried in various Monte-Carlo experiments, especially for relatively small samples or relatively large dimensions (measured by total number of inputs and outputs) of the underlying production model. This approach therefore is expected to be valuable (and at almost no additional computational costs) for practitioners wishing to perform statistical inference about production efficiency using DEA or FDH approaches.
C10|Econometric Analysis of Productivity: Theory and Implementation in R|Our chapter details a wide variety of approaches used in estimating productivity and efficiency based on methods developed to estimate frontier production using Stochastic Frontier Analysis (SFA) and Data Envelopment Analysis (DEA). The estimators utilize panel, single cross section, and time series data sets. The R programs include such approaches to estimate firm efficiency as the time invariant fixed effects, correlated random effects, and uncorrelated random effects panel stochastic frontier estimators, time varying fixed effects, correlated random effects, and uncorrelated random effects estimators, semi-parametric efficient panel frontier estimators, factor models for cross-sectional and time-varying efficiency, bootstrapping methods to develop confidence intervals for index number-based productivity estimates and their decompositions, DEA and Free Disposable Hull estimators. The chapter provides the professional researcher, analyst, statistician, and regulator with the most up to date efficiency modeling methods in the easily accessible open source programming language R.
C10|Delphic and Odyssean monetary policy shocks: Evidence from the euro-area|In this paper, we study the impact of the ECB announcements on the market-based expectations of interest rates and of in ation rates. We nd that the impact of the ECB announcements on in ation expectations has changed over the last fteen years. In particular, while in the central part of our sample the ECB announcements were read as a signal about the economic conditions (i.e. Delphic component), in latest episodes they have been interpreted as a commitment device on future monetary policy accommodation (i.e. Odyssean component). We propose an approach to separately identify the Delphic and Odyssean component of the ECB monetary policy announcements and we measure their dynamic impact on the economy.
C10|Twitter versus Traditional News Media: Evidence for the Sovereign Bond Markets|This paper compares news in Twitter with traditional news outlets and then emphasizes their differential impact on Eurozone's sovereign bond market for a homogeneous news topic. We find a two-way information flow between Twitter's “Grexit” tweets and the respective mentions in traditional news outlets. The influence of Twitter on the traditional news is consistently more prolonged, especially in high-activity periods. We also assess the differential impact of the two news sources on sovereign spreads over and above the impact of economic/financial fundamentals, namely measures of default risk, liquidity risk and global financial risk. Our focus is on the borrowing costs of Eurozone's periphery; for comparison reasons, we also consider France as a core Eurozone country. The effect of Twitter on the Greek sovereign spread is positive and of higher magnitude than that of traditional news outlets. Weak contagion effects are recorded primarily for the case of Portugal and Ireland.
C10|Estimation bayésienne d’un modèle néo-keynésien pour l’économie marocaine|Ce travail porte sur l'estimation d'un modèle hybride néo-keynesien (HNKM) formé de trois équations structurelles caractérisant l'économie marocaine. Il s'agit de la courbe de demande, de la courbe d'o¤re et d'une règle Taylor augmentée des réserves de change. Le modèle est estimé par une approche bayésienne à partir des données trimestrielles couvrant la période 1998Q1-2016Q4. Parallèlement et s'inspirant des travaux de Del Negro et Schorfheide (2004), un modèle BVAR-DSGE a été estimé en exploitant les priors issus du modèle HNKM. Les fonctions de réponse impulsionnelles ont été comparées et les performances prédictives de ces deux modèles structurels ont été confrontées à des modèles statistiques alternatifs: le VAR classique et le BVAR. Il ressort des résultats des modèles HNKM et BVAR-DSGE que les réactions des variables aux di¤érents chocs sont globalement similaires et conformes aux prédictions de la théorie économique. L'étude de la qualité prévisionnelle des di¤érents modèles indique que le BVAR-DSGE et le HNKM présentent des avantages comparatifs mais sans dominer, en tous points, les modèles statistiques tels que le VAR classique et le VAR bayésien.
C10|Spatial Dynamic Panel Data Models with Correlated Random Effects|In this paper, M-estimation and inference methods are developed for spatial dynamic panel data models with correlated random effects, based on short panels. The unobserved individual-specific effects are assumed to be correlated with the observed time-varying regressors linearly or in a linearizable way, giving the so-called correlated random effects model, which allows the estimation of effects of time-invariant regressors. The unbiased estimating functions are obtained by adjusting the conditional quasi-scores given the initial observations, leading to M-estimators that are consistent, asymptotically normal, and free from the initial conditions except the process starting time. By decomposing the estimating functions into sums of terms uncorrelated given idiosyncratic errors, a hybrid method is developed for consistently estimating the variance-covariance matrix of the M-estimators, which again depends only on the process starting time. Monte Carlo results demonstrate that the proposed methods perform well in finite sample.
C10|Economic Diversity and Regional Economic Performance: A Methodological Concern from Model Uncertainty|Although the role of spatial dependence has been considered in studying the relationship between economic diversity and regional economic performance, the existing literature seldom mentions model uncertainty, which mainly arises from at least two sources. One source of model uncertainty is the choice of an appropriate spatial weight matrix that describes the spatial interactions between two regions, which can be specified in a variety of ways. The second source of model uncertainty is choosing a set of control variables to model the diversity-performance relationship. To overcome these limitations, a Bayesian Model Averaging (BMA) method is used to address model uncertainty when studying the effects of economic diversity on short-term employment growth and long-term economic stability among 359 Metropolitan Statistical Areas (MSA) in the contiguous U.S. The potential spatial spillovers are also considered through spatial regression models. This empirical analysis suggests that ignoring model uncertainty can impact the estimates and our understanding of economic diversity, and it also confirms that economic diversity of neighbors plays an important role in regional economic development.
C10|Pc Complex: Pc Algorithm For Complex Survey Data|PC algorithm is one of the most known procedures for Bayesian networks structural learning. The structure is inferred carrying out several independence tests on a database and building a Bayesian network in agreement with the tests results. The PC algorithm is based on the assumption of independent and identically distributed observations. In practice, sample selection in surveys involves more complex sampling designs, then the standard test procedure is not valid even asymptotically. In order to avoid misleading results about the true causal structure the sample selection process must be taken into account in the structural learning process. In this paper, a modi ed version of the PC algorithm is proposed for inferring casual structure from complex survey data. It is based on resampling techniques for nite population. A simulation experiment showing the robustness with respect to departures from the assumptions and the good performance of the proposed algorithm is carried out.
C10|Ambiguous economic news and heterogeneity: What explains asymmetric consumption responses?|We study information and consumption and whether consumers respond symmetrically to good and bad news. We define a news variable and show that it has explanatory power. We, then, test the hypothesis that consumers react more to bad news than to good news using the PSID to analyze the response of households’ consumption to news about aggregate future income.We find that our news variable helps one predict households’ consumption change and that consumption responses are larger following negative (bad) news than positive (good) news and suggest that observed asymmetric consumption responses could be due to agents’ aversion to ambiguous information.
C10|Wavelet analysis for temporal disaggregation|A problem often faced by economic researchers is the interpolation or distribution of economic time series observed at low frequency into compatible higher frequency data. A method based on wavelet analysis is presented to temporal disaggregate time series. A standard `plausible' method is applied, not to the original time series, but to the smooth components resulting from a discrete wavelet transformation. This first step generates a smoothed component at the desired frequency. Subsequently, a noisy component is added to the smooth series to enforce the natural constraint of the series. The method is applied to national accounts for Euro Area, to study both ow and stock variables, and it outperforms other standard methods, as Stram and Wei or low pass interpolation when the series of interest is volatile.
C10|Propriétés cycliques des transferts de fonds des migrants marocains|Le phénomène de transferts de fonds suscite un débat passionnant au sein de la classe politique, des chercheurs et des universitaires. Ceci est dû au fait qu’il touche à des aspects humains, économiques et financiers. Les transferts de fonds sont considérés comme étant une source de financement importante pour la majorité des pays en développement. Le Maroc n’échappe pas à cette logique. Il est, en effet, l’un des pays qui reçoit le plus de transferts de fonds dans le monde. L’objet de cet article est d’étudier les propriétés cycliques des transferts de fonds vers le Maroc. En utilisant des filtres économétriques adéquats et pertinents ainsi que des données trimestrielles nous arrivons à mettre en lumière des aspects empiriques importants des transferts de fonds. En effet, les transferts de fonds constituent un enjeu majeur pour l’économie marocaine, de même, jouer les rôles importants de stabilisateur et d’amortisseurs. Remittances raise a fascinating debate in the political class, researchers and academics. This is because they affect human, economic and financial aspects. In Morocco, remittances are considered an important source of funding. It is, indeed, one of the countries that receive the most remittances in the world. The aim of this paper is to study the cyclical properties of remittances to Morocco. Using appropriate data and relevant econometric filters we can highlight important empirical aspects of remittances. Indeed, remittances constitute a major challenge for the Moroccan economy as well, play important roles in stabilizing and as a shock absorber
C10|Standard Budgets in Spanish Economic History: a User’s Guide to Sources and Methods|In this paper the author documents a near absence of household budget microdata in the sources for nineteenth century Spain, both published and archival. The sources do however contain a rich set of standard budgets, which can contribute to a better understanding of the history of Spanish living standards. The paper is divided in three parts: first, the author describes standard budgets and their usefulness for cliometricians; secondly, he traces their history in the Spanish sources; finally, he sketches a few applications to issues in the period 1850-1905. The latter analysis suggests: a) a substantial variation in cost of living and expenditure patterns across provinces, sectors, and socioeconomic status; b) a sensible impact of alternative CPI weights on national price indices; c) poverty lines ranging between 1,110 and 1,300 euros per year – in 2016 prices – for the years 1850 and 1856.
C10|Sample statistics as convincing evidence: A tax fraud case|This report deals with the analysis of data used by tax officers to support their claim of tax fraud at a pizzeria. The possibilities of embezzlement under study are overreporting of take-away sales and underreporting of cash payments. Several modelling approaches are explored, ranging from simple well-known methods to presumably more precise tools. More specifically, we contrast common methods based on normal assumptions and models based on Gamma-assumptions. For the latter, both maximum likelihood and Bayesian approaches are covered. Several criteria for the choice of method in practice are discussed, among them, how easy the method is to understand, justify and communicate to the parties. Some dilemmas present itself: the choice of statistical method, its role in building the evidence, the choice of risk factor, the application of legal principles like “clear and convincing evidence” and “beyond reasonable doubt”. The insights gained may be useful for both tax officers and defenders of the taxpayer, as well as for expert witnesses.
C10|Tangency portfolio weights for singular covariance matrix in small and large dimensions: estimation and test theory|In this paper we derive the nite-sample distribution of the esti- mated weights of the tangency portfolio when both the population and the sample covariance matrices are singular. These results are used in the derivation of a statistical test on the weights of the tangency port- folio where the distribution of the test statistic is obtained under both the null and the alternative hypotheses. Moreover, we establish the high-dimensional asymptotic distribution of the estimated weights of the tangency portfolio when both the portfolio dimension and the sam- ple size increase to in nity. The theoretical ndings are implemented in an empirical application dealing with the returns on the stocks included into the S&P 500 index.
C10|Bayesian inference for the tangent portfolio|In this paper we consider the estimation of the weights of tangent portfolios from the Bayesian point of view assuming normal conditional distributions of the logarithmic returns. For di↵use and conjugate priors for the mean vector and the covariance matrix, we derive stochastic representations for the posterior distributions of the weights of tangent portfolio and their linear combinations. Separately we provide the mean and variance of the posterior distributions, which are of key importance for portfolio selection. The analytic results are evaluated within a simulation study, where the precision of coverage intervals is assessed.
C10|Benefits of real-time pricing and rooftop solar PV generation: Explorations using Swedish micro-data|Previous empirical literature on residential dynamic pricing for the Nordic market has questioned whether households will in fact appropriately respond, in view of the low price variability and price responsiveness in the Swedish setting. Household demand response is an issue of some importance in view of increasingly smart grids in which high shares of renewable supply are being promoted partly in view of these possibilities. In addition, an important development in the Nordic market relates to increasing thrust on household PV panels. In view of the interaction between RTP-driven and PV generation-driven load changes, an analysis of the combined effects in relation to system timing is important to understand, not least because this can affect the nature of benefits to households and the electric grid. Using a unique and very detailed dataset on household electricity consumption, in combination with simulated solar panel micro-generation data, these aspects are explored in an empirical framework similar to that used in the prior literature. Our findings indicate that even with minimal price responsiveness, household response to dynamic pricing can lead to load changes with sizeable benefits. In addition, the introduction of PV panels, contrary to what may be assumed at a first glance, appear to be beneficial to the electric grid, largely due to the time pattern of winter PV generation. Overall, our empirical findings provide tentative evidence to indicate that RTP, by incentivizing households to provide demand response at appropriate times, can aid in integration of renewables.
C10|Estimating the Intergenerational Elasticity of Expected Income with Short-Run Income Measures: A Generalized Error-in-Variables Model|The intergenerational income elasticity (IGE), ubiquitously estimated in the economic mobility literature, has been misinterpreted as pertaining to the expectation of children’s income when it actually pertains to its geometric mean. The (implicit) reliance on the geometric mean to index conditional income distributions greatly hinders the study of gender and marriage dynamics in intergenerational processes, and leads to IGE estimates affected by substantial selection biases. For these reasons, it has been recently proposed that the conventional IGE be replaced by the IGE of the expectation as the workhorse intergenerational elasticity. To make this possible, mobility scholars need to have available a generalized error-in-variables model for the estimation of the latter IGE with short-run income measures. This paper derives a Taylor-series-based closed-form expression for the probability limit of the Poisson Pseudo Maximum Likelihood (PPML) estimator, and uses it to develop the needed error-in-variables model. It also evaluates the model with data from the Panel Study of Income Dynamics. The results of the empirical analyses offer clear support for the account of lifecycle and attenuation biases provided by the model, and show that the strategy most commonly employed to estimate the conventional IGE by Ordinary Least Squares can also be used for the estimation of the IGE of the expectation with the PPML estimator.
C10|Predicting Retirement Savings Using Survey Measures Of Exponential‐Growth Bias And Present Bias|In a nationally representative sample, we predict retirement savings using survey‐based elicitations of exponential‐growth bias (EGB) and present bias (PB). We find that EGB, the tendency to neglect compounding, and PB, the tendency to value the present over the future, are highly significant and economically meaningful predictors of retirement savings. These relationships hold controlling for cognitive ability, financial literacy, and a rich set of demographic controls. We address measurement error as a potential confound and explore mechanisms through which these biases may operate. Back of the envelope calculations suggest that eliminating EGB and PB would increase retirement savings by approximately 12%. (JEL D91, D14)
C10|Zooming the Ins and Outs of the U.S. Unemployment with a Wavelet Lens|To better understand unemployment dynamics it is key to assess the role played by job creation and job destruction. Although the U.S. case has been studied extensively, the importance of job finding and employment exit rates to unemployment variability remains unsettled. The aim of this paper is to contribute to this debate by adopting a novel lens, wavelet analysis. We resort to wavelet analysis to unveil time- and frequency-varying features regarding the contribution of the job finding and job separation rates for the U.S. unemployment rate dynamics. Drawing on this approach, we are able to reconcile some apparently contradictory findings reported in previous literature. We find that the job finding rate is more influential for the overall unemployment behavior but the job separation rate also plays a critical role, especially during recessions.
C10|Interdependent Hazards, Local Interactions, and the Return Decision of Recent Migrants|Consider the duration of stay of migrants in a host country. We propose a statistical model of locally interdependent hazards in order to examine whether interactions at the level of the neighbourhood are present and lead to social multipliers. To this end, we propose and study a new two-stage estimation strategy based on an inverted linear rank test statistic. Using a unique large administrative panel dataset for the population of recent labour immigrants to the Netherlands, we quantify the local social multipliers in several factual and counterfactual experiments, and demonstrate that these can be substantial.
C10|The Sources and Methods Used in the Creation of the Levy Institute Measure of Economic Well-Being for the United States, 1959-2013|This paper documents the sources of data used in the construction of the estimates of the Levy Institute Measure of Economic Wellbeing (LIMEW) for the years 1959, 1972, 1982, 1989, 1992, 1995, 2000, 2001, 2004, 2007, 2010, and 2013. It also documents the methods used to combine the various sources of data into the synthetic dataset used to produce each year's LIMEW estimates.
C10|Energy policy tools in Luxembourg - Assessing their impact on households’ space heating energy consumption and CO2 emissions by means of the LuxHEI model|In the Grand Duchy of Luxembourg, the residential building sector is a major energy consumer and greenhouse gases emitter that is considered key in achieving the country’s climate goals. The purpose of this paper is to assess the effectiveness of the most important policy instruments in achieving savings in the final energy consumption and direct CO2 emissions of Luxembourgish households. Our study is based on the LuxHEI model, which is an enhanced and upgraded version of the well-known French simulation model Res-IRF. This variant has been adjusted to the particular problems of a small country with growing economy and a quickly increasing population. The LuxHEI model goes beyond standard energy-economy models by incorporating global warming as a decision-making factor. The model outcomes reveal that total environmental and economic effectiveness increases if energy policy tools are applied concurrently. In 2060, and compared to the no-policy baseline scenario, the most aspirational policy mix enables energy savings of 42% and an emission mitigation of 60%. From our results, we can draw the following policy implications: for a significant improvement of the sector’s energy efficiency and sufficiency, (1) the implementation of a remediation duty for existing buildings and (2) the tightening of the performance standards for new constructions, (3) combined with a national carbon tax, are crucial.
C10|Combining uncertainty with uncertainty to get certainty? Efficiency analysis for regulation purposes|Data envelopment analysis (DEA) and stochastic frontier analysis (SFA), as well as combinations thereof, are widely applied in incentive regulation practice, where the assessment of efficiency plays a major role in regulation design and benchmarking. Using a Monte Carlo simulation experiment, this paper compares the performance of six alternative methods commonly applied by regulators. Our results demonstrate that combination approaches, such as taking the maximum or the mean over DEA and SFA efficiency scores, have certain practical merits and might offer a useful alternative to strict reliance on a singular method. In particular, the results highlight that taking the maximum not only minimizes the risk of underestimation, but can also improve the precision of efficiency estimation. Based on our results, we give recommendations for the estimation of individual efficiencies for regulation purposes and beyond.
C10|The Digital World: I - Bitcoin: from history to real live|Bitcoin can be considered as a medium exchange restricted to online markets, but it is not a unit of account and a store of value, and thus cannot be considered as a money. Bitcoin value is very volatile and traded for different prices in different exchanges platforms, and thus can be used for arbitrage purpose. His behavior can be associated with a high volatile stock, and most transactions in Bitcoin are aimed to speculative instruments. The high volatility in Bitcoin and the occurrence of speculative bubble depend on positive sentiment and confidence about Bitcoin market: several variables may be considered as indicators (volume of transactions, number of transactions, number of Google research, wikipedia requests). The star of the crypto-currencies has attained the 19 716 dollars in December 2017 and decreased to 6 707 dollars March 29, 2018. In capitalization it is at this time the 30th mondial currency. We explain some limits and interests of the Bitcoin system and why the central bankers and regulators need to take some decision on its existence, and what could be the possible evolution of the Bitcoin Blockchain
C10|The Digital World: II - Alternatives to the Bitcoin Blockchain?|In a previous paper (The Digital World: I - Bitcoin: from history to real live, Guégan, 2018), we explain some limits and interests of the Bitcoin system and why the central bankers and regulators need to take some decision on its existence. In this article, we develop some alternatives to the Bitcoin blockchain which are considered by the banking system and industries
C10|Expanding tidy data principles to facilitate missing data exploration, visualization and assessment of imputations|Despite the large body of research on missing value distributions and imputation, there is comparatively little literature on how to make it easy to handle, explore, and impute missing values in data. This paper addresses this gap. The new methodology builds upon tidy data principles, with a goal to integrating missing value handling as an integral part of data analysis workflows. New data structures are defined along with new functions (verbs) to perform common operations. Together these provide a cohesive framework for handling, exploring, and imputing missing values. These methods have been made available in the R package naniar.
C10|FFORMA: Feature-based forecast model averaging|We propose an automated method for obtaining weighted forecast combinations using time series features. The proposed approach involves two phases. First, we use a collection of time series to train a meta-model to assign weights to various possible forecasting methods with the goal of minimizing the average forecasting loss obtained from a weighted forecast combination. The inputs to the meta-model are features extracted from each series. In the second phase, we forecast new series using a weighted forecast combination where the weights are obtained from our previously trained meta-model. Our method outperforms a simple forecast combination, and outperforms all of the most popular individual methods in the time series forecasting literature. The approach achieved second position in the M4 competition.
C10|Meta-learning how to forecast time series|A crucial task in time series forecasting is the identification of the most suitable forecasting method. We present a general framework for forecast-model selection using meta-learning. A random forest is used to identify the best forecasting method using only time series features. The framework is evaluated using time series from the M1 and M3 competitions and is shown to yield accurate forecasts comparable to several benchmarks and other commonly used automated approaches of time series forecasting. A key advantage of our proposed framework is that the time-consuming process of building a classifier is handled in advance of the forecasting task at hand.
C10|Linking Individuals Across Historical Sources: a Fully Automated Approach|Linking individuals across historical datasets relies on information such as name and age that is both non-unique and prone to enumeration and transcription errors. These errors make it impossible to find the correct match with certainty. In the first part of the paper, we suggest a fully automated probabilistic method for linking historical datasets that enables researchers to create samples at the frontier of minimizing type I (false positives) and type II (false negatives) errors. The first step guides researchers in the choice of which variables to use for linking. The second step uses the Expectation-Maximization (EM) algorithm, a standard tool in statistics, to compute the probability that each two records correspond to the same individual. The third step suggests how to use these estimated probabilities to choose which records to use in the analysis. In the second part of the paper, we apply the method to link historical population censuses in the US and Norway, and use these samples to estimate measures of intergenerational occupational mobility. The estimates using our method are remarkably similar to the ones using IPUMS’, which relies on hand linking to create a training sample. We created an R code and a Stata command that implement this method.
C10|The Simple Empirics of Optimal Online Auctions|We study reserve prices computed to maximize the expected profit of the seller based on historical observations of incomplete bid data typically available to the auction designer in online auctions for advertising or e-commerce. This direct approach to computing reserve prices circumvents the need to fully recover distributions of bidder valuations. We derive asymptotic results and also provide a new bound, based on the empirical Rademacher complexity, for the number of historical auction observations needed in order for revenue under the estimated reserve price to approximate revenue under the optimal reserve arbitrarily closely. This simple approach to estimating reserves may be particularly useful for auction design in Big Data settings, where traditional empirical auctions methods may be costly to implement. We illustrate the approach with e-commerce auction data from eBay. We also demonstrate how this idea can be extended to estimate all objects necessary to implement the Myerson (1981) optimal auction.
C10|Using Social Network Activity Data to Identify and Target Job Seekers|An important challenge for many firms is to identify the life transitions of its customers, such as job searching, being pregnant, or purchasing a home. Inferring such transitions, which are generally unobserved to the firm, can offer the firm opportunities to be more relevant to its customers. In this paper, we demonstrate how a social network platform can leverage its longitudinal user data to identify which of its users are likely job seekers. Identifying job seekers is at the heart of the business model of professional social network platforms. Our proposed approach builds on the hidden Markov model (HMM) framework to recover the latent state of job search from noisy signals obtained from social network activity data. Specifically, our modeling approach combines cross-sectional survey responses to a job seeking status question with longitudinal user activity data. Thus, in some time periods, and for some users, we observe the “true” job seeking status. We fuse the observed state information into the HMM likelihood, resulting in a partially HMM. We demonstrate that the proposed model can not only predict which users are likely to be job seeking at any point in time, but also what activities on the platform are associated with job search, and how long the users have been job seeking. Furthermore, we find that targeting job seekers based on our proposed approach can lead to a 42% increase in profits of a targeting campaign relative to the approach that was used at the time of the data collection.
C10|Detrending and financial cycle facts across G7 countries: mind a spurious medium term!|I show that the detrending of financial variables with the Hodrick and Prescott (1981, 1997) (HP) and band-pass filters leads to spurious cycles. I find that distortions become especially severe when considering medium-term cycles, i.e., cycles that exceed the duration of regular business cycles. In particular, these medium-term filters amplify the variances of cycles of duration around 20 to 30 years up to a factor of 204, completely cancelling out shorter-term fluctuations. This is important because it is common practice, and recommended under Basel III, to extract medium-term cycles using such filters; e.g., the HP filter with a smoothing parameter of 400,000. In addition, I find that financial cycle facts, i.e., differing amplitude, duration, and synchronisation of cycles in financial variables relative to cycles in GDP, are robust. For HP and band-pass filters, differences to GDP become marginal due to spurious cycles. JEL Classification: C10, E32, E44, E58, G01
C10|Econometric Analysis of Productivity: Theory and Implementation in R|Our chapter details a wide variety of approaches used in estimating productivity and efficiency based on methods developed to estimate frontier production using Stochastic Frontier Analysis (SFA) and Data Envelopment Analysis (DEA). The estimators utilize panel, single cross section, and time series data sets. The R programs include such approaches to estimate firm efficiency as the time invariant fixed effects, correlated random effects, and uncorrelated random effects panel stochastic frontier estimators, time varying fixed effects, correlated random effects, and uncorrelated random effects estimators, semi-parametric efficient panel frontier estimators, factor models for cross-sectional and time-varying efficiency, bootstrapping methods to develop confidence intervals for index number-based productivity estimates and their decompositions, DEA and Free Disposable Hull estimators. The chapter provides the professional researcher, analyst, statistician, and regulator with the most up to date efficiency modelling methods in the easily accessible open source programming language R.
C10|An innovative feature selection method for support vector machines and its test on the estimation of the credit risk of default|Support vector machines (SVM) have been extensively used for classification problems in many areas such as gene, text and image recognition. However, SVM have been rarely used to estimate the probability of default (PD) in credit risk. In this paper, we advocate the application of SVM, rather than the popular logistic regression (LR) method, for the estimation of both corporate and retail PD. Our results indicate that most of the time SVM outperforms LR in terms of classification accuracy for the corporate and retail segments. We propose a new wrapper feature selection based on maximizing the distance of the support vectors from the separating hyperplane and apply it to identify the main PD drivers. We used three datasets to test the PD estimation, containing (1) retail obligors from Germany, (2) corporate obligors from Eastern Europe, and (3) corporate obligors from Poland. Total assets, total liabilities, and sales are identified as frequent default drivers for the corporate datasets, whereas current account status and duration of the current account are frequent default drivers for the retail dataset.
C10|Fiscal and education spillovers from charter school expansion|The fiscal and educational consequences of charter expansion for non-charter students are central issues in the debate over charter schools. Do charter schools drain resources and high-achieving peers from non-charter schools? This paper answers these questions using an empirical strategy that exploits a 2011 reform that lifted caps on charter schools for underperforming districts in Massachusetts. We use complementary synthetic control instrumental variables (IV-SC) and differences-in-differences instrumental variables (IV-DiD) estimators. The results suggest greater charter attendance increases per-pupil expenditures in traditional public schools and induces them to shift expenditure from support services to instruction and salaries. At the same time, charter expansion has a small positive effect on non-charter students’ achievement.
C10|The Impact of Agricultural Subsidies on Farm Production: A Synthetic Control Method Approach|Czech farmers experienced an enormous exogenous shock when they joined the common agricultural market (CAM) and the Common Agricultural Policy (CAP) in 2004. Using the World Bank's dataset, we apply the synthetic control method to establish a counterfactual case of the Czech Republic food production index in the absence of the CAM and CAP. The results show that the Czech Republic would have had a higher food index if it had not entered the CAM and CAP. Moreover, we show that the CAP and CAM had different impacts on farms in the Czech Republic and Bulgaria, which have the most comparable agriculture according to the results of the synthetic control method.
C10|A Closer Look at the Behavior of Uncertainty and Disagreement: Micro Evidence from the Euro Area|This paper examines point and density forecasts of real GDP growth, inflation and unemployment from the European Central Bank’s Survey of Professional Forecasters. We present individual uncertainty measures and introduce individual point- and density-based measures of disagreement. The data indicate substantial heterogeneity and persistence in respondents’ uncertainty and disagreement, with uncertainty associated with prominent respondent effects and disagreement associated with prominent time effects. We also examine the co-movement between uncertainty and disagreement and find an economically insignificant relationship that is robust to changes in the volatility of the forecasting environment. This provides further evidence that disagreement is not a reliable proxy for uncertainty.
C10|A Closer Look at the Behavior of Uncertainty and Disagreement: Micro Evidence from the Euro Area|This paper examines point and density forecasts of real GDP growth, inflation and unemployment from the European Central Bank’s Survey of Professional Forecasters. We present individual uncertainty measures and introduce individual point- and density-based measures of disagreement. The data indicate substantial heterogeneity and persistence in respondents’ uncertainty and disagreement, with uncertainty associated with prominent respondent effects and disagreement associated with prominent time effects. We also examine the co-movement between uncertainty and disagreement and find an economically insignificant relationship that is robust to changes in the volatility of the forecasting environment. This provides further evidence that disagreement is not a reliable proxy for uncertainty.
C10|Selection Without Exclusion|It is well understood that classical sample selection models are not semiparametrically identified without exclusion restrictions. Lee (2009) developed bounds for the parameters in a model that nests the semiparametric sample selection model. These bounds can be wide. In this paper, we investigate bounds that impose the full structure of a sample selection model with errors that are independent of the explanatory variables but have unknown distribution. We find that the additional structure in the classical sample selection model can significantly reduce the identified set for the parameters of interest. Specifically, we construct the identified set for the parameter vector of interest. It is a one-dimensional line-segment in the parameter space, and we demonstrate that this line segment can be short in principle as well as in practice. We show that the identified set is sharp when the model is correct and empty when model is not correct. We also provide non-sharp bounds under the assumption that the model is correct. These are easier to compute and associated with lower statistical uncertainty than the sharp bounds. Throughout the paper, we illustrate our approach by estimating a standard sample selection model for wages.
C10|Easy bootstrap-like estimation of asymptotic variances|The bootstrap is a convenient tool for calculating standard errors of the parameter estimates of complicated econometric models. Unfortunately, the bootstrap can be very time-consuming. In a recent paper, Honoré and Hu (2017), we propose a “Poor (Wo)man’s Bootstrap” based on one-dimensional estimators. In this paper, we propose a modified, simpler method and illustrate its potential for estimating asymptotic variances.
C10|A Composite Likelihood Approach for Dynamic Structural Models|We describe how to use the composite likelihood to ameliorate estimation, computational, and inferential problems in dynamic stochastic general equilibrium models. We present a number of situations where the methodology has the potential to resolve well-known problems. In each case we consider, we provide an example to illustrate how the approach works and its properties in practice.
C10|New empirical evidence on E.M.H.: case of developed and emerging markets – a microeconomic approach|In today’s globalized world, with interconnected global markets, and implicitly a higher level of sensitivity, one of the most important issues to be addressed is represented by the way market mechanisms are functioning. The main purpose of the present study is to answer the question of whether the selected markets are consistent with the Efficient Market Hypothesis, at a microeconomic level, by creating an Efficiency Index, using L. Kristoufek si M. Vosvrda (L. Kristoufek, M. Vosvrda, 2013, 184) method.We use estimators of long term memory, fractal dimension, and approximateentropy, in order to create the Efficiency Index. The results are commented both at a macroeconomic level and at a microeconomic level, as we apply the methodology on 150 companies, part of 12 stock market indices from developed and emerging economies. Wefind that the results are consistent with those obtained by L. Kristoufek si M. Vosvrda (L. Kristoufek, M. Vosvrda, 2013, 184), with most of the efficient companies being part of the developed markets, while the least efficient companies part of emerging economies. This implies the existence of a market dynamics characterized by going through areas with distinctive levels of “informational efficiency”. The present study contributes to a better understanding of financial market mechanisms at a microeconomic level, by testing the Efficient Market Hypothesis, and constructing the Efficiency Index.
C10|Maximizing profit increase in manufacturing based on an Information Theory model|US Patent 10 054 929 B1
C10|Mental Health before, during and in the aftermath of the Great Recession in Canada|Using province level panel data from the Canadian Community Health Survey (2007-2014), this study compares the mental health of Canadian before the 2008-2009 Great Recession with that of during the recession and after the recession. Mental health is measured using following indicators: self- reported mental health status, self-perceived life stress, and mental health care utilization. In the estimation, the study utilized following econometric techniques: ordered probit method, OLS method, panel data fixed effects method, logit method and conditional fixed effects logit method. The study found that self-reported mental health declined during recession and it remained at a lower level even after the end of recession. Similarly, self-perceived life stress increased during recession and it remained at a higher level in the aftermath of recession. Compared to pre-recession level, mental health care utilization increased during recession and after the end of recession. The study divided sample on the basis of gender, age categories and employment status. In all cases, the findings are similar: compared to pre-recession level, mental health declined during recession and mental health did not recover to pre-recession level even after the end of recession. The results of the study have important policy implications. The results suggest that recession not only has direct economic costs in terms of loss of employment and production, but it also has indirect costs in terms of its adverse impact on mental health.
C10|A test of the applicability of the Theory of Planned Behaviour in the Botswana context using multiple regression|Culturally sensitive and effective interventions to reduce the high risky sexual behaviours remain one promising approach to easing the effects of HIV and AIDS on African adolescents.In order to test the applicability of the theory of planned behaviour (TPB) in preventing HIV and AIDS among adolescents within the Botswana context, multiple regression modelling was applied to baseline data from a randomised control trial involving about 800 Batswana in-school adolescents. The predictors of interest were all derived from the TPB. The results indicated that there was significant correlation (p
C10|Quantitative methods in the triangulation process|The following article presents an analysis of selected quantitative methods in the research process. The author highlighted the importance of quantitative analysis in the research process, which requires to collect a great number of data, as well as to analyse and interpret them. Selected research methods were stipulated: SPACE (Strategic Position and Action Evaluation), network methods and the Delphi method. An analysis of the use of triangulation method in order to enhance the reliability of research results was carried out.The Author attempted to answer the following research questions: What research methods should be employed in the research processes within the area of economics? Is the methodological triangulation a prerequisite in research processes? Does the triangulation enhance the reliability of employed quantitative methods in research processes?
C10|Analysis of Factors Affecting Employee Job Satisfaction|The aim of this study is to understand and determine the factors situations that may occur in the workplace affecting job satisfaction. In particular, the experiences of the employees at the workplace (quantitative workload and interpersonal conflict) and their emotional abilities are considered as reasons. The study group consisted of 199 employees. The length of employment in the workplace at the time of the study was with a range of 2-26 years. According to the results obtained, these four variables have a statistically significant effect in explaining job satisfaction. 30% of the variance of job satisfaction is explained by these variables. Results show that the workplace experiences and the emotional ability of the people are effective on job satisfaction. Empathy and self-esteem positively affected job satisfaction, however, conflict and workload negatively affected. The most important variable explaining job satisfaction had found empathy.As a result of this study, empathy, self-esteem, quantitative workload and interpersonal conflict were identified as important variables to increase job satisfaction.
C10|Self-Esteem, Empathy and Jealousy in the Workplace|The purpose of this study is to examine the relationships among emotional states of self-esteem, empathy and employee jealousy in private sector employees using structural equation modeling. The study participants ranged in age from 28 years to 53 years, approximately 70% were male. The study group consisted of 216 employees. Printed questionnaires were sent to the entire private sector employees of a single fabric and were filled out anonymously. Relevant emotional states addressed in this study are self-esteem, empathy, and employee jealousy. A conceptual model was created in which self-esteem directly affects empathy and employee jealousy, and employee jealousy directly affects empathy. Then it was estimated by structural equation modeling. The results were ?2/df=3.070; RMSEA=0.09; SRMR=0.09. All the results are statistically significant. In particular, this study has shown that the self-esteem of employees is significant effects on the emotional states that can occur at work, such as empathy and employee jealousy. The employee jealousy plays a mediating role for self-esteem.
C10|Investigation of Customer Loyalty in Tourism|Aim: The aim of this study is to investigate customer loyalty in the tourism sector by taking into consideration quality of service and customer satisfaction.Method: A questionnaire was applied to measure customer loyalty, quality of service and customer satisfaction. 369 customers answered the questionnaire voluntarily. Approximately 42% of customers who filled out the questionnaire were male. The data were analyzed using the structural equations modeling. Results: The customer satisfaction and quality of service have been used to explain customer loyalty. A structural equation model containing these latent variables have been constructed and estimated. The signs of the standardized coefficients have been found to be in accordance with the expectations. Quality of service has directly positively affected customer loyalty (?=0.51; p
C10|On sampling from a rectangular grid|The problem of estimating the population total of a variable of interest by sampling from a finite population with units arranged in a rectangular array is considered. As an alternative to simple random sampling, a two-step sampling procedure is proposed. The procedure first chooses some rows (columns) by simple random sampling without replacement, then some columns (rows) by simple random sampling without replacement and uses the sample consisting of the units corresponding to the intersection of rows and columns selected. An unbiased estimator of the population total, using this step-wise sampling procedure, is proposed. The variance of the estimator is derived and further, an unbiased estimator for that variance is obtained.
C10|Causal Inference from Strip-Plot Designs: Methodology and Applications in a Potential Outcomes Framework|Randomization based causal inference in a potential outcomes framework has been of significant interest in recent years. The principal advantage of such inference is that it does not involve rigid model assumptions. We develop the methodology of causal inference from strip-plot designs that are very useful when the responses are influenced by treatments having a factorial structure and the factor levels are hard-to-change, so that they have to be applied to larger clusters of experimental units. Our results can have application to diverse fields such as sociology, agriculture, and urban management, to name only a few. For example, in an agricultural field experiment with two factors, irrigation and harvesting, both requiring larger plots, the experimental units can be laid out in several blocks, each block being a rectangular array of rows and columns. One can then employ a strip-plot design that randomizes the methods of irrigation among the rows and the methods of harvesting among the columns, in each block. Similarly, a strip-plot design is a natural choice in urban traffic management where each block is rectangular grid of streets, and within any such grid, signaling conditions are randomized among the north-south streets while traffic rules are randomized among east-west streets. With a strip-plot design under a potential outcomes framework, we propose an unbiased estimator for any treatment contrast and work out an expression for its sampling variance. We next obtain a conservative estimator of the sampling variance. This conservative estimator has a nonnegative bias, and becomes unbiased under a condition that is much milder than the age-old Neymannian strict additivity. A minimaxity property of this variance estimator is also established. Simulation results on the coverage of resulting confidence intervals lend support to theoretical considerations.
C10|Application Of The Positional Pot-Topsis Method To The Assessment Of Financial Self-Sufficiency Of Local Administrative Units|In this paper we propose new approach to the construction of synthetic measure, where the objects are described by the characteristics with strong asymmetry and outliers. The aim of the paper is to present the application potential of the tools of the Extreme Values Theory (EVT) i.e. Peaks over Threshold Model (POT) in constructing a synthetic measure based of positional Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS), that utilize the spatial median of Weber. POT model has been used for identification of outliers and to determination of the positive and negative ideal solutions of financial self-sufficiency of local administrative units (LAUs). This approach is used in the assessment of financial self-sufficiency of LAUs in Poland in 2016.
C10|Dinámicas de Pobreza en México, 2008-2014|Ante la poca disponibilidad de datos longitudinales para México, en este artículo se busca mostrar la dinámica de las personas en situación de pobreza con base en encuestas de corte transversal de ingresos y gastos de los hogares. Usando la metodología de pseudopanel de Dang y Lanjouw (2013), se estiman las dinámicas de pobreza en México entre 2008 y 2014. Los resultados de movilidad absoluta para todo el periodo estudiado, muestran que cerca de la mitad de la población se encontró en situación de pobreza por ingresos. Así mismo, los resultados de movilidad relativa muestran que si un hogar estuvo en situación de pobreza en algún año tiene altas probabilidades de seguir en esta situación dos años más tarde. Se identifica que los hogares más afectados son aquellos con rezago educativo, carencia por acceso a servicios básico en la vivienda, carencia por acceso a servicios de salud, o que han estado en situación de pobreza durante cuatro o más años.
C10|A novel statistical approach to marketing campaigns|We propose a novel heterogeneous choice model to deal with heterogeneity issues in decision making across individuals for marketing campaigns. Based on the proposed model, we further develop a novel classification method that enables us to identify an optimal group of individuals in a database for, e.g. future bank marketing campaigns. Our results show that, in the presence of heterogeneity, our model outperforms some commonly used benchmark models with respect to model specification errors, and our classification method outperforms the conventional method in respect of the positive response rate in marketing campaigns.
C10|A novel approach to modelling the distribution of financial returns|We develop a novel quantile function threshold GARCH model for studying the distribution function, rather than the volatility function, of financial returns that follow a threshold GARCH model. We propose a Bayesian method to do estimation and forecasting simultaneously, which allows us to handle multiple thresholds easily and ensures the forecasts can take account of the variation of model parameters. We apply the method to simulated data and Nasdaq returns. We show that our model is robust to model specification errors and outperforms some commonly used threshold GARCH models.
C10|The Evolution of Forecast Density Combinations in Economics|Increasingly, professional forecasters and academic researchers present model-based and subjective or judgment-based forecasts in economics which are accompanied by some measure of uncertainty. In its most complete form this measure is a probability density function for future values of the variables of interest. At the same time combinations of forecast densities are being used in order to integrate information coming from several sources like experts, models and large micro-data sets. Given this increased relevance of forecast density combinations, the genesis and evolution of this approach, both inside and outside economics, is explored. A fundamental density combination equation is specified which shows that various frequentist as well as Bayesian approaches give different specific contents to this density. In its most simplistic case, it is a restricted finite mixture, giving fixed equal weights to the various individual densities. The specification of the fundamental density combination is made more flexible in recent literature. It has evolved from using simple average weights to optimized weights and then to `richer' procedures that allow for time-variation, learning features and model incompleteness. The recent history and evolution of forecast density combination methods, together with their potential and benefits, are illustrated in a policy making environment of central banks.
C10|Impact of advertizing on brand’s market-shares in the automobile market:: a multi-channel attraction model with competition and carry-over effects|This article presents a new approach to measure the impact of multi-channel advertising investments on brands’ market shares in the main segment of the French automobile market. We propose a multi-channel attraction model with adstock, in order to take into account the advertising carryover effect and the competition. This model allows to distinguish between short term and long term effect of the advertising. As, from a mathematical point of view, a vector of market shares is a composition belonging to the simplex space, i.e. subject to positivity and summing up to one contraints, we take benefit from the compositional data analysis (CODA) literature to estimate properly this model. We show how to determine the carryover parameters for each channel (outdoor, press, radio and television) in a multivariate way. We consider several model specifications with more or less complexity (cross effects between brands), including Dirichlet models, and we compare them using goodness-of-fit and prediction accuracy measures. We explain how to built confidence and prediction ellipsoids in the space of market shares. The impact of each channel on market shares is measured in terms of direct and cross elasticities. We conclude that in this market, radio only has a contemporaneous impact whereas outdoor, press and television have a large decay effect. Moreover, the advertising elasticities vary across brands and channels, and can be negative. It also turns out that positive interactions do exist between certain brands for certain media.
C10|"""Generalized Measures of Correlation for Asymmetry, Nonlinearity, and Beyond"": Comment"|This note comments on the Generalised Measure of Correlation (GMC) suggested by Zheng et al. (2012). The GMC concept was largely anticipated in a publication 115 years earlier, undertaken by Yule (1897), in the proceedings of the Royal Society. The note is directed at giving Yule (1897) credit for covering the foundations of the topic comprehensively.
C10|On the decline of war|For the past 70 years, there has been a downward trend in the size of wars, but the idea of an enduring ‘long peace’ remains controversial. Some recent contributions suggest that observed war patterns,including the long peace, could have come from a long-standing and unchanging war-generating process, an idea rooted in Lewis F Richardson’s pioneering work on war. Aaron Clauset has tested the hypothesis that the war sizes after the Second World War are generated by the same mechanism that generated war sizes before the Second World War and fails to reject the ‘no-change’ hypothesis. In this paper, we transform the war-size data into units of battle deaths per 100,000 or world population rather than absolute battle deaths – units appropriate for investigating the probability that a random person will die in a war. This change tilts the evidence towards rejecting the no-change hypothesis. We also show that sliding the candidate break point slightly forward in time, to 1950 rather than 1945, leads us further down the path toward formal rejection of the no-change hypothesis. Finally, we expand range of wars to include not just the inter-state wars considered by Clauset (2018) but also intra-state wars. Now we do formally reject the no-change hypothesis. Finally, we show that our results do not depend on the choice between two widely used war datasets.
C10|The behavior of social transfers over the business cycle: empirical evidence of Uruguay|This paper analyzes the cycle fluctuations of the social transfers in Uruguay over the period 1988.Q1 to 2016.Q3. The unobservable cyclical components are extracted from the observable time series following different empirical strategies. The results show that social transfers behave procyclical and lag the macroeconomics fluctuations. In this way, social transfers instead of contributing to stabilize the Uruguayan economy have aggravated the business cycle, and through various items of expenditure, expose the vulnerable groups of society to more adverse economic conditions.
C10|On the minimum correlation between symmetrically distributed random variables|We show that families of symmetrically distributed Bernoulli random variables have a maximal negative correlation that almost always is strictly above the general lower limit. JEL codes: C10, C61
C10|Bitcoin is not the New Gold – A comparison of volatility, correlation, and portfolio performance|Cryptocurrencies such as Bitcoin are establishing themselves as an investment asset and are often named the New Gold. This study, however, shows that the two assets could barely be more different. Firstly, we analyze and compare conditional variance properties of Bitcoin and Gold as well as other assets and find differences in their structure. Secondly, we implement a BEKK-GARCH model to estimate time-varying conditional correlations. Gold plays an important role in financial markets with flight-to-quality in times of market distress. Our results show that Bitcoin behaves as the exact opposite and it positively correlates with downward markets. Lastly, we analyze the properties of Bitcoin as portfolio component and find no evidence for stable hedging capabilities. We conclude that Bitcoin and Gold feature fundamentally different properties as assets and linkages to equity markets. Our results hold for the broad cryptocurrency index CRIX. As of now, Bitcoin does not reflect any distinctive properties of Gold other than asymmetric response in variance.
C10|Exogenous Drivers of Cryptocurrency Volatility - A Mixed Data Sampling Approach To Forecasting|We apply the GARCH-MIDAS framework to forecast the daily, weekly, and monthly volatility of four highly capitalized Cryptocurrencies (Bitcoin, Etherium, Litecoin, and Ripple) as well as the Cryptocurrency index CRIX. Based on the prediction quality, we determine the most important exogenous drivers of volatility in Cryptocurrency markets. We ?nd that the Global Real Economic Activity outperforms all other economic and ?nancial drivers under investigation. Only the average forecast combination results in lower loss functions. This indicates that the information content of exogenous factors is time-varying and the model averaging approach diversi?es the impact of single drivers.
C10|Coexistence of Symmetry Properties for Bayesian Confirmation Measures|"Many Bayesian Confirmation Measures have been proposed so far. They are used to assess the degree to which an evidence (or premise) E supports or contradicts an hypothesis (or conclusion) H, making use of prior probability P(H), posterior probability P(H|E) and of probability of evidence P(E). Many kinds of comparisons of those measures have already been made. Here we focus on symmetry properties of confirmation measures, which are partly inspired by classical geometric symmetries. We define symmetries relating them to the dihedral group of symmetries of the square, determining the symmetries that can coexist and reconsidering desirable/undesirable symmetry properties for a Bayesian Confirmation Measure."
C10|On the cyclical properties of Hamilton's regression filter|Hamilton (2017) criticises the Hodrick and Prescott (1981, 1997) filter (HP filter) because of three drawbacks (i. spurious cycles, ii. end-of-sample bias, iii. ad hoc assumptions regarding the smoothing parameter) and proposes a regression filter as an alternative. I demonstrate that Hamilton's regression filter shares some of these drawbacks. For instance, Hamilton's ad hoc formulation of a 2-year regression filter implies a cancellation of two-year cycles and an amplification of cycles longer than typical business cycles. This is at odds with stylised business cycle facts, such as the one-year duration of a typical recession, leading to inconsistencies, for example, with the NBER business cycle chronology. Nonetheless, I show that Hamilton's regression filter should be preferred to the HP filter for constructing a credit-to-GDP gap. The filter extracts the various medium-term frequencies more equally. Due to this property, a regression-filtered credit-to-GDP ratio indicates that imbalances prior to the global financial crisis started earlier than shown by the Basel III creditto-GDP gap.
C10|The Relationship between Energy Use, GDP, Carbon Dioxide Emissions, Population, Financial Development, and Industrialization: The Case of Turkey|This study investigates the relationship between energy use, GDP, carbon dioxide emissions, population, financial development, and industrialization utilizing ARDL and artificial neural network for Turkey. The data covers the period from 1968 to 2013. The study performed a two stage analysis. At the first stage, we examined the long run relationship and causality between variables. The variables are found to be cointegrated. The Granger causality test results shows that there is a unidirectional causality running from energy use to both carbon dioxide emissions and industrialization. According to the artificial neural network results, the most important effect on energy use comes from GDP. The predicted energy use from 1968 to 2013 has maximum absolute error of % 11. 31 and minimum absolute error of %0.07. Neural network evidence shows that the R-square coefficient is 98% for the sample period.
C10|Polish Sugar Sector After Abolishing Sugar Production Quotas|The sugar market was one of the most regulated markets in the agri-food sector in the EU, The basis of said regulation was an administrative restriction of supply (production quotas), protectionist foreign trade policies, a minimum procurement price for sugar beet and the reference price of sugar. The sugar sector in Poland has a long tradition and is of great economic social and environmental significance, as well as an important element of the food security policy. Sugar beets are characterized by the greatest productivity per area unit, and by-products are used as fodder or for energy production purposes. The sugar industry is a strategic part of the food economy. Sugar remains the main sweetener, despite the development of the starch syrup and low-calorific sweetener markets. The social importance of the sector results from the fact that sugar beet production remains the source of income for planters, and the sugar industry and numerous service providers create both national income and jobs. Sugar beet cultivations are a crucial element of the sustainable development of agriculture as they increase biodiversity and maintain agricultural land in good condition.Abolishing production quotas will result in big changes in the sugar sector, which in turn will have multiple economic, social and environmental effects. Accounting for international conditions, including competition from cane sugar and obligations resulting from trade contracts, it is assessed that the sugar balance may undergo significant changes. Under the changing external conditions, the foreign trade policies and international economic relations will have a very significant impact on the situation of the domestic sugar sector. Situational development in the sugar sector should include two basic elements: changes in the EU market after the market regulations reform and changes in the economic situation on the international market, and foreign trade conditions. The significant economic social and environmental importance indicates that future policy with regards to the sector should include solutions which will allow to maintain sugar beet cultivation and sugar production at least in the most effective and competitive regions of the country.
C10|The Determinant Of Success In Basic Economics Courses Taught By Department Of Economics At The University Of Economics In Prague|In comparison with other basic courses at the University of Economics in Prague there are basic economics courses taught by the Department of Economics that regularly show higher fail rates, specifically more than 35 %. A standard evaluation should be done in time and shall use different methods. Using quantitative methods, our analysis tries to identify key determinants of students? success. The data were obtained via a questionnaire during the last lecture in WS 2016/17. As opposed to existing studies, we also consider variables that weren?t previously possible to observe, such as the use of IT technologies during lectures and studying from materials in electronic form.
C10|A microeconometric analysis of climate change drivers for coffee crops transition to cacao in Mesoamerican countries|Climate change will have a permanent impact over Mesoamerican agricultural sector. Present day crops such as coffee may not be enough to secure agricultural subsistence levels, therefore, the first stages of crop diversification are being observed in countries such as Nicaragua. Implementation of new crops such as cocoa may lead to new impacts over the environmental structure of the Mesoamerican ecosystem. These impacts may be of different, nature, but being diversification an already undergoing process attention must be paid to the underlying motivation and decision-making processes involved. This study analyses subjacent motivations and contexts that lead to the potential incorporation of cocoa crops in present-day Nicaraguan coffee farms. In order to achieve that, three main motivations were identified: climatic, economic and governmental. An econometric analyse was performed over the variables that affect farmers? motivations and decisions, in order first to analyse this decision-making process, and second, to understand how social and climatic evolution over the next decades will impact the context under which agricultural output is shaped. It was found that climatic perspectives are most closely affecting the smallholders? decision of incorporating cocoa plantations into their farms. Therefore, climate change will most certainly have a major role in the reshaping of agricultural structure in most of Nicaraguan geography. Moreover, results show a lower impact of market conditions and public subsidies over farmers? choices and decisions. These results favour the intuition that risk-reduction is a preferred strategy among Nicaraguan smallholders.
C10|Currency demand and MIMIC models: towards a structured hybrid method of measuring the shadow economy|Abstract Model-based econometric techniques of the shadow economy estimation have been increasingly popular, but a systematic approach to getting the best of their complementarities has so far been missing. We review the dominant approaches in the literature—currency demand analysis and MIMIC model—and propose a hybrid procedure that addresses their previous critique, in particular the misspecification issues in CDA equations and the vague transformation of the latent variable obtained via MIMIC model into interpretable levels and paths of the shadow economy. We propose a new identification scheme for the MIMIC model, referred to as ‘reverse standarization’. It supplies the MIMIC model with the panel-structured information on the latent variable’s mean and variance obtained from the CDA estimates, treating this information as given in the restricted full information maximum likelihood function. This approach allows avoiding some controversial steps, such as choosing an externally estimated reference point for benchmarking or adopting other ad hoc identifying assumptions. We estimate the shadow economy for up to 43 countries, with the results obtained in the range of 2.8–29.9% of GDP. Various versions of our models remain robust as regards changes in the level of the shadow economy over time and the relative position of the analysed countries. We also find that the contribution of (a correctly specified) MIMIC model to the measurement of trends in the shadow economy is marginal as compared to the contribution of the CDA model, confirming the scepticism of some previous literature towards this method.
C10|Jump-robust estimation of volatility with simultaneous presence of microstructure noise and multiple observations|Abstract In this paper, we develop the multipower estimators for the integrated volatility in (Barndorff-Nielsen and Shephard in J. Financ. Econom. 2:1–37, 2004); these estimators allow the presence of jumps in the underlying driving process and the simultaneous presence of microstructure noise and multiple records of observations. By multiple records we mean more than one observation recorded on a single time stamp, as often seen in stock markets, in particular, for heavily traded securities, for a data set with even millisecond frequency. We establish the consistency and asymptotic normality of the estimators for both noise-free and noise-present cases. Simulation studies confirm our theoretical results. We apply the estimators to a real high-frequency data set.
C10|Endogenous environmental variables in stochastic frontier models|This paper considers a stochastic frontier model that contains environmental variables that affect the level of inefficiency but not the frontier. The model contains statistical noise, potentially endogenous regressors, and technical inefficiency that follows the scaling property, in the sense that it is the product of a basic (half-normal) inefficiency term and a parametric function of the environmental variables. The environmental variables may be endogenous because they are correlated with the statistical noise or with the basic inefficiency term.
C10|Sequential Probability Ration Tests : Conservative and Robust|In practice, most computers generate simulation outputs sequentially, so it is attractive to analyze these outputs through sequential statistical methods such as sequential probability ratio tests (SPRTs). We investigate several SPRTs for choosing between two hypothesized values for the mean output (response). One SPRT is published in Wald (1945), and allows general distribution types. For a normal (Gaussian) distribution this SPRT assumes a known variance, but in our modified SPRT we estimate the variance. Another SPRT is published in Hall (1962), and assumes a normal distribution with an unknown variance estimated from a pilot sample. We also investigate a modification, replacing this pilot-sample estimator by a fully sequential estimator. We present a sequence of Monte Carlo experiments for quantifying the performance of these SPRTs. In experiment #1 the simulation outputs are normal. This experiment suggests that Wald (1945)’s SPRT with estimated variance gives significantly high error rates. Hall (1962)’s original and modified SPRTs are conservative; i.e., the actual error rates are much smaller than the prespecified (nominal) rates. The most efficient SPRT is our modified Hall (1962) SPRT. In experiment #2 we examine the robustness of the various SPRTs in case of nonnormal output. If we know that the output has a specific nonnormal distribution such as the exponential distribution, then we may also apply Wald (1945)’s original SPRT. Throughout our investigation we pay special attention to the design and analysis of these experiments.
C10|Do Remittances Promote Labor Productivity Growth in Mexico? An Empirical Analysis, 1970-2014|This paper investigates remittance flows to Mexico during the 1980-2014 period in absolute terms, relative to GDP, in comparison to FDI inflows, and in terms of their regional destination. Next, the paper reviews the growing literature that assesses the impact of remittances on investment spending and economic growth. Third, it presents a simple endogenous growth model that explicitly incorporates the potential impact of remittance flows on economic and labor productivity growth. Fourth, it presents a modified empirical counterpart to the simple model that tests for both single- and two-break unit root tests, as well as performs cointegration tests with an endogenously determined level shift over the 1970-2014 period. The error-correction model estimates suggest that remittance flows to Mexico have a positive and significant effect, albeit small, on both economic growth and labor productivity growth. The concluding section summarizes the major results and discusses potential avenues for future research on this important topic
C10|Set Identification, Moment Restrictions, and Inference|For the past 10 years, the topic of set identification has been much studied in the econometric literature. Classical inference methods have been generalized to the case in which moment inequalities and equalities define a set instead of a point. We review several instances of partial identification by focusing on examples in which the underlying economic restrictions are expressed as linear moments. This setting illustrates the fact that convex analysis helps not only for characterizing the identified set but also for inference. From this perspective, we review inference methods using convex analysis or inversion of tests and detail how geometric characterizations can be useful.
C10|Using compositional and Dirichlet models for market share regression| When the aim is to model market shares, the marketing literature proposes some regression models which can be qualified as attraction models. They are generally derived from an aggregated version of the multinomial logit model. But aggregated multinomial logit models (MNL) and the so-called generalized multiplicative competitive interaction models (GMCI) present some limitations: in their simpler version they do not specify brand-specific and cross effect parameters. In this paper, we consider alternative models: the Dirichlet model (DIR) and the compositional model (CODA). DIR allows to introduce brand-specific parameters and CODA allows additionally to consider cross effect parameters. We show that these two models can be written in a similar fashion, called attraction form, as the MNL and the GMCI models. As market share models are usually interpreted in terms of elasticities, we also use this notion to interpret the DIR and CODA models. We compare the properties of the models in order to explain why CODA and DIR models can outperform traditional market share models. An application to the automobile market is presented where we model brands market shares as a function of media investments, controlling for the brands price and scrapping incentive. We compare the quality of the models using measures adapted to shares.
C10|Interpreting the impact of explanatory variables in compositional models|Regression models have been developed for the case where the dependent variable is a vector of shares. Some of them, from the marketing literature, are easy to interpret but they are quite simple and can only be complexified at the expense of a very large number of parameters to estimate. Other models, from the mathematical literature, are called compositional regression models and are based on the simplicial geometry (a vector of shares is called a composition, shares are components, and a composition lies in the simplex). These models are transformation models: they use a log-ratio transformation of shares. They are very flexible in terms of explanatory variables and complexity (component-specific and cross-effect parameters), but their interpretation is not straightforward, due to the fact that shares add up to one. This paper combines both literatures in order to obtain a performing market-share model allowing to get relevant and appropriate interpretations, which can be used for decision making in practical cases. For example, we are interested in modeling the impact of media investments on automobile manufacturers sales. In order to take into account the competition, we model the brands market-shares as a function of (relative) media investments. We furthermore focus on compositional models where some explanatory variables are also compositional. Two specifications are possible: in Model A, a unique coefficient is associated to each compositional explanatory variable, whereas in Model B a compositional explanatory variable is associated to component-specific and cross-effect coefficients. Model A and Model B are estimated for our application in the B segment of the French automobile market, from 2003 to 2015. In order to enhance the interpretability of these models, we present different types of impact assessment measures (marginal effects, elasticities and odds ratios) and we show that elasticities are particularly useful to isolate the impact of an explanatory variable on a particular share. We show that elasticities can be equivalently computed from the transformed model and from the model in the simplex and that they are linked to directional C-derivatives of simplex-valued function of a simplex variable. Direct and cross effects of media investments are computed for both models. Model B shows interesting non-symmetric synergies between brands, and Renault seems to be the most elastic brand to its own media investments. In order to determine if component-specific and cross-effect parameters are needed to improve the quality of the model (Model B) or if a global parameter is reasonable (Model A), we compare the goodness-of-fit of the two models using (out-of-sample) quality measures adapted for share data.
C10|Human capital accumulation in France at the dawn of the XIXth century: Lessons from the Guizot Inquiry|Building on the results of the Guizot Inquiry, carried out in autumn 1833 on the initiative of François Guizot, the minister of public instruction, this article examines the process of human capital accumulation in early nineteenth-century France. We rely on an original proxy for human capital – student achievement – to highlight the high level of heterogeneity in human capital accumulation in this period. We identify two types of schools in the French educational landscape: first, large schools, well-endowed in human and material resources, which contributed a great deal to human capital accumulation; second, small schools, characterised by some degree of amateurism and improvisation, which weakly contributed to human capital formation. We note that the use of literacy rates or school enrollment rates can be misleading with regard to the estimation of French human capital endowments, laying emphasis instead on the heterogeneity in the French educational landscape at the dawn of the nineteenth century, as the country embarked on the process of industrialisation.<br><small>(This abstract was borrowed from another version of this item.)</small>
C10|Which tests not witch hunts: A diagnostic approach for conducting replication research|Replication research can be used to explore original study results that researchers consider questionable, but it should also be a tool for reinforcing the credibility of results that are important to policies and programs. The challenge is to design a replication plan open to both supporting the original findings and uncovering potential problems. The purpose of this paper is to provide replication researchers with an objective list of checks or tests to consider when planning a replication study. The authors present tips for diagnostic replication exercises in four groups: validity of assumptions, data transformations, estimation methods, and heterogeneous impacts. For each group, the authors present an introduction to the issues, a list of replication tests and checks, some examples of how these checks are employed in replication studies of development impact evaluations, and a set of resources that provide statistical and econometric details. The authors also provide a list of don'ts for how to conduct and report replication research.
C10|You can't always get what you want? A Monte Carlo analysis of the bias and the efficiency of dynamic panel data estimators|We assess the bias and the efficiency of state-of-the-art dynamic panel data estimators by means of model-based Monte Carlo simulations. The underlying data-generating process consists of a standard theoretical growth model of income convergence based on capital accumulation. While we impose a true underlying speed of convergence of around 5% in our simulated data, the results obtained with the different panel data estimators range from 0.03% to 17%. This implies a range of the half life of a given income gap from 4 years up to several hundred years. In terms of the squared percent error, the pooled OLS, fixed effects, random effects, and difference GMM estimators perform worst, while the system GMM estimator with the full matrix of instruments and the corrected least squares dummy variable (LSDVC) estimator perform best relative to the other methods under consideration. The LSDVC estimator, initialized by the system GMM estimator with the full matrix of instruments, is the only one capturing the true speed of convergence within the 95% confidence interval for all scenarios. All other estimators yield point estimates that are substantially different from the true values and confidence intervals that do not include the true value in most scenarios.
C10|Une nouvelle approche expérimentale pour tester les modèles quantiques de l’erreur de conjonction|In classical probability theory, the probability of the conjunction of two events is smaller than the probability of only one of these events. Yet, agents do not always empirically judge in this way: this is the traditional conjunction fallacy. One of the currently promising accounts of this paradox relies on so-called quantum-like models, which have been developed from mathematical tools used in quantum theory. But are these models empirically adequate? Which versions of these models can be used? In particular, can the simplest versions, the non-degenerate ones, be sufficient? We propose here an original experimental protocol to test the quantum-like models for the conjunction fallacy in the lab. The results we obtain suggest that the non-degenerate models are not empirically adequate, and that future research on quantum-like models should consider degenerate ones. Classification JEL : C60, C91, D03.
C10|The future development of world records|We conduct an innovative analysis of sporting world records by a) using economic instead of sporting determinants and b) by using multivariate stochastic frontier functions. Using data from 48 different disciplines between 1970 and 2014, we show that world records are close to full efficiency and therefore actual athletic frontiers. Forecasts including economic determinants imply that the dynamics of world records largely depend on the dynamics of the frontiers and their driving forces, i.e., socio-economic developments.
C10|On some properties of elliptical distributions|We look at a characterization of elliptical distributions in the case when finiteness of moments of the random vector is not assumed. Some additional results regarding elliptical distributions are also presented.
C10|Creaming - and the depletion of resources: A Bayesian data analysis|This paper considers sampling in proportion to size from a partly unknown distribution. The applied context is the exploration for undiscovered resources, like oil accumulations in different deposits, where the most promising deposits are likely to be drilled first, based on some geologic size indicators (“creaming”). A Log-normal size model with exponentially decaying creaming factor turns out to have nice analytical features in this context, and fits well available data, as demonstrated in Lillestøl and Sinding-Larsen (2017). This paper is a Bayesian follow-up, which provides posterior parameter densities and predictive densities of future discoveries, in the case of uninformative prior distributions. The theory is applied to the prediction of remaining petroleum accumulations to be found on the mature part of the Norwegian Continental Shelf.
C10|Higher order moments of the estimated tangency portfolio weights|In this paper we consider the estimated weights of tangency portfolio. The returns are assumed to be independently and multivariate normally distributed. We derive analytical expressions for the higher order non-central and central moments of these weights. Moreover, the expressions for mean, variance, skewness and kurtosis of the estimated weights are obtained in closed-forms. Finally, we complement our result with an empirical study where we analyze a portfolio with actual returns of eight nancial indexes listed in NASDAQ stock exchange.
C10|Confidence Intervals for Projections of Partially Identified Parameters|We propose a bootstrap‐based calibrated projection procedure to build confidence intervals for single components and for smooth functions of a partially identified parameter vector in moment (in)equality models. The method controls asymptotic coverage uniformly over a large class of data generating processes. The extreme points of the calibrated projection confidence interval are obtained by extremizing the value of the function of interest subject to a proper relaxation of studentized sample analogs of the moment (in)equality conditions. The degree of relaxation, or critical level, is calibrated so that the function of θ, not θ itself, is uniformly asymptotically covered with prespecified probability. This calibration is based on repeatedly checking feasibility of linear programming problems, rendering it computationally attractive. Nonetheless, the program defining an extreme point of the confidence interval is generally nonlinear and potentially intricate. We provide an algorithm, based on the response surface method for global optimization, that approximates the solution rapidly and accurately, and we establish its rate of convergence. The algorithm is of independent interest for optimization problems with simple objectives and complicated constraints. An empirical application estimating an entry game illustrates the usefulness of the method. Monte Carlo simulations confirm the accuracy of the solution algorithm, the good statistical as well as computational performance of calibrated projection (including in comparison to other methods), and the algorithm's potential to greatly accelerate computation of other confidence intervals.
C10|What happens when econometrics and psychometrics collide? An example using the PISA data|International large-scale assessments such as PISA are increasingly being used to benchmark the academic performance of young people across the world. Yet many of the technicalities underpinning these datasets are misunderstood by applied researchers, who sometimes fail to take their complex sample and test designs into account. The aim of this paper is to generate a better understanding among economists about how such databases are created, and what this implies for the empirical methodologies one should (or should not) apply. We explain how some of the modeling strategies preferred by economists seem to be at odds with the complex test design, and provide clear advice on the types of robustness tests that are therefore needed when analyzing these datasets. In doing so, we hope to generate a better understanding of international large-scale education databases, and promote better practice in their use.
C10|Finance growth nexus across Indian states: evidences from panel cointegration and causality tests|Abstract The paper deals with finance-growth relationship across Indian states over 1980–2011 in panel cointegration and causality framework. We apply Engle–Granger two-step procedure for cointegration test in panel setting which takes care of cross-sectional dependence and heterogeneity across states. For panel Granger causality analysis, we employ Dumitrescu and Hurlin (Econ Model 29:1450–1460, 2012) method and apply bootstrapping to account for cross-sectional dependence. We find robust evidence of cointegration between per capita income and credit per capita. Using panel FMOLS, we find that 1 % change in credit per capita results in 0.14 % change in per capita income. Panel Granger causality test reveals that there is bi-directional causality (feedback effects) in the absence of cross-sectional dependence. However, with cross-sectional dependence, we find evidence in favour of supply leading hypothesis. Probable policy implication calls for inclusive financial development and growth strategies in order to mitigate uneven income levels across states.
C10|Rolling Regression Analysis of the Pástor-Stambaugh Model: Evidence from Robust Instrumental Variables|Abstract The capital asset pricing model (CAPM), Fama-French (FF), and Pástor-Stambaugh (PS) factor models are examined using a new dynamic rolling regression version of the generalized method of moments (GMM) method. This rolling regression framework not only allows us to investigate phases of the business cycle, but also permits regression estimates to vary through time due to changes in the development and efficiency of the sectors. The principal reasons for using the dynamic GMM with robust instruments is that some of these factors are measured with errors and the disturbances may be non-spherical. The CAPM appears as the most parsimonious model to explain the FF sector returns. Furthermore, the rolling GMM approach is clearly more sensitive to dynamic financial episodes than the ordinary least squares approach. In particular, liquidity has some anticipatory power, as it is able to forecast the 2007–2009 crises with heightened volatility starting in late 2005.
C10|The European Railway Sectors: Understanding and Assessing Change|This paper aims at explaining and assessing the process of change that European railway sectors have experienced over the last decades. It has been organized in two parts: the first part (section 2) explains change and the second part (section 3) empirically assesses change. In the first part we argue that the roots of this change process date back to the 1950s when, because of competitiveness reasons, the market share of most European railways began to decrease compared to other transport modes. Since national reformers ascribed these performance shortcomings to the traditional organization of the railway sector which was that of vertically-integrated state- owned monopoly. Therefore, between the 1970s and 1980s many governments in Europe adopted reforms that tried to inject competition in national railway sectors by supporting liberalization policies. Then, since early-1990s the European Commission has released a number of regulatory instruments aimed at pursuing three main goals: 1) liberalization of the rail services; 2) unbundling of infrastructure and service level, and 3) creation of the common transport market. The empirical analysis conducted in the second part of this paper suggests that state-controlled structures still play a crucial role in the management and performance of national railway industries as lower service prices are associated with greater presence of the state in the sector. Results show that liberalization-friendly regulation does not necessarily benefit railway service consumers since increased levels of market access, unbundling and privatization are not necessarily associated with lower prices. Our analysis shows that the long-lasting transition from state-owned monopolies to competitive market arrangements in the railway sectors provided mitigated results. We conclude this paper by launching a reflection on the future developments of railways in Europe.
C10|The Relationship Between Trade Openness and Economic Growth: The Case of Ghana and Nigeria|This study purposed to determine the long run relationship between trade openness and economic growth in Ghana and Nigeria covering the period between 1980 and 2016. It incorporated investment, exchange rates and inflation as the additional variables. To test for stationarity of the data, the augments Dickey-Fuller (ADF) (Dickey and Fuller, 1981), the Phillips and Perron (1988) and the DF-GLS test proposed by Elliot, Rothenberg and Stock (1996) were used. The Autoregressive distributed lag (ARDL) model was employed in this study to examine the long run relationship between the variables. The findings of the study suggested existence of a long run relationship among the variables for both countries. The results further showed that trade openness has a positive impact on economic growth and significant at the 1% level in Ghana while in Nigeria trade openness has a negative but insignificant effect on economic growth. These results imply that different policy measures should be put into place for each of these two countries.
C10|Does Efficiency Trump Legality? The Case of the German Constitutional Court|The US Supreme Court has the power of certiorari. It may pick its fights. As a beneficial side effect, the court may allocate its resources, in particular the time and energy the justices spend on a case, to worthy causes. In economic parlance, this discretion makes the court more efficient. Efficiency comes at a political cost, though. This discretion also gives the court political power. It may direct its verdict to causes that are politically most relevant, or it may put an issue on the political agenda. Officially German constitutional law does not have certiorari. The Constitutional Court must decide each and every case that is brought. Yet over time the court has crafted a whole arsenal of more subtle measures for managing the case load. This paper shows that it uses these tools to engage in its version of allocating resources to cases. It investigates whether the ensuing efficiency gain comes at the cost of biasing the court’s jurisprudence. The paper exploits a new comprehensive data set. It consists of all (mostly only electronically) published cases the court has heard in 2011. While the data is rich, in many technical ways it is demanding. The paper uses a factor analysis to create a latent variable: to which degree has the court taken an individual case seriously? It then investigates whether observed indicators for bias explain this latent variable. Since the paper essentially investigates a single (independent) case, in statistical terms the findings are to be interpreted with caution. The paper can only aim at finding smoking guns.
C10|Statistical Matching for Combining Time-Use Surveys with Consumer Expenditure Surveys: An Evaluation on Real Data|Performing a statistical match to combine two surveys made over the same population by traditional methods is shown to give biased estimates and variance of the imputed values. A method proposed by Rubin (1986) allows imputing an unobserved variable using observations in another dataset by taking into account the partial correlation between the variables that are jointly unobserved for any unit. We use a dataset where households report their expenditures and time-uses to show that fusioning expenditure and time-use surveys by Rubin's procedure allows to recover the true distribution of the missing variables and to yield minimally biased estimates
C10|Multivariate Reflection Symmetry of Copula Functions|We propose a multivariate nonparametric copula test of reflection symmetry. The test is valid in any number of dimensions, extending previous results that cover the bivariate case. Furthermore, the asymptotic theory for the test relies on recent results on the dependent multiplier bootstrap, valid for sub-exponentially strongly mixing data. Consequently to the introduction of those two features, the procedure is suitable for financial time series whose asymmetric dependence, in distressed periods, has already been documented elsewhere. We conduct an extensive simulation study of empirical size and power and provide several examples of applications. In particular, we investigate the use of the statistic as a financial stress indicator by comparing it with the CISS, the leading ECB indicator.
C10|Mis-classified, Binary, Endogenous Regressors: Identification and Inference|This paper studies identification and inference for the effect of a mis-classified, binary, endogenous regressor when a discrete-valued instrumental variable is available. We begin by showing that the only existing point identification result for this model is incorrect. We go on to derive the sharp identified set under mean independence assumptions for the instrument and measurement error, and that these fail to point identify the effect of interest. This motivates us to consider alternative and slightly stronger assumptions: we show that adding second and third moment independence assumptions suffices to identify the model. We then turn our attention to inference. We show that both our model, and related models from the literature that assume regressor exogeneity, suffer from weak identification when the effect of interest is small. To address this difficulty, we exploit the inequality restrictions that emerge from our derivation of the sharp identified set under mean independence only. These restrictions remain informative irrespective of the strength of identification. Combining these with the moment equalities that emerge from our identification result, we propose a robust inference procedure using tools from the moment inequality literature. Our method performs well in simulations.
C10|How Well Do Structural Demand Models Work? Counterfactual Predictions in School Choice|Discrete choice demand models are widely used for counterfactual policy simulations, yet their out-of-sample performance is rarely assessed. This paper uses a large-scale policy change in Boston to investigate the performance of discrete choice models of school demand. In 2013, Boston Public Schools considered several new choice plans that differ in where applicants can apply. At the request of the mayor and district, we forecast the alternatives' effects by estimating discrete choice models. This work led to the adoption of a plan which significantly altered choice sets for thousands of applicants. Pathak and Shi (2014) update forecasts prior to the policy change and describe prediction targets involving access, travel, and unassigned students. Here, we assess how well these ex ante counterfactual predictions compare to actual outcome under the new choice sets. We find that a simple ad hoc model performs as well as the more complicated structural choice models for one of the two grades we examine. However, the structural models' inconsistent performance is largely due to prediction errors in applicant characteristics, which are auxiliary inputs. Once we condition on the actual applicant characteristics, the structural choice models outperform the ad hoc alternative in predicting both choice patterns and policy relevant outcomes. Moreover, refitting the models using the new choice data does not significantly improve their prediction accuracy, suggesting that the choice models are indeed “structural.” Our findings show that structural demand models can effectively predict counterfactual outcomes, as long there are accurate forecasts about auxiliary input variables.
C10|Human capital accumulation in France at the dawn of the XIXth century: Lessons from the Guizot Inquiry|Building on the results of the Guizot Inquiry, carried out in autumn 1833 on the initiative of François Guizot, the minister of public instruction, this article examines the process of human capital accumulation in early nineteenth-century France. We rely on an original proxy for human capital – student achievement – to highlight the high level of heterogeneity in human capital accumulation in this period. We identify two types of schools in the French educational landscape: first, large schools, well-endowed in human and material resources, which contributed a great deal to human capital accumulation; second, small schools, characterised by some degree of amateurism and improvisation, which weakly contributed to human capital formation. We note that the use of literacy rates or school enrollment rates can be misleading with regard to the estimation of French human capital endowments, laying emphasis instead on the heterogeneity in the French educational landscape at the dawn of the nineteenth century, as the country embarked on the process of industrialisation.
C10|Burden of Climate Change on Malaria Mortality|In 2015, an estimated 429,000 deaths and 212 million cases of malaria occurred worldwide, while 70% of the deaths occurred in children under five years old. Changes in climatic exposure such as temperature and precipitation makes malaria one of the most climate sensitive outcomes. Using a global malaria mortality dataset for 105 countries between 1980 and 2010, we estimate that the global optimal temperature maximizing all-age malaria mortality is 20.6, lower than previously predicted in the literature. While in the case of child mortality, a significantly lower optimum temperature of 19.3° is estimated. Our results also suggest that in Africa and Asia, the continents where malaria is most prevalent malaria, mortality is maximized at 28.4 and 26.3, respectively. Furthermore, we estimate that child mortality (ages 0-4) is likely to increase by up to 20 percent in some areas due to climate change by the end of the 21st century.
C10|Meta-Analysis of Multiple Treatments Experiments| Limited theoretical and empirical work have been conducted to synthetize multiple treatment experiments commonly used in agricultural field sciences. The main objective of this study was to extend the current meta-analysis literature by developing a flexible econometric model to evaluate multiple treatment data. The proposed method is based on random effects meta-regression and mixed effects models. The model is illustrated on a publicly available series of nitrogen field experiments on corn production in the upper Midwestern United States. Stochastic plateau theory integrated with econometric meta-analysis methodology was used to determine the agronomic optimal nitrogen application rate and shape of the functional form of corresponding yield potential.
C10|Decision structure of risky choice|As we know, there is a controversy about the decision making under risk between economists and psychologists. We discuss to build a unified theory of risky choice, which would explain both of compensatory and non-compensatory theories. For risky choice, according to cognition ability, we argue that people could not build a continuous and accurate subjective probability world, but several order concepts, such as small, middle and large probability. People make decisions based on information, experience, imagination and other things. All of these things are so huge that people have to prepare some strategies. That is, people have different strategies when facing to different situations. The distributions of these things have different decision structures. More precisely, decision making is a process of simplifying the decision structure. However, the process of decision structure simplifying is not stuck in a rut, but through different path when facing problems repeatedly. It is why preference reversal always happens when making decisions. The most efficient way to simplify the decision structure is calculating expected value or making decisions based on one or two dimensions. We also argue that the deliberation time at least has four parts, which are consist of substitution time, first order time, second order time and calculation time. Decision structure also can simply explain the phenomenon of paradoxes and anomalies. JEL Codes: C10, D03, D81
C10|Analysis on the Determinants of Exit of Self-Employed Businesses in Korea (in Korean)|"The proportion of self-employed and unpaid family workers in Korea is 25.9% as of 2015, which is in a decreasing trend but still very high compared to other OECD countries. In addition, there exist a lot of concerns over a high shutdown rate of self-employed businesses while only a few studies have dealt with their survival. Therefore, we empirically analyze the determinants of exit of self-employed businesses in Korea, using panel data on wholesale & retail trade, restaurant & lodging, repair and other individual service industries from ""Census on Establishments"" provided by the Statistics Korea. With Cox's proportional hazards model, we estimate the effects of various demand, cost, and competition factors on the exit rate of businesses in major industries of self-employment. According to estimation results, cost factors: rents, lending interest rates, and fixed personnel expenses as well as demand factors: consumer price index (CPI) and gross regional domestic product (GRDP) have considerable effects on the shutdown rate of self-employed businesses. The number of competitors and business-specific characteristics such as the age and size of business are also estimated to have significant effect."
C10|A Primer on the “Reproducibility Crisis” and Ways to Fix It|This article uses the framework of Ioannidis (2005) to organize a discussion of issues related to the “reproducibility crisis.” It then goes on to use that framework to evaluate various proposals to fix the problem. Of particular interest is the “post-study probability”, the probability that a reported research finding represents a true relationship. This probability is inherently unknowable. However, a number of insightful results emerge if we are willing to make some conjectures about reasonable parameter values. Among other things, this analysis demonstrates the important role that replication can play in improving the signal value of empirical research.
C10|Identification and estimation of heterogeneous agent models: A likelihood approach|In this paper, we study the statistical properties of heterogeneous agent models with incomplete markets. Using a Bewley-Hugget-Aiyagari model we compute the equilibrium density function of wealth and show how it can be used for likelihood inference. We investigate the identifiability of the model parameters based on data representing a large cross-section of individual wealth. We also study the finite sample properties of the maximum likelihood estimator using Monte Carlo experiments. Our results suggest that while the parameters related to the household's preferences can be correctly identified and accurately estimated, the parameters associated with the supply side of the economy cannot be separately identified leading to inferential problems that persist even in large samples. In the presence of partially identification problems, we show that an empirical strategy based on fixing the value of one of the troublesome parameters allows us to pin down the other unidentified parameter without compromising the estimation of the remaining parameters of the model. An empirical illustration of our maximum likelihood framework using the 2013 SCF data for the U.S. confirms the results from our identification experiments.
C10|Assessing Cumulative Net Nutrition and the Transition from 19th Century Bound to Free-Labor by Ethnic Status|Average stature reflects cumulative net nutrition and health during economic development. This study introduces a difference-in-decompositions approach to show that although 19th century African-American cumulative net nutrition was comparable to working class whites, it was made worse-off with the transition to free-labor. Average stature reflects net nutrition over the life-course, and slave children’s BMIs increased more with age than whites as they approached entry into the adult slave labor force. Agricultural worker’s net nutrition was better than workers in other occupations but was worse-off under free-labor and industrialization. Within-group stature variation was greater than across-group variation, and white within-group stature variation associated with socioeconomic status was greater than African-Americans.
C10|Structural breaks in panel data: Large number of panels and short length time series| The detection of (structural) breaks or the so called change point problem has drawn increasing attention from the theoretical, applied economic and financial fields. Much of the existing research concentrates on the detection of change points and asymptotic properties of their estimators in panels when N, the number of panels, as well as T, the number of observations in each panel are large. In this paper we pursue a different approach, i.e., we consider the asymptotic properties when N→∞ while keeping T fixed. This situation is typically related to large (firm-level) data containing financial information about an immense number of firms/stocks across a limited number of years/quarters/months. We propose a general approach for testing for break(s) in this setup. In particular, we obtain the asymptotic behavior of test statistics. We also propose a wild bootstrap procedure that could be used to generate the critical values of the test statistics. The theoretical approach is supplemented by numerous simulations and by an empirical illustration. We demonstrate that the testing procedure works well in the framework of the four factors CAPM model. In particular, we estimate the breaks in the monthly returns of US mutual funds during the period January 2006 to February 2010 which covers the subprime crises.
C10|Identification And Inference On Regressions With Missing Covariate Data|This paper examines the problem of identification and inference on a conditional moment condition model with missing data, with special focus on the case when the conditioning covariates are missing. We impose no assumption on the distribution of the missing data and we confront the missing data problem by using a worst case scenario approach. We characterize the sharp identified set and argue that this set is usually too complex to compute or to use for inference. Given this difficulty, we consider the construction of outer identified sets (i.e. supersets of the identified set) that are easier to compute and can still characterize the parameter of interest. Two different outer identification strategies are proposed. Both of these strategies are shown to have non-trivial identifying power and are relatively easy to use and combine for inferential purposes.<br><small>(This abstract was borrowed from another version of this item.)</small>
C10|On the Choice of Test Statistic for Conditional Moment Inequalities|This paper derives asymptotic approximations to the power of Cramer-von Mises (CvM) style tests for inference on a finite dimensional parameter defined by conditional moment inequalities in the case where the parameter is set identified. Combined with power results for Kolmogorov- Smirnov (KS) tests, these results can be used to choose the optimal test statistic, weighting function and, for tests based on kernel estimates, kernel bandwidth. The results show that, in the setting considered here, KS tests are preferred to CvM tests, and that a truncated variance weighting is preferred to bounded weightings.This paper derives asymptotic approximations to the power of Cramer-von Mises (CvM) style tests for inference on a finite dimensional parameter defined by conditional moment inequalities in the case where the parameter is set identified. Combined with power results for Kolmogorov-Smirnov (KS) tests, these results can be used to choose the optimal test statistic, weighting function and, for tests based on kernel estimates, kernel bandwidth. The results show that, in the setting considered here, KS tests are preferred to CvM tests, and that a truncated variance weighting is preferred to bounded weightings.
C10|Econometric Measurement of Earth's Transient Climate Sensitivity|How sensitive is Earth’s climate to a given increase in atmospheric greenhouse gas (GHG) concentrations? This long-standing and fundamental question in climate science was recently analyzed by dynamic panel data methods using extensive spatiotemporal data of global surface temperatures, solar radiation, and GHG concentrations over the last half century to 2010 (Storelvmo et al, 2016). These methods revealed that atmospheric aerosol effects masked approximately one-third of the continental warming due to increasing GHG concentrations over this period, thereby implying greater climate sensitivity to GHGs than previously thought. The present study provides asymptotic theory justifying the use of these methods when there are stochastic process trends in both the global forcing variables, such as GHGs, and station-level trend effects from such sources as local aerosol pollutants. These asymptotics validate con dence interval construction for econometric measures of Earth’s transient climate sensitivity. The methods are applied to observational data and to data generated from three leading global climate models (GCMs) that are sampled spatio-temporally in the same way as the empirical observations. The fi ndings indicate that estimates of transient climate sensitivity produced by these GCMs lie within empirically determined con dence limits but that the GCMs uniformly underestimate the effects of aerosol induced dimming. The analysis shows the potential of econometric methods to calibrate GCM performance against observational data and to reveal the respective sensitivity parameters (GHG and non-GHG related) governing GCM temperature trends.
C10|Identification-Robust Subvector Inference|This paper introduces identification-robust subvector tests and confidence sets (CS’s) that have asymptotic size equal to their nominal size and are asymptotically efficient under strong identification. Hence, inference is as good asymptotically as standard methods under standard regularity conditions, but also is identification robust. The results do not require special structure on the models under consideration, or strong identification of the nuisance parameters, as many existing methods do. We provide general results under high-level conditions that can be applied to moment condition, likelihood, and minimum distance models, among others. We verify these conditions under primitive conditions for moment condition models. In another paper, we do so for likelihood models. The results build on the approach of Chaudhuri and Zivot (2011), who introduce a C(a)-type Lagrange multiplier test and employ it in a Bonferroni subvector test. Here we consider two-step tests and CS’s that employ a C(a)-type test in the second step. The two-step tests are closely related to Bonferroni tests, but are not asymptotically conservative and achieve asymptotic efficiency under strong identification
C10|Maximum Entropy Estimation of Statistical Equilibrium in Economic Quantal Response Models|Many problems in empirical economic analysis involve systems in which the quantal actions of a large number of participants determine the distribution of some social outcome. In many of these cases key model variables are un- observed. From the statistical perspective, when observed variables depend non-trivially on unobserved variables the joint distribution of the variables of interest is underdetermined and the model is ill-posed due to incomplete information. In this paper we examine the class of models de ned by a joint distribution of discrete individual actions and an outcome variable, where one of the variables is unobserved, so that the joint distribution is underdetermined. We derive a general maximum entropy based method to infer the underdetermined joint distribution in this class of models. We apply this method to the classical Smithian theory of competition where firms' profit rates are observed but the entry and exit decisions that determine the distribution of profit rates is unobserved.
C10|Survey of Volatility and Spillovers on Financial Markets|In this survey article, we present a rich extent of literature on volatility and its propagation on financial markets via spillovers. We document how new approaches or improved existing methodologies lead to results that offer richer insights than those derived from standard econometric techniques. Moreover, the implications of the results can be related to a wide set of markets as the surveyed articles cover emerging and developed European markets as well as the United States.
C10|Technological effectiveness of urban transport in selected Polish cities|Research background: An efficient and effectively functioning transport in the city is of great importance both for the people residing in its territory, as well as companies doing business there. However, apart from a positive impact, transport also carries many social costs including congestion, traffic accidentsand a negative influence on the natural environment. Consequently, urban transport is an increasingly important area of city management. Purpose of the article:The aim of this study is to analyze and to assess the transport technological effectivenessin selected Polish cities.The author received a ranking of cities and identified ways to improve the efficiency. Methodology:The test procedure used non-parametric method of Data Envelopment Analysis. Data for analysis were draw from the Local Data Bank of the Central Statistical Office defining expenses in the transport section as well as data on the condition and use of transport infrastructure. The calculations have been made using Frontier Analyst Application software. The performance results were determined using the BCC model. Findings& Value added:The main result is the author’s rankingof transport effectivenessin Polish cities. The analysis showed that urban transport characterized by a rather low technological effectiveness.
C10|Social Convergence in Nordic NUTS-3 Regions|Geographical proximity, common historical roots and collaboration within the Nordic Council make the Nordic countries, often wrongly treated as monoliths. However, in reality, Nordic regions differ in terms of broadly defined social and economic development. Issues concerning the standard of living are one of the priorities of the Helsinki Treaty signed by Nordic countries. The main goal of this paper is to analyze the existence of the social convergence in the Nordic NUTS-3 regions over the 2000-2015 period. The social convergence refers to a reduction in the dispersion of the standard of living across regions. Result of this analysis may be helpful in evaluating the efficiency of the activities under third and fourth Nordic Strategy for Sustainable Development. The spatial taxonomy measure of development proposed by Pietrzak was used as the standard of living approximation. Inclusion of spatial relationships in the construction of taxonomic measure of development is justified as regions are not isolated in space and can be affected by other units. The existence of beta-, sigma- and gamma convergence was tested for global spatial aggregate measure and as well for sub-groups of determinants forming the standard of living. The analysis showed that the regions with the highest standard of living are those situated on the west coast of Norway. Regions with the lowest standard of living were regions located in central Finland. However the most important part of this research was to investigate the existence of beta-, sigma- and gamma- social convergence. The results show that there is no convergence for global standard of living measure. However the convergence occurs in groups of determinants of education and health care.
C10|Duration Model of Enterprises – Analysis of Territorial Groups|The popular term of business demography or demography of the firm denotes a relatively young area of science which focuses on the structures of cohorts of firms and the changes that undergo within these structures. As both the terms suggest, the studies use research methods traditionally applied in demographic studies. Survival analysis is increasingly used in business demography. The purpose of the present study was to build the enterprises survival models for territorial groups in Zachodniopomorskie (Poland). In the first stage the Kaplan-Meier estimator was calculated and the test to verify the similarity of the survival function for poviats was conducted. Poviats were classified into groups. Next, the tables of enterprises survival were built and the business liquidation intensity was analysed in individual groups. In this study the continuous-time non-parametric models were used: Kaplan-Meier estimator, Gehan test and duration table. Those methods were employed to model the survival time and find differences in the survival of firms in the poviats of the Zachodniopomorskie. In keeping with the above scheme five territorial groups with similar enterprises survival time models were distinguished. The study results presented in this article reveal the differentiation of enterprises survival models in the territorial groups. Five groups of poviats were distinguished. These groups, as a result of the study, have been characterized.
C10|Nowcasting Building Permits with Google Trends|We propose a useful way to predict building permits in the US, exploiting rich real-time data from web search queries. The time series on building permits is usually considered as a leading indicator of economic activity in the construction sector. Nevertheless, new data on building permits are released with a lag close to two months. Therefore, an accurate now-cast of this leading indicator is desirable. We show that models including Google search queries nowcast and forecast better than our good, not naïve, univariate benchmarks both in-sample and out-of-sample. We also show that our results are robust to different specifications, the use of rolling or recursive windows and, in some cases, to the forecasting horizon. Since Google queries information is free, our approach is a simple and inexpensive way to predict building permits in the United States.
C10|Estimates of Net Capital Stock and Consumption of Fixed Capital for Australian States and Territories, 1990–2013|Being an important input for many economic models and being widely used in economic decision making by the federal and regional government, capital stock data are not readily available at the sub-national level for most countries, including Australia. The study closely follows the methodology of the Australian Bureau of Statistics and presents a complete set of capital stock data for the states and territories of Australia for the period 1990–2013. The robustness of the method is assessed by comparing the aggregated data by type of asset estimates for the states, with the capital stock data by type of asset for Australia as a whole published by the Australian Bureau of Statistics.
