C65|The Qualitative Expectations Hypothesis: Model Ambiguity, Consistent Representations Of Market Forecasts, And Sentiment|We introduce the Qualitative Expectations Hypothesis (QEH) as a new approach to modeling macroeconomic and financial outcomes. Building on John Muth's seminal insight underpinning the Rational Expectations Hypothesis (REH), QEH represents the market's forecasts to be consistent with the predictions of an economist's model. However, by assuming that outcomes lie within stochastic intervals, QEH, unlike REH, recognizes the ambiguity faced by an economist and market participants alike. Moreover, QEH leaves the model open to ambiguity by not specifying a mechanism determining specific values that outcomes take within these intervals. In order to examine a QEH model's empirical relevance, we formulate and estimate its statistical analog based on simulated data. We show that the proposed statistical model adequately represents an illustrative sample from the QEH model. We also illustrate how estimates of the statistical model's parameters can be used to assess the QEH model's qualitative implications.
C65|Implementable Mechanisms for discrete utility functions, a solution using Tropical Geometry|For mechanisms with two possible outcomes and a unique agent with two posible types, we state sufficient and necessary conditions over the type space that guarantees the existence of an incentive compatible mechanism. By using tropical geometry, we found that these conditions rely only on the relative valuations to be increasing over types. This result extends implementability for discrete utility functions.
C65|Fake news and propaganda: Trump's Democratic America and Hitler's National Socialist (Nazi) Germany|This paper features an analysis of President Trump's two State of the Union addresses, which are analysed by means of various data mining techniques including sentiment analysis. The intention is to explore the contents and sentiments of the messages contained, the degree to which they differ, and their potential implications for the national mood and state of the economy. In order to provide a contrast and some parallel context, analyses are also undertaken of President Obama's last State of the Union address and Hitler's 1933 Berlin Proclamation. The structure of these four political addresses is remarkably similar. The three US Presidential speeches are more positive emotionally than Hitler's relatively shorter address, which is characterized by a prevalence of negative emotions. However, it should be said that the economic circumstances in contemporary America and Germany in the 1930s are vastly different.
C65|Reconstruction of regional and national population using intermittent census-type data: the case of Portugal, 1527-1864|We offer a new methodology for the construction of annual population stocks over the very long run. Our method does not require the assumption of a closed economy, and can be used in situations in which local annual gross flows are obtainable. Combining gross flows with intermittent census-type data, it is possible to arrive at local, regional and national population stock estimates at annual frequencies. We provide an application to early modern and nineteenth century Portugal, using a large sample of parish-level statistics up to the first modern census of 1864. All six major regions of the country are considered. ghlight the effect of the second nature geography changes as well as protectionism on well-being. We use a novel database of Danish and Swedish real wages to investigate the impact of these changes on Scanian living standards by employing a difference in difference approach to show that wages fell more in Scania than those in surrounding regions in relation to the border change and associated protectionism.
C65|Resolutions to flip-over credit risk and beyond|Abstract Given a risk outcome y over a rating system {R_i }_(i=1)^k for a portfolio, we show in this paper that the maximum likelihood estimates with monotonic constraints, when y is binary (the Bernoulli likelihood) or takes values in the interval 0≤y≤1 (the quasi-Bernoulli likelihood), are each given by the average of the observed outcomes for some consecutive rating indexes. These estimates are in average equal to the sample average risk over the portfolio and coincide with the estimates by least squares with the same monotonic constraints. These results are the exact solution of the corresponding constrained optimization. A non-parametric algorithm for the exact solution is proposed. For the least squares estimates, this algorithm is compared with “pool adjacent violators” algorithm for isotonic regression. The proposed approaches provide a resolution to flip-over credit risk and a tool to determine the fair risk scales over a rating system.
C65|Complex assessment of Ukrainian agroholdings’ economic security|Retrospective diagnostics of main financial results of activity (profit, earnings and yield of shares); business capital, market capitalization, EBITDA, operating margin and net profit margin, P / E, ROA, ROE, ROC as of 2018 and an average of 5 years; Consideration of Montier C-Score and Piotroski F-Score indicators on financial issues of companies; liquidity of the balance of agroholdings on the criterion of the ratio of assets and liabilities; the coefficient analysis of enterprises and the diagnosis of bankruptcy probabilities using models of multiple discriminant analysis (MDA model), logistic regression (Logit-model) and rating methods (Beaver's indicators and definition of the class of the potential beneficiary of the investment prokect) helped to formulate a generalized conclusion on the probability of bankruptcy according to models and a general assessment of the financial condition of agricultural companies. Based on the analysis, it has been determined that, according to all methods of the agroholding, the sample collectively generates a series from the highest to the lowest: MHP / Kernel, IMC, Agroton and Avangard. In view of this, it is necessary to formulate and continuously improve integrated risk management systems (integrated systems of enterprise economic security), rather than separate procedures at the level of different departments of the company.
C65|Комплексне Оцінювання Економічної Безпеки Агрохолдингів України<BR>[Complex assessment of Ukrainian agroholdings’ economic security]|У статті на основі даних фінансової звітності, консолідованої інформації на сайтах фондових бірж тощо здійснено аналіз фінансового стану 5 агрохолдингів України. Проведено оцінювання фінансових показників та діагностику стану системи економічної безпеки холдингового підприємства агробізнесу (за МСФЗ та НПСБО) за 5 параметрами: загальний фінансовий стан, основні індикатори, ліквідність балансу, коефіцієнтний аналіз, ймовірність банкрутства. Проблема. У контексті забезпечення національної безпеки поряд з її політичною та воєнною важливими складовими частинами є економічна та продовольча. Агросектор України, маючи унікальні ресурси, разом з розвитком інноваційних інформаційних технологій в змозі забезпечити якісне економічне зростання країни. В Україні найбільшими та найвпливовішими представниками агросектору є аграрні холдинги. Тому необхідним є оцінювання стану економічної безпеки цих економічних суб’єктів. Утім, у цій предметній області є ряд проблемних моментів: серед дослідників відсутня єдність поглядів на широке коло питань економічної безпеки, існує брак досліджень про систему економічної безпеки підприємств конкретних галузей (зокрема аграрної), не сформовано єдиної методики щодо оцінювання стану економічної безпеки суб’єктів господарювання, агрохолдинги практично не вивчаються в контексті економічної безпеки. У зв’язку з цим, актуалізується вироблення вітчизняних напрацювань, прилаштованих до умов сучасної української економіки. Мета статті – здійснити комплексний аналіз економічної безпеки агрохолдингів України у рамках розробки теоретико-методичного базису й обґрунтування практичних рекомендацій щодо вдосконалення систем економічної безпеки агрохолдингів в Україні та визначення головних напрямів інноваційного розвитку цих систем. Методи дослідження. Економічний та фінансовий аналіз, коефіцієнтний метод, методи комплексної оцінки (для оцінювання економічної безпеки та фінансового стану досліджуваних агрокомпаній); табличний та графічний (діаграми) – для візуалізації результатів дослідження та систематизації статистичних даних. Результати. Коефіцієнти використано для оцінювання фінансового стану і банкрутства як найнадійніші і найважливіші показники – найбільші проблеми агрохолдингів в останні роки були саме з фінансового складника системи економічної безпеки підприємства. Для подальшого аналізу обрано 5 агрокорпорацій: Авангард, Агротон, ІМК, Кернел, МХП. Критеріями вибору компаній були географічна представленість (діяльність в різних областях України) та інформаційна доступність (в т.ч. фінансової звітності, звітів для інвесторів, дані про системи ризик-менеджменту). Три агрохолдинги вибірки за розміром земельного банку входять одночасно і до рейтингу топ-35 найбільших латифундистів світу за підсумками 2017 року: Uklandfarming (Авангард входить до його складу), Кернел, Миронівський хлібопродукт. Крім того, за різними рейтингами якості управління, репутації і т.п. ці компанії зазвичай є лідерами. Фінансові прорахунки відповідно до проведених оцінок присутні в усіх аналізованих агрокомпаніях, під час діагностики ймовірності банкрутства навіть вітчизняні методи (що зазвичай показують найприйнятніші результати) показали необхідність санаційних процедур чи антикризового управління. Наукова новизна. У статті на основі даних фінансової звітності, консолідованої інформації на сайтах фондових бірж тощо здійснено аналіз фінансового стану 5 агрохолдингів України. Проведено оцінювання фінансових показників та діагностику стану системи економічної безпеки холдингового підприємства агробізнесу (за МСФЗ та НПСБО) за 5 параметрами: загальний фінансовий стан, основні індикатори, ліквідність балансу, коефіцієнтний аналіз, ймовірність банкрутства. Висновки. Ретроспективна діагностика основних фінансових результатів діяльності (прибутку, виторгу та дохідності акцій); показників вартості підприємства, ринкової капіталізації, EBITDA, маржі операційного та чистого прибутку, P/E, ROA, ROE, ROC станом на 2018 рік та в середньому за 5 років; розгляд індикаторів Montier C-Score та Piotroski F-Score щодо фінансових проблем компаній; ліквідності балансу агрохолдингів за критерієм співвідношення активів та пасивів; коефіцієнтний аналіз підприємств та діагностика ймовірності банкрутства з використанням моделей множинного дискримінантного аналізу (MDA-моделі), логістичної регресії (Logit-моделі) та рейтингових методик (показники Бівера та визначення класу потенційного бенефіціара інвестиційного проекту) допомогли сформувати узагальнений висновок щодо ймовірності настання банкрутства згідно моделей і загальну оцінку фінансового стану агрокомпаній. На основі проведеного аналізу визначено, що за всіма методиками агрохолдинги вибірки загалом формують ряд від найвищих до найнижчих показників: МХП/Кернел, ІМК, Агротон та Авангард. З огляду на це необхідне формування та постійне вдосконалення систем комплексного управління ризиками (інтегрованих систем економічної безпеки підприємства), а не окремі процедури на рівні різних відділів компанії.
C65|A Conic Approach to the Implementation of Reduced-Form Allocation Rules|We examine the implementation of reduced-form allocation rules in mechanism design problems. To handle the problem, we adopt a conic approach which uses a lift-and-project method to construct a projection cone and find its finite generators. This results in a set of implementable reduced forms for implementation. We then characterize projection cones for several typical mechanism design problems including single-item auctions, bilateral trade, compromise, and multiple-item auctions with group capacity constraints. We find that the implementation condition in general has a linear characterization by a class of sign functions, which is larger and richer than the well-known class of characteristic functions found by Border. These results admit meaningful economic interpretations.
C65|Individual upper semicontinuity and subgame perfect ϵ-equilibria in games with almost perfect information|We study games with almost perfect information and an infinite time horizon. In such games, at each stage, the players simultaneously choose actions from finite action sets, knowing the actions chosen at all previous stages. The payoff of each player is a function of all actions chosen during the game. We define and examine the new condition of individual upper semicontinuity on the payoff functions, which is weaker than upper semicontinuity. We prove that a game with individual upper semicontinuous payoff functions admits a subgame perfect ϵ-equilibrium for every ϵ > 0, in eventually pure strategy profiles.
C65|Methods for Analytical Barrier Option Pricing with Multiple Exponential Time-Varying Boundaries|We develop novel methods for efficient analytical solution of all types of partial time barrier options with both single and double exponential and time varying boundaries, and specifically to treat forward-starting partial double barrier options, which present the simplest non-trivial example of the multiple exponential time-varying barrier case. Our methods reduce the pricing of all barrier options with time-varying boundaries to the pricing of a single European option. We express our novel results solely in terms of European first and second order Gap options. We are motivated by similar structures appearing in Structural Credit Risk models for firm default.
C65|Policies for decarbonizing a liberalized power sector|Given the agreed urgency of decarbonizing electricity and the need to guide decentralized private decisions, an adequate and credible carbon price appears essential. The paper models and quantifies the break-even carbon price for mature zero-carbon electricity investments. It appears an attractive alternative given the difficulty of measuring the social cost of carbon, but modelling shows it extremely sensitive to projected fuel prices, the rate of interest, and the capital cost of generation options, all of which are very uncertain. This has important implications, and justifies combining a carbon price floor with suitable long-term contracts for electricity investments.
C65|Linking Tax Morale and Personal Income Tax in Spain|The paper presents a study of the relationship between the tax morale and the individual payments of personal income tax using the statistical matching of opinion polls with a representative sample of the personal income tax returns in Spain. As an initial step, the method selected to execute the match -imputations using Bayesian Networks- is described. The relationship between a proxy variable of the individual tax morale and other variables in the declared income tax file is later analyzed using the matched files. A first result is that tax morale increases with the level of declared wages, salaries and capital gains, while it has no link with declared business income.
C65|Approximate super-resolution and truncated moment problems in all dimensions|We study the problem of reconstructing a discrete measure on a compact set K subset Rn from a finite set of moments (possibly known only approximately) via convex optimization. We give new uniqueness results, new quantitative estimates for approximate recovery and a new sum-of-squares based hierarchy for approximate super-resolution on compact semi-algebraic sets.
C65|Relationships between the stochastic discount factor and the optimal omega ratio|The omega ratio is an interesting performance measure because it fo- cuses on both downside losses and upside gains, and nancial markets are re ecting more and more asymmetry and heavy tails. This paper focuses on the omega ratio optimization in general Banach spaces, which applies for both in nite dimensional approaches related to continuous time stochastic pricing models (Black and Scholes, stochastic volatility, etc.) and more classical problems in portfolio selection. New algorithms will be provided, as well as Fritz John-like and Karush-Kuhn-Tucker-like optimality conditions and duality results, despite the fact that omega is neither di¤er- entiable nor convex. The optimality conditions will be applied to the most important pricing models of Financial Mathematics, and it will be shown that the optimal value of omega only depends on the upper and lower bounds of the pricing model stochastic discount factor. In particular, if the stochastic discount factor is unbounded (Black and Scholes, Heston, etc.) then the optimal omega ratio becomes unbounded too (it may tend to in nity), and the introduction of several nancial constraints does not overcome this caveat. The new algorithms and optimality conditions will also apply to optimize omega in static frameworks, and it will be illustrated that both in nite- and nite-dimensional approaches may be useful to this purpose.
C65|Golden options in financial mathematics|This paper deals with the construction of smooth good deals (SGD), i.e., sequences of self- nancing strategies whose global risk diverges to ∞ and such that every security in every strategy of the sequence is a smooth derivative with a bounded delta. If the selected risk measure is the value at risk then these sequences exist under quite weak conditions, since one can involve risks with both bounded and unbounded expectation, as well as non-friction-free pricing rules. Moreover, every strategy in the sequence is composed of an European option plus a position in a riskless asset. The strike of the option is easily computed in practice, and the ideas may also apply in some actuarial problems such as the selection of an optimal reinsurance contract. If the chosen risk measure is a coherent one then the general setting is more limited. Indeed, though frictions are still accepted, expectations and variances must remain nite. The existence of SGDs will be characterized, and computational issues will be properly addressed as well. It will be shown that SGDs often exist, and for the conditional value at risk they are composed of the riskless asset plus easily replicable European puts. Numerical experiments will be presented in all of the studied cases.
C65|A network-based method to harmonize data classifications|A frequent problem in research is the harmonization of data to a common classification, whether that is in terms of ? to name a few examples ? industries, commodities, occupations, or geograph- ical areas. Statistical offices often provide concordance tables, to match data through time or with different classifications, but these concordance tables alone are often not sufficient to define a clear methodology on how the matching should be performed. In fact, the concordance tables have, in numerous occasions, a many-to-many mapping of classifications. The issue is exacerbated when two or more concordance tables are concatenated. In this Jupyter notebook, I discuss a network- based abstraction of this problem and propose, as a general solution, a method that identifies the network components (or the network communities) to make data converge to a new classification. The method simplifies the issue and reduces greatly conversion errors.
C65|Reliably Computing Nonlinear Dynamic Stochastic Model Solutions: An Algorithm with Error Formulas|This paper provides a new technique for representing discrete time nonlinear dynamic stochastic time invariant maps. Using this new series representation, the paper augments the usual solution strategy with an additional set of constraints thereby enhancing algorithm reliability. The paper also provides general formulas for evaluating the accuracy of proposed solutions. The technique can readily accommodate models with occasionally binding constraints and regime switching. The algorithm uses Smolyak polynomial function approximation in a way which makes it possible to exploit a high degree of parallelism.
C65|From Methodology to Practice (and Back): Georgescu-Roegen's Philosophy of Economics and the Flow-Fund Model|Despite his early contribution to the rise of mathematics in economics, Georgescu-Roegen's later methodological criticism of models has received little attention from historians and philosophers of economics. This paper attempts to fill this gap following two lines. First, I examine his explicitly methodological claims and connect them with related topics in economic methodology. Building on the distinction between dialectical and arithmomorphic concepts, I characterise his approach to theory-making as a three steps process of idealisation, isolation and arithmetisation. In this framework, models perform two functions, checking for logical consistency and facilitating understanding, which can be related to the idea of modelling as theorising. I then confront these general principles with Georgescu-Roegen's flow-fund model of production. I use the methodology as a reading grid of this theory, while examining its limits and complementary principles in practice. This shows a great deal of consistency, where idealisation provides conceptual foundations, isolation determines the relevant problems, and models are built according to structural consistency. The two functions of models are then illustrated by the logical derivation of older principles formulated by Babbage and Smith, and the understanding of the different organisational patterns of production. But some slightly different functions also appear when specific configurations of the model enable to check the conceptual consistency of other theories, or the understanding provided by the model contributes to the formation of new concepts. Hence, the consistency and the complementarity between Georgescu-Roegen's methodology and practice of theory-making provide interesting insights and a useful background for further investigations
C65|Supply Function Equilibrium over a Constrained Transmission Line I: Calculating Equilibria|Competition between oligopolist electricity generators is inhibited by transmission constraints. I present a supply function equilibrium (SFE) model of an electricity market with a single lossless, but constrained, transmission line. The market admits equilibria in which generator withhold energy in order to induce congestion, which further increases their local market power. Under appropriate assumptions on cost and demand functions, I obtain a planar autonomous system of ordinary differential equations for the SFE. Computational methods are developed to solve the system while respecting monotonicity constraints on the supply functions. Using these methods I can calculate SFE in network markets that range from fully isolated to fully integrated. I also find network markets for which the SFE is not unique.
C65|From Methodology to Practice (and Back): Georgescu-Roegen's Philosophy of Economics and the Flow-Fund Model|Despite his early contribution to the rise of mathematics in economics, Georgescu-Roegen's later methodological criticism of models has received little attention from historians and philosophers of economics. This paper attempts to fill this gap following two lines. First, I examine his explicitly methodological claims and connect them with related topics in economic methodology. Building on the distinction between dialectical and arithmomorphic concepts, I characterise his approach to theory-making as a three steps process of idealisation, isolation and arithmetisation. In this framework, models perform two functions, checking for logical consistency and facilitating understanding, which can be related to the idea of modelling as theorising. I then confront these general principles with Georgescu-Roegen's flow-fund model of production. I use the methodology as a reading grid of this theory, while examining its limits and complementary principles in practice. This shows a great deal of consistency, where idealisation provides conceptual foundations, isolation determines the relevant problems, and models are built according to structural consistency. The two functions of models are then illustrated by the logical derivation of older principles formulated by Babbage and Smith, and the understanding of the different organisational patterns of production. But some slightly different functions also appear when specific configurations of the model enable to check the conceptual consistency of other theories, or the understanding provided by the model contributes to the formation of new concepts. Hence, the consistency and the complementarity between Georgescu-Roegen's methodology and practice of theory-making provide interesting insights and a useful background for further investigations.
C65|Mirage on the Horizon: Geoengineering and Carbon Taxation Without Commitment|We show that, in a model without commitment to future policies, geoengineering breakthroughs can have adverse environmental and welfare effects because they change the (equilibrium) carbon taxes. In our model, energy producers emit carbon, which creates a negative environmental externality, and may decide to switch to cleaner technology. A benevolent social planner sets carbon taxes without commitment. Higher future carbon taxes both reduce emissions given technology and encourage energy producers to switch to cleaner technology. Geoengineering advances, which reduce the negative environmental effects of the existing stock of carbon, decrease future carbon taxes and thus discourage private investments in conventional clean technology. We characterize the conditions under which these advances diminish - rather than improve - environmental quality and welfare.
C65|Quantifier Elimination for Deduction in Econometrics|When combined with the logical notion of partially interpreted functions, many nonparametric results in econometrics and statistics can be understood as statements about semi-algebraic sets. Tarski’s quantifier elimination (QE) theorem therefore guarantees that a universal algorithm exists for deducing such results from their assumptions. This paper presents the general framework and then applies QE algorithms to Jensen’s inequality, omitted variable bias, partial identification of the classical measurement error model, point identification in discrete choice models, and comparative statics in the nonparametric Roy model. This paper also discusses the computational complexity of real QE and its implementation in software used for program verification, logic, and computer algebra. I expect that automation will become as routine for abstract econometric reasoning as it already is for numerical matrix inversion.
C65|Non-linear Real Arithmetic Benchmarks derived from Automated Reasoning in Economics|We consider problems originating in economics that may be solved automatically using mathematical software. We present and make freely available a new benchmark set of such problems. The problems have been shown to fall within the framework of non-linear real arithmetic, and so are in theory soluble via Quantifier Elimination (QE) technology as usually implemented in computer algebra systems. Further, they all can be phrased in prenex normal form with only existential quantifiers and so are also admissible to those Satisfiability Module Theory (SMT) solvers that support the QF_NRA logic. There is a great body of work considering QE and SMT application in science and engineering, but we demonstrate here that there is potential for this technology also in the social sciences.
C65|Living Standards Analysis Model: The First Prototype|How do we understand the synergies and trade-offs of a given policy on area beyond that policy, such as the effect of housing on health and on income? How do we choose between policies in completely different areas, such as an education policy and a health policy? Treasury’s Living Standards Framework provides one possible starting point, but it provides little assistance tracing the many dependencies between policy areas. A model that includes those dependencies could help. The Living Standards Analysis Model (LSAM) is designed to do this. A first prototype of the model has just been developed. This model includes all eleven aspects of wellbeing as described by the OECD’s How’s Life? framework and linkages between the different aspects for a small open economy. Most models for studying wellbeing only include one or two aspects, missing the rich set of interactions that can occur with greater coverage. As an early prototype, this version of the model does have many flaws and requires significant further development, but it forms a basis for creating an improved model as well as providing some qualitatively useful results. The model is loosely based on a stocks-and-flows type of model, with a small general equilibrium model covering the market economy part of the model. This paper is focussed on the description of the model.
C65|On the extension of a preorder under translation invariance|This paper proves the existence, for any preorder on a real vector space satisfying translation invariance, of a complete preorder extending the preorder and satisfying translation invariance. As application, the existence of a translation-invariant complete preorder on infinite utility streams satisfying strong Pareto and fixed-step anonymity, is established.
C65|Models of Continuous Dynamics on the 2-Simplex and Applications in Economics|In this paper, we discuss the models of continuous dynamics on the 2-simplex that arise when different qualitative restrictions are imposed on the (continuous) functions that generate the dynamics on the 2-simplex. We consider three types of qualitative restrictions: inequality (or set-theoretical) conditions, monotonicity/curvature (or differential-geometrical) conditions, and topological conditions (referring to (transversal) non-(self-)intersection of trajectories). We discuss the implications of these restrictions for transitional and limit dynamics on the 2-simplex and the wide range of potential and existing applications of the resulting system-theoretical models in economics and, in particular, in economic growth and development theory.
C65|Zur mathematischen Struktur der Wertformen von Karl Marx in 'Das Kapital'<BR>[About the mathematical structure of the form of value of Karl Marx in 'Das Kapital']|In the first section of Das Kapital by Karl Marx different forms of values are analysed. From a mathematical point of view one can find therein structures, which correspond to elements of the mathematical theory of categories. These are especially the limit of cones and the definition of subobjects as morphisms. Using the limit cone, the concept of money contains the categorial product of commodities. The concept of the value of a commodity contains the categorial definition of a subobject.
C65|A Frequency-Domain Approach to Dynamic Macroeconomic Models|This article is concerned with frequency-domain analysis of dynamic linear models under the hypothesis of rational expectations. We develop a unified framework for conveniently solving and estimating these models. Unlike existing strategies, our starting point is to obtain the model solution entirely in the frequency domain. This solution method is applicable to a wide class of models and permits straightforward construction of the spectral density for performing likelihood-based inference. To cope with potential model uncertainty, we also generalize the well-known spectral decomposition of the Gaussian likelihood function to a composite version implied by several competing models. Taken together, these techniques yield fresh insights into the model’s theoretical and empirical implications beyond what conventional time-domain approaches can offer. We illustrate the proposed framework using a prototypical new Keynesian model with fiscal details and two distinct monetary-fiscal policy regimes. The model is simple enough to deliver an analytical solution that makes the policy effects transparent under each regime, yet still able to shed light on the empirical interactions between U.S. monetary and fiscal policies along different frequencies.
C65|On the Extension and Decomposition of a Preorder under Translation Invariance|We prove the existence, for a translation-invariant preorder on a divisible commutative group, of a complete preorder extending the preorder in question and satisfying translation invariance (theorem 1). We also prove that the extension may inherit a property of continuity (theorem 2). This property of continuity may lead to scalar invariance. By seeking to clarify the relationship between continuity and scalar invariance under translation invariance, we are led to formulate a theorem that asserts the existence of a continuous linear weak representation under a certain condition (theorem 3). The application of these results in a space of infinite real sequences shows that this condition is weaker than the axiom super weak Pareto, and that the latter is itself weaker than the axiom monotonicity for non-constant preorders. Thus, theorem 3 is a strengthening of theorem 4 of Mabrouk 2011. It also makes it possible to show the existence of a sequence of continuous linear preorders whose lexicographic combination constitutes the finest combination coarser than the preorder in question (theorem 4). This decomposition makes it possible to handle continuous functions instead of preoders when one looks for optima, which may be more practical. Finally we apply this decomposition to the preorder catching-up. Several examples are provided.
C65|Ambiguous economic news and heterogeneity: What explains asymmetric consumption responses?|We study information and consumption and whether consumers respond symmetrically to good and bad news. We define a news variable and show that it has explanatory power. We, then, test the hypothesis that consumers react more to bad news than to good news using the PSID to analyze the response of households’ consumption to news about aggregate future income.We find that our news variable helps one predict households’ consumption change and that consumption responses are larger following negative (bad) news than positive (good) news and suggest that observed asymmetric consumption responses could be due to agents’ aversion to ambiguous information.
C65|Wavelet analysis for temporal disaggregation|A problem often faced by economic researchers is the interpolation or distribution of economic time series observed at low frequency into compatible higher frequency data. A method based on wavelet analysis is presented to temporal disaggregate time series. A standard `plausible' method is applied, not to the original time series, but to the smooth components resulting from a discrete wavelet transformation. This first step generates a smoothed component at the desired frequency. Subsequently, a noisy component is added to the smooth series to enforce the natural constraint of the series. The method is applied to national accounts for Euro Area, to study both ow and stock variables, and it outperforms other standard methods, as Stram and Wei or low pass interpolation when the series of interest is volatile.
C65|Investment Decisions with Two-Factor Uncertainty|This paper considers investment problems in real options with non-homogeneous two-factor uncertainty. It shows that, despite claims made in the literature, the method used to derive an analytical solution in one dimensional problems cannot be straightforwardly extended to problems with two stochastic processes. To illustrate, we analyze an investment problem with two stochastic revenue streams and a constant sunk cost. We show that a semi-analytical approach leads to a sub-optimal investment policy. The main message of our paper is that non-homogeneous investment problems can only be solved numerically
C65|The Network View: applications to international trade and bank exposures|Systems of interconnected elements are increasingly important in economic applications. This paper elaborates on some ideas of network analysis and its application to the study of systems of economic interest. It focuses on the Identification of influential and vulnerable elements, from both a global and a local perspective.
C65|Cartagena libre de pobreza extrema en el 2033|En 2016, Cartagena fue la tercera ciudad con mayor incidencia de la pobreza monetaria en Colombia, situación que contrasta con el dinamismo de sus principales sectores económicos como la industria, el turismo, la construcción y la actividad portuaria. Este documento propone una intervención integral para que Cartagena supere la pobreza extrema en el mediano plazo. Entre las medidas que se consideran prioritarias en este estudio se encuentran: i) la identificación, reubicación y protección de la población que vive en zonas de riesgo no mitigable, ii) la cobertura de servicios públicos esenciales como agua y alcantarillado, iii) la educación y iv) el empleo y la informalidad laboral. También se presenta una estimación preliminar del costo de esta propuesta. ******ABSTRACT: In 2016, Cartagena was the third city with the highest incidence of monetary poverty in Colombia, a situation that contrasts with the economic dynamism of sectors such as industry, tourism, construction, and port activity. This document proposes an integral public intervention to overcome extreme poverty in Cartagena in the medium term. Among the measures considered to be priorities in this study are: (i) identification, relocation, and protection of the population living in areas of nonmitigable risk; (ii) coverage of essential public services such as water and sewage; (iii) education and iv) employment and job informality. We also present a preliminary estimate of the cost of this proposal.
C65|Cartagena libre de pobreza extrema en el 2033|En 2016, Cartagena fue la tercera ciudad con mayor incidencia de la pobreza monetaria en Colombia, situación que contrasta con el dinamismo de sus principales sectores económicos como la industria, el turismo, la construcción y la actividad portuaria. Este documento propone una intervención integral para que Cartagena supere la pobreza extrema en el mediano plazo. Entre las medidas que se consideran prioritarias en este estudio se encuentran: i) la identificación, reubicación y protección de la población que vive en zonas de riesgo no mitigable, ii) la cobertura de servicios públicos esenciales como agua y alcantarillado, iii) la educación y iv) el empleo y la informalidad laboral. También se presenta una estimación preliminar del costo de esta propuesta.
C65|Differential equations connecting VaR and CVaR|The Value at Risk (VaR) is a very important risk measure for practitioners, supervisors and researchers. Many practitioners draw on VaR as a critical instrument in Risk Management and other Actuarial/Financial problems, while super- visors and regulators must deal with VaR due to the Basel Accords and Solvency II, among other reasons. From a theoretical point of view VaR presents some drawbacks overcome by other risk measures such as the Conditional Value at Risk (CVaR). VaR is neither differentiable nor sub-additive because it is neither continuous nor convex. On the contrary, CVaR satis es all of these properties, and this simpli es many ana- lytical studies if VaR is replaced by CVaR. In this paper several differential equations connecting both VaR and CVaR will be presented. They will allow us to address several important issues involving VaR with the help of the CVaR properties. This new methodology seems to be very efficient. In particular, a new VaR Representation Theorem may be found, and optimization problems involving VaR or probabilistic constraints always have an equivalent differentiable optimization problem. Applications in VaR, marginal VaR, CVaR and marginal CVaR estimates will be addressed as well. An illustrative actuarial numerical example will be given.
C65|Point Optimal Testing with Roots That Are Functionally Local to Unity|Limit theory for regressions involving local to unit roots (LURs) is now used extensively in time series econometric work, establishing power properties for unit root and cointegration tests, assisting the construction of uniform confidence intervals for autoregressive coefficients, and enabling the development of methods robust to departures from unit roots. The present paper shows how to generalize LUR asymptotics to cases where the localized departure from unity is a time varying function rather than a constant. Such a functional local unit root (FLUR) model has much greater generality and encompasses many cases of additional interest, including structural break formulations that admit subperiods of unit root, local stationary and local explosive behavior within a given sample. Point optimal FLUR tests are constructed in the paper to accommodate such cases. It is shown that against FLUR\ alternatives, conventional constant point optimal tests can have extremely low power, particularly when the departure from unity occurs early in the sample period. Simulation results are reported and some implications for empirical practice are examined.
C65|Boundary Limit Theory for Functional Local to Unity Regression|This article studies functional local unit root models (FLURs) in which the autoregressive coefficient may vary with time in the vicinity of unity. We extend conventional local to unity (LUR) models by allowing the localizing coefficient to be a function which characterizes departures from unity that may occur within the sample in both stationary and explosive directions. Such models enhance the flexibility of the LUR framework by including break point, trending, and multidirectional departures from unit autoregressive coefficients. We study the behavior of this model as the localizing function diverges, thereby determining the impact on the time series and on inference from the time series as the limits of the domain of definition of the autoregressive coefficient are approached. This boundary limit theory enables us to characterize the asymptotic form of power functions for associated unit root tests against functional alternatives. Both sequential and simultaneous limits (as the sample size and localizing coefficient diverge) are developed. We find that asymptotics for the process, the autoregressive estimate, and its tâ€ statistic have boundary limit behavior that differs from standard limit theory in both explosive and stationary cases. Some novel features of the boundary limit theory are the presence of a segmented limit process for the time series in the stationary direction and a degenerate process in the explosive direction. These features have material implications for autoregressive estimation and inference which are examined in the article.
C65|Kernel-Based Inference In Time-Varying Coefficient Cointegrating Regression|This paper studies nonlinear cointegrating models with time-varying coefficients and multiple nonstationary regressors using classic kernel smoothing methods to estimate the coefficient functions. Extending earlier work on nonstationary kernel regression to take account of practical features of the data, we allow the regressors to be cointegrated and to embody a mixture of stochastic and deterministic trends, complications which result in asymptotic degeneracy of the kernel-weighted signal matrix. To address these complications new \textsl{local} and \textsl{global rotation} techniques are introduced to transform the covariate space to accommodate multiple scenarios of induced degeneracy. Under certain regularity conditions we derive asymptotic results that differ substantially from existing kernel regression asymptotics, leading to new limit theory under multiple convergence rates. For the practically important case of endogenous nonstationary regressors we propose a fully-modified kernel estimator whose limit distribution theory corresponds to the prototypical pure (i.e., exogenous covariate) cointegration case, thereby facilitating inference using a generalized Wald-type test statistic. These results substantially generalize econometric estimation and testing techniques in the cointegration literature to accommodate time variation and complications of co-moving regressors. Finally an empirical illustration to aggregate US data on consumption, income, and interest rates is provided.
C65|Relevant states and memory in Markov chain bootstrapping and simulation|Markov chain theory is proving to be a powerful approach to bootstrap and simulate highly nonlinear time series. In this work, we provide a method to estimate the memory of a Markov chain (i.e. its order) and to identify its relevant states. In particular, the choice of memory lags and the aggregation of irrelevant states are obtained by looking for regularities in the transition probabilities. Our approach is based on an optimization model. More specifically, we consider two competing objectives that a researcher will in general pursue when dealing with bootstrapping and simulation: preserving the “structural” similarity between the original and the resampled series, and assuring a controlled diversification of the latter. A discussion based on information theory is developed to define the desirable properties for such optimal criteria. Two numerical tests are developed to verify the effectiveness of the proposed method.
C65|Discontinuous payoff option pricing by Mellin transform: A probabilistic approach|The Mellin transform technique is applied for solving the Black-Scholes equation with time-dependent parameters and discontinuous payoff. We show that the option pricing is equivalent to recovering a probability density function on the positive real axis based on its moments, which are integer or fractional Mellin transform values. Then the Mellin transform can be effectively inverted from a collection of appropriately chosen fractional (i.e. non-integer) moments by means of the Maximum Entropy (MaxEnt) method. An accurate option pricing is guaranteed by previous theoretical results about MaxEnt distributions constrained by fractional moments. We prove that typical drawbacks of other numerical techniques, such as Finite Difference schemes, are bypassed exploiting the Mellin transform properties. An example involving discretely monitored barrier options is illustrated and the accuracy, efficiency and time consuming are discussed.
C65|Risk, ambiguity, and the exercise of employee stock options|We investigate the importance of ambiguity, or Knightian uncertainty, in executives’ stock option exercise decisions. We develop an empirical estimate of ambiguity and include it in regression models alongside the traditional measure of risk, equity volatility. We show that each variable has a significant effect on the timing of option exercises, with volatility causing executives to hold options longer to preserve option value, and ambiguity increasing the tendency for executives to exercise early.
C65|Global Dynamics in a Search and Matching Model of the Labor Market|We study global and local dynamics of a simple search and matching model of the labor market. We show that the model can be locally indeterminate or have no equilibrium at all, but only for parameterizations that are empirically implausible. In contrast to the local results, we show that the model exhibits chaotic and periodic dynamics for reasonable parameter values both in backward and forward time. In contrast to earlier work, we establish these results analytically without placing numerical restrictions on the parameters.
C65|Longevity, age-structure, and optimal schooling|The mechanism stating that longer life implies larger investment in human capital, is premised on the view that individual decision-making governs the relationship between longevity and education. This relationship is revisited here from the perspective of optimal period school life expectancy, obtained from the utility maximization of the whole population characterized by its age structure and its age-specific fertility and mortality. Realistic life tables such as model life tables are mandatory, because the age distribution of mortality matters, notably at infant and juvenile ages. Optimal period school life expectancy varies with life expectancy and fertility. The application to French historical data from 1806 to nowadays shows that the population age structure has indeed modified the relationship between longevity and optimal schooling.
C65|Improved Inference on Cointegrating Vectors in the Presence of a near Unit Root Using Adjusted Quantiles|It is well known that inference on the cointegrating relations in a vector autoregression (CVAR) is difficult in the presence of a near unit root. The test for a given cointegration vector can have rejection probabilities under the null, which vary from the nominal size to more than 90%. This paper formulates a CVAR model allowing for multiple near unit roots and analyses the asymptotic properties of the Gaussian maximum likelihood estimator. Then two critical value adjustments suggested by McCloskey (2017) for the test on the cointegrating relations are implemented for the model with a single near unit root, and it is found by simulation that they eliminate the serious size distortions, with a reasonable power for moderate values of the near unit root parameter. The findings are illustrated with an analysis of a number of different bivariate DGPs.
C65|The Qualitative Expectations Hypothesis: Model Ambiguity, Consistent Representations of Market Forecasts, and Sentiment|We introduce the Qualitative Expectations Hypothesis (QEH) as a new approach to modeling macroeconomic and financial outcomes. Building on John Muth's seminal insight underpinning the Rational Expectations Hypothesis (REH), QEH represents the market's forecasts to be consistent with the predictions of an economist's model. However, by assuming that outcomes lie within stochastic intervals, QEH, unlike REH, recognizes the ambiguity faced by an economist and market participants alike. Moreover, QEH leaves the model open to ambiguity by not specifying a mechanism determining specific values that outcomes take within these intervals. In order to examine a QEH model's empirical relevance, we formulate and estimate its statistical analog based on simulated data. We show that the proposed statistical model adequately represents an illustrative sample from the QEH model. We also illustrate how estimates of the statistical model's parameters can be used to assess the QEH model's qualitative implications.
C65|Comparing forecasts for tourism dynamics in Medellín, Colombia|Tourism is a topic of interest to many economies around the world, but it has received limited attention in Colombia. Knowing the periods of larger tourist inflows is important for predicting coverage in services for tourists. In this paper, we compare the estimation between classical and Bayesian regression in order to choose the best alternative to predict the number of tourist arrivals to Medellin. We also identify the most significant variables affecting the influx of tourists and the models providing better fit to the associated dynamics. According to our results, the Bayesian approach shows better estimates than the classic one. In addition, the variable month is significant to explain the demands for both Colombians and foreigners. The periods with the highest incidence of visits to the city are December-January and June-July, a pattern that repeats itself every year, which is crucial for planning hotel resources
C65|The grant element method of measuring the concessionality of loans and debt relief|The grant element is the “gift portion” of a financial transaction. The mathematical technique for arriving at a precise grant element percentage was first proposed by John Pincus of the RAND Corporation in 1963, and developed mathematically by Göran Ohlin of the Development Centre in 1966. Pincus also advocated expressing foreign aid in terms of its grant equivalent – i.e. the grant element expressed as a monetary value instead of a percentage. Grant element methodology was first used officially in 1969, in a target for softening the terms of aid. A grant element test was then introduced into the definition of official development assistance in 1972. Grant element methodology was subsequently applied to regulate the terms of export credits, to help assess the sustainability of developing country borrowing, and to calculate the level of debt relief and ensure comparability of effort in relevant Paris Club debt rescheduling operations. Central to grant element calculations is the selection of an appropriate discount rate to reflect financial market conditions. The present low interest rate environment raises challenges in this respect. This paper offers a layman’s introduction to the nature and mechanics of grant element methodology, and to the history of its application in practice.
C65|Approaches and Techniques to Validate Internal Model Results|The development of risk model for managing portfolio of financial institutions and insurance companies require both from the regulatory and management points of view a strong validation of the quality of the results provided by internal risk models. In Solvency II for instance, regulators ask for independent validation reports from companies who apply for the approval of their internal models. Unfortunately, the usual statistical techniques do not work for the validation of risk models as we lack enough data to significantly test the results of the models. We will certainly never have enough data to statistically estimate the significance of the VaR at a probability of 1 over 200 years, which is the risk measure required by Solvency II. Instead, we need to develop various strategies to test the reasonableness of the model. In this paper, we review various ways, management and regulators can gain confidence in the quality of models. It all starts by ensuring a good calibration of the risk models and the dependencies between the various risk drivers. Then applying stress tests to the model and various empirical analysis, in particular the probability integral transform, we build a full and credible framework to validate risk models.
C65|Point-in-Time PD Term Structure Models with Loan Credit Quality as a Component|Most point-in-time PD term structure models used in industry for stress testing and IFRS9 expected loss estimation apply only to macroeconomic scenarios. Loan level credit quality is not a factor in these models. In practice, credit profile at assessment time plays an important role in the performance of the loan during its lifetime. A forward-looking point-in-time PD term structure model with loan credit quality as a component is widely expected. In this paper, we propose a forward-looking point-in-time PD term structure model based on forward survival probability, extending the model proposed in [8] by including a loan specific credit quality score as a component. The model can be derived under the Merton model framework. Under this model, the forward survival probability for a forward term is driven by a loan credit quality score in addition to macroeconomic factors. Empirical results show, the inclusion of the loan specific credit score can significantly improve the performance of the model. The proposed approaches provide a tool for modeling point-in-time PD term structure in cases where loan credit profile is essential. The model can be implemented easily by using, for example, the SAS procedure PROC NLMIXED.
C65|On the predictability of economic structural change by the Poincaré-Bendixson theory|The three-sector framework (relating to agriculture, manufacturing, and services) is one of the major concepts for studying the long-run change of the economic structure. We discuss the system-theoretical classification of the structural change phenomenon and, in particular, the predictability of the structural change in the three-sector framework by the Poincaré-Bendixson theory. To do so, we compare the assumptions of the Poincaré-Bendixson theory to (a) the typical axioms of structural change modelling, (b) the empirical evidence on the geometrical properties of structural change trajectories, and (c) some methodological arguments referring to the laws of structural change. The results of this comparison support the assumption that the structural change phenomenon is representable by a dynamic system that is predictable by the Poincaré-Bendixson theory. Moreover, we discuss briefly the implications of this result for structural change modelling and prediction as well as topics for further research.
C65|Handbook of Game Theory and Industrial Organization, Volume I: Theory. An Introduction|We introduce here the first volume of Handbook of Game Theory and Industrial Organization: Theory, by L. C. Corchón and M. A. Marini (eds.), Edward Elgar, Cheltenam, UK and Northampton, MA, by describing its main aim and its basic structure.
C65|Sender-Receiver Games with Cooperation|"We consider generalized sender-receiver games in which the sender also has a decision to make, but this decision does not directly affect the receiver. We introduce specific perfect Bayesian equilibria, in which the players agree on a joint decision after that a message has been sent (\""talk and cooperate equilibrium\"", TCE). We establish that a TCE exists provided that the receiver has a \""uniform punishment decision\"" (UPD) against the sender."
C65|Infinite supermodularity and preferences|Abstract Chambers and Echenique (J Econ Theory 144:1004–1014, 2009) proved that preferences in a wide class cannot disentangle the usual economic assumptions of quasisupermodularity and supermodularity. This paper further studies the ordinal content of the much stronger assumption of infinite supermodularity in the same context. It is shown that weakly increasing binary relations on finite lattices fail to disentangle infinite supermodularity from quasisupermodularity and supermodularity. Moreover, for a complete preorder, the mild requirement of strict increasingness is shown to imply the existence of infinitely supermodular representations.
C65|Infinite Supermodularity and Preferences|This chapter studies the ordinal content of supermodularity on lattices. This chapter is a generalization of the famous study of binary relations over finite Boolean algebras obtained by Wong, Yao and Lingras. We study the implications of various types of supermodularity for preferences over finite lattices. We prove that preferences on a finite lattice merely respecting the lattice order cannot disentangle these usual economic assumptions of supermodularity and infinite supermodularity. More precisely, the existence of a supermodular representation is equivalent to the existence of an infinitely supermodular representation. In addition, the strict increasingness of a complete preorder on a finite lattice is equivalent to the existence of a strictly increasing and infinitely supermodular representation. For wide classes of binary relations, the ordinal contents of quasisupermodularity, supermodularity and infinite supermodularity are exactly the same. In the end, we extend our results from finite lattices to infinite lattices.
C65|A Standardized Treatment of Binary Similarity Measures with an Introduction to k-Vector Percentage Normalized Similarity|This paper attempts to codify a standard nomenclature for similarity measures based on recent literature and to advance the field of similarity measures through the introduction of non-binary similarity between more than two attribute vectors. The nomenclature standardization is accomplished through the integration of common terminology into non-binary similarity measures, and the refinement of the terminology with regard to k-vector binary and non-binary measures. This nomenclature standardization lays the groundwork for the introduction of k-vector percentage normalized similarity measures that follow the same fundamental form as pre-existing binary measures; a method not previously documented.Mathematics Subject Classification: 68; 91Keywords: Binary Similarity; Nonbinary Similarity; Nonparametric SimilarityÂ Testing; Multivector Similarity
C65|Equilibria in symmetric games: theory and applications|This article presents a new approach to analyze the equilibrium set of symmetric, differentiable games by separating between multiple symmetric equilibria and asymmetric equilibria. This separation allows to investigate, for example, how various parameter constellations affect the scope for multiple symmetric or asymmetric equilibria, or how the equilibrium set depends on the nature of the strategies. The approach is particularly helpful in applications because (1) it allows to reduce the complexity of the uniqueness-problem to a two-player game, (2) boundary conditions are less critical compared to standard procedures, and (3) best-replies need not be everywhere differentiable. The usefulness of the separation approach is illustrated with several examples, including an application to asymmetric games and to a two-dimensional price-information game.
C65|Behavioural types in public goods games: A re-analysis by hierarchical clutering|We re-analyse participant behaviour in standard economics experiments studying voluntary contributions to a public good. Previous approaches were based in part on a priori models of decision-making, such as maximising personal earnings, or reciprocating the behaviour of others. Many participants however do not conform to one of these models exactly, requiring ad hoc adjustments to the theoretical baselines to identify them as belonging to a given behavioural type. We construct a typology of behaviour based on a similarity measure between strategies using hierarchical clustering analysis. We identify four clearly distinct behavioural types which together account for over 90% of participants in six experimental studies. The resulting type classification distinguishes behaviour across groups more consistently than previous approaches.
C65|Hyperbolic grids and discrete random graphs|We present an efficient algorithm for computing distances in hyperbolic grids. We apply this algorithm to work efficiently with a discrete variant of the hyperbolic random graph model. This model is gaining popularity in the analysis of scale-free networks, which are ubiquitous in many fields, from social network analysis to biology. We present experimental results conducted on real world networks.
C65|The algebraic approach to some ranking problems|The problem of ranking a set of elements, namely giving a “rank” to the elements of the set, may arise in very different contexts and may be handled in some possible different ways, depending on the ways these elements are set in competition the ones against the others. For example there are contexts in which we deal with an even paired competition, in the sense the pairings are evenly matched: if we think for example of a national soccer championship, each team is paired with every other team the same number of times. Sometimes we may deal with an uneven paired competition: think for example of the UEFA Champions League, in which the pairings are not fully covered, but just some pairings are set, by means of a random selection process for example. Mathematically based ranking schemes can be used and may show interesting connections between the ranking problems and classical theoretical results. In this working paper we first show how a linear scheme in the ranking process directly takes to some fundamental Linear Algebra concepts and results, mainly the eigenvalues and eigenvectors of linear transformations and Perron–Frobenius theorem. We apply also the linear ranking model to a numerical simulation taking the data from the Italian soccer championship 2015-2016. We finally point out some interesting differences in the final ranking by comparing the actual placements of the teams at the end of the contest with the mathematical scores provided to teams by the theoretical model.
C65|Is there any link between level of instruction and financial choices? A study on a Generation Y-based survey|Generally investors use heuristics in their process of decision-making with the aim of finding short-cuts and simplified roads to quite sophisticated answers. We conducted a survey on about 250 young people (18-27 years old) concerning their financial literacy and economic choices, given an education level which is predominantly very high (73% enrolled in a bachelor degree, 80% took part to at least some basic finance or economics courses). The survey was designed to study the influence of financial-economic literacy on the flaws occurring in financial decisions of young people (the so called generation Y), mainly with reference to biases, overconfidence, framing. The results of the survey give an insight into the behavior of a new and educated generation in typical economic decision frameworks, which could be a useful tool for stakeholders. In fact, being aware of the psychological component of the financial decision is a key factor to better understand and manage risk.
C65|PAMS.py: a GAMS-like Modeling System based on Python and SAGE|This paper presents an external module for the Python programming language and for the SAGE open source mathematical software, which allows the realization of models based on constrained optimization or non-linear systems. The module, which is freely available for download, allows describing the structure of a model using a syntax similar to that of popular modeling systems like GAMS, AIMMS or GEMPACK; in particular by allowing the automatic replication of equations, variable and parameter definitions on the basis of some specified sets. Many applied models, especially in economics, are based on non-linear constrained optimization and system solving. Years ago, the standard way to realize simulations for this kind of models involved writing your own code, using a programming language like FORTRAN, possibly making calls to external math library subroutines. Subsequently, the introduction of packages like Matlab, GAUSS, Octave and many others have made this process somewhat simpler, because vectors and matrices could be treated as single variables, and complex numerical tasks could be performed with a single instruction. However, one fundamental problem remained: the model code still looked much different from the more familiar mathematical notation one would have used in a paper. Therefore, checking and modifying the model code written by another researcher was a rather daunting task. To address this issue, GAMS (General Algebraic Modeling System) was developed by Alexander Meeraus and many of his collaborators at the World Bank in Washington D.C., since the late '70s (Meeraus, 1983). The main purpose of GAMS was (and still is) “providing a high-level language for the compact representation of large and complex models” and “permitting model descriptions that are independent of solution algorithms”. This paper presents an external module for the Python programming language and for the SAGE open source mathematical software, based on the same principles underlying GAMS and other similar packages. The purpose is providing a tool that takes the best of both worlds: the simplicity and clarity of GAMS-like systems combined with the flexibility and power of Python and SAGE. The paper is structured as follows. In the next section, some key characteristics of GAMS and other popular Modeling Systems are reviewed in some detail. Section 3 introduces the Python programming language and the closely related SAGE system for symbolic and numerical computation. Section 4 illustrates the basics of the PAMS.py syntax, and in Section 5 a practical example is provided. A discussion follows in Section 6 and a final section concludes. The paper presents an external module for programs written with the Python language and for the SAGE mathematical software. This module allows the definition and solution of non-linear systems and optimization problems, described in a way very similar to GAMS and programs alike. The key common characteristic of PAMS.py and GAMS is the automatic indexing of parameters, equations and variables. Since many elements of this kind can be defined with only one instruction (as one would normally do, for instance when the model is illustrated in a scientific paper), understanding how the model works directly by reading the program code is normally quite straightforward. The latter feature turns out to be particularly critical when the model code needs to be understood and manipulated by others, which may occur either in a team work or when replication and validation of some results is called for.
C65|Trust model for social network using singular value decomposition|For effective interactions to take place in a social network, trust is important. We model trust of agents using the peer to peer reputation ratings in the network that forms a real valued matrix. Singular value decomposition discounts the reputation ratings to estimate the trust levels as trust is the subjective probability of future expectations based on current reputation ratings. Reputation and trust are closely related and singular value decomposition can estimate trust using the real valued matrix of the reputation ratings of the agents in the network. Singular value decomposition is an ideal technique in error elimination when estimating trust from reputation ratings. Reputation estimation of trust is optimal at the discounting of 20 %.
C65|The Linear Systems Approach To Linear Rational Expectations Models|This paper considers linear rational expectations models from the linear systems point of view. Using a generalization of the Wiener-Hopf factorization, the linear systems approach is able to furnish very simple conditions for existence and uniqueness of both particular and generic linear rational expectations models. As applications of this approach, the paper provides results for existence of sequential solutions to block triangular systems and provides an exhaustive description of stationary and unit root solutions, including a generalization of Granger's representation theorem. In addition, the paper provides an innovative numerical solution to the Wiener-Hopf factorization and its generalization.<br><small>(This abstract was borrowed from another version of this item.)</small>
C65|Distributional comparative statics with heterogeneous agents|"We propose a formal way to systematically study the differential effects of exogenous shocks in economic models with heterogeneous agents. Our setting applies to models that can be rephrased as ""competition for market shares"" in a broad sense. We show that even in presence of any number of arbitrarily heterogeneous agents, a single recursion relation characterizes the distributional pattern of equilibrium market shares and related measures. We identify the general conditions under which the market share function rotates, thereby either causing more or less equality among the agents. Our setting highlights the exceptional rule that power functions play for the distributional effects. We apply our method across economic models, including examples from monopolistic competition, discrete choice, partial and general equilibrium theory and contest theory."
C65|A fuzzy multi-criteria approach for assessing sustainability of Italian farms| This work defines a procedure to assess the socio-economic and environmental sustainability of agricultural systems with a particular attention to conventional and organic farming. Firstly, a mathematical programming model calculates the different multi-dimensional outcomes of Italian farms depending on various levels of prices affecting organic products. Those outcomes are the input data for a fuzzy multi-criteria analysis, which processes the various criteria, takes into account different sets of weights for criteria, and, by a ranking of price scenarios, identifies the most desirable and the least desirable level of prices for five groups of regions. The method adopted proves to be sensitive to geographical location and different perspectives. In particular, when the farmers’ set of weights is adopted, the highest level of prices represents the most desirable scenario in all groups of regions. On the other side, in all other sets of weights, the lowest level of prices seems to be the most preferable scenario for North-Western regions.
C65|On the use of agricultural system models for exploring technological innovations across scales in Africa: A critical review| The major challenge of the 21st century is to achieve food security under, roughly, a doubling in food demand by 2050 compared to present, and producing the additional food under marked shifts in climatic risks and with environmentally sound farming practices. Sustainable intensification of agricultural production is required that meets the dual goal of improved environmental sustainability and economic efficiency. Ex ante evaluation of technological innovations to support agricultural production and food security taking into account the various future risks can substantially contribute to achieve this. Here we perceive technological innovations as new or improved agro-technologies and –management practices, such as new breeds, integrated soil fertility practices or labour-saving technologies meeting the goals of sustainable intensification. In this report we present results from three systematic reviews: one on the use of biophysical modelling, the second and third on the use of bio-economic modelling at farm scale and agro-economic modelling at higher aggregation levels, for ex ante evaluation of the effects of (agro ) Technological Innovations (ag-TIs) on sustainable agriculture and food security indicators. To this end, we searched the SCOPUS database for journal articles published between 1996 and 2015. We considered modelling studies at different spatial scales with particular attention to local to national scale studies for the twelve PARI focal countries in Africa . But we also included studies for all other African countries as well as a few studies at supra-national/continental scale. Both, “quick wins” as well as long term benefits from ag-TIs were of interest. The various ag-TIs were furthermore grouped into four classes: (1) water/soil moisture (2) soil nutrients/conservation (3) crop/cropping system, (4) other ag-TIs or (5) combinations of 1 to 4. For each paper, we tried to identify the primary ag-TI analysed, and if there was equal emphasis to more than one, we classified them as combinations. It should be borne in mind that there is some subjectivity in classifying the papers in this way. Results. After various steps of refining “search strings”, screening on relevance and supplementing databases from additional sources, we found 140 relevant biophysical modelling studies, whereby coverage of sub-regions and ag-TIs varied markedly. Most studies were found for East and West Africa, followed by Southern Africa; hardly anything was found for Northern and Middle Africa . A similar pattern appeared for the integrated agro-economic modelling studies at farm scale, for which we found 40 relevant ones. Agro-economic modelling studies at higher aggregation levels showed a somewhat different pattern – and more generally contained little detail on technological innovations. Regarding the share of different primary agro-technologies explored in the biophysical studies we found 45 on crop management, 35 on combined agro-technologies, 31 on soil nutrient management and conservation, 23 on water/soil moisture management, and 6 on other technologies. We found similar shares among the various agro-technology groups for the integrated agro-economic modelling studies at farm scale. Looking at the outcomes from ex ante evaluations we found that many studies are (mostly) positive on effects of single and “conventional” ag-Tis. The majority of biophysical studies is performed at “field scale” and focuses on the effects on productivity (sometimes yield stability); many of these studies were performed in climate variability and change /adaptation research context. Most agro-economic modelling studies that look specifically at ex ante evaluations of ag-TIs are performed at farm or regional (sub-national) scales. While the number of biophysically oriented studies has grown exponentially over the considered period 1996-2015, this is not the case for the agro-economic modelling studies. Looking in more detail at the twelve focal countries of PARI (=Programme of Accompanying Research on Agricultural Innovations) we also find an unbalanced distribution, with most studies found in Kenya, Ethiopia, Mali and Ghana (biophysical modelling studies), and respectively in Kenya and Uganda (agro-economic modelling studies), whereas nothing or little was found for both types of studies in Togo, Zambia and Nigeria. Very few of the biophysically-oriented studies include other information than effects on crop yields, and there are few studies for both biophysical and agro-economic modelling that comprise multi-scale or higher scale analyses; if multi-scale, there are more studies that scale up from field/farm to regional/sub-national level than from field/farm to nation scale or beyond. There is definitely a need to overcome the lack of meaningful integrated multi-scale modelling along the lines proposed in chapters 5-6 of this report. Moreover, less than half of all integrated /agro-economic modelling studies at farm scale explicitly address risk – another clear shortcoming, which requires attention by the research community. A more general conclusion is that there is no application yet of true transdisciplinary research approaches in practice. Hence, there is need for participatory, collaborative (cross-sectoral) and combined modelling approaches with adequate stakeholder involvement throughout the research process. In this respect, some lessons might be learned from pioneering work conducted in Asia and Europe.
C65|The role of consumer networks in firmsÂ’ multi-characteristics competition and market share inequality|We develop a location analysis spatial model of firmsÂ’ competition in multi-characteristics space, where consumersÂ’ opinions about the firmsÂ’ products are distributed on multilayered networks. Firms do not compete on price but only on location upon the productsÂ’ multi-characteristics space, and they aim to attract the maximum number of consumers. Boundedly rational consumers have distinct ideal points/tastes over the possible available firm locations but, crucially, they are affected by the opinions of their neighbors. Proposing a dynamic agent-based analysis on firmsÂ’ location choice we characterize multi-dimensional product differentiation competition as adaptive learning by firmsÂ’ managers and we argue that such a complex systems approach advances the analysis in alternative ways, beyond game-theoretic calculations.
C65|Alternative versions of the global competitive industrial performance ranking constructed by methods from social choice theory|The Competitive Industrial Performance index (developed by experts of the UNIDO) is designed as a measure of national competitiveness. Index is an aggregate of eight observable variables, representing different dimensions of competitive industrial performance. Instead of using a cardinal aggregation function, what CIP’s authors do, it is proposed to apply ordinal ranking methods borrowed from social choice: either direct ranking methods based on the majority relation (e.g. the Copeland rule, the Markovian method) or a multistage procedure of selection and exclusion of the best alternatives, as determined by a majority relation-based social choice solution concept (tournament solution), such as the uncovered set and the minimal externally stable set. The same method of binary comparisons based on the majority rule is used to analyze rank correlations. It is demonstrated that the ranking is robust but some of the new aggregate rankings represent the set of criteria better than the original ranking based on the CIP.
C65|La exclusión en los tiempos del auge: El caso de Cartagena|Cartagena atraviesa por un momento de auge económico importante debido al dinamismo de sectores como la industria, el turismo y la actividad portuaria. En este trabajo se analizan la exclusión social y la vulnerabilidad ambiental en Cartagena en el escenario de crecimiento económico actual. Se describen los principales indicadores socioeconómicos de la ciudad, comparándola con las demás ciudades principales de Colombia y sus áreas metropolitanas, lo que refleja un rezago relativo de Cartagena en materia de pobreza y cobertura de servicios públicos básicos. En términos ambientales, la vulnerabilidad es mayor en las zonas donde habita la población de menos ingresos, que coincide con los barrios con mayor proporción de afrodescendientes. Empleando un análisis por componentes principales, se construye un indicador de exclusión social en que Cartagena ocupa el primer lugar entre las trece principales ciudades. Este resultado refleja la necesidad de aprovechar la coyuntura actual y diseñar políticas que permitan que la ciudad reduzca la exclusión social al tiempo que se prepare mejor para afrontar el cambio climático.
C65|Transboundary Capital and Pollution Flows and the Emergence of Regional Inequalities|We seek to explain the emergence of spatial heterogeneity regarding development and pollution on the basis of interactions associated with the movement of capital and polluting activities from one economy to another. We use a simple dynamical model describing capital accumulation along the lines of a ?fixed-savings-ratio Solow-type model capable of producing endogenous growth and convergence behavior, and pollution accumulation in each country with pollution diffusion between countries or regions. The basic mechanism underlying the movements of capital across space is the quest for locations where the marginal productivity of capital is relatively higher than the productivity at the location of origin. The notion that capital moves to locations of relatively higher productivity but not necessarily from locations of high concentration to locations of low concentration, does not face difficulties associated with the Lucas paradox. We show that, for a wide range of capital and pollution rates of flow, spatial heterogeneity emerges even between two economies with identical fundamental structures. These results can be interpreted as suggesting that the neoclassical convergence hypothesis might not hold under differential rates of fl?ow of capital and polluting activities among countries of the same fundamental structure.
C65|Transboundary Capital and Pollution Flows and the Emergence of Regional Inequalities| We seek to explain the emergence of spatial heterogeneity regarding development and pollution on the basis of interactions associated with the movement of capital and polluting activities from one economy to another. We use a simple dynamical model describing capital accumulation along the lines of a fixed-savings-ratio Solow-type model capable of producing endogenous growth and convergence behavior, and pollution accumulation in each country with pollution diffusion between countries or regions. The basic mechanism underlying the movements of capital across space is the quest for locations where the marginal productivity of capital is relatively higher than the productivity at the location of origin. The notion that capital moves to locations of relatively higher productivity but not necessarily from locations of high concentration to locations of low concentration, does not face difficulties associated with the Lucas paradox. We show that, for a wide range of capital and pollution rates of flow, spatial heterogeneity emerges even between two economies with identical fundamental structures. These results can be interpreted as suggesting that the neoclassical convergence hypothesis might not hold under differential rates of flow of capital and polluting activities among countries of the same fundamental structure.
C65|The Laffer Effect in a Product's Market in the Case of a Specific Tax|Traditionally, the Laffer effect has been discussed in context of macroeconomic endogenous growth models or in the situation of labor market whether a tax cut on wages would persuade people to work more and also increase income tax revenues of the government. In this paper, we are firstly interested in the Laffer effect in a single product¡¯s market rather than a general macroeconomic situation. Secondly, we provide a general formula mathematically in that particular commodity market for the optimal tax amount of the government in the case of a specific tax using non-linear demand and supply curves, which is the most possible extension. It turns out that the optimal tax amount depends on after-tax demand elasticity and not on before-tax elasticity as it is commonly assumed in the economics literature and also on after-tax demand price and on consumers¡¯ share of burden of tax. Some important novel concepts such as the proportional increase in equilibrium price relative to initial equilibrium price ... are defined and discussed. Finally, we believe that any government should consider the issues discussed in this paper before taking a fiscal step in a micro market!
C65|Transition to Clean Technology|We develop an endogenous growth model in which clean and dirty technologies compete in production. Research can be directed to either technology. If dirty technologies are more advanced, the transition to clean technology can be difficult. Carbon taxes and research subsidies may encourage production and innovation in clean technologies, though the transition will typically be slow. We estimate the model using microdata from the US energy sector. We then characterize the optimal policy path that heavily relies on both subsidies and taxes. Finally, we evaluate various alternative policies. Relying only on carbon taxes or delaying intervention has significant welfare costs.
C65|Advantages of an Ellipse when Modeling Leisure Utility|Abstract This paper characterizes a specification for the utility of leisure that is based on the general equation for an ellipse. We show that this functional form has multiple benefits. The elliptical utility function provides Inada conditions at both the upper-bound and lower-bound constraints on labor supply, which is not the case for the two most common alternative functions. The presence of these two Inada conditions in the elliptical utility of leisure specification speeds up the computation by a factor between three and six times. We further show that the elliptical utility of leisure function is a close approximation to the constant relative risk aversion and constant Frisch elasticity functions in terms of marginal utilities, microeconomic outcomes in a life cycle model, and macroeconomic outcomes in a simple real business cycle model.
C65|A Result on Integral Functionals with Infinitely Many Constraints|A classic paper of Borwein/Lewis (1991) studies optimisation problems over L^p_+ with finitely many linear equality constraints, given by scalar products with functions from L^q. One key result shows that if some x in L^p_+ satisfies the constraints and if the constraint functions are pseudo-Haar, the constraints can also be realised by another function y in the interior of L^\infty_+ . We establish an analogue of this result in a setting with infinitely many, measurably parametrised constraints, and we briefly sketch an application in arbitrage theory.
C65|Basic Analysis of The Hex Game|The objective of this paper is to analyze the game of Hex through the use of Game Theory and Graph Theory. Hex is a game where each player must connect two opposite sides by a continuous path of pieces in a hexagonal grid within a rhombus-shaped board. The size of the board is usually 14×14 but the game can be found in a wide range of sizes such as 11×11 and 17×17. Although the strategy is not as deep as in chess, it is still complex, and just like chess, Hex is a no-chance game, and thus it is a perfect candidate to be examined using some formal tools. This game has some interesting features that make it more interesting; unlike chess, the game will never end in a tie, second, the number of possible movements is finite and third, the second player will always win, through this paper, we will show some of these features.
C65|La mixtura de conjuntos difusos y sus aplicaciones en la administración|Como es bien sabido, las operaciones con conjuntos difusos se basan en cálculos usando los valores de funciones miembros o el grado de pertenencia de los elementos. En este trabajo, una nueva operación con conjuntos difusos se introduce y se denomina mezcla de conjuntos difusos, el cual opera no sólo con los valores antes mencionados, sino que también se basa en la mezcla de elementos de varios conjuntos difusos. A través de dos ejemplos desde la perspectiva de la gestión, una relacionada con el diagnóstico de habilidades de trabajo y otra relativa a la prospectiva de desarrollo local, se muestra la aplicación de la contribución teórica de la investigación, concluyendo con la demostración de su utilidad para la ciencia en la práctica. ****** As is well known, operations with fuzzy sets are based on calculations using the values of membership or the degree belonging to the elements. In this paper, we will present a new operation with fuzzy sets, named mixture of fuzzy sets, which operates not only with the values mentioned above, but also with the mixture of elements of several fuzzy sets. Moreover, we will bring up two examples from the management perspective, in which one example is related to the job skills diagnosis, while the other to the local development prospective. We will also present the implementation of the theoretical contribution of our research and its utility, in practice, for science.
C65|A theorem on aggregating classifications|Suppose that a group of individuals must classify objects into three or more categories, and does so by aggregating the individual classifications. We show that if the classifications, both individual and collective, are required to put at least one object in each category, then no aggregation rule can satisfy a unanimity and an independence condition without being dictatorial. This impossibility theorem extends a result that Kasher and Rubinstein (1997) proved for two categories and complements another that Dokow and Holzman (2010) obtained for three or more categories under the condition that classifications put at most one object in each category. The paper discusses an interpretation of its result both in terms of Kasher and Rubinstein’s group identification problem and in terms of Dokow and Holzman’s task assignment problem.
C65|Solving generalized multivariate linear rational expectations models|We generalize the linear rational expectations solution method of Whiteman (1983) to the multivariate case. This facilitates the use of a generic exogenous driving process that must only satisfy covariance stationarity. Multivariate cross-equation restrictions linking the Wold representation of the exogenous process to the endogenous variables of the rational expectations model are obtained. We argue that this approach offers important insights into rational expectations models. We give two examples in the paper—an asset pricing model with incomplete information and a monetary model with observationally equivalent monetary-fiscal policy interactions. We relate our solution methodology to other popular approaches to solving multivariate linear rational expectations models, and provide user-friendly code that executes our approach.
C65|Predictability dynamics of Islamic and conventional equity markets|This study undertakes the challenging task of comparing the weak form efficiency of conventional and Islamic equity markets. Using 12 different Dow Jones indexes that cover 16 years of daily data, we compare the time-varying non-linear predictability patterns of conventional market indexes and their Islamic counterparts at country and continent level by using permutation entropy. Accordingly, we find that all indexes in our analysis have different degrees of time-varying predictability and all conventional markets are found to be more efficient compared to their Islamic counterparts. However, in some of the cases, this difference in efficiency is almost indistinguishable. Our findings reveal that compared to their conventional counterparts, Islamic markets do not necessarily need to carry a more deterministic or predictable structure since efficiency in these markets depends mostly on liquidity, market quality, institutional characteristics and the country/continent specific investment behavior.
C65|Fine structure of the price–demand relationship in the electricity market: Multi-scale correlation analysis|In this research we investigate the problems of dynamic relationship between electricity price and demand over different time scales for two largest price zones of the Russian wholesale electricity market. We use multi-scale correlation analysis based on a modified method of time-dependent intrinsic correlation and the complete ensemble empirical mode decomposition with adaptive noise for this purpose. Three hypotheses on the type and strength of correlations in the short-, medium- and long-runs were tested. It is shown that price zones significantly differ in internal price–demand correlation structure over the comparable time scales, and not each of the theoretically formulated hypotheses is true for each of them. We can conclude that the answer to the question whether it is necessary to take into account the influence of demand-side on electricity spot prices over different time scales, is significantly dependent on the structure of electricity generation and consumption on the corresponding market.
C65|Counterparty risk for CDS: Default clustering effects|We derive a closed-form expression for the bilateral credit valuation adjustment of a credit default swap in presence of simultaneous defaults. We develop our analysis under a default intensity model specified by a class of three-dimensional subordinators, allowing for default dependence through common risk factors. We performance a suitable decomposition of the bilateral price into debit and credit valuation adjustment components. Those components do not have a symmetric impact on the price because of the joint event occurrences. Our analysis indicates that simultaneous defaults have material impact on the size and directionality of the adjustments. Our findings suggest policymakers to consider default clustering when designing counterparty valuation procedures, especially during periods of financial distress.
C65|Can we neutralize social preference in experimental games?|We propose an experimental method whose purpose is to remove social concerns in games. The core idea is to adapt the binary-lottery incentive scheme, so that an individual payoff is a probability to see one's preferred social allocation implemented. For a large class of social preference models, the method induces payoffs in the game that are in line with subjects’ (social) preferences. We test the method in several popular experimental games, contrasting behaviors with and without our methodology. Our results suggest that a substantial part of the difference between predictions based on selfishness and observed behaviors seems driven by such preferences, since our method does induce more “selfish” behaviors. But they also indicate that a considerable share is left unexplained, perhaps giving weight to alternative explanations or other types of social concerns.
C65|Limits to rational learning|A long-standing open question raised in the seminal paper of Kalai and Lehrer (1993) is whether or not the play of a repeated game, in the rational learning model introduced there, must eventually resemble the play of exact equilibria, and not just the play of approximate equilibria as demonstrated there. This paper shows that play may remain distant – in fact, mutually singular – from the play of any equilibrium of the repeated game. We further show that the same inaccessibility holds in Bayesian games, where the play of a Bayesian equilibrium may continue to remain distant from the play of any equilibrium of the true game.
C65|Transport user benefits calculation with the “Rule of a Half” for travel demand models with constraints|The importance of user benefits in transport projects assessments is well-known by transport planners and economists. Generally they have the greatest impact on the result of cost-benefit analysis. It is common practice to adopt the consumer surplus measure for calculating transport user benefits. Normally the well-known “Rule of a Half”, as a practical approximation for the integral of the demand curve, is used to determine the change of consumer surplus. Changes in travel demand and consumer surplus are influenced by all modeled changing variables, which primarily comprise the generalized costs. However, travel demand models with multiple constraints additionally contain variable “shadow prices”, added to the generalized costs. Such models are often used for travel demand modeling of large-scale areas. The most discussed and well-known model in the field of transport modeling is the doubly constrained gravity model. Beside this model with inelastic constraints, there are also more flexible models with elastic constraints. In this paper we enter into the question of whether applying the Rule of a Half with regard only to the generalized costs and neglecting the shadow prices is valid in the case of travel demand models with multiple constraints. For this purpose, a theoretical analysis in this paper provides a mathematical proof.
C65|A geometrical approach to structural change modelling|A large body of research has studied structural change, particularly the dynamics of labour allocation in multi-sector growth models, based on different economic theories. We present a meta-model of structural change that combines information from empirical evidence (stylized facts) with information on the geometrical properties of typical trajectories studied in structural change theory. In particular, our approach is based on three facts: (1) structural change in three-sector models is defined on a 2-simplex; (2) the trajectory of past structural change partitions the 2-simplex into economically interpretable sections; (3) the typical properties of structural change trajectories (e.g. non-self-intersection) prohibit some movements from one section to another. Jointly, these facts imply that structural change is path-dependent and that the number of feasible structural change scenarios can be reduced significantly. While we focus on labour allocation dynamics, our approach can be applied to other topics (e.g. income distribution dynamics).
C65|Macroeconomic benefits of farmer-pastoralist peace in Nigeria’s Middle Belt: An input-output analysis approach|This article reports on the potential macroeconomic benefits of peace stemming from a reduction in farmer-pastoralist violence in four Middle Belt states of Nigeria (Benue, Kaduna, Nasarawa, and Plateau). Farmers and pastoralists routinely clash over access to farmland, grazing areas, stock routes, and water points for both animals and households. Farmer-pastoralist violence in these states is a relatively low-intensity form of conflict, but it is regionally widespread and chronic, and its incidence is arguably increasing. Using estimates of potential income benefits of peace at the household-level derived from a related study, we herein derive macroeconomic benefits via an input-output model of the Nigerian economy. We estimate these benefits to amount to around 2.8 percent of the nominal Nigerian GDP (or around 0.8 percent of the total Nigerian GDP, inclusive of the informal sector), representing a major macroeconomic opportunity. We break out these benefits by sector, showing that the sectors that stand to gain most from peace are the crop production, food and beverage, livestock, and chemical and petroleum industries.
C65|Fuzzy Similarity And Counterfactuals In The Assessment Of Default Risk: The Eurozone Crisis And The Argentinean Solution|We present an approach, based on the notion of fuzzy similarity, to the comparison of economies under default risk. We run a counterfactual analysis of the perspectives of the most troubled countries in the Eurozone compared with Argentina in 2001. This allows us to assess the possibility that these economies will end up following the path of Argentina.
C65|Searching for information|This paper provides a search-based information acquisition framework using an urn model with an asymptotic approach. The underlying intuition of the model is simple: when the scope of information search is more limited, marginal search efforts produce less useful information due to redundancy, but commonality of information among different agents increases. Consequently, limited information searchability induces a trade-off between an information source's precision and its commonality. In a “beauty contest” game with endogenous information acquisition, this precision-commonality trade-off generates non-fundamental volatility through the channel of information acquisition.
C65|Multiproduct Model Decomposition of Components of Russian GDP|This paper proposes a method for a multiproduct model decomposition of GDP components by expenditure which allows the use of several different price indices in the same model. The decomposition does not link the products to imports or exports, therefore, it imposes no restrictions on the behaviour of these series and their deflators. The theoretical reasoning, the estimation methodology and the estimation results for Russian GDP data are presented. A method of the decomposition of changes in inventories is also presented.
C65|Approximating Innovation Potential With Neurofuzzy Robust Model / Aproximación Al Potencial Innovador Con Un Modelo Robusto De Neuro-Fuzzy|In a remarkably short time, economic globalisation has changed the world’s economic order, bringing new challenges and opportunities to SMEs. These processes pushed the need to measure innovation capability, which has become a crucial issue for today’s economic and political decision makers. Companies cannot compete in this new environment unless they become more innovative and respond more effectively to consumers’ needs and preferences – as mentioned in the EU’s innovation strategy. Decision makers cannot make accurate and efficient decisions without knowing the capability for innovation of companies in a sector or a region. This need is forcing economists to develop an integrated, unified and complete method of measuring, approximating and even forecasting the innovation performance not only on a macro but also a micro level. In this recent article a critical analysis of the literature on innovation potential approximation and prediction is given, showing their weaknesses and a possible alternative that eliminates the limitations and disadvantages of classical measuring and predictive methods. / En un plazo increíblemente corto, la globalización económica ha cambiado el orden de la economía, creando nuevos retos y oportunidades a las pequeñas y medianas empresas. Por ello se esta dando la necesidad de crear maneras de medir capacidad de innovación que resulta fundamental para quien debe tomar decisiones politico-economicas. Las compañías no pueden competir en este nuevo entorno a no ser que sean mas innovadoras y respondan de manera más eficiente a las necesidades y preferencias del consumidor-como de hecho se ha mencionado en la Estrategia de Innovación de la UE. Las decisiones no pueden ser tomadas de manera eficiente y adecuada sin el conocimiento de la capacidad de innovación de compañías de un determinada región y/o sector. Esta necesidad está forzando a los economistas a desarrollar un método completo integrado y unificador de medir, aproximar e incluso predecir el rendimiento innovativo tanto a micro como a macro niveles. En este reciente articulo se ha hecho un análisis critico de la literatura que trata sobre aproximaciones y/o predicciones del potencial innovador, mostrando sus defectos y posibles alternativas que eliminarían las limitaciones y desventajas de las mediciones clásicas y métodos predictivos.
C65|An improvement to Jensen’s inequality and its application to mating market clearing when paternity is uncertain|Jensen’s inequality can be tightened in the context of discrete random variables. The suggested new inequality coincides with Jensen’s inequality only when the expected value of the random variable is an integer, in all other cases the inequality is tighter than Jensen’s. This improvement is used to derive a welfare result in the context of a mating market where male agents are exposed to paternal uncertainty. It is shown that social welfare is highest when paternity is certain, a result that cannot be obtained when relying on Jensen’s inequality.
C65|Production sharing, demand spillovers and CO2 emissions : the case of Chinese regions in GVCs|This study adopts the perspective of demand spillovers to provide new insights regarding Chinese domestic-regions' production position in global value chains and their associated CO2 emissions. To this end, we constructed a new type of World Input-Output Database in which China's domestic interregional input-output table for 2007 is endogenously embedded. Then, the pattern of China's regional demand spillovers across both domestic regions and countries are revealed by employing this new database. These results were further connected to endowments theory, which help to make sense of the empirical results. It is found that China's regions locate relatively upstream in GVCs, and had CO2 emissions in net exports, which were entirely predicted by the environmental extended HOV model. Our study points to micro policy instruments to combat climate change, for example, the tax reform for energy inputs that helps to change the production pattern thus has impact on trade pattern and so forth.
C65|Quadratic minimization with portfolio and terminal wealth constraints|We address a problem of stochastic optimal control drawn from the area of mathematical finance. The goal is to minimize the expected value of a general quadratic loss function of the wealth at close of trade when there is a specified convex constraint on the portfolio over the trading interval, together with a specified almost-sure lower-bound on the wealth at close of trade. We use a variational approach of Rockafellar which leads naturally to an appropriate vector space of dual variables, a dual functional on the space of dual variables such that the dual problem of maximizing the dual functional is guaranteed to have a solution (i.e. a Lagrange multiplier) when a simple and natural Slater condition holds for the terminal wealth constraint, and obtain necessary and sufficient conditions for optimality of a candidate wealth process. The dual variables are pairs, each comprising an Itô process paired with a member of the adjoint of the space of essentially bounded random variables measurable with respect to the event $$\sigma $$ σ -algebra at close of trade. The necessary and sufficient conditions are used to construct an optimal portfolio in terms of the Lagrange multiplier. The dual problem simplifies to maximization of a concave function over the real line when the portfolio is unconstrained but the terminal wealth constraint is maintained. Copyright Springer-Verlag Berlin Heidelberg 2015
C65|Bounds for path-dependent options|We develop new semiparametric bounds on the expected payoffs and prices of European call options and a wide range of path-dependent contingent claims. We first focus on the trinomial financial market model in which, as is well-known, an exact calculation of derivative prices based on no-arbitrage arguments is impossible. We show that the expected payoff of a European call option in the trinomial model with martingale-difference log-returns is bounded from above by the expected payoff of a call option written on an asset with i.i.d. symmetric two-valued log-returns. We further show that the expected payoff of a European call option in the multiperiod trinomial option pricing model is bounded by the expected payoff of a call option in the two-period model with a log-normal asset price. We also obtain bounds on the possible prices of call options in the (incomplete) trinomial model in terms of the parameters of the asset’s distribution. Similar bounds also hold for many other contingent claims in the trinomial option pricing model, including those with an arbitrary convex increasing payoff function as well as for path-dependent ones such as Asian options. We further obtain a wide range of new semiparametric moment bounds on the expected payoffs and prices of path-dependent Asian options with an arbitrary distribution of the underlying asset’s price. These results are based on recently obtained sharp moment inequalities for sums of multilinear forms and U-statistics and provide their first financial and economic applications in the literature. Similar bounds also hold for many other path-dependent contingent claims. Copyright Springer-Verlag Berlin Heidelberg 2015
C65|A Model of Stock Manipulation Ramping Tricks|Ramping tricks of trade-based stock manipulation have evolved greatly in the fight with stricter market regulation, and can be extremely complicated nowadays. Despite the rigidity and soundness, theoretical models proposed in extant literature can hardly be applied directly to real market data, due to their assumptions being far away from reality. On the other hand, empirical studies of ramping manipulation still lack guidance and support from theories that can better reflect ramping details in practice. This paper addresses this gap by constructing a theoretical model that is closely linked to practical detection, in the framework of behavioral finance. New insights into concrete ramping manipulation tricks are also contributed to the literature. The potential of the model for manipulation detection is demonstrated by applying it to the two most infamous manipulation cases in the history of Chinese stock market. Copyright Springer Science+Business Media New York 2015
C65|Multiscale Analysis of the Liquidity Effect in the UK Economy|This study examines the existence of a liquidity effect in the UK economy over different time-scales. This analysis draws from the liquidity preference framework, an approach to interest rate determination, and uses wavelet multiscale analysis in the context of a standardised regression model. The modelling framework is similar to the one proposed by Cochrane ( 1989 ), however, instead of using a band-pass filter the model’s variables are analyzed with a wavelet multiresolution analysis which enables a more accurate estimation of the liquidity effect. The results suggest that, in short-term cycles, interest rates are influenced primarily by changes in the money supply (i.e., the liquidity effect). In medium- and long-term cycles, the liquidity effect becomes less important and interest rates are found to be more sensitive to income and price effects. Copyright Springer Science+Business Media New York 2015
C65|Finding similar places using the observation-to-generalization place model|In this article, a novel observation-to-generalization place model is proposed. It is shown how this model can be used to formally define the problem of finding geographically similar places. The observation-to-generalization model differentiates between observations of phenomena in the environment at a specific location and time, and generalizations about places that are inferred from these observations. A suite of operations is defined to find similar places based on the invariance of generalized place properties, and it is demonstrated how these functions can be applied to the problem of finding similar places based on the topics that people write about in place descriptions. One use for similar-place search is for exploratory research that will enable investigators to perform case–control studies on place data. Copyright Springer-Verlag Berlin Heidelberg 2015
C65|Local volatility calibration during turbulent periods|We propose a methodology to calibrate the local volatility function under a continuous time setting. For this purpose, we used the Markov chain approximation method built on the well-established idea of local consistency. The chain was designed to approximate jump–diffusions coupled with a local volatility function. We found that this method outperforms traditional numerical algorithms that require time discretization. Furthermore, we showed that a local volatility jump–diffusion model outperformed the in- and out-of-sample pricing that the market practitioners benchmark, namely the Practitioners Black–Scholes, in turbulent periods during which at-the-money implied volatilities have risen substantially. Hedging experiments show a moderate portfolio risk under the local volatility jump–diffusion case. As in previous literature concerning local volatility estimation, we represent the local volatility function using a space-time cubic spline. Copyright Springer Science+Business Media New York 2015
C65|On strategic complementarities in discontinuous games with totally ordered strategies|This paper studies the existence of a pure strategy Nash equilibrium in games with strategic complementarities where the strategy sets are totally ordered. By relaxing the conventional conditions related to upper semicontinuity and single crossing, we enlarge the class of games to which monotone techniques are applicable. The results are illustrated with a number of economics-related examples.
C65|Mathematics of Predicting Growth|Mathematical methods of analysis of data andofpredicting growth are discussed. The starting point is the analysis of the growth rates, which can be expressed as a function of time or as a function of the size of the growing entity. Application of these methods is illustrated using the world economic growth but they can be applied to any type of growth.
C65|The Insecure Future of the World Economic Growth|Growth rate of the world Growth Domestic Product (GDP) is analysed to determine possible pathways of the future economic growth. The analysis is based on using the latest data of the World Bank and it reveals that the growth rate between 1960 and 2014 was following a trajectory approaching asymptotically a constant value. The most likely prediction is that the world economic growth will continue to increase exponentially and that it will become unsustainable possibly even during the current century. A more optimistic but less realistic prediction is based on the assumption that the growth rate will start to decrease linearly. In this case, the world economic growth is predicted to reach a maximum, if the growth rate is going to decrease linearly with time, or to follow a logistic trajectory, if the growth rate is going to decrease linearly with the size of the world GDP.
C65|Network Price Identity|We model a closed-loop network of agents distributed among subnetworks and study the conditions that satisfy the time-dependent stability of network connectedness in presence of random perturbations. We show that the evolutionary stability of the network structure depends on the prevalence of perturbations on between-subnetwork coupling. Our findings permit to unveil the conservation value of the topological structure, as well as to situate the Price theorem, both in its standard and expanded forms, in the context of network evolutionary variational identity. Whenever the dynamics deals with interconnected structures, the network-based framework is more suitable to address the questions that revolve around the stability of input-output systems.
C65|Dynamics of Cluster Structures in Stock Market Networks|In recent 15 years network analysis has been actively applied for studying financial markets. In this paper we present a network-based analysis of stock markets of USA and Sweden. We extract and study special cluster structures of networks built from correlation matrices of stock returns for these stock markets. A cluster structure of a network is extracted by solving the p-median problem which chooses p central stocks (medians) and partitions all stocks into p clusters around these medians - centers. The objective function maximizes the sum of correlations between each stock and the median of its cluster. The obtained cluster structure is represented by an undirected disconnected weighted graph, which components are star-graphs with one central vertex (median) and several leaf vertices connected only with the median by weighted edges. Our main observation is that in non-crisis periods cluster structures of stock market networks change more chaotically, while during crises they demonstrate more stable behavior and smaller changes. Thus an increase in stability of the cluster structure for a stock market network obtained by means of the p-median problem solution can serve as an indicator of a coming crisis.
C65|Path Dependency|Path dependency is defined, and three different specific concepts of path dependency – cumulative causation, lock in, and hysteresis – are analyzed. The relationships between path dependency and equilibrium, and path dependency and fundamental uncertainty are also discussed. Finally, a typology of dynamical systems is developed to clarify these relationships.
C65|Existence of SPE in Discounted Stochastic Games; Revisited and Simplified|Mertens and Parthasarathy (1987) proved the existence of sub-game perfect equilibria in discounted stochastic games. Their method involved new techniques in dynamic programming, which were presented in a very general framework, with no expense spared in highlighting versatility and scope. This paper presents the fundamentals of their technique which are necessary, as well as identifies and elaborates on the components of their method, hence giving the core of the proof in a much more concise, direct, and illuminating manner.
C65|Looking into the future of complex dynamic systems|The desire to know and foresee the future is naturally bound to human nature. Traditional forecasting methods have looked after reductionist linear approaches: variables and relationships are monitored in order to foresee future outcomes with simplified models and to derive theoretical and practical implications. The limitations of this attitude have become apparent in many cases, mainly when dealing with dynamic evolving complex systems, that encompass numerous factors and activities which are interdependent and whose relationships might be highly nonlinear, resulting in an inherent unpredictability of their long-term behavior. Complexity science ideas are important interdisciplinary research themes emerged in the last few decades that allow to tackle the issue, at least partially. This paper presents a brief overview of the complexity framework as a means to understand structures, characteristics, relationships, and explores the most important implications and contributions of the literature on the predictability of a complex system. The objective is to allow the reader to gain a deeper appreciation of this approach.
C65|How Market Economies Come to Live and Grow on the Edge of Chaos|Summary: In a Hayek-Friedman-Lucas world, market economies are assumed to be natural, stable, and ergodic; hence, government policies are harmful to their efficiency. We develop a nonlinear dissipative dynamic model that shows that market economies instead live on the edge of chaos. We next appeal to the theory of differential equation to show that if they do not usually dissipate the totality of the information produced by their evolution it is due to a far-off self-organized equilibrium brought about by a spontaneous phase change originating in an optimal government policy.
C65|A decomposition for the space of games with externalities|The main goal of this paper is to present a different perspective than the more `traditional' approaches to study solutions for games with externalities. We provide a direct sum decomposition for the vector space of these games and use the basic representation theory of the symmetric group to study linear symmetric solutions. In our analysis we identify all irreducible subspaces that are relevant to the study of linear symmetric solutions and we then use such decomposition to derive some applications involving characterizations of classes of solutions.
C65|Simultaneous optimization: sectorization and congolese air traffic assignment by the method of preferential reference of dominance|Air controllers encounter aeronautical problems everyday. Those problems complexity is growing as the latter problems emerge in aerial navigations sectors at the time of the air traffic assignment. As a long time as the number of aircraft in a sector is high, the controller-related load will increase in a nonlinear way. Currently, one counts, on the congolese territory, and especially in the vicinity of areas with wars, many planes movements. This would represent in a near future a difficult bulk of control for controllers. In order to avoid saturation in sectors, the congolese airspace must be divided in increasingly small sectors while distributing the workloads. To clarify the analysis, one is interested in the multicriteria optimization which deals with the case of the simultaneous presence of several objectives by the preferential reference of dominance method proposed by Joseph Okitonyumbe and Berthold Ulungu (2014). The latter method is based on a new characterization of the efficient solutions by building the probable assignments in order to minimize the load of control in the sector.
C65|Effects of Long Cycles in Cash Flows on Present Value|This paper explores how present value varies over time when the underlying cash flow has a deterministic period. I assume that cash flows are known with certainty and follow a cycle with a long or short period. When the cash flow has a short period, the present value is relatively stable over time because the present value calculation smooths out several cycles. However, when the cash flow has a long period the present value itself develops a long and large cycle. These results are driven by the mathematical definition of the present value and are relevant to the use of present value as a pricing tool in situations where the cash flows of an investment have a long cycle.
C65|Location Quotient,Coefficient of Specialization and Shift-Share|This technical document describes the foundations for three different regional economic functions implemented in MATLAB and R. These functions are Location Quotients, Coefficients of Localization, and Shift-Share Analysis.
C65|Информационные методы исследования неопределенности в экономических моделях. Information methods of investigation of uncertainty in economic models|В инновационных процессах существенную роль играют два типа неопределенности: неопределенность целей и неопределенность располагаемых ресурсов. На математическом языке неопределенность целей обычно описывается представлением цели в виде функции со случайными параметрами (или представлением цели как случайной величины). Располагаемые ресурсы обычно описываются некоторым множеством допустимых значений. Неопределенность располагаемых ресурсов в этом случае должна быть описана случайным множеством. При решении многих прикладных задач необходимо учитывать наличие статистической взаимосвязи между отдельными факторами неопределенности. С информационной точки зрения эта связь определяется через изменение энтропии каждого фактора в результате их взаимного влияния друг на друга. Свойства энтропии, порождаемой случайными величинами, к настоящему времени достаточно хорошо изучены. Применение энтропии случайных величин к исследованию неопределенности экономических процессов содержится в ряде работ. Однако, неопределенность, порождаемая случайными точечно-множественными отображениями, изучена значительно меньше. В то же время, этот тип неопределенности с прикладной точки зрения не менее значим. В данной статье описывается предлагаемый автором метод исследования неопределенности, порождаемой случайными точечно-множественными отображениями, т.е. неопределенности располагаемых ресурсов. Основным инструментом в предлагаемом методе является функция энтропии случайного множества, которая обобщает понятие энтропии случайной величины. На базе функции энтропии определяется функция информационной связи случайных множеств и все другие характеристики. In innovation processes play an important role are two types of uncertainty: uncertainty of goals and uncertainty of available resources. In mathematical language uncertainty of goals is usually described as a representation of the target function with random parameters (or a representation of the target as a random variable). The available resources are usually described by a set of valid values. Uncertainty of the resources in this case must be described by a random set. When dealing with many applications, be aware of the statistical relationship between the individual uncertainties. From the information point of view, this relationship is determined by a change in the entropy of each factor as a result of their mutual influence on each other. Properties of the entropy generated by the random variables, by now fairly well-known. Application of entropy of random variables to study the uncertainty of economic processes is contained, for example, in [3,4]. However, the uncertainty generated by the random point-set maps, studied far less. At the same time, this type of uncertainty from the applied point of view is not less important. This article describes the method proposed by the authors study the uncertainty generated by the random point-set mappings, i.e. uncertainty of available resources. The main tool in the proposed method is a function of the entropy of the random set, which generalizes the notion of entropy of a random variable. On the basis of the entropy function is determined by the function of information link of random sets and all other features.
C65|Newton´s Laws of Motion and Price Theory|The paper focuses on factual research in bibliographic and biographical databases showing that representatives of the Czech School of Economics took a leading role in the methodological use of applied and theoretical physics in the basic economic research, especially in the second half of the twentieth century.The linear and non-linear analytical structures of theoretical physics are compared with the analytical structures of commodity price theory in a market with nearly perfect competition. Newton´s equations of motion for the non-relativistic speed of instantaneous relative depreciation and instantaneous relative commodity prices over time are analyzed. Assuming that the market value of a commodity is fully determined exclusively by the value of the instantaneous commodity price, the price jerk equation acquires a form corresponding to the non-relativistic equation for jerk in mechanics, following from Newton´s second law of motion. In this paper price jounce and price crackle are defined.
C65|System Design Of Recycle Management Using Ahp And Topsis In Buyukcekmece Municipality|Local governments are researching efficient and sustainable solutions to the problem of increasing amount of solid waste. Recycling is proposed as one of these solutions considering increasing environmental concerns. Recycling operation system starts with separating recyclable waste at household level. Though an efficient recycling system's success largely depends on the participation rate of inhabitants.Büyükçekmece is a district and municipality in the suburbs of Istanbul, Turkey on the Sea of Marmara coast of the European side, western part of the city. It is largely an industrial area with a population of 380,000. Although Büyükçekmece is a non-large district of ?stanbul in terms of the amount of population, the population is growing in the summer months. Department of Environmental Protection and Control of Büyükçekmece Municipality conducts activities for air pollution measurement, noise control and environmental impact assessment. In addition to these activities, office engages in recycling activities of vegetable wastes, packaging wastes, batteries, electronic wastes, plastics, glass and paper. Recycling activities at the municipality are handled by a private companies. Currently these recycling activities and operations are carried out in an unsystematic way. This recycling activity is irregular though awareness level of the inhabitant is extremely low. In this research, three alternative recycle collection methods are evaluated on three different locations. Alternatives are evaluated using AHP and TOPSIS methodologies. First important factors for a successful application of recycling were identified by AHP methodology. In AHP application, three main criteria are identified as economic factors, social factors and operational factors. There exists ten sub-criteria in the AHP tree. AHP surveys are filled out with project partner company managers, municipality engineers and administrators. After that TOPSIS methodology is used while comparing and ranking alternative recycle collection methods.After selecting the application sites application started with handing out recycling flyers, brochures of information about the study. Next a survey is prepared in order to understand the inhabitants? perception of recycling and demographics. Application lasted for three weeks starting with collecting survey papers and distribution of recycling bags to the residents periodically. During that period observations are made by the regional partners such as trash collector, site administrators as well as project team.
C65|Driving Forces of CO 2 Emissions in Emerging Countries: LMDI Decomposition Analysis on China and India’s Residential Sector|The main objective of this paper is to identify and analyze the key drivers behind changes of CO 2 emissions in the residential sectors of the emerging economies, China and India. For the analysis, we investigate to what extent changes in residential emissions are due to changes in energy emissions coefficients, energy consumption structure, energy intensity, household income, and population size. We decompose the changes in residential CO 2 emissions in China and India into these five contributing factors from 1990 to 2011 by applying the Logarithmic Mean Divisia Index (LMDI) method. Our results show that the increase in per capita income level was the biggest contributor to the increase of residential CO 2 emissions, while the energy intensity effect had the largest effect on CO 2 emissions reduction in residential sectors in both countries. This implies that investments for energy savings, technological improvements, and energy efficiency policies were effective in mitigating CO 2 emissions. Our results also depict that the change in CO 2 emission coefficients for fuels which include both direct and indirect emission coefficients slowed down the increase of residential emissions. Finally, our results demonstrate that changes in the population and energy consumption structure drove the increase in CO 2 emissions.
C65|The Multi-faceted Character of Risk in Maritime Freight Markets (Panamax) 1996-2012| The paper deals with maritime risk, which we consider important, no doubt, for ship-owners acting in volatile markets. Traditionally, risk is measured by ‘standard deviation’. Other risk measures like ‘excess kurtosis’, ‘excess skewness’, ‘long-term dependence’ and the ‘catastrophe propensity’ were ignored. Risk in 1900 was based on the mathematical laws of Chance and influenced greatly by Probability theory due to Pascal and Fermat (1654). Economists, but maritime ones, have understood, however, that the ‘random walk’ model, and the ‘efficient market hypothesis’, failed to interpret reality since Black Monday (1987) at least. The traditional treatment of risk assumes that 95% of the observations fall within 2ó from their mean. However, the daily data of 4 time-charter routes (‘Baltic Panamax Index’, May 1996-February 2012) showed otherwise. Moreover, variance varies from one decade to next, even under stable mean. Risk is related to dispersion, which is defined the same in ‘normal’ and ‘chaotic dynamic systems. All maritime studies (1997-2013), however, reported excess skewness, excess kurtosis, absence of normality and serial correlation...but no remedy provided. As far as the reference to the assumption that observations are ‘independent and identically distributed’ is concerned, maritime time series analysis shows ‘long term dependence’ indicated by a high ‘Hurst exponent’~1. The paper uses ‘Rescaled Range Analysis’-a nonparametric method, to identify the ‘Noah effect’ (i.e. the propensity of time series towards catastrophe; measured by alpha exponent). Combined with nonlinear forecasting methods, short and long term risk is thus in this paper forecast. Finally, it shows using daily data, that ‘risk and dependence’ vary on data’s calendar time used.
C65|Decision making in phantom spaces|This paper introduces a new model of decision making under uncertainty. Aiming to provide a more realistic depiction of decision making, it generalizes the von Neumann–Morgenstern theory by including additional tiers of uncertainty. In this model, beliefs about the probabilities of events are ambiguous and their consequential utilities are vague; both are naturally formulated in the phantom space using phantom numbers. The degree of uncertainty, determined by the decision maker’s beliefs, is distinguished from the attitude toward uncertainty, which is drawn from her preferences. Decision making under ambiguity is a particular case of our model in which probabilities are ambiguous, but resulting utilities of events are knowable. Copyright Springer-Verlag Berlin Heidelberg 2015
C65|Social evaluation functionals: a gateway to continuity in social choice|This paper develops social choice theory aggregating individual utility functions to a social utility function. Such a tool allows me to deal with a natural notion of continuity in social choice theory. In addition, and in order to have the choice problem as close as possible to its beginnings, the social evaluation functionals considered are assumed to satisfy both ordinal measurability and interpersonal non-comparability, and unanimity. I present two results concerning the characterization of projective social evaluation functionals (which means that the social utility function is exactly the utility of the dictator). The first one needs a strong form of welfarism called social state separability. The second one uses continuity in combination with a new axiom called ordinal-scale-preserving. Copyright Springer-Verlag Berlin Heidelberg 2015
C65|Strategic uncertainty and the ex-post Nash property in large games|This paper elucidates the conceptual role that independent randomization plays in non-cooperative game theory. In the context of large (atomless) games in normal form, we present precise formalizations of the notions of a mixed strategy equilibrium (MSE), and of a randomized strategy equilibrium in distributional form (RSED). We offer a resolution of two long-standing open problems and show: (i) any MSE {\it induces} a RSED, and any RSED can be {\it lifted} to a MSE, (ii) a mixed strategy profile is a MSE if and only if it has the ex-post Nash property. Our substantive results are a direct consequence of an {\it exact} law of large numbers (ELLN) that can be formalized in the analytic framework of a Fubini extension. We discuss how the \lq measurability' problem associated with a MSE of a large game is automatically resolved in such a framework. We also illustrate our ideas by an approximate result pertaining to a sequence of large but finite games.
C65|Textual Analysis in Real Estate|This paper incorporates text data from MLS listings from Atlanta, GA into a hedonic pricing model. Text is found to decrease pricing error by more than 25%. Information from text is incorporated into a linear model using a tokenization approach. By doing so, the implicit prices for various words and phrases are estimated. The estimation focuses on simultaneous variable selection and estimation for linear models in the presence of a large number of variables. The LASSO procedure and variants are shown to outperform least-squares in out-of-sample testing.
C65|Analiza svojstava konveksnosti obveznica bez primjene diferencijalnog računa|U radu Gardijan, M., Kojić, V., Šego, B. (2012) Trajanje obveznica: pravila i primjene. Zbirna znanstvena knjiga (urednici: Aljinović, Z., Marasović, B.), Sveučilište u Splitu, Ekonomski fakultet, Split, ISBN 978-953-281-049-3, str. 5-26 (1. poglavlje) analizirana su svojstva trajanja kuponskih obveznica kao jedne od temeljnih mjera rizičnosti. Druga vrlo bitna mjera rizičnosti je svakako konveksnost obveznice, pa je u skladu s tim bitno poznavati svojstva konveksnosti. Iako se u literaturi ta svojstva deskriptivno objašnjavaju, analitički izvodi i dokazi relevantnih zaključaka gotovo da i nema. Stoga je cilj ovog rada dati pregledan opis svojstava konveksnosti obveznica, te ih matematički dokazati. Jedan od načina dokazivanja je svakako primjenom diferencijalnog računa. Međutim, primjena diferencijalnog računa neizbježno vodi na komplicirane međurezultate koji su nužni kako bi se došlo do krajnjeg zaključka, pa se stoga dokazi provode na elementaran, algebarski način, razumljiv i široj publici koja nema nužno predznanje iz područja matematičke analize.
C65|Using support vector machines for measuring democracy|We present a novel approach for measuring democracy, which enables a very detailed and sensitive index. This method is based on Support Vector Machines, a mathematical algorithm for pattern recognition. Our implementation evaluates 188 countries in the period between 1981 and 2011. The Support Vector Machines Democracy Index (SVMDI) is continuously on the 0-1-Interval and robust to variations in the numerical process parameters. The algorithm introduced here can be used for every concept of democracy without additional adjustments, and due to its exibility it is also a valuable tool for comparison studies.
C65|A Theory for Complex Systems Social Change: An Application of a General 'Criticality' Model|"Within the developed nations deterioration in the basis of society, as dramatically demonstrated by the Lehman collapse, has reached extreme levels, and currently the formation of pro-change agents is approaching a decisive stage. Here, we will construct a complex systems 'criticality' model, apply it to social change, and examine its reliability and validity. The model derived a power law distribution of the output of social change. The validity of the model was verified by examining vote shares of parties in Japan. Based on the results of this examination, we propose a new quantitative strategy ""information entropy enhancement"" for social change."
C65|Multifractality in Finance: A deep understanding and review of Mandelbrot's MMAR|"Beno�t Mandelbrot, the father of Fractal Geometry, developed a multifractal model for describing price changes. Despite the commonly used models, such as the Brownian motion, the Mutifractal Model of Asset Return (MMAR) takes into account scale-consistency, long-range dependence and heavy tails, thus having a great flexibility in depicting the real-market peculiarities. In section 2 a review of the mathematics involved into multifractals is presented; Section 3 is addresses to the extension of multifractality towards stochastic processes, introducing the crucial concept of local H\""older exponent of a function. Finally, Section 4 deeply analyze the mathematical properties of the scaling function which drives the ``wildeness'' of the process. The proof of Theorem 4.4 is unpublished and the generalization of a Mandelbrot's result, which highlights a possible alternative motivation for the presence of heavy tails and a connection with the Extreme Value Theory. Section 5 is devoted to the analysis of the connection between the scaling function, Multifractal Formalism and Large Deviation Theory, suggesting possible ways in order to estimate the quantities involved. Finally in Section 6 the MMAR is presented, listing all the theorems that make it a suitable model for financial modelling."
C65|Some notes on divisibility rules|A divisibility rule is a shorthand way of determining whether a given number is divisible by a fixed divisor without performing the division, usually by performing a simple calculation on its digits. There is no similarity among these rules, in the sense that for example the rule for testing the divisibility by 3 is very different from the rule for 7. In this working paper we present a general rule for testing divisibility by two digit integer numbers. The general rule is characterized by a pair of conditions, but in some cases just the main condition of the two is necessary and sufficient for divisibility. In the second part we investigate on this aspect and we show in general what are the cases in which just one condition is enough. Although there are divisibility tests for numbers in any base, and they are usually different, in this paper we concentrate just on the base 10.
C65|Confidence Bands for Impulse Responses: Bonferroni versus Wald|In impulse response analysis estimation uncertainty is typically displayed by constructing bands around estimated impulse response functions. These bands may be based on frequentist or Bayesian methods. If they are based on the joint distribution in the Bayesian framework or the joint asymptotic distribution possibly constructed with bootstrap methods in the frequentist framework often individual confidence intervals or credibility sets are simply connected to obtain the bands. Such bands are known to be too narrow and have a joint confidence content lower than the desired one. If instead the joint distribution of the impulse response coefficients is taken into account and mapped into the band it is shown that such a band is typically rather conservative. It is argued that a smaller band can often be obtained by using the Bonferroni method. While these considerations are equally important for constructing forecast bands, we focus on the case of impulse responses in this study.
C65|Solving the production cost minimization problem with the Cobb – Douglas production function without the use of derivatives|In this paper, we propose a new original method to solve the production cost minimization problem with Cobb-Douglas production function by using the weighted arithmetic-geometric-mean inequality (weighted AM-GM inequality). Instead of using derivatives or the Lagrange multiplier method, the minimum costs and global minimizers in the case of the Cobb-Douglas production function are derived in the direct way. The result is first derived for the case of two inputs and then generalized for the problem with n inputs.
C65|Identifying Clusters of Regions in the European South, based on their Economic, Social and Environmental Characteristics|Regional development has been in the centre of interest among both academics but also decision makers in the central and local governments of many European countries. Identifying the key problems that regions face and considering how these findings could be effectively used as a basis for planning their development process are essential in order to improve the conditions in the European Union regions. For a long period of time a country's or a region's development has been synonymous with its economic growth. Over the last years, however, economies and societies have been undergoing dramatic changes. These changes have led to the concept of sustainable development, which refers to the ability of our societies to meet the needs of the present without sacrificing the ability of future generations to meet their own needs. Measuring sustainable development means going beyond a purely economic description of human activities; requires integration of economic, social and environmental concerns. New techniques are required in order to benchmark performance, highlight leaders and laggards on various aspects of development and facilitate efforts to identify best practices. Furthermore, new tools have to be designed so as to make sustainability decision-making more objective, systematic and rigorous. The growth or decline of a country or region depends on its power to pull and retain both business and the right blend of people to run them. Working in this context, we have so far defined a variable which is called the image of a region and quantifies this pulling power. The region's image is a function of a multitude of factors physical, economic, social and environmental, some common for all potential movers and some specific for particular groups of them and expresses its present state of development and future prospects. The paper examines a number of south European countries and focuses on their NUTS 2 level regions. Its objective is to: * Estimate the Basic Image values of those regions. * Group those regions into different clusters on the basis of the values of the various factors used to define their respective Basic Images. * Present and discuss the results.
C65|Stop Waiting Problem: Decision Rule with Ψ function and Application with Share Prices|In this paper the stop-waiting strategy of Franz Bruss is set into a simple probabilistic framework and applied to the apple share prices from 1984 to 2013. Within the probabilistic framework a heuristic and a mathematical decision rule using the $\Psi$ function is developed. The results are in line with Bruss's theory. We apply the stop-waiting strategy to the Apple share prices and compare the results with a simple start-end and chart technique strategy.
C65|Last Success Problem: Decision Rule and Application|Where is the most likely position for the last success in n events, if each event has the same probability Pr(A)? What is the probability for the last success? This situation assumes returning successes which is different to the stop waiting problem where a single best event is assumed. We set F. Thomas Bruss's theory into a simple probability framework and develop a decision rule for the last success. The results are applied to the Apple share prices.
C65|The role of fund size in the performance of mutual funds assessed with DEA models| This contribution studies the role of the size of mutual funds in the evaluation of the fund performance with a data envelopment analysis (DEA) approach, with the aim of studying the issue from different angles and with different statistical tools and investigating the presence of a positive or negative size effect in mutual funds market. Firstly, we discuss the role of fund size in the performance evaluation and wonder whether it is appropriate to include size information among the variables of DEA models. Secondly, we analyse the presence of a relationship between the performance scores and the size of mutual funds using different statistical tests and carry out an empirical investigation on a set of European equity mutual funds. Thirdly, we study scale efficiency and investigate whether the European mutual funds analysed exhibit constant, increasing or decreasing returns to scale.
C65|A mapping associated to a quadratic optimization problem with linear constraints|In this working paper we go on with the study of a mapping in Rn associated to a quadratic optimization problem with one equality linear constraint. After showing some general properties related to homogeneity and the inverse mapping, we present some results regarding how the mapping behaves with norms of vectors. Then some aspects related either to invariant points or invariant subspaces are investigated.
C65|On the mathematical background of Google PageRank algorithm|The PageRank algorithm, the kernel of the method used by Google Search to give us the answer of a search we are asking in the web, contains a lot of mathematics. Maybe one could say that graphs and Markov chains theories are in the background, while the crucial steps are in a linear algebra context, as the eigenvalues of a matrix are involved. In this working paper we deal with all the mathematics we need to explain how the PageRank method works.
C65|The importance of Perron-Frobenius Theorem in ranking problems|The problem of ranking a set of elements, namely giving a ``rank'' to the elements of the set, may be tackled in many different ways. In particular a mathematically based ranking scheme can be used and sometimes it may be interesting to see how different can be the results of a mathematically based method compared with some more heuristic ways. In this working paper some remarks are presented about the importance, in a mathematical approach to ranking schemes, of a classical result from Linear Algebra, the Perron--Frobenius theorem. To give a motivation of such an importance two different contexts are taken into account, where a ranking problem arises: the example of ranking football/soccer teams and the one of ranking webpages in the approach proposed and implemented by Google's PageRank algorithm.
C65|Alternative subsidy scenarios for different agricultural practices: A sustainability assessment using fuzzy multi-criteria analysis| The recent Common Agricultural Policy reform (CAP14) at the European level links the granting of aid to farmers to adhering to environmentally-friendly farming practices. It therefore becomes important to assess the overall effectiveness of such a policy by taking into account different economic and environmental criteria. In this work, an ex ante assessment of different agricultural policy scenarios in Italy is undertaken at the national level, through the adoption of a fuzzy multi-criteria analysis approach, to account for the different economic and environmental aspects (indicators) of each scenario. Italian agricultural holdings were divided into homogeneous groups (according to farm typology, location, and environment), in order to determine the most preferable scenario for each group. Results are extremely heterogeneous across the macro areas, the farm typologies, and the climatic zones, and it is not possible to determine a ‘good-for-all’ scenario. However, we can observe that when all indicators are assigned an equal weight and also when environmental indicators are assigned a higher weight, the preferred scenario for the majority of groups is the alternative scenario where a tax of 30% on pesticides is added to the CAP14. On the other side, when economic indicators have a higher weight, the situation of subsidies preceding CAP14 (base subsidies and environmental subsidies, with no differentiation among conventional and organic farming) seems to be the ‘best’ scenario for all groups, with one exception.
C65|Generating structured music using quality metrics based on Markov models|In this research, a first order Markov model is built from a corpus of bagana music, a traditional lyre from Ethiopia. Different ways in which low order Markov models can be used to build quality assessment metrics for an optimization algorithm are explained. These are then implemented in a variable neighbourhood search algorithm that generates bagana music. The results are examined and thoroughly evaluated. Due to the size of many datasets it is often only possible to get rich and reliable statistics for low order models, yet these do not handle structure very well and their output is often very repetitive. A method is proposed that allows the enforcement of structure and repetition within music, thus handling long term coherence with a first order model.
C65|Reducción del ruido y predicción de series temporales de alta frecuencia mediante sistemas dinámicos no lineales y técnicas neurales|The analysis of economic phenomena from direct observation can lead to incorrect conclusions because the data surveyed as an expression of the magnitude studied is often contaminated by multiple factors that introduce noise and prevent clearly perception of the underlying evolutionary patterns that seeks to analyze. It is essential to decompose the magnitude observed in terms of variations not directly observable. To do this methods in nonlinear dynamical systems are studied to remove noise that contaminates high frequency time series with eventual chaotic behavior
C65|The Effect of Secondary Markets on Equity-Linked Life Insurance With Surrender Guarantees|" type=""main"" xml:lang=""en""> Many equity-linked life insurance products offer the possibility to surrender policies prematurely. Secondary markets for policies with surrender guarantees influence both policyholders and insurers. We show that secondary markets lead to a gap in policy value between insurer and policyholder. Insurers increase premiums to adjust for higher surrender rates of customers and optimized surrender behavior by investors acquiring the policies on secondary markets. Hence, the existence of secondary markets is not necessarily profitable for the primary policyholders. The result depends on the demand for and the supply of the contracts brought to the secondary markets."
C65|Viable Ramsey economies|The Ramsey model of economic growth is revisited from the perspective of viability theory. The Ramsey model, augmented with minimal consumption and sustainability criteria, becomes a viability problem. The framework allows for a clear picture of optimal viable, optimal nonviable, and viable nonoptimal paths. The drastic sacrifices in terms of present consumption required by the implementation of Brundtland sustainability are visualized, the rich countries bearing the major part of the burden. The econometric analysis of viability sets enhances the role of technological progress in ensuring Brundtland sustainability. Preference parameters such as the pure time preference rate are statistically nonsignificant.
C65|Ecuaciones Diferenciales Estocásticas con Condición Final y Soluciones de Viscosidad de EDPS Semilineales de Segundo Orden|El objetivo de este documento es recopilar algunos resultados clásicos sobre existencia y unicidad de soluciones de ecuaciones diferenciales estocásticas (EDEs) con condición final (en inglés Backward stochastic differential equations) con particular énfasis en el caso de coeficientes monótonos, y su conexión con soluciones de viscosidad de sistemas de ecuaciones diferenciales parciales (EDPs) parabólicas y elípticas semi-lineales de segundo orden.
C65|Judgment Aggregation Theory Can Entail New Social Choice Results|Judgment (or logical) aggregation theory is logically more powerful than social choice theory and has been put to use to recover some classic results of this field. Whether it could also enrich it with genuinely new results is still controversial. To support a positive answer, we prove a social choice theorem by using the advanced nonbinary form of judgment aggregation theory developed by Dokow and Holzman (2010c). This application involves aggregating classifications (specifically assignments) instead of preferences, and this focus justifies shifting away from the binary framework of standard judgement aggregation theory to a more general one.
C65|Single-period cutting planes for inventory routing problems|IRP involves the distribution of one or more products from a supplier to a set of clients over a discrete planning horizon. Each client has a known demand to be met in each period and can only hold a limited amount of stock. The product is shipped through a distribution network by one or more vehicles of limited capacity. The objective is to find replenishment decisions minimizing the sum of the storage and distribution costs. In this paper we present reformulations of IRP, under the Maximum Level replenishment policy, derived from a single-period substructure. We define a generic family of valid inequalities, and then introduce two specific subclasses for which the separation problem of generating violated inequalities can be solved effectively. A basic Branch-and-Cut algorithm has been implemented to demonstrate the strength of the single-period reformulations. Computational results are presented for the benchmark instances with 50 clients and three periods and 30 clients and six periods.
C65|Securely solving classical network flow problems|We investigate how to solve several classical network flow problems using secure multi-party computation. We consider the shortest path problem, the Minimum Mean Cycle problem and the Minimum Cost Flow problem. To the best of our knowledge, this is the first time the two last problems have been addressed in a general multi-party computation setting. Furthermore, our study highlights the complexity gaps between traditional and secure implementations of the solutions, to later test its implementation. It also explores various trade-offs between performance and security. Additionally it provides protocols that can be used as building blocks to solve complex problems. Applications of our work can be found in: communication networks, routing data from rival company hubs; benchmarking, comparing several IT appliances configurations of rival companies; distribution problems, retailer/supplier selection in multi-level supply chains that want to share routes without disclosing sensible information; amongst others.
C65|Weak Convergence To Stochastic Integrals For Econometric Applications|Limit theory involving stochastic integrals is now widespread in time series econometrics and relies on a few key results on function space weak convergence. In establishing weak convergence of sample covariances to stochastic integrals, the literature commonly uses martingale and semimartingale structures. While these structures have wide relevance, many applications in econometrics involve a cointegration framework where endogeneity and nonlinearity play a major role and lead to complications in the limit theory. This paper explores weak convergence limit theory to stochastic integral functionals in such settings. We use a novel decomposition of sample covariances of functions of I(1) and I(0) time series that simplifies the asymptotic development and we provide limit results for such covariances when linear process, long memory, and mixing variates are involved in the innovations. The limit results extend earlier findings in the literature, are relevant in many econometric applications, and involve simple conditions that facilitate implementation in practice. A nonlinear extension of FM regression is used to illustrate practical application of the methods.<br><small>(This abstract was borrowed from another version of this item.)</small>
C65|Judgment aggregation theory can entail new social choice results|Judgment (or logical) aggregation theory is logically more powerful than social choice theory and has been put to use to recover some classic results of this field. Whether it could also enrich it with genuinely new results is still controversial. To support a positive answer, we prove a social choice theorem by using the advanced nonbinary form of judgment aggregation theory developed by Dokow and Holzman (2010c). This application involves aggregating classifications (specifically assignments) instead of preferences, and this focus justifies shifting away from the binary framework of standard judgement aggregation theory to a more general one.
C65|Escape dynamics: A continuous-time approximation|We extend a continuous-time approximation approach to the analysis of escape dynamics in economic models with constant gain adaptive learning. This approach is based on the application of the results of continuous-time version of large deviations theory to the linear diffusion approximation of the original discrete-time dynamics under learning. We characterize escape dynamics by analytically deriving the most probable escape point and mean escape time. The approximation is tested on the Phelps problem of a government controlling inflation while adaptively learning a misspecified Phillips curve, studied previously by Sargent (1999) and Cho et al. (2002) (henceforth, CWS), among others. We compare our results with simulations extended to very low values of the constant gain and show that, for the lowest gains, our approach approximates simulations relatively well. We express reservations regarding the applicability of any approach based on large deviations theory to characterizing escape dynamics for economically plausible values of constant gain in the model of CWS when escapes are not rare. We show that for these values of the gain it is possible to derive first passage times for learning dynamics reduced to one dimension without resort to large deviations theory. This procedure delivers mean escape time results that fit the simulations closely. We explain inapplicability of large deviations theory by insufficient averaging near the point of self-confirming equilibrium for relatively large gains which makes escapes relatively frequent, suggest the changes which might help approaches based on the theory to work better in this gain interval, and describe a simple heuristic method for determining the range of constant gain values for which large deviations theory could be applicable.
C65|Recovering default risk from CDS spreads with a nonlinear filter|We propose a nonlinear filter to estimate the time-varying default risk from the term structure of credit default swap (CDS) spreads. Based on the numerical solution of the Fokker–Planck equation (FPE) using a meshfree interpolation method, the filter performs a joint estimation of the risk-neutral default intensity and CIR model parameters. As the FPE can account for nonlinear functions and non-Gaussian errors, the proposed framework provides outstanding flexibility and accuracy. We test the nonlinear filter on simulated spreads and apply it to daily CDS data of the Dow Jones Industrial Average component companies from 2005 to 2010 with supportive results.
C65|Validating an agent-based model of the Zipf׳s Law: A discrete Markov-chain approach|This study discusses the validation of an agent-based model of emergent city systems with heterogeneous agents. To this end, it proposes a simplified version of the original agent-based model and subjects it to mathematical analysis. The proposed model is transformed into an analytically tractable discrete Markov model, and its city size distribution is examined. Its discrete nature allows the Markov model to be used to validate the algorithms of computational agent-based models. We show that the Markov chains lead to a power-law distribution when the ranges of migration options are randomly distributed across the agent population. We also identify sufficient conditions under which the Markov chains produce the Zipf׳s Law, which has never been done within a discrete framework. The conditions under which our simplified model yields the Zipf׳s Law are in agreement with, and thus validate, the configurations of the original heterogeneous agent-based model.
C65|The state's role and position in international trade: A complex network perspective|Based on statistical physics and graph theory, the research paradigm of a complex network, which has sprung up in the last decade, provides us with new global perspective to discuss the topic of international trade. In this paper, we engage in the issue of countries' roles and positions in international trade using the latest complex network theories. On a mid-level structure, countries are classified into three communities that reflect the structure of the “core/periphery” using the weighted extremal optimisation algorithm and the coarse graining process. On a micro-level, countries' rankings are provided with the aid of network's node centralities, which presents world trade as a closed, imbalanced, diversified and multi-polar development. Further, we firstly introduce the improved bootstrap percolation to simulate cascading influences following the breaking down of bilateral trade relations. We find that the breakdown of EU's export relations can more easily form a cascading reaction, which would result in a global collapse of world trade. All the results highlight the important positions of the EU, USA and Japan in the international trade system, which plays a positive role in promoting the world economy.
C65|Controlling portfolio skewness and kurtosis without directly optimizing third and fourth moments|In spite of their importance, third or higher moments of portfolio returns are often neglected in portfolio construction problems due to the computational difficulties associated with them. In this paper, we propose a new robust mean–variance approach that can control portfolio skewness and kurtosis without imposing higher moment terms. The key idea is that, if the uncertainty sets are properly constructed, robust portfolios based on the worst-case approach within the mean–variance setting favor skewness and penalize kurtosis.
C65|On the relation between regular variation and the asymptotic elasticity of substitution|This paper characterizes a class of regularly varying production functions with an asymptotic elasticity of substitution equal to one. In particular, it is shown that these functions asymptotically approximate the Cobb–Douglas form. The results generalize and unify existing results in the literature.
C65|Constant and variable returns to scale DEA models for socially responsible investment funds|In order to evaluate the performance of socially responsible investment (SRI) funds, we propose some models which use data envelopment analysis (DEA) and can be computed in all phases of the business cycle. These models focus on the most crucial elements of an investment in mutual funds.
C65|A Fuzzy Goal Programming model for solving aggregate production-planning problems under uncertainty: A case study in a Brazilian sugar mill|This paper proposes a Fuzzy Goal Programming model (FGP) for a real aggregate production-planning problem. To do so, an application was made in a Brazilian Sugar and Ethanol Milling Company. The FGP Model depicts the comprehensive production process of sugar, ethanol, molasses and derivatives, and considers the uncertainties involved in ethanol and sugar production. Decision-makings, related to the agricultural and logistics phases, were considered on a weekly-basis planning horizon to include the whole harvesting season and the periods between harvests. The research has provided interesting results about decisions in the agricultural stages of cutting, loading and transportation to sugarcane suppliers and, especially, in milling decisions, whose choice of production process includes storage and logistics distribution.
C65|Time-varying long range dependence in energy futures markets|This study aims to investigate the presence of long-range dependence in energy futures markets. Using a daily dataset covering from 1990 to 2013 (which includes crucial events for energy markets such as invasion of Iraq and global financial crisis of 2008), we estimate time-varying generalized Hurst exponents of several energy futures contracts with different times to maturity using a rolling window approach. Results reveal that efficiency of energy futures markets is clearly time-varying and changes drastically over the sample period. For futures contracts with 1–4months to maturities, crude oil and gasoline are found to be more efficient compared to others. On the other hand, for contracts with 5–9months to maturities, crude oil and natural gas futures are more efficient. For almost every different month to maturity, heating oil and gas oil futures are found to be the least efficient markets. Moreover in general, the efficiency of energy futures markets is found to be decreasing dramatically when time to maturity is increasing. Several implications are discussed.
C65|Provincial allocation of carbon emission reduction targets in China: An approach based on improved fuzzy cluster and Shapley value decomposition|An approach to determine carbon emission reduction target allocation based on the particle swarm optimization (PSO) algorithm, fuzzy c-means (FCM) clustering algorithm, and Shapley decomposition (PSO–FCM–Shapley) is proposed in this study. The method decomposes total carbon emissions into an interaction result of four components (i.e., emissions from primary, secondary, and tertiary industries, and from residential areas) which composed totally by 13 macro influential factors according to the KAYA identity. Then, 30 provinces in China are clustered into four classes according to the influential factors via the PSO–FCM clustering method. The key factors that determine emission growth in the provinces representing each cluster are investigated by applying Shapley value decomposition. Finally, based on guaranteed survival emissions, the reduction burden is allocated by controlling the key factors that decelerate CO2 emission growth rate according to the present economic development level, energy endowments, living standards, and the emission intensity of each province. A case study of the allocation of CO2 intensity reduction targets in China by 2020 is then conducted via the proposed method. The per capita added value of the secondary industry is the primary factor for the increasing carbon emissions in provinces. Therefore, China should limit the growth rate of its secondary industry to mitigate emission growth. Provinces with high cardinality of emissions have to shoulder the largest reduction, whereas provinces with low emission intensity met the minimum requirements for emission in 2010. Fifteen provinces are expected to exceed the national average decrease rates from 2011 to 2020.
C65|Credit risk assessment of fixed income portfolios using explicit expressions|We propose a model to assess the credit risk features of fixed income portfolios assuming they can be characterized by two parameters: their default probability and their default correlation. We rely on explicit expressions to assess their credit risk and demonstrate the benefits of our approach in a complex leveraged structure example. We show that using expected loss as a proxy for credit risk is misleading as it does not capture the dispersion effects introduced by correlation. The implications of these findings are relevant for improving current risk management practices and for regulation purposes.
C65|The international syndicated loan market network: An “unholy trinity”?|This paper provides a descriptive analysis of the international network of syndicated loan lenders through an examination of its topology and structure using network theory measures. The author studies both the undirected and directed graphs, weighted and unweighted connections, as well as different sub-networks based on lender type and geographical regions. Results show that the networks and sub-networks generally display an “unholy trinity” of structural properties that can be related to network robustness and stability. Specifically, lender networks have high complexity and connectivity, have a small-world structure characterized by proximity and clustering and also display scale-free characteristics with preferential attachment.
C65|Equal weights coauthorship sharing and the Shapley value are equivalent|The publication credit allocation problem is one of the fundamental problems in bibliometrics. There are two solutions which do not use any additional information: equal weights measure and the Shapley value. The paper justifies the equal weights measure by showing equivalence with the Shapley value approach for sharing co-authors performance in specific games.
C65|Diversification and systemic risk|Portfolio diversification makes investors individually safer but creates connections between them through common asset holdings. Such connections create “endogenous covariances” between assets and investors, and enhance systemic risk by propagating shocks swiftly through the system. We provide a theoretical model in which shocks spread through constrained selling from N diversified portfolio investors in a network of asset holdings with home bias, and study the desirability of diversification by comparing the multivariate distribution of implied losses for every level of diversification. There may be a region on the parameter set for which the propagation effect dominates the individually safer one. We derive analytically the general element of the covariance between two assets i and j. We find agents may minimize their exposure to endogenous risk by spreading their wealth across more and more distant assets. The resulting network enhances systemic stability.
C65|Self-organization of hexagonal agglomeration patterns in new economic geography models|Self-organization of agglomeration patterns for economic models in a two-dimensional economic space is studied from a multi-disciplinary viewpoint of new economic geography, central place theory, and bifurcation theory. Emergence of hexagonal distributions of various sizes in a homogeneous space is predicted theoretically for core–periphery models. The existence of hexagonal distributions as stable equilibria is demonstrated by a comparative static analysis with respect to transport costs for specific core–periphery models. These distributions are the ones envisaged by central place theory and also inferred to emerge by Krugman (1996) for a core–periphery model in two dimensions.
C65|On the space of players in idealized limit games|This paper demonstrates the class of atomless spaces that accurately models the space of players in a large game which represents an idealized limit of a sequence of finite-player games. Through two examples, we show that arbitrary atomless probability spaces, in particular, the Lebesgue unit interval, may not be appropriate to model the space of players of an idealized limit. This inappropriateness hinges on the fact there is a convergent sequence of exact pure-strategy Nash equilibria in the sequence of finite-player games, while the idealized limit game of the sequence does not have any equilibrium. Instead, a saturated probability space is shown to be not only sufficient but also necessary, to model the space of players in any proper idealized limit. This complements the study of large games with a bio-social typology in Khan et al. [10] as such a connection between finite-limiting and idealized continuum-limit games was not able to be obtained in their framework.
C65|On the existence of mixed strategy Nash equilibria|The focus of this paper is on developing verifiable sufficient conditions for the existence of a mixed strategy Nash equilibrium for both diagonally transfer continuous and better-reply secure games. First, we show that employing the concept of diagonal transfer continuity in place of better-reply security might be advantageous when the existence of a mixed strategy Nash equilibrium is concerned. Then, we study equilibrium existence in better-reply secure games possessing a payoff secure mixed extension. With the aid of an example, we show that such games need not have mixed strategy Nash equilibria. We provide geometric conditions for the mixed extension of a two-person game that is reciprocally upper semicontinuous and uniformly payoff secure to be better-reply secure.
C65|Social influence and the Matthew mechanism: The case of an artificial cultural market|We show that the Matthew effect, or Matthew mechanism, was present in the artificial cultural market Music Lab in one-fourth of the “worlds” when social influence between individuals was allowed, whereas this effect was not present in the “world” that disallowed social influence between individuals. We also sketch on a class of social network models, derived from social influence theory, that may generate the Matthew effect. Thus, we propose a theoretical framework that may explain why the most popular songs could be much more popular, and the least popular songs could be much less popular, than when disallowing social influence between individuals.
C65|Destabilizing a stable crisis: Employment persistence and government intervention in macroeconomics|The basic Keen model is a three-dimensional dynamical system describing the time evolution of the wage share, employment rate, and private debt in a closed economy. In the absence of government intervention this system admits, among others, two locally stable equilibria: one with a finite level of debt and nonzero wages and employment rate, and another characterized by infinite debt and vanishing wages and employment. We show how the addition of a government sector, modelled through appropriately selected functions describing spending and taxation, prevents the equilibrium with infinite debt. Specifically, we show that, by countering the fall in private profits with sufficiently high government spending at low employment, the extended system can be made uniformly weakly persistent with respect to the employment rate. In other words, the economy is guaranteed not to stay in a permanently depressed state with arbitrarily low employment rates.
C65|Utilización de la lógica borrosa en la selección de personas e ideas para la participación en programas públicos de ayuda a la creación de empresas| [ES] Se propone en este trabajo un modelo de control borroso que ayude a filtrar y seleccionar las solicitudes de subvención que pueda recibir una institución pública en un programa de fomento para la creación y desarrollo de nuevas iniciativas empresariales. Creemos que la utilización de la lógica borrosa presenta ventajas sobre los procedimientos ordinarios ya que nos movemos en un escenario de actuación complejo y vago. El control borroso introduce el conocimiento de los expertos de un modo muy natural mediante variables lingüísticas y procesos de inferencia propios del lenguaje ordinario, lo que facilita la toma de decisiones en situaciones complejas. Nuestro modelo considera por un lado la idea empresarial y por otro la persona . Los indicadores y criterios que los expertos consideran relevantes para la evaluación de la subvención son modelados mediante variables lingüísticas y tratados como antecedentes y consecuentes de un motor de inferencia borroso, cuya salida nos proporciona la valoración final de la solicitud. Al final de nuestro trabajo resolvemos un caso práctico sencillo para aclarar el procedimiento.
C65|Time scales and mechanisms of economic cycles: a review of theories of long waves|This paper explores long wave theory, including Kondratieff's theory of cycles in production and relative prices; Kuznets's theory of cycles arising from infrastructure investments; Schumpeter's theory of cycles due to waves of technological innovation; Goodwin's theory of cyclical growth based on employment and wage share dynamics; Keynesâ€“Kaldorâ€“Kalecki's demand and investment-oriented theories of cycles; and Minsky's financial instability hypothesis whereby capitalist economies show a genetic propensity to boomâ€“bust cycles. This literature has been out of favor for many years but recent developments suggest a re-examination is warranted and timely.
C65|La cópula GED bivariada. Una aplicación en entornos de crisis|The General Error Distribution (GED) has been extensively used in time series econometrics applications, due to its great flexibility in the estimation of financial stylized facts. However, there has been no attempt to employ this statistical distribution in the construction of copulas. Copulas are probability functions that link one multivariate distribution to univariate distribution functions called marginals. These marginals are continuous and follow a uniform behavior within [0,1]. In this paper we introduce the bivariate GED copula to investigate financial contagion in Latinamerica during the 2008 crisis. We examine contagion in foreign exchange, equity, bonds and sovereign markets in Latinamerica. Standard decision criteria provide strong evidence in favor of the GED copula, against other widely used elliptical and arquimidean alternatives.// Mientras que la distribución de error general (GED) ha sido usada extensamente en aplicaciones de series de tiempo y ha demostrado una gran flexibilidad en la estimación de series de tiempo financieras, no se ha intentado utilizarla en la construcción de cópulas. Las cópulas son funciones de probabilidad que unen una función de distribución multivariada a funciones de distribución univariadas llamadas marginales. Se parte del supuesto de que las marginales son continuas y uniformes en el intervalo [0,1]. En este artículo proponemos la cópula GED bivariada, la cual, de acuerdo con nuestra revisión, no ha sido usada en la bibliografía. Esta función abarca otras funciones de distribución, como la gausiana o la doble exponencial, empleadas frecuentemente en el análisis de fenómenos financieros. Con el fin de probar el desempeño de esta nueva cópula investigamos el contagio financiero en la crisis de 2008 empleando tipos de cambio, acciones, bonos y mercados de deuda soberana en América Latina. Los criterios usuales de decisión proveen fuerte evidencia a favor de la cópula GED sobre otras alternativas elípticas o arquimideanas.
C65|Three-Valued Modal Logic for Qualitative Comparative Policy Analysis with Crisp-Set QCA|Contradictory and missing outcomes are problems common to many qualitative comparative studies, based on the methodology of crisp-set QCA. They also occur in public policy analyses, e.g. if important background variables are omitted or outcomes of new policies are technically censored. As a new solution to these problems, this article proposes the use of three-valued modal logic, originally introduced by the Polish philosopher Jan Lukasiewicz (1970). In addition to true and false, indeterminate is the third truth-value in this alternative approach, which serves to code missing or contradictory data. Moreover, modal operators allow a differentiation between strict and possible triggers and inhibitors of policy outcomes. The advantages of three-valued modal logic in crisp-set QCA are illustrated by an empirical example from comparative welfare policy analysis. Its conclusions allow comparisons with the corresponding results from a conventional crisp-set QCA of the same data-set.
C65|Postal financial services, development and inclusion: Building on the past and looking to the future|Post offices, inherited from the Industrial Revolution, were monolithic telephone and postal administrations. They were intimately linked to the fabric of nations and made significant contributions to state finances. From the 1960s onwards, integrators, such as UPS and FEDEX, started offering end-to-end express services, thus challenging the postal monopoly in new high added value services. Gradually, the liberalization paradigm gained ground. Telecommunications and sometimes financial services were spun off from postal operations. More recently, new policies and priorities started to emerge especially on the development agenda where financial inclusion has become a top priority in the developing world. The question to be addressed is which role, if any, the posts play or could play in ensuring inclusion. Despite an exceptionally scarce research in the field, this paper provides an overview of how these shifts in paradigm have affected postal policy, the postal financial services regulatory framework, the status of the organizations delivering those services and the offerings themselves in developing as well as in developed countries. After a research review, including the regulatory dimension, the paper focuses on how postal financial services institutions in their legal framework have developed bringing to the fore a panorama of a dozen of promising transformations of financial postal services in developing countries.
C65|Strategic Analysis Of Forest Investments Using Real Option: The Fuzzy Pay-Off Model (Fpom)|This paper introduces a fuzzy forestry investment decision-making tool based. It will be applied to choose optimal levels of investment when three possible scenarios are conceived, a base case and two extreme alternatives: optimistic and pessimistic. Decision-makers can be seen as being either owners of a forest or investors. For each of these roles the possibility degrees of the scenarios may be represented by means of fuzzy numbers, representing ambiguous net present values (NPV). Real option values (ROV) are then computed based on them. The application to a potential forestry project in Argentina shows that, while in the case of an owner of forestry project the expected benefits are both positive under NPV and ROV, an investor would discard the project if she assumes equal weights for the scenarios in a traditional analysis but would accept it under the fuzzy approach.
C65|Mathematical definition, mapping, and detection of (anti)fragility|"We provide a mathematical definition of fragility and antifragility as negative or positive sensitivity to a semi-measure of dispersion and volatility (a variant of negative or positive ""vega"") and examine the link to nonlinear effects. We integrate model error (and biases) into the fragile or antifragile context. Unlike risk, which is linked to psychological notions such as subjective preferences (hence cannot apply to a coffee cup) we offer a measure that is universal and concerns any object that has a probability distribution (whether such distribution is known or, critically, unknown). We propose a detection of fragility, robustness, and antifragility using a single ""fast-and-frugal"", model-free, probability free heuristic that also picks up exposure to model error. The heuristic lends itself to immediate implementation, and uncovers hidden risks related to company size, forecasting problems, and bank tail exposures (it explains the forecasting biases). While simple to implement, it outperforms stress testing and other such methods such as Value-at-Risk.<br><small>(This abstract was borrowed from another version of this item.)</small><br><small>(This abstract was borrowed from another version of this item.)</small>"
C65|Movilidad endógena y variaciones demográficas: una aplicación para Ecuador|Se establece un modelo que describe la movilidad interna y externa a nivel provincial de la población ecuatoriana, considerando su autoidentificación étnica (indígena y no indígena). El estudio utiliza las cadenas de Markov; para desarrollar el modelo estocástico, se han tomado como base los datos del Censo de Población y Vivienda 2010, elaborado por el Instituto Nacional de Estadística y Censos (INEC).
C65|Un estudio exploratorio sobre el crecimiento de la industria mexicana y el comercio exterior, periodo 2004–2012|Se muestra la evolución de la industria nacional por medio del comportamiento de las ramas económicas evaluadas a través de un modelo de ponderación de la tasa de crecimiento de variables económico-financieras, y se seleccionan las mejores actividades económicas nacionales. Esto establece las áreas de oportunidad para llevar a cabo estrategias de desarrollo a nivel nacional para ser competitivos globalmente. El aporte principal es encontrar la relación entre los factores económicos locales y el comercio exterior para establecer estrategias a mediano y a largo plazo con la evolución empresarial en México y alcanzar la competitividad a nivel internacional./ This paper studies the development of the domestic industry through the behavior of the economic activity. It was evaluated by the growth of economic-financial variables rate, which allows us to state which the healthiest industries. This provides the opportunity areas for carrying out development strategies at national level to be competitive globally. The main contribution is to find the relationship between the local economic factors and the foreign trade to set up strategies in the medium and long term business developments in Mexico to be internationally competitive.
C65|Majorized correspondences and equilibrium existence in discontinuous games|This paper is aimed at widening the scope of applications of majorized correspondences. A new class of majorized correspondences -- domain U-majorized correspondences -- is introduced. For them, a maximal element existence theorem is established. Then, sufficient conditions for the existence of an equilibrium in qualitative games are provided. They are used to show the existence of a pure strategy Nash equilibrium in compact quasiconcave games that are either correspondence secure or correspondence transfer continuous.
C65|On Kronecker Products, Tensor Products And Matrix Differential Calculus|The algebra of the Kronecker products of matrices is recapitulated using a notation that reveals the tensor structures of the matrices. It is claimed that many of the difficulties that are encountered in working with the algebra can be alleviated by paying close attention to the indices that are concealed beneath the conventional matrix notation. The vectorisation operations and the commutation transformations that are common in multivariate statistical analysis alter the positional relationship of the matrix elements. These elements correspond to numbers that are liable to be stored in contiguous memory cells of a computer, which should remain undisturbed. It is suggested that, in the absence of an adequate index notation that enables the manipulations to be performed without disturbing the data, even the most clear-headed of computer programmers is liable to perform wholly unnecessary and time-wasting operations that shift data between memory cells.
C65|Risk Appetite in Practice: Vulgaris Mathematica|The ultimate goal of risk management is the generation of efficient incomes. The objective is to generate the maximum return for a unit of risk taken or to minimise the risk taken to generate the return expected i.e. it is the optimisation of a financial institution strategy. Therefore, by measuring its exposure against its appetite, a financial institution is assessing its couple risk-return. But this taskk may be difficult as banks face various types of risks, for instance, Operational, Market, Credit, Liquidity, etx. and these cannot be evaluated on a stand alone basis, interaction and contagion effects should be taken into account. In this paper, methodologies to evaluate banks' exposures are presented along their management implications, as the purpose of the risk appetite evaluation process is the transformation of risk metrics into effective management decisions
C65|Soving Problems of Linear Algebra and Linear Programming Using MS Excel|This article demonstrates the benefits of solving the general problem of linear algebra and linear programming by use of MS Excel algorithms. MS Excel from MS Office is one of the most accessible software package. Studying Excel in depth gives the opportunity for easy assimilation of complex algorithms and elaborate on mathematical models. Solving the assigned problems becomes faster and it is easier to illustrate the basic principles and practical applications without bearing the burden of calculation procedures.
C65|Predicted Number of the Rural Population by 2020 by Planning Regions|The serious challenges to the implementation of the Programme for Rural Development in the new programming period 2014 2020 (PRD) raises due to the heavy demographic crisis of country and especially of the rural areas. This defines the purpose of the article. On the one hand, the main forecasts the expected total number of rural population incl. in sex-age groups by 2020 for six planning regions are presented and analyzed. On the other hand, recommendations regarding the need for urgent, complex and unusual measures to get out of the demographic crisis are made. The component method of population is used for preparing the forecasts. Years from 2001 to 2010. are selected for basic period. The specific features of the statistical regions in relation to the rural demographic development are discovered. A further tendency of the rural population decrease and continuing village depopulation are contoured. This trend is typical for all planning areas but level and pace of ongoing demographic processes are different. The impact of population growth on demographic development in the rural areas will continue to have a dominant influence than the impact of migration processes. The summarizing conclusions from analysis of the expected demographic situation in rural areas are derived. An attempt to lay down recommendations to solve this with essential, as complex and difficult problem is made. Among them is a reasonable idea of creating a special fund under certain requirements to help the demographic salvation of Bulgarian villages. Among them is motivated the idea of creating a special fund subject to certain requirements to help the demographic rescue of Bulgarian villages.
C65|Limits to Rational Learning|A long-standing open question raised in the seminal paper of Kalai and Lehrer (1993) is whether or not the play of a repeated game, in the rational learning model introduced there, must eventually resemble play of exact equilibria, and not just play of approximate equilibria as demonstrated there. This paper shows that play may remain distant - in fact, mutually singular - from the play of any equilibrium of the repeated game. We further show that the same inaccessibility holds in Bayesian games, where the play of a Bayesian equilibrium may continue to remain distant from the play of any equilibrium of the true game.
C65|On the use of palynological data in economic history: New methods and an application to agricultural output in Central Europe, 0–2000AD|In this paper we introduce a new source of data to economic history: palynological data or, in other words, information about pollen grains which are preserved in the bottom sediments of various water basins. We discuss how this data is collected and how it should be interpreted; develop new methods for aggregating this information into regional trends in agricultural output; construct an extensive dataset with a large number of pollen sites from Central Europe; and use our methods to study the economic history of Greater Poland, Lesser Poland, Bohemia, Brandenburg, and Lower Saxony since the first century AD.
C65|Non-Associativity of Lorentz Transformation and Associative Reciprocal Symmetric Transformation|Lorentz transformation is not associative. The non-associativity makes it frame dependent; and it does not fulfill relativistic requirements including reciprocity principle. The non associativity also leads to ambiguities when three or more velocities are involved. We have proposed an associative Reciprocal Symmetric Transformation (RST) to replace Lorentz transformation. RST is complex and is compatible with Pauli and Dirac algebra.
C65|A Comparison Between Direct and Indirect Seasonal Adjustment of the Chilean GDP 1986–2009 with X-12-ARIMA|Abstract It is well known among practitioners that the seasonal adjustment applied to economic time series involves several decisions to be made by the econometrician. As such, it would always be desirable to have an informed opinion on the risks taken by each of those decisions. In this paper, I assess which disaggregation strategy delivers the best results for the case of the Chilean 1986–2009 GDP quarterly dataset (base year: 2003). This is done by performing an aggregate-by-disaggregate analysis under different schemes, as the fixed base year dataset allows this fair comparison. The analysis is based on seasonal adjustment diagnostics contained in the X-12-ARIMA program plus some statistical tests for robustness. This exercise is relevant for conjunctural economic assessment, as it concerns signal extraction from seasonal, noisy series, direction of change detection, and econometric applications based on reliable and accurate unobserved variables. The results show that it is preferable, in terms of stability, to use the first block of supply-side disaggregation, while demand-side disaggregation tends to be less reliable. This result carries important implications for policymakers aiming to evaluate its short-term effectiveness in both households and firms.
C65|On Infinite Dimensional Linear-Quadratic Problem with Fixed Endpoints. Continuity Question|In a Hilbert space setting, necessary and sufficient conditions for the minimum norm solution u to the equation Su = Rz to be continuously dependent on z are given. These conditions are used to study continuity of the minimum energy and linear-quadratic control problems for infinite dimensional linear systems with fixed endpoints.
C65|Assessing Impact of Large-Scale Distributed Residential HVAC Control Optimization on Electricity Grid Operation and Renewable Energy Integration|Demand management is an important component of the emerging Smart Grid, and a potential solution to the supply-demand imbalance occurring increasingly as intermittent renewable electricity is added to the generation mix. Model predictive control (MPC) has shown great promise for controlling HVAC demand in commercial buildings, making it an ideal solution to this problem. MPC is believed to hold similar promise for residential applications, yet very few examples exist in the literature despite a growing interest in residential demand management. This work explores the potential for residential buildings to shape electric demand at the distribution feeder level in order to reduce peak demand, reduce system ramping, and increase load factor using detailed sub-hourly simulations of thousands of buildings coupled to distribution power ow software. More generally, this work develops a methodology for the optimization of residential HVAC operation using a distributed but directed MPC scheme that can be applied today's programmable thermostat technologies to address the increasing variability in electric supply and demand. Case studies incorporating varying levels of renewable energy generation demonstrate the approach and highlight important considerations for large-scale residential model predictive control.
C65|Analyzing the Effect of Real Exchange Rate on Petrochemicals Exporting|The export of petrochemical products -as a type of non-oil export- plays a key role in the economic development of our country. This is of special importance in light of the structure of Iran's economy that is oil-based. Identifying the factors affecting the export of petrochemical products can improve their export. Using Johansen-Juselius co-integration method and the error correction model, the present study purports to investigate the effects of the real foreign exchange rate and the total value of petrochemical products on the export of these products in Iran. This research used data from 1989 to 2012. It was found that the real foreign exchange rate and the real value of total petrochemical products positively affect their export in the long run, and the effect of the former is greater than that of the latter. However, in the short run the effect of the foreign exchange rate on the export of petrochemical products is more significant.
C65|Mean-median compromise method as an innovating voting rule in social choice theory|This paper aims at presenting a new voting function which is obtained in Balinski-Laraki's framework and benefits mean and median advantages. The so-called Mean-Median Comprise Method (MMCM) has fulfilled criteria such as unanimity, neutrality, anonymity, monotonicity, and Arrow's independence of irrelevant alternatives. It also generalizes approval voting system.
C65|Generating Function for M(m, n)|This paper shows that the coefficient of x in the right hand side of the equation for M(m, n) for all n >1 is an algebraic relation in terms of z. The exponent of z represents the crank of partitions of a positive integral value of n and also shows that the sum of weights of corresponding partitions of n is the sum of ordinary partitions of n and it is equal to the number of partitions of n with crank m. This paper shows how to prove the Theorem “The number of partitions π of n with crank C(π) = m is M(m, n) for all n >1.”
C65|Elementos microeconómicos para gravar el consumo de la marihuana en México|El propósito de este artículo es describir y analizar el mercado de la marihuana en Estados Unidos y en México desde el punto de vista microeconómico para señalar algunos elementos que pudieran permitir legalizar, despenalizar, prohibir o gravar indirectamente su consumo. Para tal efecto, se realiza una aproximación a las elasticidades de la demanda de la droga y, a través de un modelo de clústeres, clasificando en dos grupos los datos de acuerdo a su similitud, muestra que la tendencia de la ingesta de marihuana en México (nacional) ha ido en aumento en el país, mientas que en la región norte con drogas ilegales (anfetamínicos y estupefacientes) su incremento es muy pequeño.
C65|A Proof Without Words and a Maximum without Calculus|We define a standard optimization problem with quadratic objective function and provide a rigorous visual proof for its solution without using calculus. We then show that such standard problem is a building block for several economic models related to microeconomics, game theory and pricing strategies.
C65|Shifting martingale measures and the birth of a bubble as a submartingale|In an incomplete financial market model, we study a flow in the space of equivalent martingale measures and the corresponding shifting perception of the fundamental value of a given asset. This allows us to capture the birth of a perceived bubble and to describe it as an initial submartingale which then turns into a supermartingale before it falls back to its initial value zero. Copyright Springer-Verlag Berlin Heidelberg 2014
C65|Rationalizability in large games|This paper characterizes both point-rationalizability and rationalizability in large games when societal responses are formulated as distributions or averages of individual actions. The sets of point-rationalizable and rationalizable societal responses are defined and shown to be convex, compact and equivalent to those outcomes that survive iterative elimination of never best responses, under point-beliefs and probabilistic beliefs, respectively. Given the introspection and mentalizing that rationalizability notions presuppose, one motivation behind the work is to examine their viability in situations where the terms rationality and full information can be given a more parsimonious, and thereby a more analytically viable, expression. Copyright Springer-Verlag Berlin Heidelberg 2014
C65|The signs of change in economic evolution|Neo-Schumpeterian evolutionary economics has, since the early works of Nelson and Winter, defined evolution as the change of the mean of a characteristic of a population. This paper trancends the previous paradigm and explores novel aspects of evolution in economics. Within the traditional paradigm change is provided by directional selection (and directional innovation). However, the full definition of evolutionary processes has to include two important types of selection that change the variance without necessarily changing the mean. Stabilizing selection removes any outlier and diversifying selection promotes the coexistence of behavioural variants. This paper emphasizes the need for an integrated analysis of all three types of selection. It also demonstrates that the evolutionary algebra provided by Price’s equation increases the intellectual coherence and power of thinking about selection and other aspects of evolutionary processes. Directional, stabilizing and diversifying selection are then related to fitness functions that can produce the different types of selection; and the functions are used for simple simulations of the change of the population distribution of a quantitative characteristic. Finally, the paper adds to evolutionary economics a novel way of using Price’s equation to decompose the statistics of the changes of the frequency distributions. The changes of mean, variance, skewness and kurtosis are all decomposed as the sum of a selection effect and an intra-member effect. It is especially the signs of these effects that serve to define and characterize the different types of selection. Both this result and the general analysis of the types of selection are of relevance for applied evolutionary economics. Copyright Springer-Verlag Berlin Heidelberg 2014
C65|Preference, topology and measure|A symmetric difference metric topology on the collection of binary relations on a countably infinite set provides a new setting for the study of properties of preferences and, as an illustration, is used to lend credence and meaning to some simple intuitions about properties of binary relations. A finite measure on a $$\sigma $$ σ -algebra over the same collection of binary relations is used to provide support for the topological results. Copyright Springer-Verlag Berlin Heidelberg 2014
C65|Directional, stabilizing and disruptive selection: An analysis of aspects of economic evolution based on Price’s equation|This paper tries to demonstrate that the well developed analysis of directional selection within evolutionary economics can be complemented by analyses of stabilizing selection and disruptive selection. It also tries to demonstrate that the evolutionary algebra provided by Price’s equation increases the intellectual coherence and power of thinking about selection and other aspects of evolutionary processes. The paper combines these aims by analysing the types of selection by means of the algebra of evolution provided by Price’s equation. To prepare for this task, the paper starts by reviewing recent discussions in relation to Price’s equation. This review includes the presentation of framework for analysing evolution that then is used for the definition and analysis of directional, stabilizing and disruptive selection. These types of selection are then related to fitness functions that can produce the different types of selection; and the functions are used for simple simulations of the change of the population distribution of a quantitative characteristic. Finally, Price’s equation is used to decompose the statistics of the changes of the frequency distributions. The changes of mean, variance, skewness and kurtosis are all decomposed as the sum of a selection effect and an intra-member effect. It is especially the signs of these effects that serve to define and characterize the different types of selection. Both this result and the general analysis of the types of selection seem to be of relevance for applied evolutionary economics.
