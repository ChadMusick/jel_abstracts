C14|Consistent Inference for Predictive Regressions in Persistent VAR Economies|This paper studies the properties of standard predictive regressions in model economies, characterized through persistent vector autoregressive dynamics for the state variables and the associated series of interest. In particular, we consider a setting where all, or a subset, of the variables may be fractionally integrated, and note that this induces a spurious regression problem. We then propose a new inference and testing procedure - the local spectrum (LCM) approach - for the joint significance of the regressors, which is robust against the variables having different integration orders. The LCM procedure is based on (semi-)parametric fractional-filtering and band spectrum regression using a suitably selected set of frequency ordinates. We establish the asymptotic properties and explain how they differ from and extend existing procedures. Using these new inference and testing techniques, we explore the implications of assuming VAR dynamics in predictive regressions for the realized return variation. Standard least squares predictive regressions indicate that popular financial and macroeconomic variables carry valuable information about return volatility. In contrast, we find no significant evidence using our robust LCM procedure, indicating that prior conclusions may be premature. In fact, if anything, our results suggest the reverse causality, i.e., rising volatility predates adverse innovations to key macroeconomic variables. Simulations are employed to illustrate the relevance of the theoretical arguments for finite-sample inference.
C14|Inference for Local Distributions at High Sampling Frequencies: A Bootstrap Approach|"We study inference for the local innovations of It^o semimartingales. Specifically, we construct a resampling procedure for the empirical CDF of high-frequency innovations that have been standardized using a nonparametric estimate of its stochastic scale (volatility) and truncated to rid the effect of ""large"" jumps. Our locally dependent wild bootstrap (LDWB) accommodate issues related to the stochastic scale and jumps as well as account for a special block-wise dependence structure induced by sampling errors. We show that the LDWB replicates first and second-order limit theory from the usual empirical process and the stochastic scale estimate, respectively, as well as an asymptotic bias. Moreover, we design the LDWB sufficiently general to establish asymptotic equivalence between it and and a nonparametric local block bootstrap, also introduced here, up to second-order distribution theory. Finally, we introduce LDWB-aided Kolmogorov-Smirnov tests for local Gaussianity as well as local von-Mises statistics, with and without bootstrap inference, and establish their asymptotic validity using the second-order distribution theory. The finite sample performance of CLT and LDWB-aided local Gaussianity tests are assessed in a simulation study as well as two empirical applications. Whereas the CLT test is oversized, even in large samples, the size of the LDWB tests are accurate, even in small samples. The empirical analysis verifies this pattern, in addition to providing new insights about the distributional properties of equity indices, commodities, exchange rates and popular macro finance variables."
C14|Edgeworth expansion for Euler approximation of continuous diffusion processes|In this paper we present the Edgeworth expansion for the Euler approximation scheme of a continuous diffusion process driven by a Brownian motion. Our methodology is based upon a recent work [22], which establishes Edgeworth expansions associated with asymptotic mixed normality using elements of Malliavin calculus. Potential applications of our theoretical results include higher order expansions for weak and strong approximation errors associated to the Euler scheme, and for studentized version of the error process.
C14|Modelling time-varying income elasticities of health care expenditure for the OECD|Income elasticity dynamics of health expenditure is considered for the OECD and the Eurozone over the period 1995-2014. This paper studies a novel non-linear cointegration model with fixed effects, controlling for cross-section dependence and unobserved heterogeneity. Most importantly, its coefficients can vary over time and its variables can be non-stationary. The resulting asymptotic theory is fundamentally different with a faster rate of convergence to similar kernel smoothing methodologies. A fully modified kernel regression method is also proposed to reduce the asymptotic bias. Results show a steep increase in the income elasticity for the OECD and a small increase for the Eurozone.
C14|Testing Nonlinearity through a Logistic Smooth Transition AR Model with Logistic Smooth Transition GARCH Errors|This paper analyzes the cyclical behavior of CAC 40 by testing the existence of nonlinearity through a logistic smooth transition AR model with logistic smooth transition GARCH errors. We study the daily returns of CAC 40 from 1990 to 2018. We estimate several models using nonparametric maximum likelihood, where the innovation distribution is replaced by a nonparametric estimate for the density function. We find that the rate of transition and the threshold value in both the conditional mean and conditional variance are highly significant. The forecasting results show that the informational shocks have transitory effects on returns and volatility and confirm nonlinearity.
C14|Memory that Drives! New Insights into Forecasting Performance of Stock Prices from SEMIFARMA-AEGAS Model|Stock price forecasting, a popular growth-enhancing exercise for investors, is inherently complex – thanks to the interplay of financial economic drivers which determine both the magnitude of memory and the extent of non-linearity within a system. In this paper, we accommodate both features within a single estimation framework to forecast stock prices and identify the nature of market efficiency commensurate with the proposed model. We combine a class of semiparametric autoregressive fractionally integrated moving average (SEMIFARMA) model with asymmetric exponential generalized autoregressive score (AEGAS) errors to design a SEMIRFARMA-AEGAS framework based on which predictive performance of this model is tested against competing methods. Our conditional variance includes leverage effects, jumps and fat tail-skewness distribution, each of which affects magnitude of memory in a stock price system. A true forecast function is built and new insights into stock price forecasting are presented. We estimate several models using the Skewed Student-t maximum likelihood and find that the informational shocks have permanent effects on returns and the SEMIFARMA-AEGAS is appropriate for capturing volatility clustering for both negative (long Value-at-Risk) and positive returns (short Value-at-Risk). We show that this model has better predictive performance over competing models for both long and/or some short time horizons. The predictions from SEMIRFARMA-AEGAS model beats comfortably the random walk model. Our results have implications for market-efficiency: the weak efficiency assumption of financial markets stands violated for all stock price returns studied over a long period.
C14|On binscatter|Binscatter is very popular in applied microeconomics. It provides a flexible, yet parsimonious way of visualizing and summarizing “big data” in regression settings, and it is often used for informal testing of substantive hypotheses such as linearity or monotonicity of the regression function. This paper presents a foundational, thorough analysis of binscatter: We give an array of theoretical and practical results that aid both in understanding current practices (that is, their validity or lack thereof) and in offering theory-based guidance for future applications. Our main results include principled number of bins selection, confidence intervals and bands, hypothesis tests for parametric and shape restrictions of the regression function, and several other new methods, applicable to canonical binscatter as well as higher-order polynomial, covariate-adjusted, and smoothness-restricted extensions thereof. In particular, we highlight important methodological problems related to covariate adjustment methods used in current practice. We also discuss extensions to clustered data. Our results are illustrated with simulated and real data throughout. Companion general-purpose software packages for Stata and R are provided. Finally, from a technical perspective, new theoretical results for partitioning-based series estimation are obtained that may be of independent interest.
C14|The Empirical Content of Binary Choice Models|Empirical demand models used for counterfactual predictions and welfare analysis must be rationalizable, i.e. theoretically consistent with utility maximization by heterogeneous consumers. We show that for binary choice under general unobserved heterogeneity, rationalizability is equivalent to a pair of Slutsky-like shape-restrictions on choice-probability functions.The forms of these restrictions differ from Slutsky-inequalities for continuous goods. Unlike McFadden-Richter's stochastic revealed preference, our shape-restrictions (a) are global, i.e. their forms do not depend on which and how many budget-sets are observed, (b) are closed-form, hence easy to impose on parametric/semi/non-parametric models in practical applications, and (c) provide computationally simple, theory-consistent bounds on demand and welfare predictions on counterfactual budget-sets.
C14|Income Effects and Rationalizability in Multinomial Choice Models|In multinomial choice settings, Daly-Zachary (1978) and Armstrong-Vickers (2015) provided closedform conditions, under which choice probability functions can be rationalized via random utility models. A key condition is Slutsky symmetry. We first show that in the multinomial context, Daly-Zachary’s Slutsky symmetry is equivalent to absence of income-effects. Next, for general multinomial choice that allows for income-effects, we provide global shape restrictions on choice probability functions, which are shown to be sufficient for rationalizability. Finally, we outline nonparametric identification of preference distributions using these results. The theory of linear partial differential equations plays a key role in our analysis.
C14|Inference for first-price auctions with Guerre, Perrigne, and Vuong’s estimator|We consider inference on the probability density of valuations in the first-price sealed-bid auctions model within the independent private value paradigm. We show the asymptotic normality of the two-step nonparametric estimator of Guerre et al. (2000) (GPV), and propose an easily implementable and consistent estimator of the asymptotic variance. We prove the validity of the pointwise percentile bootstrap confidence intervals based on the GPV estimator. Lastly, we use the intermediate Gaussian approximation approach to construct bootstrap-based asymptotically valid uniform confidence bands for the density of the valuations.
C14|Random Forest Estimation of the Ordered Choice Model|In econometrics so-called ordered choice models are popular when interest is in the estimation of the probabilities of particular values of categorical outcome variables with an inherent ordering, conditional on covariates. In this paper we develop a new machine learning estimator based on the random forest algorithm for such models without imposing any distributional assumptions. The proposed Ordered Forest estimator provides a flexible estimation method of the conditional choice probabilities that can naturally deal with nonlinearities in the data, while taking the ordering information explicitly into account. In addition to common machine learning estimators, it enables the estimation of marginal effects as well as conducting inference thereof and thus providing the same output as classical econometric estimators based on ordered logit or probit models. An extensive simulation study examines the finite sample properties of the Ordered Forest and reveals its good predictive performance, particularly in settings with multicollinearity among the predictors and nonlinear functional forms. An empirical application further illustrates the estimation of the marginal effects and their standard errors and demonstrates the advantages of the flexible estimation compared to a parametric benchmark model.
C14|More insecure and less paid? The effect of perceived job insecurity on wage distribution|This article employs a Counterfactual Decomposition Analysis (CDA) using both a semi-parametric and a non-parametric method to examine the pay gap, over the entire wage distribution, between secure and insecure workers on the basis of perceived job insecurity. Using the 2015 INAPP Survey on Quality of Work, our results exhibit a mirror J-shaped pattern in the pay gap, with a significant sticky floor effect, i.e. the job insecurity more relevant at the lowest quantiles. This pattern is mainly due to the characteristics effect, while the relative incidence of the coefficient component accounts roughly for 22 up to 36% of the total difference, being more relevant at the bottom of the wage distribution.
C14|Low-performing student responses to state merit scholarships|State merit scholarships may affect academic outcomes for low-performing college students, yet low-performers are generally overlooked in existing literature. New Mexico’s lottery scholarship provides tuition-free college to residents meeting a uniquely “low-bar” eligibility criteria. Using administrative data, a discontinuity in eligibility rules identifies local average treatment effects on degree completion and course taking behavior for students with below-average college grades. Results suggest a reduction in time-to-degree corresponding to the scholarship’s funding cap, with no overall change in degree completion. Despite modest eligibility requirements related to credit completion, the scholarship increased credit completion among low-achieving students. Some students appear to manipulate scholarship eligibility by taking fewer courses or strategically dropping courses during a qualifying semester in order to secure aid. A bounding exercise suggests partial manipulation of eligibility rules results in selection bias which underestimates the true effect of the scholarship on time to degree and credit completion.
C14|Tail index estimation: quantile driven threshold selection|The selection of upper order statistics in tail estimation is notoriously difficult. Most methods are based on asymptotic arguments, like minimizing the asymptotic mse, that do not perform well in finite samples. Here we advance a data driven method that minimizes the maximum distance between the fitted Pareto type tail and the observed quantile. To analyse the finite sample properties of the metric we organize a horse race between the other methods. In most cases the finite sample based methods perform best. To demonstrate the economic relevance of choosing the proper methodology we use daily equity return data from the CRSP database and find economic relevant variation between the tail index estimates.
C14|Effectiveness of FX Intervention and the Flimsiness of Exchange rate Expectations|Most of the foreign exchange intervention literature overlooks the influence of market uncertainty when evaluating effectiveness. In this paper we take a fresh new look at how this uncertainty amplifies exchange rate effects. Our contribution is twofold. We first posit a partial equilibrium model with frictions to illustrate that when uncertainty is low, intervention is less effective, for agents are willing to bet against the central bank. Conversely, when uncertainty is high, intervention faces a weaker countervailing force from speculators and arbitragers. Second, we empirically test for the incremental effects of flimsy exchange rate fundamentals by using a sharp policy discontinuity in the way the Central Bank of Colombia intervened in the FX market. Our results indicate that market uncertainty increases depreciation of domestic currency in approximately 1% following central bank purchases of foreign currency and extends its duration in up to 2 weeks. Additionally, these purchases have an incremental effect in stemming exchange rate volatility in up to 7%. **** RESUMEN: En este trabajo examinamos la influencia de la incertidumbre sobre la tasa de cambio futura en la efectividad de la intervención cambiaria. Nuestra contribución consta de dos partes. Primero, desarrollamos un modelo teórico de equilibrio parcial para ilustrar cómo la efectividad cambiaria aumenta a medida que aumenta la incertidumbre sobre la tasa de cambio futura o de sus determinantes. Segundo, presentamos evidencia empírica de esta relación haciendo uso del esquema de intervención cambiaria del Banco de la República, empleado durante el periodo 2002-2012, a través de opciones put de volatilidad. Nuestros resultados indican que, en presencia de alta incertidumbre en los fundamentales de la tasa de cambio, la efectividad de la intervención esterilizada aumenta en aproximadamente 1% y su duración se extiende hasta por dos semanas. Adicionalmente, encontramos que, en períodos de alta incertidumbre, la intervención reduce la volatilidad cambiaria hasta en un 7% adicional.
C14|The Phillips Multiplier|We propose a model-free approach for determining the inflation-unemployment trade-off faced by a central bank, i.e., the ability of a central bank to transform unemployment into inflation (and vice versa) via its interest rate policy. We introduce the Phillips multiplier as a statistic to non-parametrically characterize the trade-off and its dynamic nature. We compute the Phillips multiplier for the US, UK and Canada and document that the trade-off went from being very large in the pre-1990 sample period to being small (but significant) post-1990 with the onset of inflation targeting and the anchoring of inflation expectations.
C14|Identifying modern macro equations with old shocks|"Despite decades of research, the consistent estimation of structural forward looking macroeconomic equations remains a formidable empirical challenge because of pervasive endogeneity issues. Prominent cases |the estimation of Phillips curves, of Euler equations for consumption or output, or of monetary policy rules| have typically relied on using pre-determined variables as instruments, with mixed success. In this work, we propose a new approach that consists in using sequences of independently identi ed structural shocks as instrumental variables. Our approach is robust to weak instruments and is valid regardless of the shocks' variance contribution. We estimate a Phillips curve using monetary shocks as instruments and nd that conventional methods (i) substantially under-estimate the slope of the Phillips curve and (ii) over-estimate the role of forward-looking in ation expectations."
C14|Time varying cointegration and the UK Great Ratios|We re-examine the great ratios associated with balanced growth models and ask whether they have remained constant over time. We first use a benchmark DSGE model to explore how plausible smooth variations in structural parameters lead to movements in great ratios that are comparable to those seen in the UK data. We then employ a nonparametric methodology that allows for slowly varying coefficients to estimate trends over time. To formally test for stable relationships in the great ratios, we propose a statistical test based on these nonparametric estimators devised to detect time varying cointegrating relationships. Small sample properties of the test are explored in a small Monte Carlo exercise. Generally, we find no evidence for cointegration when parameters are constant, but strong evidence when allowing for time variation. The implications are that in macroeconometric models allowance should be made for shifting long-run relationships, including DSGE models where smooth variation should be allowed in the deep structural relationships.
C14|Changing Preferences: An Experiment and Estimation of Market-Incentive EÂ§ects on Altruism|This paper studies how altruistic preferences are changed by markets and incentives. We conduct a laboratory experiment in a within-subject design. Subjects are asked to choose health care qualities for hypothetical patients in monopoly, duopoly, and quadropoly. Prices, costs, and patient benefits are experimental incentive parameters. In monopoly, subjects choose quality to tradeoff between profits and altruistic patient benefits. In duopoly and quadropoly, we model subjects playing a simultaneous-move game. Each subject is uncertain about an opponentÌ s altruism, and competes for patients by choosing qualities. Bayes-Nash equilibria describe subjects' quality decisions as functions of altruism. Using a nonparametric method, we estimate the population altruism distributions from Bayes-Nash equilibrium qualities in diÂ§erent markets and incentive conOÌˆgurations. Markets tend to reduce altruism, although duopoly and quadropoly equilibrium qualities are much higher than those in monopoly. Although markets crowd out altruism, the disciplinary powers of market competition are stronger. Counterfactuals confirm markets change preferences.
C14|Efficient Estimation of Nonparametric Regression in The Presence of Dynamic Heteroskedasticity|We study the efficient estimation of nonparametric regression in the presence of heteroskedasticity. We focus our analysis on local polynomial estimation of nonparametric regressions with conditional heteroskedasticity in a time series setting. We introduce a weighted local polynomial regression smoother that takes account of the dynamic heteroskedasticity. We show that, although traditionally it is adviced that one should not weight for heteroskedasticity in nonparametric regressions, in many popular nonparametric regression models our method has lower asymptotic variance than the usual unweighted procedures. We conduct a Monte Carlo investigation that confirms the efficiency gain over conventional nonparametric regression estimators infinite samples.
C14|Nonparametric Recovery of the Yield Curve Evolution from Cross-Section and Time Series Information|We develop estimation methodology for an additive nonparametric panel model that is suitable for capturing the pricing of coupon-paying government bonds followed over many time periods. We use our model to estimate the discount function and yield curve of nominally riskless government bonds. The novelty of our approach is the combination of two different techniques: cross-sectional nonparametric methods and kernel estimation for time varying dynamics in the time series context. The resulting estimator is able to capture the yield curve shapes and dynamics commonly observed in the fixed income markets. We establish the consistency, the rate of convergence, and the asymptotic normality of the proposed estimator. A Monte Carlo exercise illustrates the good performance of the method under different scenarios. We apply our methodology to the daily CRSP bond dataset, and compare with the popular Diebold and Li (2006) method.
C14|Nonparametric Predictive Regressions for Stock Return Prediction|We propose two new nonparametric predictive models: the multi-step nonparametric predictive regression model and the multi-step additive predictive regression model, in which the predictive variables are locally stationary time series. We define estimation methods and establish the large sample properties of these methods in the short horizon and the long horizon case. We apply our methods to stock return prediction using a number of standard predictors such as dividend yield. The empirical results show that all of these models can substantially outperform the traditional linear predictive regression model in terms of both in-sample and out-of-sample performance. In addition, we _nd that these models can always beat the historical mean model in terms of in-sample fitting, and also for some cases in terms of the out-of-sample forecasting. We also compare our methods with the linear regression and historical mean methods according to an economic metric. In particular, we show how our methods can be used to deliver a trading strategy that beats the buy and hold strategy (and linear regression based alternatives) over our sample period.
C14|Inference on a Distribution from Noisy Draws|We consider a situation where the distribution of a random variable is being estimated by the empirical distribution of noisy measurements of that variable. This is common practice in, for example, teacher value-added models and other fixed-effect models for panel data. We use an asymptotic embedding where the noise shrinks with the sample size to calculate the leading bias in the empirical distribution arising from the presence of noise. The leading bias in the empirical quantile function is equally obtained. These calculations are new in the literature, where only results on smooth functionals such as the mean and variance have been derived. Given a closed-form expression for the bias, bias-corrected estimator of the distribution function and quantile function can be constructed. We provide both analytical and jackknife corrections that recenter the limit distribution and yield confidence intervals with correct coverage in large samples. These corrections are non-parametric and easy to implement. Our approach can be connected to corrections for selection bias and shrinkage estimation and is to be contrasted with deconvolution. Simulation results confirm the much-improved sampling behavior of the corrected estimators.
C14|Dependent Microstructure Noise and Integrated Volatility: Estimation from High-Frequency Data|In this paper, we develop econometric tools to analyze the integrated volatility (IV) of the efficient price and the dynamic properties of microstructure noise in high-frequency data under general dependent noise. We first develop consistent estimators of the variance and autocovariances of noise using a variant of realized volatility. Next, we employ these estimators to adapt the pre-averaging method and derive consistent estimators of the IV, which converge stably to a mixed Gaussian distribution at the optimal rate n1/4. To improve the finite sample performance, we propose a multi-step approach that corrects the finite sample bias, which turns out to be crucial in applications. Our extensive simulation studies demonstrate the excellent performance of our multi-step estimators. In an empirical study, we analyze the dependence structures of microstructure noise and provide intuitive economic interpretations; we also illustrate the importance of accounting for both the serial dependence in noise and the finite sample bias when estimating IV.
C14|Asymptotic F Tests under Possibly Weak Identification|This paper develops asymptotic F tests robust to weak identification and temporal dependence. The test statistics are modified versions of the S statistic of Stock and Wright (2000) and the K statistic of Kleibergen (2005), both of which are based on the continuous updating generalized method of moments. In the former case, the modification involves only a multiplicative degree-of-freedom adjustment. In the latter case, the modification involves an additional multiplicative adjustment that uses a J statistic for testing overidentification. By adopting fixed-smoothing asymptotics, we show that both the modified S statistic and the modified K statistic are asymptotically F-distributed. The asymptotic F theory accounts for the estimation errors in the underlying heteroskedasticity and autocorrelation robust variance estimators, which the asymptotic chi-squared theory ignores. Monte Carlo simulations show that the F approximations are much more accurate than the corresponding chi-squared approximations in finite samples.
C14|Average Derivative Estimation Under Measurement Error|In this paper, we derive the asymptotic properties of average derivative estimators when the regressors are contaminated with classical measurement error and the density of this error is unknown. Average derivatives of conditional mean functions are used extensively in economics and statistics, most notably in semiparametric index models. As well as ordinary smooth measurement error, we provide results for supersmooth error distributions. This is a particularly important class of error distribution as it includes the popular Gaussian density. We show that under this ill-posed inverse problem, despite using nonparametric deconvolution techniques and an estimated error characteristic function, we are able to achieve a \sqrt{n} rate of convergence for the average derivative estimator. Interestingly, if the measurement error density is symmetric, the asymptotic variance of the average derivative estimator is the same irrespective of whether the error density is estimated or not.
C14|On the uniform convergence of deconvolution estimators from repeated measurements|This paper studies the uniform convergence rates of Li and Vuong's (1998) nonparametric deconvolution estimator and its regularized version by Comte and Kappus (2015) for the classical measurement error model, where repeated measurements are available. Our assumptions are weaker than existing results, such as Li and Vuong (1998) which requires bounded support, and a specialization of Bonhomme and Robin (2010) which requires the existence of moment generating functions of certain observables. Moreover, our uniform convergence rates are typically faster than those obtained in these papers.
C14|Jackknife, small bandwidth and high-dimensional asymptotics|This paper sheds light on problems of statistical inference under alternative or nonstandard asymptotic frameworks from the perspective of jackknife empirical likelihood (JEL). Examples include small bandwidth asymptotics for semiparametric inference, many covariates asymptotics for regression models, and many-weak instruments asymptotics for instrumental variable regression. We first establish Wilks' theorem for the JEL statistic on a general semiparametric inference problem under the conventional asymptotics. We then show that the JEL statistics lose asymptotic pivotalness under the above nonstandard asymptotic frameworks, and argue that these phenomena are understood as emergence of Efron and Stein's (1981) bias of the jackknife variance estimator in the first order. Finally we propose a modification of JEL to recover asymptotic pivotalness under both the conventional and nonstandard asymptotics. Our modification works for all above examples and provides a unified framework to investigate nonstandard asymptotic problems.
C14|How “Big” Should Government Be?|We assess how “big” government should reasonably be in a number of advanced countries. First, we will link the recent findings of Data Envelope Analysis on efficient public expenditure with the question of the size of the government. Second, we report descriptive analysis of various government performance indicators in relation to public expenditure to provide indications of overall “optimal” across spending categories. In principle, the highest savings potential is in the biggest expenditure categories, public consumption and social expenditure.
C14|A Theory of Scenario Generation|We show how distributions can be reduced to low-dimensional scenario trees. Applied to intertemporal distributions, the scenarios and their probabilities become time-varying factors. From S&P 500 options, two or three time-varying scenarios suffice to forecast returns, implied variance or skewness on par, or better, than extant multivariate stochastic volatility jump-diffusion models, while reducing the computational effort to fractions of a second.
C14|Arbitrage Free Dispersion|We develop a theory of arbitrage-free dispersion (AFD) that characterizes the testable restrictions of asset pricing models. AFD measures Jensen’s gap in the cumulant generating function of pricing kernels and returns. It implies a wide family of model-free dispersion constraints, which extend dispersion and co-dispersion bounds in the literature and are applicable with a unifying approach in multivariate and multiperiod settings. Empirically, the dispersion of stationary and martingale pricing kernel components in the benchmark long-run risk model yields a counterfactual dependence of short- vs. long-maturity bond returns and is insufficient for pricing optimal portfolios of market equity and short-term bonds.
C14|Dealing with a Technological Bias: The Difference-in-Difference Approach|I construct a nonlinear model for causal inference in the empirical settings where researchers observe individual-level data for few large clusters over at least two time periods. It allows for identification (sometimes partial) of the counterfactual distribution, in particular, identifying average treatment effects and quantile treatment effects. The model is exible enough to handle multiple outcome variables, multidimensional heterogeneity, and multiple clusters. It applies to the settings where the new policy is introduced in some of the clusters, and a researcher additionally has information about the pretreatment periods. I argue that in such environments we need to deal with two different sources of bias: selection and technological. In my model, I employ standard methods of causal inference to address the selection problem and use pretreatment information to eliminate the technological bias. In case of one-dimensional heterogeneity, identification is achieved under natural monotonicity assumptions. The situation is considerably more complicated in case of multidimensional heterogeneity where I propose three di erent approaches to identification using results from transportation theory.
C14|The Role of the Propensity Score in Fixed Effect Models|We develop a new approach for estimating average treatment effects in the observational studies with unobserved cluster-level heterogeneity. The previous approach relied heavily on linear fixed effect specifications that severely limit the heterogeneity between clusters. These methods imply that linearly adjusting for differences between clusters in average covariate values addresses all concerns with cross-cluster comparisons. Instead, we consider an exponential family structure on the within-cluster distribution of covariates and treatments that implies that a low-dimensional sufficient statistic can summarize the empirical distribution, where this sufficient statistic may include functions of the data beyond average covariate values. Then we use modern causal inference methods to construct flexible and robust estimators.
C14|Eficiencia del gasto en salud en la OCDE y ALC: un análisis envolvente de datos|Este artículo mide la eficiencia del gasto en salud en sesenta y dos países de América Latina y el Caribe (ALC) y de la Organización para la Cooperación y el Desarrollo Económico (OCDE), a partir de la relación entre el nivel de gasto total (como porcentaje del PIB) y algunos resultados en salud (esperanza de vida en años y mortalidad en menores de cinco años por cada mil nacidos vivos). Con este fin, se aplicó el método no paramétrico de análisis envolvente de datos, usando para cada grupo datos de 1995, 2005 y 2014. Los resultados permiten identificar la eficiencia y la posición relativa del conjunto de países dentro de ambos grupos. Para el año 2014, los países más eficientes de ALC fueron Chile, Cuba, República Dominicana, Venezuela y Jamaica, y de la OCDE fueron Japón, Luxemburgo y Turquía. El promedio de la eficiencia de los países de ALC fue inferior a la de los países la OCDE (0,938 y 0,974, respectivamente).
C14|Firm size and concentration inequality: A flexible extension of Gibrat’s law|No abstract is available for this item.
C14|Uncertainty in Electricity Markets from a seminonparametric Approach|No abstract is available for this item.
C14|Una breve aplicación a la predicción de la fragilidad de empresas colombianas, mediante el uso de modelos estadísticos|Resumen: Este trabajo estima diferentes modelos estadísticos para medir la probabilidad de riesgo de quiebra empresarial e identificar cuál de ellos presenta un mejor desempeño predictivo. Para lograr este objetivo, se emplean los estados financieros de las empresas colombianas para el 2015 con el fin construir indicadores financieros como variables explicativas en los modelos empleados. Las variables más relevantes para medir la probabilidad de quiebra fueron la rentabilidad del patrimonio y el nivel de endeudamiento. Entre los modelos estimados (logístico, logístico heterocedástico, logístico robusto y logístico mixto), el logístico mixto fue el que presentó el mejor desempeño para predecir la fragilidad empresarial. / Abstract : This manuscript estimates different statistical models to measure the probability of risk business failure, and identifies which one has better predicting performance. In order to achieve this objective, the financial statements of Colombian companies in 2015 are used in order to build financial indicators as explanatory variables in the used models. The most relevant variables to measure the probability of business failure were the return on equity and debt ratio. Among the estimated models (logistic, heteroscedastic logistic, robust logistic and mixed logistic), the mixed logistic has the best performance in predicting the corporate fragility.
C14|Time-Varying General Dynamic Factor Models and the Measurement of Financial Connectedness|Ripple effects in financial markets associated with crashes, systemic risk and contagion are characterized by non-trivial lead-lag dynamics which is crucial for understanding how crises spread and, therefore, central in risk management. In the spirit of Diebold and Yilmaz (2014), we investigate connectedness among financial firms via an analysis of impulse response functions of adjusted intraday log-ranges to market shocks involving network theory methods. Motivated by overwhelming evidence that the interdependence structure of financial markets is varying over time, we are basing that analysis on the so-called time-varying General Dynamic Factor Model proposed by Eichler et al. (2011), which extends to the locally stationary context the framework developed by Forni et al. (2000) under stationarity assumptions. The estimation methods in Eichler et al. (2011), however, present the major drawback of involving two-sided filters which make it impossible to recover impulse response functions. We therefore introduce a novel approach extending to the time-varying context the one-sided method of Forni et al. (2017). The resulting estimators of time-varying impulse response functions are shown to be consistent, hence can be used in the analysis of (time-varying) connectedness. Our empirical analysis on a large and strongly comoving panel of intraday price ranges of US stocks indicates that large increases in mid to long-run connectedness are associated with the main financial turmoils.
C14|Are Sufficient Statistics Necessary? Nonparametric Measurement of Deadweight Loss from Unemployment Insurance|Central to the welfare analysis of income transfer programs is the deadweight loss associated with possible reforms. To aid analytical tractability, its measurement typically requires specifying a simplified model of behavior. We employ a complementary “decomposition” approach that compares the behavioral and mechanical components of a policy’s total impact on the government budget to study the deadweight loss of two unemployment insurance policies. Experimental and quasi-experimental estimates using state administrative data show that increasing the weekly benefit is more efficient (with a fiscal externality of 53 cents per dollar of mechanical transferred income) than reducing the program’s implicit earnings tax.
C14|New Evidence on Long-Term Effects of Start-Up Subsidies: Matching Estimates and their Robustness|The German start-up subsidy (SUS) program for the unemployed has recently undergone a major make-over, altering its institutional setup, adding an additional layer of selection and leading to ambiguous predictions of the program’s effectiveness. Using propensity score matching (PSM) as our main empirical approach, we provide estimates of long-term effects of the post-reform subsidy on individual employment prospects and labor market earnings up to 40 months after entering the program. Our results suggest large and persistent long-term effects of the subsidy on employment probabilities and net earned income. These effects are larger than what was estimated for the pre-reform program. Extensive sensitivity analyses within the standard PSM framework reveal that the results are robust to different choices regarding the implementation of the weighting procedure and also with respect to deviations from the conditional independence assumption. As a further assessment of the results’ sensitivity, we go beyond the standard selection-on-observables approach and employ an instrumental variable setup using regional variation in the likelihood of receiving treatment. Here, we exploit the fact that the reform increased the discretionary power of local employment agencies in allocating active labor market policy funds, allowing us to obtain a measure of local preferences for SUS as the program of choice. The results based on this approach give rise to similar estimates. Thus, our results indicating that SUS are still an effective active labor market program after the reform do not appear to be driven by “hidden bias”.
C14|A Correction for Regression Discontinuity Designs with Group-Specific Mismeasurement of the Running Variable|"When the running variable in a regression discontinuity (RD) design is measured with error, identification of the local average treatment effect of interest will typically fail. While the form of this measurement error varies across applications, in many cases the measurement error structure is heterogeneous across different groups of observations. We develop a novel measurement error correction procedure capable of addressing heterogeneous mismeasurement structures by leveraging auxiliary information. We also provide adjusted asymptotic variance and standard errors that take into consideration the variability introduced by the estimation of nuisance parameters, and honest confidence intervals that account for potential misspecification. Simulations provide evidence that the proposed procedure corrects the bias introduced by heterogeneous measurement error and achieves empirical coverage closer to nominal test size than ""naïve"" alternatives. Two empirical illustrations demonstrate that correcting for measurement error can either reinforce the results of a study or provide a new empirical perspective on the data."
C14|Long-Run Effects of Dynamically Assigned Treatments: A New Methodology and an Evaluation of Training Effects on Earnings|"We propose and implement a new method to estimate treatment effects in settings where individuals need to be in a certain state (e.g. unemployment) to be eligible for a treatment, treatments may commence at different points in time, and the outcome of interest is realized after the individual left the initial state. An example concerns the effect of training on earnings in subsequent employment. Any evaluation needs to take into account that some of those who are not trained at a certain time in unemployment will leave unemployment before training while others will be trained later. We are interested in effects of the treatment at a certain elapsed duration compared to ""no treatment at any subsequent duration"". We prove identification under unconfoundedness and propose inverse probability weighting estimators. A key feature is that weights given to outcome observations of non-treated depend on the remaining time in the initial state. We study earnings effects of WIA training in the US and long-run effects of a training program for unemployed workers in Sweden. Estimates are positive and sizeable, exceeding those obtained by using common static methods, and suggesting a reappraisal of training."
C14|Searching for the optimal territorial structure: The case of Spanish provincial councils|Modern states are organized in multi-level governance structures with economic and political authorities dispersed across them. However, although there is relatively widespread consensus that this form of organization is preferable to a centralized authority, the same cannot be said about its jurisdictional design, that is, how to transfer authority from central states to both supranational and subnational levels. This lack of consensus also exists in contexts with explicit initiatives to strengthen political ties such as the European Union (EU), and even within EU member countries, a situation that is aggravated by the relative scarcity of contributions that measure the advantages and disadvantages of different territorial organizations. We explore these issues through a study of one EU country, Spain, whose provincial councils (diputaciones) are often the subject of debate and controversy due to their contribution to increasing public spending and their purported inefficiencies, corruption, and lack of transparency. Specifically, we combine a variety of activity analysis techniques to evaluate how they impact on local government performance. Results suggest that, in general, the presence of a provincial council has a positive impact on local government performance, but when their activity levels are too high the effect can become pernicious.
C14|Employment Effect of Innovation|The present paper estimates and decomposes the employment effect of innovation by R&D intensity levels. Our microeconometric analysis is based on a large international panel data set from the EU Industrial R&D Investment Scoreboard. Employing flexible semi-parametric methods - the generalised propensity score - allows us to recover the full functional relationship between R&D investment and firm employment, and to address important econometric issues, which is not possible in the standard estimation approach used in the previous literature. Our results suggest that modest innovators do not create and may even destruct jobs by raising their R&D expenditures. Most of the jobs in the economy are created by innovation followers: increasing innovation by 1% may increase employment up to 0.7%. The job creation effect of innovation reaches its peak when R&D intensity is around 100% of the total capital expenditure, after which the positive employment effect declines and becomes statistically insignificant. Innovation leaders do not create jobs by further increasing their R&D expenditures, which are already very high.
C14|Statistical Analysis and Evaluation of Macroeconomic Policies: A Selective Review|In this paper, we highlight some recent developments of a new route to evaluate macroeconomic policy effects, which are investigated under the framework with potential out- comes. First, this paper begins with a brief introduction of the basic model setup in modern econometric analysis of program evaluation. Secondly, primary attention goes to the focus on causal effect estimation of macroeconomic policy with single time series data together with some extensions to multiple time series data. Furthermore, we examine the connection of this new approach to traditional macroeconomic models for policy analysis and evaluation. Finally, we conclude by addressing some possible future research directions in statistics and econometrics.
C14|Testing Unconfoundedness Assumption Using Auxiliary Variables|In this paper, we propose an alternative test procedure for testing the conditional independence assumption which is an important identication condition commonly imposed in the literature of program analysis and policy evaluation. We transform the conditional independence test to a nonparametric conditional moment test using an auxiliary variable which is independent of the treatment assignment variable conditional on potential outcomes and observable covariates. The proposed test statistic is shown to have a limiting normal distribution under null hypotheses of conditional independence. Furthermore, the suggested method is shown to be valid under time series framework and thus the corresponding test statistic and its limiting distribution are also established. Monte Carlo simulations are conducted to examine the finite sample performances of the proposed test statistics. Finally, the proposed test method is applied to test the conditional independence in real examples: the 401(k) participation program and return to college education.
C14|Time-Varying General Dynamic Factor Models and the Measurement of Financial Connectedness|Ripple effects in financial markets associated with crashes, systemic risk and contagion are characterized by non-trivial lead-lag dynamics which is crucial for understanding how crises spread and, therefore, central in risk management. In the spirit of Diebold and Yilmaz (2014), we investigate connectedness among financial firms via an analysis of impulse response functions of adjusted intraday log-ranges to market shocks involving network theory methods. Motivated by overwhelming evidence that the interdependence structure of financial markets is varying over time, we are basing that analysis on the so-called time-varying General Dynamic Factor Model proposed by Eichler et al. (2011), which extends to the locally stationary context the framework developed by Forni et al. (2000) under stationarity assumptions. The estimation methods in Eichler et al. (2011), however, present the major drawback of involving two-sided filters which make it impossible to recover impulse response functions. We therefore introduce a novel approach extending to the time-varying context the one-sided method of Forni et al. (2017). The resulting estimators of time-varying impulse response functions are shown to be consistent, hence can be used in the analysis of (time-varying) connectedness. Our empirical analysis on a large and strongly comoving panel of intraday price ranges of US stocks indicates that large increases in mid to long-run connectedness are associated with the main financial turmoils.
C14|A Semi-Parametric Approach to the Oaxaca–Blinder Decomposition with Continuous Group Variable and Self-Selection|This paper presents an extension to the Oaxaca–Blinder decomposition with continuous groups using a semiparametric approach known as varying coefficients model. To account for potential self-selection into the continuum of groups, the use of inverse mills ratios is expanded upon following the literature on endogenous selection. The flexibility of this methodology may allow detecting heterogeneity when analyzing endogenous dose treatments effects, as well as correcting for endogeneity when analyzing the heterogeneous partial effects across the continuous group variable. For illustration, the methodology is used to revisit the impact of body weight on wages, using body mass index (BMI) as the continuum of groups, finding evidence that body weight has a negative, but decreasing impact on wages for both white men and women.
C14|Time, space and hedonic prediction accuracy evidence from the Corsican apartment market|In this study, we propose a hedonic housing model to address spa- tial and temporal latent structures simultaneously. With the development of spatial econometrics and spatial statistics, economists can now better assess the impact of spatial correlation on house prices. How- ever, the simultaneous handling of spatial and temporal correlation is still under development. Since spatial econometric models are limited to account for two kinds of cor- relation simultaneously, we propose using a hierarchical spatiotemporal model from spatial statistics. Based on a Bayesian framework and a stochastic par- tial differential equation (SPDE) approach, the estimation is carried out via INLA. We then perform an empirical study on apartment transaction prices in Corsica (France) using the proposed model. The empirical results demonstrate that the prediction performance of the hierarchical spatiotemporal model is the best among all candidate models. Moreover, the hedonic housing estimates are affected by spatial effects and temporal effects. Ignoring these effects could result in serious forecasting issues.
C14|Fertility response to climate shocks|In communities highly dependent on rainfed agriculture for their livelihoods, the common oc-currence of climatic shocks such as droughts can lower the opportunity cost of having children, and raise fertility. Using longitudinal household data from Madagascar, we estimate the causal effect of drought occurrences on fertility, and explore the nature of potential mechanisms driving this effect. We exploit exogenous within-district year-to-year variation in rainfall deficits, and find that droughts occurring during the agricultural season significantly increase the number of children born to women living in agrarian communities. This effect is long lasting, as it is not reversed within four years following the drought occurrence. Analyzing the mechanism, we find that droughts have no effect on common underlying factors of high fertility such as marriage timing and child mortality. Furthermore, droughts have no significant effect on fertility if they occur during the non-agricultural season or in non-agrarian communities, and their positive effect in agrarian communities is mitigated by irrigation. These findings provide evidence that a low opportunity cost of having children is the main channel driving the fertility effect of drought in agrarian communities.
C14|Estimating Heterogeneous Reactions to Experimental Treatments|Frequently in experiments there is not only variance in the reaction of participants to treatment. The heterogeneity is patterned: discernible types of participants react differently. In principle, a finite mixture model is well suited to simultaneously estimate the probability that a given participant belongs to a certain type, and the reaction of this type to treatment. Yet often, finite mixture models need more data than the experiment provides. The approach requires ex ante knowledge about the number of types. Finite mixture models are hard to estimate for panel data, which is what experiments often generate. For repeated experiments, this paper offers a simple two-step alternative that is much less data hungry, that allows to find the number of types in the data, and that allows for the estimation of panel data models. It combines machine learning methods with classic frequentist statistics.
C14|A Feature-Based Framework for Detecting Technical Outliers in Water-Quality Data from In Situ Sensors|Outliers due to technical errors in water-quality data from in situ sensors can reduce data quality and have a direct impact on inference drawn from subsequent data analysis. However, outlier detection through manual monitoring is unfeasible given the volume and velocity of data the sensors produce. Here, we proposed an automated framework that provides early detection of outliers in water-quality data from in situ sensors caused by technical issues.The framework was used first to identify the data features that differentiate outlying instances from typical behaviours. Then statistical transformations were applied to make the outlying instances stand out in transformed data space. Unsupervised outlier scoring techniques were then applied to the transformed data space and an approach based on extreme value theory was used to calculate a threshold for each potential outlier. Using two data sets obtained from in situ sensors in rivers flowing into the Great Barrier Reef lagoon, Australia, we showed that the proposed framework successfully identified outliers involving abrupt changes in turbidity, conductivity and river level, including sudden spikes, sudden isolated drops and level shifts, while maintaining very low false detection rates. We implemented this framework in the open source R package oddwater.
C14|Nonparametric Predictive Regressions for Stock Return Prediction|We propose two new nonparametric predictive models: the multi-step nonparametric predictive regression model and the multi-step additive predictive regression model, in which the predictive variables are locally stationary time series. We define estimation methods and establish the large sample properties of these methods in the short horizon and the long horizon case. We apply our methods to stock return prediction using a number of standard predictors such as dividend yield. The empirical results show that all of these models can substantially outperform the traditional linear predictive regression model in terms of both in-sample and out-of-sample performance. In addition, we find that these models can always beat the historical mean model in terms of in-sample fitting, and also for some cases in terms of the out-of-sample forecasting. We also propose a trading strategy based on our methodology and show that it beats the buy and hold stategy provided the tuning parameters are well chosen.
C14|Are Sufficient Statistics Necessary? Nonparametric Measurement of Deadweight Loss from Unemployment Insurance|"Central to the welfare analysis of income transfer programs is the deadweight loss associated with possible reforms. To aid analytical tractability, its measurement typically requires specifying a simplified model of behavior. We employ a complementary ""decomposition"" approach that compares the behavioral and mechanical components of a policy's total impact on the government budget to study the deadweight loss of two unemployment insurance policies. Experimental and quasi-experimental estimates using state administrative data show that increasing the weekly benefit is more efficient (with a fiscal externality of 53 cents per dollar of mechanical transferred income) than reducing the program's implicit earnings tax."
C14|Ensemble Methods for Causal Effects in Panel Data Settings|In many prediction problems researchers have found that combinations of prediction methods (“ensembles”) perform better than individual methods. A simple example is random forests, which combines predictions from many regression trees. A striking, and substantially more complex, example is the Netflix Prize competition where the winning entry combined predictions using a wide variety of conceptually very different models. In macro-economic forecasting researchers have often found that averaging predictions from different models leads to more accurate forecasts. In this paper we apply these ideas to synthetic control type problems in panel data setting. In this setting a number of conceptually quite different methods have been developed, with some assuming correlations between units that are stable over time, others assuming stable time series patterns common to all units, and others using factor models. With data on state level GDP for 270 quarters, we focus on three basic approaches to predicting missing values, one from each of these strands of the literature. Rather than try to test the different models against each other and find a true model, we focus on combining predictions based on each of the separate models using ensemble methods. For the ensemble predictor we focus on a weighted average of the three individual methods, with non-negative weights determined through out-of-sample cross-validation.
C14|Nonparametric Estimates of Demand in the California Health Insurance Exchange|We estimate the demand for health insurance in the California Affordable Care Act marketplace (Covered California) without using parametric assumptions about the unobserved components of utility. To do this, we develop a computational method for constructing sharp identified sets in a nonparametric discrete choice model. The model allows for endogeneity in prices (premiums) and for the use of instrumental variables to address this endogeneity. We use the method to estimate bounds on the effects of changing premium subsidies on coverage choices, consumer surplus, and government spending. We find that a $10 decrease in monthly premium subsidies would cause between a 1.6% and 7.0% decline in the proportion of low-income adults with coverage. The reduction in total annual consumer surplus would be between $63 and $78 million, while the savings in yearly subsidy outlays would be between $238 and $604 million. Comparable logit models yield price sensitivity estimates towards the lower end of the bounds.
C14|Toward an Understanding of Corporate Social Responsibility: Theory and Field Experimental Evidence|We develop theory and a tightly-linked field experiment to explore the supply side implications of corporate social responsibility (CSR). Our natural field experiment, in which we created our own firm and hired actual workers, generates a rich data set on worker behavior and responses to both pecuniary and CSR incentives. Making use of a novel identification framework, we use these data to estimate a structural principal-agent model. This approach permits us to compare and contrast treatment and selection effects of both CSR and financial incentives. Using data from more than 110 job seekers, we find strong evidence that when a firm advertises work as socially-oriented, it attracts employees who are more productive, produce higher quality work, and have more highly valued leisure time. In terms of enhancing the labor pool, for example, CSR increases the number of applicants by 25 percent, an impact comparable to the effect of a 36 percent increase in wages. We also find an economically important complementarity between CSR and wage offers, highlighting the import of using both to hire and motivate workers. Beyond lending insights into the supply side of CSR, our research design serves as a framework for causal inference on other forms of non-pecuniary incentives and amenities in the workplace, or any other domain more generally.
C14|Bank Survival in European Emerging Markets|We analyze bank survival on large dataset covering 17 European emerging markets during the period of 2007-2015 by estimating the Cox proportional hazards model. We group banks across countries and according to their financial soundness. Our results show that progress in banking reforms positively affects bank survival. The economic impact of various determinants is largest for average banks measured by their soundness. Financial indicators predict bank survival rate with intuitively expected impact that is economically less significant in comparison to other factors. Specifically, ownership structure and legal form are the key economically significant factors that exhibit strongest economic effect on bank survival. We further document importance of banks being listed with respect to their survival. We also show that probability of exit increases after number of directors increases beyond a threshold. The results are robust no matter how bank are grouped, with respect to alternative specifications as well as alternative assumptions on survival distribution.
C14|Data-driven local polynomial for the trend and its derivatives in economic time series|The main purpose of this paper is the development of iterative plug-in algorithms for local polynomial estimation of the trend and its derivatives in macroeconomic time series. In particular, a data-driven lag-window estimator for the variance factor is proposed so that the bandwidth is selected without any parametric assumption on the stationary errors. Further analysis of the residuals using an ARMA model is discussed briefl y. Moreover, confidence bounds for the trend and its derivatives are conducted using some asymptotically unbiased estimates and applied to test possible linearity of the trend. These graphical tools also provide us further detailed features about the economic development. Practical performance of the proposals is illustrated by quarterly US and UK GDP data.
C14|New Evidence on Long-Term Effects of Start-Up Subsidies: Matching Estimates and Their Robustness|"The German start-up subsidy (SUS) program for the unemployed has recently undergone a major make-over, altering its institutional setup, adding an additional layer of selection and leading to ambiguous predictions of the program's effectiveness. Using propensity score matching (PSM) as our main empirical approach, we provide estimates of long-term effects of the post-reform subsidy on individual employment prospects and labor market earnings up to 40 months after entering the program. Our results suggest large and persistent long-term effects of the subsidy on employment probabilities and net earned income. These effects are larger than what was estimated for the pre-reform program. Extensive sensitivity analyses within the standard PSM framework reveal that the results are robust to different choices regarding the implementation of the weighting procedure and also with respect to deviations from the conditional independence assumption. As a further assessment of the results' sensitivity, we go beyond the standard selection-on-observables approach and employ an instrumental variable setup using regional variation in the likelihood of receiving treatment. Here, we exploit the fact that the reform increased the discretionary power of local employment agencies in allocating active labor market policy funds, allowing us to obtain a measure of local preferences for SUS as the program of choice. The results based on this approach give rise to similar estimates. Thus, our results indicating that SUS are still an effective active labor market program after the reform do not appear to be driven by ""hidden bias""."
C14|Resolutions to flip-over credit risk and beyond|Abstract Given a risk outcome y over a rating system {R_i }_(i=1)^k for a portfolio, we show in this paper that the maximum likelihood estimates with monotonic constraints, when y is binary (the Bernoulli likelihood) or takes values in the interval 0≤y≤1 (the quasi-Bernoulli likelihood), are each given by the average of the observed outcomes for some consecutive rating indexes. These estimates are in average equal to the sample average risk over the portfolio and coincide with the estimates by least squares with the same monotonic constraints. These results are the exact solution of the corresponding constrained optimization. A non-parametric algorithm for the exact solution is proposed. For the least squares estimates, this algorithm is compared with “pool adjacent violators” algorithm for isotonic regression. The proposed approaches provide a resolution to flip-over credit risk and a tool to determine the fair risk scales over a rating system.
C14|Lognormal city size distribution and distance|This paper analyses whether the size distribution of nearby cities is lognormally distributed by using a distance-based approach. We use data from three different definitions of US cities in 2010, considering all possible combinations of cities within a 300-mile radius. The results indicate that support for the lognormal distribution decreases with distance, although the lognormal distribution cannot be rejected in most of the cases for distances below 100 miles.
C14|Revisiting the finance-growth nexus: A socioeconomic approach|Despite the fact that financial development is recognised as a vital determinant of countries’ economic growth path, many empirical studies fail to further isolate the role of socioeconomic indicators on accelerating growth. This study attempts to fill this gap by examining the statistical significance and the behavior of several socioeconomic indicators on economic growth. We apply parametric (System GMM estimators) and semi-parametric techniques along the lines of Baltagi and Li (2002) on a panel data set of 19 EU countries over the period 1995-2017. We test for nonlinear effects on economic growth for three banking indicators (domestic credit, non-performing loans and banking capitalization). In contrast to the related literature, our findings provide sufficient evidence of nonlinear relationships between several aspects of financial development and economic growth. Our results imply significant policy implications for policy makers and regulators in their effort of balancing banking development with a resurgence in economic growth within the EU periphery.
C14|A Semiparametric Smooth Coefficient Estimator for Recreation Demand|We introduce a semiparametric smooth coefficient estimator for recreation demand data that allows more flexible modeling of preference heterogeneity. We show that our sample of visitors each has an individual statistically significant price coefficient estimate leading to clearly nonparametric consumer surplus and willingness to pay (WTP) distributions. We also show mean WTP estimates that are different in economically meaningful ways for every demographic variable we have for our sample of beach visitors. This flexibility is valuable for future researchers who can include any variables of interest beyond the standard demographic variables we have included here. And the richer results, price elasticities, consumer surplus and WTP estimates, are valuable to planners and policymakers who can easily see how all these estimates vary with characteristics of the population of interest.
C14|Statistical Inference for Aggregation of Malmquist Productivity Indices|The Malmquist Productivity Index (MPI) has gained popularity amongst studies on dynamic change of productivity of decision making units (DMUs). In practice, this index is frequently reported at aggregate levels (e.g., public and private rms) in the form of simple equally-weighted arithmetic or geometric means of individual MPIs. A number of studies have emphasized that it is necessary to account for the relative importance of individual DMUs in the aggregations of indices in general and of MPI in particular. While more suitable aggregations of MPIs have been introduced in the literature, their statistical properties have not been revealed yet, preventing applied researchers from making essential statistical inferences such as con dence intervals and hypothesis testing. In this paper, we will ll this gap by developing a full asymptotic theory for an appealing aggregation of MPIs. On the basis of this, some meaningful statistical inferences are proposed and their nite-sample performances are veri ed via extensive Monte Carlo experiments.
C14|The Growth-Finance Nexus in Brazil: Evidence from a New Dataset, 1890-2003| This study revisits the growth-finance nexus using a new econometric approach and unique data set. In particular by employing the smooth transition framework and annual time series data for Brazil from 1890 to 2003, we attempt to address on the one side, what is the relationship between financial development, trade openness, political instability and economic growth and, on the other, how it changes over time. The main finding is that financial development has a mixed positive and negative time-varying impact on economic growth, which signifi cantly depends on jointly estimated trade openness thresholds. Moreover our estimates highlight a positive impact of trade openness on growth but with interesting variation regarding their size and power, whereas the effect of political instability (both formal and informal) on growth is mainly negative. We also find that changes between regimes tend not to be smooth. Finally, our estimates show that in 57% of the years in which financial development has a below the mean effect, we find that trade openness experiences a substantial above the mean change.
C14|Macroeconomic Indicator Forecasting with Deep Neural Networks|Economic policymaking relies upon accurate forecasts of economic conditions. Current methods for unconditional forecasting are dominated by inherently linear models {{p}} that exhibit model dependence and have high data demands. {{p}} We explore deep neural networks as an {{p}} opportunity to improve upon forecast accuracy with limited data and while remaining agnostic as to {{p}} functional form. We focus on predicting civilian unemployment using models based on four different neural network architectures. Each of these models outperforms bench- mark models at short time horizons. One model, based on an Encoder Decoder architecture outperforms benchmark models at every forecast horizon (up to four quarters).
C14|Skill Biased Technical Change and Misallocation. A Unified Framework and a country-sector exercize|Due to strict reliance on perfectly competitive labor markets, standard approaches estimating skill biased technical change (SBTC) conflate ‘true’ SBTC and labor market distortions preventing firms from choosing the efficient skilled to unskilled labor ratio. To overcome this limit, we present a unified framework to estimate SBTC, net of factor accumulation (FA) effects, and quantify the discrepancy between skilled to unskilled marginal rate of technical substitution (MRTS) and wage ratio (i.e., relative misallocation). The methodology takes advantage of recent developments in nonparametric estimation methods (i.e., generalized kernel regression) that allow us to estimate the marginal productivity of inputs at the country-sector level directly from country-sector data. Using 1995-2005 data, we find a 3% yearly growth rate for the MRTS between skilled and unskilled labor and show such change to be mostly driven by SBTC, rather than FA. We then show that MRTS changes does not match the evolution of the wage ratios (quite stable over time), thus yielding substantial heterogeneity in terms of relative misallocation patterns, for which we report a 6% increase, overall (1.5% in manufacturing, against a rough 12% in Non-Manufacturing sectors). Finally, we show evidence that relative misallocation increased less in country-sectors in which it was larger at the beginning of the period and grew more in country-sectors characterized by: higher skill-intensity; lower bargaining power of skilled over unskilled workers; lower FA effects.
C14|Testing for information asymmetry in the mortgage servicing market|Our main objective is to test for evidence of information asymmetry in the mortgage servicing market. Does the sale of mortgage servicing rights (MSR) by the initial lender to a second servicing institution unveil any residual asymmetric information? We analyze the originator’s selling choice of MSR using a large sample of U.S. mortgages that were privately securitized during the period of January 2000 to December 2013 (more than 5 million observations). Our econometric methodology is mainly non-parametric and the main test for the presence of information asymmetry is driven by kernel density estimation techniques (Su and Spindler, 2013). We also employ the non-parametric testing procedure of Chiappori and Salanié (2000). For robustness, we present parametric tests to corroborate our results after controlling for observable risk characteristics, for econometric misspecification error, and for endogeneity issues using instrumental variables. Our empirical results provide strong support for the presence of second-stage asymmetric information in the mortgage servicing market.
C14|Inference in partially identiﬁed panel data models with interactive fixed eﬀects|This paper develops methods for statistical inferences in a partially identiﬁed nonparametric panel data model with endogeneity and interactive ﬁxed eﬀects. We consider the case where the number of cross-sectional units (N) is large and the number of time series periods (T).as well as the number of unobserved common factors (R) are ﬁxed. Under some normalization rules, wecan concentrateout thelarge dimen-sional parameter vector of factor loadings and specify a set of conditional moment restriction that are involved with only the ﬁnite dimensional factor parameters along with the inﬁnite dimensional nonpara-metric component. For a conjectured restriction on the parameter, we consider testing the null hypothesis that the restriction is satisﬁed by at least one element in the identiﬁed set and propose a test statistic based on a novel martingale diﬀerence divergence (MDD) measure for the distance between a conditional expectation object and zero. We derive the limiting distribution of the resultant test statistic under the null and show that it is divergent at rate-N under the global alternative based on the U-process theory. To obtain the critical values for our test, we propose a version of multiplier bootstrap and establish its asymptotic validity. Simulations demonstrate the ﬁnite sample properties of our inference procedure. We apply our method to study Engel curves for major nondurable expenditures in China by using a panel dataset from the China Family Panel Studies (CFPS).
C14|Flexible Estimation of Heteroskedastic Stochastic Frontier Models via Two-step Iterative Nonlinear Least Squares|This article illustrates a straightforward and useful method for incorporating exogenous inefficiency effects in the estimation of semiparametric stochastic frontier models. An iterative estimation algorithm based on two-step nonlinear least squares is developed allowing for any flexible and monotonic specification of the production technology. We investigate the behavior of the proposed procedure through a set of Monte Carlo experiments comparing its finite sample properties with those of available alternatives. The new algorithm provides very good performance, outperforming the competitors in small samples and in presence of small signal-to-noise ratios. Two applications to agricultural data illustrate the usefulness of the proposed algorithm, even when it is used as a tool for sensitivity analysis.
C14|Disentangling tax evasion from inefficiency in firms tax declaration: an integrated approach|"In this article we present a new methodology to support fiscal monitoring by the Italian Revenue Agency (IRA) with the aim of improving current taxpayers fiscal compliance and fighting tax evasion within small and medium enterprises. In fact, given the methodology behind the Sector Studies (Studi di Settore - SdS) system, there is room for firms to implement tax evasion strategies by simply adjusting revenues (and costs) toward an estimated average threshold (known ex-ante), the so called ""presumptive"" revenues, and achieving the fiscal ""congruity"" status. By estimating a production function through stochastic frontier analysis we avoid estimating the average threshold know ex-ante and can combine information on firm economic efficiency with those on fiscal congruity, thus being able to disentangle underreporting of revenues due to potential firm tax evasion behaviours from underreporting due to firm inefficiencies. We apply this framework to two samples of Italian firms belonging to two Sector Studies. Our results confirm the potentiality of the approach, although more work is needed before moving to a large scale implementation for policy purposes."
C14|Health and Development|In this paper we examine whether the Solow growth model is consistent with across-countries variations in standard of living once investments in education and health are explicitly and simultaneously taken into account. Using a sample of low- and middle-income economies, we provide evidence that per capita GDP is positively affected by population's health, here proxied by the life expectancy at birth. Public expenditure on health affects indirectly the level of per capita income through its positive effectect on life expectancy. Using a Finite Mixture approach, we also show that richer economies are those where the impact of unobserved factors on the level of per capita income is stronger.
C14|Average derivative estimation under measurement error|In this paper, we derive the asymptotic properties of average derivative estimators when the regressors are contaminated with classical measurement error and the density of this error is unknown. Average derivatives of conditional mean functions are used extensively in economics and statistics, most notably in semiparametric index models. As well as ordinary smooth measurement error, we provide results for supersmooth error distributions. This is a particularly important class of error distribution as it includes the popular Gaussian density. We show that under this ill-posed inverse problem, despite using nonparametric deconvolution techniques and an estimated error characteristic function, we are able to achieve a \sqrt{n} rate of convergence for the average derivative estimator. Interestingly, if the measurement error density is symmetric, the asymptotic variance of the average derivative estimator is the same irrespective of whether the error density is estimated or not.<br><small>(This abstract was borrowed from another version of this item.)</small>
C14|Estimation of Varying Coefficient Models with Measurement Error|We propose a semi-parametric estimator for varying coefficient models when the regressors in the nonparametric component are measured with error. Varying coefficient models are an extension of other popular semiparametric models, including partially linear and nonparametric additive models, and deliver an attractive solution to the curse-of-dimensionality. We use deconvolution kernel estimation in a two-step procedure and show that the estimator is consistent and asymptotically normally distributed. We do not assume that we know the distribution of the measurement error a priori, nor do we assume that the error is symmetrically distributed. Instead, we suppose we have access to a repeated measurement of the noisy regressor and use the approach of Li and Vuong (1998) based on Kotlarski�s (1967) identity. We show that the convergence rate of the estimator is significantly reduced when the distribution of the measurement error is assumed unknown and possibly asymmetric. Finally, we study the small sample behavior of our estimator in a simulation study.
C14|Firm export diversification and change in workforce composition|The objective of this paper is to show that part of the fixed cost of firms’ trade expansion is due to the acquisition of new internal capabilities (e.g. technology, production processes or skills), which imply a costly change in the firm’s internal labor organisation. We investigate the relationship between a firm’s structure of labor, in terms of relative number of managers, and the scope of its export portfolio, in terms of product-destination varieties. The empirical analysis is based on a matched employer- employee dataset covering the population of French firms from tradable sectors over theperiod 2009-2014. Our analysis suggests that market expansion, and in particular export diversification, is associated with a change in the firm’s workforce composition, namely an increase in the number of managerial layers and in the ratio of managers. We show how these results are consistent with a simple model where the complexity of a firm’s operations increases in the number of product-destination couples exported, and where managers’ role is to address the unsolved problems arising from such increased complexity of operations.
C14|Modeling temporal treatment effects with zero inflated semi-parametric regression models: the case of local development policies in France|A semi-parametric approach is proposed to estimate the variation along time of the effects of two distinct public policies that were devoted to boost rural development in France over a similar period of time. At a micro data level, it is often observed that the dependent variable, such as local employment, does not vary along time, so that we face a kind of zero inflated phenomenon that cannot be dealt with a continuous response model. We introduce a conditional mixture model which combines a mass at zero and a continuous response. The suggested zero inflated semi-parametric statistical approach relies on the flexibility and modularity of additive models with the ability of panel data to deal with selection bias and to allow for the estimation of dynamic treatment effects. In this multiple treatment analysis, we find evidence of interesting patterns of temporal treatment effects with relevant nonlinear policy effects. The adopted semi-parametric modeling also offers the possibility of making a counterfactual analysis at an individual level. The methodology is illustrated and compared with parametric linear approaches on a few municipalities for which the mean evolution of the potential outcomes is estimated under the different possible treatments.
C14|Just-noticeable difference as a behavioural foundation of the critical cost-efficiency index|Critical cost-efficiency index (or CCEI), proposed in Afriat (1973) and Varian (1990), is one of the most commonly used measures of departures from rationality. We show that this index is equivalent to a particular notion of the just-noticeable difference, that is, a measure of dissimilarity between alternatives that is sufficient for the agent to tell them apart. Therefore, we show that CCEI evaluates the consumer's cognitive inability to discriminate among options.
C14|Forecast Density Combinations with Dynamic Learning for Large Data Sets in Economics and Finance|A flexible forecast density combination approach is introduced that can deal with large data sets. It extends the mixture of experts approach by allowing for model set incompleteness and dynamic learning of combination weights. A dimension reduction step is introduced using a sequential clustering mechanism that allocates the large set of forecast densities into a small number of subsets and the combination weights of the large set of densities are modelled as a dynamic factor model with a number of factors equal to the number of subsets. The forecast density combination is represented as a large finite mixture in nonlinear state space form. An efficient simulation-based Bayesian inferential procedure is proposed using parallel sequential clustering and filtering, implemented on graphics processing units. The approach is applied to track the Standard & Poor 500 index combining more than 7000 forecast densities based on 1856 US individual stocks that are are clustered in a relatively small subset. Substantial forecast and economic gains are obtained, in particular, in the tails using Value-at-Risk. Using a large macroeconomic data set of 142 series, similar forecast gains, including probabilities of recession, are obtained from multivariate forecast density combinations of US real GDP, Inflation, Treasury Bill yield and Employment. Evidence obtained on the dynamic patterns in the financial as well as macroeconomic clusters provide valuable signals useful for improved modelling and more effective economic and financial policies.
C14|The Phillips multiplier|We propose a model-free approach for determining the inflation-unemployment trade-off faced by a central bank, i.e., the ability of a central bank to transform unemployment into inflation (and vice versa) via its interest rate policy. We introduce the Phillips multiplier as a statistic to non-parametrically characterize the trade-off and its dynamic nature. We compute the Phillips multiplier for the US, UK and Canada and document that the trade-off went from being very large in the pre-1990 sample period to being small (but significant) post-1990 with the onset of inflation targeting and the anchoring of inflation expectations.
C14|Heterogeneous Effects of Tariff and Non-tariff Trade-Policy Barriers in Quantitative General Equilibrium|Structural quantitative work in international economics typically treats trade policy as log-linearly related to trade costs and as exogenous. This paper proposes a structural approach that allows for a non-parametric relationship and treats tariff and non-tariff trade-policy variables as potentially endogenous in log-linear estimation. We document that the data reject the assumption of log-linearity of trade costs in both the tariff- and the non-tariff-policy domains. Specifically, the partial impact of a change in tariffs is strongest for low policy barriers and medium levels of tariffs but generally decreases in the level of both non-tariff barriers and tariff barriers. To give a relevant illustration, we assess the effects of a unilateral increase of US tariffs on Chinese imports by 10 percentage points and document that the estimated effects on real bilateral trade-flow changes would be largely underestimated by standard approaches.
C14|Testing Constancy in Varying Coefficient Models|This article proposes tests for constancy of coefficients in semi-varying coefficients models. The testing procedure resembles in spirit the union-intersection parameter stability tests in time series, where observations are sorted according to the explanatory variable responsible for the coefficients varying. The test can be applied to model specification checks of interactive effects in linear regression models. Because test statistics are not asymptotically pivotal, critical values and p-values are estimated using a bootstrap technique. The finite sample properties of the test are investigated by means of Monte Carlo experiments, where the new proposal is compared to existing tests based on smooth estimates of the unrestricted model. We also report an application to returns of education modeling
C14|Functional Coefficient Panel Modeling with Communal Smoothing Covariates|Behavior at the individual level in panels or at the station level in spatial models is often influenced by aspects of the system in aggregate. In particular, the nature of the interaction between individual-specific explanatory variables and an individual dependent variable may be affected by `global’ variables that are relevant in decision making and shared communally by all individuals in the sample. To capture such behavioral features, we employ a functional coefficient panel model in which certain communal covariates may jointly influence panel interactions by means of their impact on the model coefficients. Two classes of estimation procedures are proposed, one based on station averaged data the other on the full panel, and their asymptotic properties are derived. Inference regarding the functional coefficient is also considered. The finite sample performance of the proposed estimators and tests are examined by simulation. An empirical spatial model illustration is provided in which the climate sensitivity of temperature to atmospheric CO_2 concentration is studied at both station and global levels.
C14|Housing Rent Dynamics and Rent Regulation in St. Petersburg (1880-1917)| This article studies the evolution of housing rents in St. Petersburg between 1880 and 1917 covering an eventful period of Russian and world history. We collect and digitize over 5,000 rental advertisements from historic newspapers, which we use together with geo-coded addresses and detailed structural characteristics to construct a quality-adjusted rent price index in continuous time. We provide the first pre-war and pre-Soviet index based on market data for any Russian housing market. In 1915, one of the world’s earliest rent control and tenant protection policies was introduced as a response to soaring prices following the outbreak of World War I. We analyze the impact of this policy: while before the regulation rents were increasing at a similar rapid pace as other consumer prices, the policy reversed this trend. We find evidence for official compliance with the policy, document a rise in tenure duration and strongly increased rent affordability among workers after the introduction of the policy. We conclude that the immediate prelude to the October Revolution was indeed characterized by economic turmoil, but rent affordability and rising rents were no longer the prevailing problems.
C14|Drawbacks in the 3-Factor Approach of Fama and French (2018)|"This paper features a statistical analysis of the monthly three factor Fama/French return series. We apply rolling OLS regressions to explore the relationship between the 3 factors, using monthly and weekly data from July 1926 to June 2018, that are freely available on French's website. The results suggest there are significant and time-varying relationships between the factors. This is conirmed by non-parametric tests. We then switch to a sub-sample from July 1990 to July 2018, also taken from French's website. The three series and their interrelationships are analysed using two stage least squares and the Hausman test to check for issues related to endogeneity, the Sargan over-identification test and the Cragg-Donald weak instrument test. The relationship between factors is also examined using OLS, incorporating Ramsey's RESET tests of functional form misspecification, plus Naradaya-Watson kernel regression techniques. The empirical results suggest that the factors, when combined in OLS regression analysis, as suggested by Fama and French (2018), are likely to suffer from endogeneity. OLS regression analysis and the application of Ramsey's RESET tests suggest a non-linear relationship exists between the three series, in which cubed terms are significant. This non-linearity is also confirmed by the kernel regression analysis. We use two instruments to estimate the market betas, and then use the factor estimates in a second set of panel data tests using a small sample of monthly returns for US firms that are drawn from the online data source ""tingo"". These issues are analysed using methods suggested by Petersen (2009) to permit clustering in the panels by date and firm. The empirical results suggest that using an instrument to capture endogeneity reduces the standard error of market beta in subsequent cross-sectional tests, but that clustering effects, as suggested by Petersen (2009), will also impact on the estimated standard errors. The empirical results suggest that using these factors in linear regression analysis, such as suggested by Fama and French (2018), as a method of screening factor relevance, is problematic in that the estimated standard errors are highly sensitive to the correct model specification."
C14|Do African economies grow similarly?|This paper examines economic growth in 52 African countries for 1961-2016 and seeks to find if there is common growth. As all African countries have their particular features, concerning climate, harvest, industry, size, politics, and infrastructure, and more, it seems best to rely on a non-parametric method. Dynamic Time Warping is such a convenient method, also as it allows leads and lags across countries to vary over time, and as it can easily be incorporated into a clustering technique. Five clusters are found, two of which concern Equatorial Guinea and Botswana, and the three other clusters have common growth rates of about 0, 2 and 4 over more than five decades.
C14|Bayesian Nonparametric Learning of How Skill Is Distributed across the Mutual Fund Industry|In this paper, we use Bayesian nonparametric learning to estimate the skill of actively managed mutual funds and also to estimate the population distribution for this skill. A nonparametric hierarchical prior, where the hyperprior distribution is unknown and modeled with a Dirichlet process prior, is used for the skill parameter, with its posterior predictive distribution being an estimate of the population distribution. Our nonparametric approach is equivalent to an infinitely ordered mixture of normals where we resolve the uncertainty in the mixture order by partitioning the funds into groups according to the group's average ability and variability. Applying our Bayesian nonparametric learning approach to a panel of actively managed, domestic equity funds, we find the population distribution of skill to be fat-tailed, skewed towards higher levels of performance. We also find that it has three distinct modes: a primary mode where the average ability covers the average fees charged by funds, a secondary mode at a performance level where a fund loses money for its investors, and lastly, a minor mode at an exceptionally high skill level.
C14|Robust Inference in First-Price Auctions : Experimental Findings as Identifying Restrictions|In laboratory experiments bidding in first-price auctions is more aggressive than predicted by the risk-neutral Bayesian Nash Equilibrium (RNBNE) - a finding known as the overbidding puzzle. Several models have been proposed to explain the overbidding puzzle, but no canonical alternative to RNBNE has emerged, and RNBNE remains the basis of the structural auction literature. Instead of estimating a particular model of overbidding, we use the overbidding restriction itself for identification, which allows us to bound the valuation distribution, the seller's payoff function, and the optimal reserve price. These bounds are consistent with RNBNE and all models of overbidding and remain valid if different bidders employ different bidding strategies. We propose simple estimators and evaluate the validity of the bounds numerically and in experimental data.
C14|Time-Geographically Weighted Regressions and Residential Property Value Assessment|In this study, we develop and apply a new methodology for obtaining accurate and equitable property value assessments. This methodology adds a time dimension to the Geographically Weighted Regressions (GWR) framework, which we call Time-Geographically Weighted Regressions (TGWR). That is, when generating assessed values, we consider sales that are close in time and space to the designated unit. We think this is an important improvement of GWR since this increases the number of comparable sales that can be used to generate assessed values. Furthermore, it is likely that units that sold at an earlier time but are spatially near the designated unit are likely to be closer in value than units that are sold at a similar time but farther away geographically. This is because location is such an important determinant of house value. We apply this new methodology to sales data for residential properties in 50 municipalities in Connecticut for 1994-2013 and 145 municipalities in Massachusetts for 1987-2012. This allows us to compare results over a long time period and across municipalities in two states. We find that TGWR performs better than OLS with fixed effects and leads to less regressive assessed values than OLS. In many cases, TGWR performs better than GWR that ignores the time dimension. In at least one specification, several suburban and rural towns meet the IAAO Coefficient of Dispersion cutoffs for acceptable accuracy.
C14|Off-Balance Sheet Activities, Inefficiency and Market Power of U.S. Banks|The Lerner index is a well-established measure of firms’ market power, but estimation and interpretation present several challenges, especially for banks. We estimate Lerner indices for U.S. banks for 2001-2016 while (i) accounting for banks’ off-balancesheet activities, (ii) estimating cost and profit functions nonparametrically to avoid mis-specification inherent in parametric estimation of translog functions on banking data, and (iii) allowing for cost and profit inefficiency that can otherwise bias index estimates. We find that banks have more market power than previous studies found, and that failure to account for off-balance-sheet activities or inefficiency can seriously bias estimates of market power.
C14|On Binscatter|Binscatter is very popular in applied microeconomics. It provides a flexible, yet parsimonious way of visualizing and summarizing large data sets in regression settings, and it is often used for informal evaluation of substantive hypotheses such as linearity or monotonicity of the regression function. This paper presents a foundational, thorough analysis of binscatter: we give an array of theoretical and practical results that aid both in understanding current practices (i.e., their validity or lack thereof) and in offering theory-based guidance for future applications. Our main results include principled number of bins selection, confidence intervals and bands, hypothesis tests for parametric and shape restrictions of the regression function, and several other new methods, applicable to canonical binscatter as well as higher-order polynomial, covariate-adjusted and smoothness-restricted extensions thereof. In particular, we highlight important methodological problems related to covariate adjustment methods used in current practice. We also discuss extensions to clustered data. Our results are illustrated with simulated and real data throughout. Companion general-purpose software packages for \texttt{Stata} and \texttt{R} are provided. Finally, from a technical perspective, new theoretical results for partitioning-based series estimation are obtained that may be of independent interest.
C14|Exact tests on returns to scale and comparisons of production frontiers in nonparametric models|When benchmarking production units by non-parametric methods like data envelopment analysis (DEA), an assumption has to be made about the returns to scale of the underlying technology. Moreover, it is often also relevant to compare the frontiers across samples of producers. Until now, no exact tests for examining returns to scale assumptions in DEA, or for test of equality of frontiers, have been available. The few existing tests are based on asymptotic theory relying on large sample sizes, whereas situations with relatively small samples are often encountered in practical applications. In this paper we propose three novel tests based on permutations. The tests are easily implementable from the algorithms provided, and give exact significance probabilities as they are not based on asymptotic properties. The first of the proposed tests is a test for the hypothesis of constant returns to scale in DEA. The others are tests for general frontier differences and whether the production possibility sets are, in fact, nested. The theoretical advantages of permutation tests are that they are appropriate for small samples and have the correct size. Simulation studies show that the proposed tests do, indeed, have the correct size and furthermore higher power than the existing alternative tests based on asymptotic theory.
C14|Animate the cluster or subsidize collaborative R&D? A multiple overlapping treatments approach to assess the impact of the French cluster policy|This paper examines the effectiveness of the French competitiveness cluster policy on participating SMEs in terms of innovation and economic performance. Using an original dataset, we construct different measures of treatment with crossover designs. The findings indicate substantial additionality effects on R&D and employment and weak or insignificant effects on other types of economic performance. While only adhering to clusters induces much stronger positive impacts on SMEs than only participating in R&D collaborative projects, the policy is most effective when the two treatments are simultaneously used. To achieve its impact on SMEs, the cluster policy should not overlook low-cost instruments such as animation actions and common services.
C14|Machine Learning for Forecasting Excess Stock Returns – The Five-Year-View| In this paper, we apply machine learning to forecast stock returns in excess of different benchmarks, including the short-term interest rate, long-term interest rate, earnings-by-price ratio, and the inflation. In particular, we adopt and implement a fully nonparametric smoother with the covariates and the smoothing parameter chosen by cross-validation. We find that for both one-year and five-year returns, the term spread is, overall, the most powerful predictive variable for excess stock returns. Differently combined covariates can then achieve higher predictability for different forecast horizons. Nevertheless, the set of earnings-by-price and term spread predictors under the inflation benchmark strikes the right balance between the one-year and five-year horizon.
C14|Conditional variance forecasts for long-term stock returns| In this paper, we apply machine learning to forecast the conditional variance of long-term stock returns measured in excess of different benchmarks, including the short-term interest rate, long-term interest rate, earnings-by-price ratio, and inflation. In particular, we apply and implement in a two-step procedure a fully nonparametric smoother with the covariates and the smoothing parameters chosen via cross-validation. We find that volatility forecastability is much less important at longer horizons regardless of the chosen model and that the homoscedastic historical average of the squared return prediction errors gives an adequate approximation of the unobserved realized conditional variance for both the one-year and five-year horizon.
C14|Centralized and decentralized bitcoin markets: Euro vs USD vs GBP|In this study, I compared the euro, U.S. dollar, and British pound sterling (GBP) centralized and decentralized bitcoin cryptocurrency markets in terms of return volatility and interdependency. This comparison showed the decentralized bitcoin market has higher volatility and the centralized markets have higher tail dependence regarding returns. The volatility analysis results are contrary to the established leverage reasons that market drops cause volatility. The results demonstrate a higher left tail dependence is in line with the general pattern in “traditional” financial markets which more extreme dependent in downturns. It was also shown trade volume increases as prices decrease, demonstrating participants’ lack of confidence and consensus in a price-jump period.
C14|Fertility Response to Climate Shocks|In communities highly dependent on rainfed agriculture for their livelihoods, the common oc- currence of climatic shocks such as droughts can lower the opportunity cost of having children, and raise fertility. Using longitudinal household data from Madagascar, we estimate the causal effect of drought occurrences on fertility, and explore the nature of potential mechanisms driving this effect. We exploit exogenous within-district year-to-year variation in rainfall deficits, and find that droughts occurring during the agricultural season significantly increase the number of children born to women living in agrarian communities. This effect is long lasting, as it is not reversed within four years fol- lowing the drought occurrence. Analyzing the mechanism, we find that droughts have no effect on common underlying factors of high fertility such as marriage timing and child mortality. Further- more, droughts have no significant effect on fertility if they occur during the non-agricultural season or in non-agrarian communities, and their positive effect in agrarian communities is mitigated by irrigation. These findings provide evidence that a low marginal price of having children is the main channel driving the fertility effect of drought in agrarian communities.
C14|A Comparison of Semiparametric Tests for Fractional Cointegration|There are various competing procedures to determine whether fractional cointegration is present in a multivariate time series, but no standard approach has emerged. We provide a synthesis of this literature and conduct a detailed comparative Monte Carlo study to guide empirical researchers in their choice of appropriate methodologies. Special attention is paid on empirically relevant issues such as assumptions about the form of the underlying process and the ability of the procedures to distinguish between short-run correlation and long-run equilibria. It is found that several approaches are severely oversized in presence of correlated short-run components and that the methods show different performance in terms of power when applied to common-component models instead of triangular systems.
C14|Identification of Auction Models Using Order Statistics|Auction data often fail to record all bids or all relevant factors that shift bidder values. In this paper, we study the identification of auction models with unobserved heterogeneity (UH) using multiple order statistics of bids. Classical measurement error approaches require multiple independent measurements. Order statistics, by definition, are dependent, rendering classical approaches inapplicable. First, we show that models with nonseparable finite UH is identifiable using three consecutive order statistics or two consecutive ones with an instrument. Second, two arbitrary order statistics identify the models if UH provides support variations. Third, models with separable continuous UH are identifiable using two consecutive order statistics under a weak restrictive stochastic dominance condition. Lastly, we apply our methods to U.S. Forest Service timber auctions and find evidence of UH.
C14|Nonparametric Assessment of Hedge Fund Performance|We propose a new class of performance measures for Hedge Fund (HF) returns based on a family of empirically identiable stochastic discount factors (SDFs). The SDF-based measures incorporate no-arbitrage pricing restrictions and naturally embed information about higher-order mixed moments between HF and benchmark factors returns. We provide a full asymptotic theory for our SDF estimators to test for the statistical signicance of each fund's performance and for the relevance of individual benchmark factors within each proposed measure. We apply our methodology to a panel of 4815 individual hedge funds. Our empirical analysis reveals that fewer funds have a statistically signicant positive alpha compared to the Jensen's alpha obtained by the traditional linear regression approach. Moreover, the funds' rankings vary considerably between the two approaches. Performance also varies between the members of our family because of a dierent fund exposure to higherorder moments of the benchmark factors, highlighting the potential heterogeneity across investors in evaluating performance.
C14|Identification with Latent Choice Sets|In a common experimental format, individuals are randomly assigned to either a treatment group with access to a program or a control group without access. In such experiments, analyzing the average effects of the treatment of program access may be hindered by the problem that some control individuals do not comply with their assigned status and receive program access from outside the experiment. Available tools to account for such a problem typically require the researcher to observe the receipt of program access for every individual. However, in many experiments, this is not the case as data is not collected on where any individual received access. In this paper, I develop a framework to show how data on only each individual's treatment assignment status, program participation decision and outcome can be exploited to learn about the average effects of program access. I propose a nonparametric selection model with latent choice sets to relate where access was received to the treatment assignment status, participation decision and outcome, and a linear programming procedure to compute the identified set for parameters evaluating the average effects of program access in this model. I illustrate the framework by analyzing the average effects of Head Start preschool access using the Head Start Impact Study. I nd that the provision of Head Start access induces parents to enroll their child into Head Start and also positively impacts test scores, and that these effects heterogeneously depend on the availability of access to an alternative preschool.
C14|Residential Property Price Indexes: Spatial Coordinates versus Neighbourhood Dummy Variables|The paper addresses the following question: can satisfactory residential property price indexes be constructed using hedonic regression techniques where location effects are modeled using local neighbourhood dummy variables or is it necessary to use spatial coordinates to model location effects. Hill and Scholz (2018) addressed this question and found, using their hedonic regression model, that it was not necessary to use spatial coordinates to obtain satisfactory property price indexes for Sydney. However, their hedonic regression model did not estimate separate land and structure price indexes for residential properties. In order to construct national balance sheet estimates, it is necessary to have separate land and structure price indexes. The present paper addresses the Hill and Scholz question in the context of providing satisfactory residential land price indexes. The spatial coordinate model used in the present paper is a modification of Colwellâ€™s (1998) spatial interpolation method. The modification can be viewed as a general nonparametric method for estimating a function of two variables.
C14|Nonparametric Estimation of Marginal Effects in Regression-spline Random Effects Models|We consider a B-spline regression approach towards nonparametric modelling of a random effects (error component) model. We focus our attention on the estimation of marginal effects (derivatives) and their asymptotic properties. Theoretical underpinnings are provided, finite-sample performance is evaluated via Monte Carlo simulation, and an application that examines the contribution of different types of public infrastructure on private production is investigated using panel data comprising the 48 contiguous states in the US over the period 1970-1986.
C14|Testing Nonlinearity through a Logistic Smooth Transition AR Model with Logistic Smooth Transition GARCH Errors|This paper analyzes the cyclical behavior of CAC 40 by testing the existence of nonlinearity through a logistic smooth transition AR model with logistic smooth transition GARCH errors. We study the daily returns of CAC 40 from 1990 to 2018. We estimate several models using nonparametric maximum likelihood, where the innovation distribution is replaced by a nonparametric estimate for the density function. We find that the rate of transition and the threshold value in both the conditional mean and conditional variance are highly significant. The forecasting results show that the informational shocks have transitory effects on returns and volatility and confirm nonlinearity.<br><small>(This abstract was borrowed from another version of this item.)</small>
C14|Memory that Drives! New Insights into Forecasting Performance of Stock Prices from SEMIFARMA-AEGAS Model|Stock price forecasting, a popular growth-enhancing exercise for investors, is inherently complex – thanks to the interplay of financial economic drivers which determine both the magnitude of memory and the extent of non-linearity within a system. In this paper, we accommodate both features within a single estimation framework to forecast stock prices and identify the nature of market efficiency commensurate with the proposed model. We combine a class of semiparametric autoregressive fractionally integrated moving average (SEMIFARMA) model with asymmetric exponential generalized autoregressive score (AEGAS) errors to design a SEMIRFARMA-AEGAS framework based on which predictive performance of this model is tested against competing methods. Our conditional variance includes leverage effects, jumps and fat tail-skewness distribution, each of which affects magnitude of memory in a stock price system. A true forecast function is built and new insights into stock price forecasting are presented. We estimate several models using the Skewed Student-t maximum likelihood and find that the informational shocks have permanent effects on returns and the SEMIFARMA-AEGAS is appropriate for capturing volatility clustering for both negative (long Value-at-Risk) and positive returns (short Value-at-Risk). We show that this model has better predictive performance over competing models for both long and/or some short time horizons. The predictions from SEMIRFARMA-AEGAS model beats comfortably the random walk model. Our results have implications for market-efficiency: the weak efficiency assumption of financial markets stands violated for all stock price returns studied over a long period.<br><small>(This abstract was borrowed from another version of this item.)</small>
C14|Beyond RCP8.5: Marginal Mitigation Using Quasi-Representative Concentration Pathways|Assessments of decreases in economic damages from climate change mitigation typically rely on climate output from computationally expensive precomputed runs of general circulation models (GCMs) under a handful of scenarios with discretely varying targets, such as the four representative concentration pathways (RCPs) for CO2 and other anthropogenically emitted gases. Although such analyses are extremely valuable in informing scientists and policymakers about specific, well-known, and massive mitigation goals, we add to the literature by considering potential outcomes from more modest policy changes that may not be represented by any concentration pathway or GCM output. We construct computationally efficient Quasi-representative Concentration Pathways (QCPs) in order to leverage existing scenarios featuring plausible concentration pathways. Computational efficiency allows for common statistical methods for assessing model uncertainty based on iterative replication, such as bootstrapping. We illustrate by feeding two QCPs through a computationally efficient statistical emulator and dose response functions extrapolated from estimates in the recent literature in order to gauge effects of mitigation on the relative risk of heat stress mortality.
C14|Identifying Modern Macro Equations with Old Shocks|"Despite decades of research, the consistent estimation of structural forward looking macroeconomic equations remains a formidable empirical challenge because of pervasive endogeneity issues. Prominent cases |the estimation of Phillips curves, of Euler equations for consumption or output, or of monetary policy rules| have typically relied on using pre-determined variables as instruments, with mixed success. In this work, we propose a new approach that consists in using sequences of independently identified structural shocks as instrumental variables. Our approach is robust to weak instruments and is valid regardless of the shocks' variance contribution. We estimate a Phillips curve using monetary shocks as instruments and find that conventional methods (i) substantially under-estimate the slope of the Phillips curve and (ii) over-estimate the role of forward-looking inflation expectations."
C14|Estimating the Heterogeneous Impact of the Free Movement of Persons on Relative Wage Mobility|We analyse the impact of an inflow of foreign workers on positional wage mobility in a small open economy like Switzerland. We exploit the quasi-natural experiment constituted by the entry into force of the Agreement on the Free Movement of Persons between Switzerland and the EU on 1st June 2002. We compute conditional average treatment effects with machine learning methods, and we find evidence of relevant heterogeneity in the impact of this policy on wage mobility.
C14|Random Forest Estimation of the Ordered Choice Model|In econometrics so-called ordered choice models are popular when interest is in the estimation of the probabilities of particular values of categorical outcome variables with an inherent ordering, conditional on covariates. In this paper we develop a new machine learning estimator based on the random forest algorithm for such models without imposing any distributional assumptions. The proposed Ordered Forest estimator provides a flexible estimation method of the conditional choice probabilities that can naturally deal with nonlinearities in the data, while taking the ordering information explicitly into account. In addition to common machine learning estimators, it enables the estimation of marginal effects as well as conducting inference thereof and thus providing the same output as classical econometric estimators based on ordered logit or probit models. An extensive simulation study examines the finite sample properties of the Ordered Forest and reveals its good predictive performance, particularly in settings with multicollinearity among the predictors and nonlinear functional forms. An empirical application further illustrates the estimation of the marginal effects and their standard errors and demonstrates the advantages of the flexible estimation compared to a parametric benchmark model.
C14|Bilinear form test statistics for extremum estimation|This paper develops a set of test statistics based on bilinear forms in the context of the extremum estimation framework. We show that the proposed statistic converges to a conventional chi-square limit. A Monte Carlo experiment suggests that the test statistic works well in ?nite samples
C14|Housing Rent Dynamics and Rent Regulation in St. Petersburg (1880-1917)|This article studies the evolution of housing rents in St. Petersburg between 1880 and 1917, covering an eventful period of Russian and world history. We collect and digitize over 5,000 rental advertisements from a local newspaper, which we use together with geo-coded addresses and detailed structural characteristics to construct a quality-adjusted rent price index in continuous time. We provide the first pre-war and pre-Soviet index based on market data for any Russian housing market. In 1915, one of the world's earliest rent control and tenant protection policies was introduced in response to soaring prices following the outbreak of World War I. We analyze the impact of this policy: while before the regulation rents were increasing at a similar rapid pace as other consumer prices, the policy reversed this trend. We find evidence for official compliance with the policy, document a rise in tenure duration and strongly increased rent affordability among workers after the introduction of the policy. We conclude that the immediate prelude to the October Revolution was indeed characterized by economic turmoil, but rent affordability and rising rents were no longer the dominating problems.
C14|Asymptotic F Tests under Possibly Weak Identification|This paper develops asymptotic F tests robust to weak identification and temporal dependence. The test statistics are modified versions of the S statistic of Stock and Wright (2000) and the K statistic of Kleibergen (2005), both of which are based on the continuous updating generalized method of moments. In the former case, the modification involves only a multiplicative degree-of-freedom adjustment. In the latter case, the modification involves an additional multiplicative adjustment that uses a J statistic for testing overidentification. By adopting fixed-smoothing asymptotics, we show that both the modified S statistic and the modified K statistic are asymptotically F-distributed. The asymptotic F theory accounts for the estimation errors in the underlying heteroskedasticity and autocorrelation robust variance estimators, which the asymptotic chi-squared theory ignores. Monte Carlo simulations show that the F approximations are much more accurate than the corresponding chi-squared approximations in finite samples.
C14|Heterogeneous panel data models with cross-sectional dependence|This paper considers a semiparametric panel data model with heterogeneous coefficients and individual-specific trending functions, where the random errors are assumed to be serially correlated and cross-sectionally dependent. We propose mean group estimators for the coefficients and trending functions involved in the model. It can be shown that the proposed estimators can achieve an asymptotic consistency with rates of root-NT and root-NTh, respectively as (N, T) -> (∞, ∞), where N is allowed to increase faster than T. Furthermore, a statistic for testing homogeneous coefficients is constructed based on the difference between the mean group estimator and a pooled estimator. Its asymptotic distributions are established under both the null and a sequence of local alternatives, even if the difference between these estimators vanishes considerably fast (can achieve root-NT2 rate at most under the null) and consistent estimator available for the covariance matrix is not required explicitly. The finite sample performance of the proposed estimators together with the size and local power properties of the test are demonstrated by simulated data examples, and an empirical application with the OECD health care expenditure dataset is also provided.
C14|Estimating and Decomposing Conditional Average Treatment Effects: The Smoking Ban in England|We develop a practical method for estimating and decomposing conditional average treatment effects using locally-weighted regressions. We illustrate with an application to the smoking ban in England using a regression discontinuity design, based on Health Survey for England data. We estimate average treatment effects conditional on socioeconomic status and decompose these effects by smoking location. Results show, the ban had no effect on the level of active smoking, but significantly reduced average exposure to second-hand smoke among non-smokers by 1.38 hours per week. Our method reveals a complex relationship between socioeconomic status and the effect on passive smoking. Decomposition analysis shows that these effects stem primarily from exposure reductions in pubs, but also from workplace exposure reductions for high socioeconomic status individuals.
C14|Nonparametric Homogeneity Pursuit in Functional-Coefficient Models|This paper explores the homogeneity of coefficient functions in nonlinear models with functional coefficients and identifies the underlying semiparametric modelling structure. With initial kernel estimates of coefficient functions, we combine the classic hierarchical clustering method with a generalised version of the information criterion to estimate the number of clusters, each of which has a common functional coefficient, and determine the membership of each cluster. To identify a possible semi-varying coefficient modelling framework, we further introduce a penalised local least squares method to determine zero coefficients, non-zero constant coefficients and functional coefficients which vary with an index variable. Through the nonparametric kernel-based cluster analysis and the penalised approach, we can substantially reduce the number of unknown parametric and nonparametric components in the models, thereby achieving the aim of dimension reduction. Under some regularity conditions, we establish the asymptotic properties for the proposed methods including the consistency of the homogeneity pursuit. Numerical studies, including Monte-Carlo experiments and an empirical application, are given to demonstrate the finite-sample performance of our methods.
C14|Nonparametric estimation of the random coefficients model: An elastic net approach|This paper investigates and extends the computationally attractive nonparametric random coefficients estimator of Fox, Kim, Ryan, and Bajari (2011). We show that their estimator is a special case of the nonnegative LASSO, explaining its sparse nature observed in many applications. Recognizing this link, we extend the estimator, transforming it to a special case of the nonnegative elastic net. The extension improves the estimator's recovery of the true support and allows for more accurate estimates of the random coefficients' distribution. Our estimator is a generalization of the original estimator and therefore, is guaranteed to have a model fit at least as good as the original one. A theoretical analysis of both estimators' properties shows that, under conditions, our generalized estimator approximates the true distribution more accurately. Two Monte Carlo experiments and an application to a travel mode data set illustrate the improved performance of the generalized estimator.
C14|Forecasting the Realized Variance in the Presence of Intraday Periodicity|This paper examines the impact of intraday periodicity on forecasting realized volatility using a heterogeneous autoregressive model (HAR) framework. We show that periodicity inflates the variance of the realized volatility and biases jump estimators. This combined effect adversely affects forecasting. To account for this, we propose a periodicity-adjusted model, HARP, where predictors are built from the periodicity-filtered data. We demonstrate empirically (using 30 stocks from various business sectors and the SPY for the period 2000--2016) and via Monte Carlo simulations that the HARP models produce significantly better forecasts, especially at the 1-day and 5-days ahead horizons.
C14|Marshallian vs Jacobs Effects: Which One Is Stronger? Evidence for Russia Unemployment Dynamics|This paper is devoted to the study of diversification and specialization influence on one of the main indicators of Russian labour market, the unemployment growth. The purpose of the work is to find out which effects dominate in the Russian regions, Marshallian or Jacobs, and whether this predominance is stable for different time intervals. The following hypotheses were empirically tested: 1) the dependence of the unemployment rate on the degree of concentration or diversification is non-monotonic due to possible overlapping effects of urbanization and localization; 2) the influence of the degree of concentration or diversification on the level of unemployment depends on the time period. To test these hypotheses nonparametric additive models with spatial effects were used. Both hypotheses found empirical confirmation. It was shown that in Russia, depending on the period, various effects dominated: in 2008-2010, and 2013-2016 Marshallian effects predominated, while in 2010-2013, Jacobs effects dominated.
C14|Increasing inequality in lifetime earnings: A tale of educational upgrading and changing employment patterns|This paper provides a detailed decomposition analysis of rising lifetime earnings inequality in Germany using individual employment biographies constructed from high-quality administrative data. The results show that significant parts of rising lifetime earnings inequality among West German men born between the years 1955 and 1974 can be attributed to a lower labor market participation (as a consequence of longer periods of both part-time and non-employment) as well as the educational expansion among later cohorts. The paper also points towards potentially important changes in the penalty linked to employment interruptions, but only finds a moderate impact of skill-biased technological change beyond educational upgrading. The analysis reveals similarities with the development in the U.S. in the sense that the cohorts studied did not only face an increase in inequality, but also a stagnation in earnings for a major part of their career. This trend is even stronger when looking at changes within education groups.
C14|Inference under covariate-adaptive randomization with multiple treatments|" This paper studies inference in randomized controlled trials with covariate-adaptive randomization when there are multiple treatments. More specifically, we study in this setting inference about the average effect of one or more treatments relative to other treatments or a control. As in Bugni et al. (2017), covariate-adaptive randomization refers to randomization schemes that first stratify according to baseline covariates and then assign treatment status so as to achieve ""balance"" within each stratum. In contrast to Bugni et al. (2017), however, we allow for the proportion of units being assigned to each of the treatments to vary across strata. We first study the properties of estimators derived from a ""fully saturated"" linear regression, i.e., a linear regression of the outcome on all interactions between indicators for each of the treatments and indicators for each of the strata. We show that tests based on these estimators using the usual heteroskedasticity-consistent estimator of the asymptotic variance are invalid in the sense that they may have limiting rejection probability under the null hypothesis strictly greater than the nominal level; on the other hand, tests based on these estimators and suitable estimators of the asymptotic variance that we provide are exact in the sense that they have limiting rejection probability under the null hypothesis equal to the nominal level. For the special case in which the target proportion of units being assigned to each of the treatments does not vary across strata, we additionally consider tests based on estimators derived from a linear regression with ""strata fixed effects,"" i.e., a linear regression of the outcome on indicators for each of the treatments and indicators for each of the strata. We show that tests based on these estimators using the usual heteroskedasticity-consistent estimator of the asymptotic variance are conservative in the sense that they have limiting rejection probability under the null hypothesis no greater than and typically strictly less than the nominal level, but tests based on these estimators and suitable estimators of the asymptotic variance that we provide are exact, thereby generalizing results in Bugni et al. (2017) for the case of a single treatment to multiple treatments. A simulation study illustrates the practical relevance of our theoretical results."
C14|Robust cross-country analysis of inequality of opportunity|International rankings of countries based on inequality of opportunity indices may not be robust vis-à-vis the specific metric adopted to measure opportunities. Indices often aggregate relevant information and neglect to control for normatively irrelevant distributional factors. This paper shows that gap curves can be estimated from cross-sectional data and adopted to test hypotheses about robust cross-country comparisons of (in)equality of opportunity.
C14|Distributional change: Assessing the contribution of household income sources|We develop a decomposition of distributional change by factor components to quantify how changes in the association between sources of income and changes in their marginal distributions contribute to the change in the distribution of household incomes over time. The two components are further broken down to isolate the contribution of specific income sources. Application to the change in the distribution of household incomes in Luxembourg between 2004 and 2013 reveals contrasted results: increased association between spouse earnings, public transfers, and taxes depressed the income share of poor households while changes in marginal distributions increased incomes in the upper half of the distribution.
C14|Local territorial reform and regional spending efficiency|We investigate the effect of a local territorial reform, which reduced the number of parishes, on municipality spending efficiency in the period 2011-2016. We build a composite output indicator and use Data Envelopment Analysis (DEA) to compute efficiency scores, which we then analyze through a second stage regression with socio-demographic, economic factors and the reform. We find efficiency gains for around 10% of municipalities overall. In Alentejo and in Centro, more than 50% of the municipalities improved efficiency. The second stage results show that the reform did not improve local spending efficiency in Mainland Portugal, particularly in the Norte region.
C14|How “big” should government be?|We assess how “big” government should reasonably be in a number of advanced countries. First, we will link the recent findings of Data Envelope Analysis on efficient public expenditure with the question of the size of the government. Second, we report descriptive analysis of various government performance indicators in relation to public expenditure to provide indications of overall “optimal” across spending categories. In principle, the highest savings potential is in the biggest expenditure categories, public consumption and social expenditure.
C14|Taxation and Public Spending Efficiency: An International Comparison|This paper evaluates the relevance of the taxation for public spending efficiency in a sample of OECD economies in the period 2003-2017. First, we compute the data envelopment analysis (DEA) scores and the Malmquist productivity index to measure the change in total factor productivity, the change in efficiency and the change in technology. Second, we explain these newly computed public efficiency scores with tax structures using a reduced-form panel data regression specification. Looking at the period between 2007 and 2017, our main findings are as follows: inputs could be theoretically lower by approximately 32-34%; the Malmquist indices show an overall decrease in technology and in TFP. Crucial for policymaking, we find that expenditure efficiency is negatively associated with taxation, more specifically direct and indirect taxes negatively affect government efficiency performance, and the same is true for social security contributions.
C14|Multi-step non- and semi-parametric predictive regressions for short and long horizon stock return prediction|In this paper, we propose three new predictive models: the multi-step nonparametric predictive regression model and the multi-step additive predictive regression model, in which the predictive variables are locally stationary time series; and the multi-step time-varying coefficient predictive regression model, in which the predictive variables are stochastically nonstationary. We also establish the estimation theory and asymptotic properties for these models in the short horizon and long horizon case. To evaluate the effectiveness of these models, we investigate their capability of stock return prediction. The empirical results show that all of these models can substantially outperform the traditional linear predictive regression model in terms of both in-sample and out-of-sample performance. In addition, we find that these models can always beat the historical mean model in terms of in-sample fitting, and also for some cases in terms of the out-of-sample forecasting.
C14|High dimensional semiparametric moment restriction models|Moment restriction semiparametric models, where both the dimension of parameter and the number of restrictions are divergent and an unknown function is involved, are studied using the generalized method of moments (GMM) and sieve method dealing with the nonparametric parameter. The consistency and normality for the GMM estimators are established. Meanwhile, a new test statistic is proposed for overidentification issue. Numerical examples are used to verify the established theory.
C14|Inference on a Semiparametric Model with Global Power Law and Local Nonparametric Trends|This paper studies a model with both a parametric global trend and a nonparametric local trend. This model may be of interest in a number of applications in economics, finance, ecology, and geology. The model nests the parametric global trend model considered in Phillips (2007) and Robinson (2012), and the nonparametric local trend model. We first propose two hypothesis tests to detect whether either of the special cases are appropriate. For the case where both null hypotheses are rejected, we propose an estimation method to capture both aspects of the time trend. We establish consistency and some distribution theory in the presence of a large sample. Moreover, we examine the proposed hypothesis tests and estimation methods through both simulated and real data examples. Finally, we discuss some potential extensions and issues when modelling time effects.
C14|Nonseparable Sample Selection Models with Censored Selection Rules|We consider identification and estimation of nonseparable sample selection models with censored selection rules. We employ a control function approach and discuss different objects of interest based on(1)local effects conditional on the control function, and (2)global effects obtained from integration over ranges of values of the control function. We provide conditions under which these objects are appropriate for the total population. We also present results regarding the estimation of counterfactual distributions. We derive conditions for identification for these different objects and suggest strategies for estimation. We also provide the associated asymptotic theory.These strategies are illustrated in an empirical investigation of the determinants of female wages and wage growth in the United Kingdom.
C14|Permutation tests for equality of distributions of functional data| Economic data are often generated by stochastic processes that take place in continuous time, though observations may occur only at discrete times. For example, electricity and gas consumption take place in continuous time. Data generated by a continuous time stochastic process are called functional data. This paper is concerned with comparing two or more stochastic processes that generate functional data. The data may be produced by a randomized experiment in which there are multiple treatments. The paper presents a test of the hypothesis that the same stochastic process generates all the functional data. In contrast to existing methods, the test described here applies to both functional data and multiple treatments. The test is presented as a permutation test, which ensures that in a finite sample, the true and nominal probabilities of rejecting a correct null hypothesis are equal. The paper also presents the asymptotic distribution of the test statistic under alternative hypotheses. The results of Monte Carlo experiments and an application to an experiment on billing and pricing of natural gas illustrate the usefulness of the test.
C14|Testing continuity of a density via g -order statistics in the regression discontinuity design| In the regression discontinuity design (RDD), it is common practice to assess the credibility of the design by testing the continuity of the density of the running variable at the cut-off, e.g., McCrary (2008). In this paper we propose a new test for continuity of a density at a point based on the so-called g-order statistics, and study its properties under a novel asymptotic framework. The asymptotic framework is intended to approximate a small sample phenomenon: even though the total number n of observations may be large, the number of effective observations local to the cut-off is often small. Thus, while traditional asymptotics in RDD require a growing number of observations local to the cut-off as n ? 8, our framework allows for the number q of observations local to the cut-off to be fixed as n ? 8. The new test is easy to implement, asymptotically valid under weaker conditions than those used by competing methods, exhibits finite sample validity under stronger conditions than those needed for its asymptotic validity, and has favorable power properties against certain alternatives. In a simulation study, we find that the new test controls size remarkably well across designs. We finally apply our test to the design in Lee (2008), a well-known application of the RDD to study incumbency advantage.
C14|GEL-based inference with unconditional moment inequality restrictions| This paper studies the properties of generalised empirical likelihood (GEL) methods for the estimation of and inference on partially identifi ed parameters in models specifi ed by unconditional moment inequality constraints. The central result is, as in moment equality condition models, a large sample equivalence between the scaled optimised GEL objective function and that for generalised method of moments (GMM) with weight matrix equal to the inverse of the efficient GMM metric for moment equality restrictions. Consequently, the paper provides a generalisation of results in the extant literature for GMM for the non-diagonal GMM weight matrix setting. The paper demonstrates that GMM in such circumstances delivers a consistent estimator of the identi fied set, i.e., those parameter values that satisfy the moment inequalities, and derives the corresponding rate of convergence. Based on these results the consistency of and rate of convergence for the GEL estimator of the identifi ed set are obtained. A number of alternative equivalent GEL criteria are also considered and discussed. The paper proposes simple conservative consistent confi dence regions for the identi fied set and the true parameter vector based on both GMM with a non-diagonal weight matrix and GEL. A simulation study examines the efficacy of the non-diagonal GMM and GEL procedures proposed in the paper and compares them with the standard diagonal GMM method.
C14|Locally robust semiparametric estimation| This paper shows how to construct locally robust semiparametric GMM estimators, meaning equivalently moment conditions have zero derivative with respect to the first step and the first step does not affect the asymptotic variance. They are constructed by adding to the moment functions the adjustment term for first step estimation. Locally robust estimators have several advantages. They are vital for valid inference with machine learning in the first step, see Belloni et. al. (2012, 2014), and are less sensitive to the specification of the first step. They are doubly robust for affine moment functions, where moment conditions continue to hold when one first step component is incorrect. Locally robust moment conditions also have smaller bias that is flatter as a function of first step smoothing leading to improved small sample properties. Series first step estimators confer local robustness on any moment conditions and are doubly robust for affine moments, in the direction of the series approximation. Many new locally and doubly robust estimators are given here, including for economic structural models. We give simple asymptotic theory for estimators that use cross-fitting in the first step, including machine learning.
C14|Identifying Effects of Multivalued Treatments|Multivalued treatment models have typically been studied under restrictive assumptions: ordered choice, and more recently, unordered monotonicity. We show how treatment effects can be identified in a more general class of models that allows for multidimensional unobserved heterogeneity. Our results rely on two main assumptions: treatment assignment must be a measurable function of threshold‐crossing rules, and enough continuous instruments must be available. We illustrate our approach for several classes of models.
C14|Improved Density and Distribution Function Estimation|Given additional distributional information in the form of moment restrictions, kernel density and distribution function estimators with implied generalised empirical likelihood probabilities as weights achieve a reduction in variance due to the systematic use of this extra information. The particular interest here is the estimation of densities or distributions of (generalised) residuals in semi-parametric models defined by a finite number of moment restrictions. Such estimates are of great practical interest, being potentially of use for diagnostic purposes, including tests of parametric assumptions on an error distribution, goodness-of-fit tests or tests of overidentifying moment restrictions. The paper gives conditions for the consistency and describes the asymptotic mean squared error properties of the kernel density and distribution estimators proposed in the paper. A simulation study evaluates the small sample performance of these estimators. Supplements provide analytic examples to illustrate situations where kernel weighting provides a reduction in variance together with proofs of the results in the paper.
C14|Control Variables, Discrete Instruments, and Identification of Structural Functions|Control variables provide an important means of controlling for endogeneity in econometric models with nonseparable and/or multidimensional heterogeneity. We allow for discrete instruments, giving identification results under a variety of restrictions on the way the endogenous variable and the control variables affect the outcome. We consider many structural objects of interest, such as average or quantile treatment effects. We illustrate our results with an empirical application to Engel curve estimation.
C14|Semiparametrically Efficient Estimation of the Average Linear Regression Function|Let Y be an outcome of interest, X a vector of treatment measures, and W a vector of pre-treatment control variables. Here X may include (combinations of) continuous, discrete, and/or non-mutually exclusive “treatments”. Consider the linear regression of Y onto X in a subpopulation homogenous in W = w (formally a conditional linear predictor). Let b 0 (w) be the coefficient vector on X in this regression. We introduce a semiparametrically efficient estimate of the average β 0 = Ε[b 0 (W)]. When X is binary-valued (multi-valued) our procedure recovers the (a vector of) average treatment effect(s). When X is continuously-valued, or consists of multiple non-exclusive treatments, our estimand coincides with the average partial effect (APE) of X on Y when the underlying potential response function is linear in X, but otherwise heterogenous across agents. When the potential response function takes a general nonlinear/heterogenous form, and X is continuously-valued, our procedure recovers a weighted average of the gradient of this response across individuals and values of X. We provide a simple, and semiparametrically efficient, method of covariate adjustment for settings with complicated treatment regimes. Our method generalizes familiar methods of covariate adjustment used for program evaluation as well as methods of semiparametric regression (e.g., the partially linear regression model).
C14|A Lattice Test for Additive Separability|We derive necessary and sufficient conditions for a finite data set of price and demand observations to be consistent with an additively separable preference. We do so without imposing concavity on any of the subutility functions or convexity of the budget set a priori, thereby generalizing earlier results. Our simple and intuitive lattice test easily accommodates departures from rationality, or errors, which subsequently facilitates a rich empirical analysis. We apply our econometric techniques to the food consumption of a panel of British households. The primary empirical finding is that additive separability has considerable success in explaining the data.
C14|Policy discontinuity and duration outcomes|A comparison of hazard rates of duration outcomes before and after policy changes is hampered by non-identification if there is unobserved heteogeneity in the effects and no model structure is imposed. We develop a discontinuity approach that overcomes this by exploiting variation in the moment at which different cohorts are exposed to the policy change, i.e. by considering spells crossing the policy change. We prove identification of average treatment effect on hazard rates without model structure. We estimate these effects by local linear kernel hazard regression. We use the introduction of the NDYP programme for young unemployed individuals to estimate average programme participation effects on the exit rate to work.
C14|Adaptive Bayesian Estimation of Mixed Discrete-Continuous Distributions under Smoothness and Sparsity|We consider nonparametric estimation of a mixed discrete-continuous distribution under anisotropic smoothness conditions and possibly increasing number of support points for the discrete part of the distribution. For these settings, we derive lower bounds on the estimation rates in the total variation distance. Next, we consider a nonparametric mixture of normals model that uses continuous latent variables for the discrete part of the observations. We show that the posterior in this model contracts at rates that are equal to the derived lower bounds up to a log factor. Thus, Bayesian mixture of normals models can be used for optimal adaptive estimation of mixed discrete-continuous distributions.
C14|Central Bank Policy Announcements and Changes in Trading Behavior: Evidence from Bond Futures High Frequency Price Data|We present a theoretical model to explain how financial traders incorporate public and private information into security prices. We explain that the model enables us to simultaneously identify when public information caused surprises and how large an impact it had on the market. By applying the model to the tick-by-tick data on Japanese government bond futures prices, we show that the Bank of Japan fs introduction of quantitative and qualitative monetary easing was one of the most surprising episodes during the period from 2005 to 2016. We also show that the sensitivity to the Bank fs announcements has strengthened since the introduction of the negative interest rate policy, whereas the sensitivity to economic indicators and surveys has weakened substantially.
C14|“A new metric of consensus for Likert scales”|In this study we present a metric of consensus for Likert-type scales. The measure gives the level of agreement as the percentage of consensus among respondents. The proposed framework allows to design a positional indicator that gives the degree of agreement for each item and for any given number of reply options. In order to assess the performance of the proposed metric of consensus, in an iterated one-period ahead forecasting experiment we test whether the inclusion of the degree of agreement in consumers’ expectations regarding the evolution of unemployment improves out-of-sample forecast accuracy in eight European countries. We find evidence that the degree of agreement among consumers contains useful information to predict unemployment rates in most countries. The obtained results show the usefulness of consensus-based metrics to track the evolution of economic variables.
C14|“Flexible maximum conditional likelihood estimation for single-index models to predict accident severity with telematics data”|Estimation in single-index models for risk assessment is developed. Statistical properties are given and an application to estimate the cost of traffic accidents in an innovative insurance data set that has information on driving style is presented. A new kernel approach for the estimator covariance matrix is provided. Both, the simulation study and the real case show that the method provides the best results when data are highly skewed and when the conditional distribution is of interest. Supplementary materials containing appendices are available online.
C14|How does the achievement gap between immigrant and native-born pupils progress from primary to secondary education?|This paper documents the change in educational achievement differences between native and foreign background students between the ages of 10 and 15, as they progress from primary to secondary education. We examine three cohorts of students in a number of Western European and traditional English-speaking immigration countries using combinations of PIRLS, TIMSS and PISA survey data. While the performance of students with mixed parents is not markedly different from native students?, foreign background children?both first- and second-generation?exhibit a large achievement gap at age 10 in continental Europe, even when accounting for observable differences in socio-economic characteristics. The gap tends to narrow down by age 15 in reading, but no catching up is observed in mathematics. By contrast, we do not find significant differences between the academic achievements of immigrant children and their native-born peers in traditional immigration countries.
C14|Generalised Empirical Likelihood Kernel Block Bootstrapping|This article unveils how the kernel block bootstrap method of Parente and Smith (2018a,2018b) can be applied to make inferences on parameters of models de ned through moment restrictions. Bootstrap procedures that resort to generalised empirical likelihood implied probabilities to draw observations are also introduced. We prove the rst-order asymptotic validity of bootstrapped test statistics for overidentifying moment restrictions, parametric restrictions and additional moment restrictions. Resampling methods based on such probabilities were shown to be efficient by Brown and Newey (2002). A set of simulation experiments reveals that the statistical tests based on the proposed bootstrap methods perform better than those that rely on first-order asymptotic theory.
C14|Quasi-Maximum Likelihood and the Kernel Block Bootstrap for Nonlinear Dynamic Models|"This paper applies a novel bootstrap method, the kernelblockbootstrap, to quasi-maximum likelihood estimation of dynamic models with stationary strong mixing data. The method rst kernel weights the components comprising the quasi-log likelihood function in an appropriate way and then samples the resultant transformed components using the standard ""m out of n""bootstrap. We investigate the first order asymptotic properties of the KBB method for quasi-maximum likelihood demonstrating, in particular, its consistency and the rst-order asymptotic validity of the bootstrap approximation to the distribution of the quasi-maximum likelihood estimator. A set of simulation experiments for the mean regression model illustrates the efficacy of the kernel block bootstrap for quasi-maximum likelihood estimation."
C14|Does market structure trigger efficiency? Evidence for the USA before and after the financial crisis|This paper investigates the relationship between efficiency and market structure for a sample of industrial facilities dispersed among the USA states. In order to measure the relevant efficiency scores, we use a Data Development Analysis (DEA) allowing for the inclusion of desirable and undesirable (toxic chemical releases) outputs in the production function. In the next stage, we utilise the bootstrapped quantile regression methodology to uncover possible non-linear relationships between efficiency and competition at the mean and at various quantiles before and after the global financial crisis (2002 and 2012). In this way, we impose no functional form constraints on parameter values over the conditional distribution of the dependent variable (efficiency). At the same time, we estimate at which part of its conditional distribution function, the efficiency is located and draw substantial conclusions about the range of policy measures obtained. The empirical findings, indicate that the relationship between efficiency and market concentration did not remain unchanged in the aftermath of the economic crisis. The empirical results survived robustness checks under the inclusion of an alternative market concentration indicator (CR8).
C14|Testing Happiness Hypothesis among the Elderly|Se emplea un amplio conjunto de datos que permite evaluar de diferentes formas la hipótesis de la felicidad, empleando cuatro enfoques metodológicos. Se constata que las personas de mayor edad en Uruguay tienen una tendencia a reconocerse felices cuando están casadas, cuando tienen un buen estado de salud y si tienen altos ingresos monetarios o estiman que su ingreso es conveniente para su nivel de vida. Contrariamente, señalan niveles más bajos de felicidad cuando viven solos o cuando sunutrición es insuficiente. Se evidencia que la educación no tiene un impacto claro sobre su percepción de felicidad. Este trabajo es una contribución al estudio de los factores que pueden explicar la felicidad entre las personas de la tercera edad en los países de América Latina. El trabajo futuro seconcentrará sobre un análisis empírico mejorado y sobre la expansión del estudio a otros países.
C14|The Causal Relationships between Inflation and Inflation Uncertainty|Since the publication of Friedman’s (1977) Nobel lecture, the relationship between the mean function of the inflation stochastic process and its uncertainty has been the subject of much research. Friedman postulated that high inflation causes increased inflation uncertainty. Ball (1992) produces macroeconomic theory that could justify that causality. But other researchers have found the converse causality, from increased inflation uncertainty to increased mean inflation, and postulated macroeconomic theory that could support their views. In addition, some researchers have found inverse correlation between mean inflation and inflation volatility with causation in either direction. These controversies are important, since they have different implications for economic theory and policy. We conduct a systematic econometric study of the relationship among the first two moments of the inflation stochastic process using state of the art approaches. We propose a time-varying inflation uncertainty measure based on stochastic volatility to take into account unpredictable shocks. Further, we extend previous related literature by providing a new econometric specification of this relationship using two semi-parametric approaches: the frequency evolutionary co-spectral approach and the continuous wavelet methodology. We theoretically justify their use through an extension of Ball's (1992) model. These frequency approaches have two advantages, they provide the analyses for different frequency horizons and do not impose restriction on the data. While related literature always focused on the US data, our study explores this relationship for five major developed and emerging countries (the US, the UK, the Euro area, South Africa, and China) over the last five decades to investigate robustness of our inferences and investigate sources of prior inconsistencies in inferences among prior studies. This selection of countries permits investigation of the inflation versus inflation uncertainty relationship under different hypotheses, including explicit versus implicit inflation targets, conventional versus unconventional monetary policy, independent versus dependent central banks, and calm versus crisis periods. Our findings depict a significant relationship between inflation and inflation uncertainty that varies with time and frequency and offer an improved comprehension of the ambiguous inflation versus inflation uncertainty relationship. This relationship seems positive in the short and medium terms during stable periods, confirming the Friedman-Ball theory, while it is negative during crisis periods. In addition, our analysis identifies the phases of leading and lagging inflation uncertainty. Our general approach nests within it the earlier approaches, permitting explanation of the prior appearances of ambiguity in the relationship and identifies the conditions associated with the various outcomes.
C14|Non-parametric Estimation of GARCH (2, 2) Volatility model: A new Algorithm|The main objective of this paper is to provide an estimation approach for non-parametric GARCH (2, 2) volatility model. Specifically the paper, by combining the aspects of multivariate adaptive regression splines(MARS) model estimation algorithm proposed by Chung (2012) and an algorithm proposed by Buhlman and McNeil(200), develops an algorithm for non-parametrically estimating GARCH (2,2) volatility model. Just like the MARS algorithm, the algorithm that is developed in this paper takes a logarithmic transformation as a preliminary analysis to examine a nonparametric volatility model. The algorithm however differs from the MARS algorithm by assuming that the innovations are i.d.d. The algorithm developed follows similar steps to that of Buhlman and McNeil (200) but starts by semi parametric estimation of the GARCH model and not parametric while relaxing the dependency assumption of the innovations to avoid exposing the estimation procedure to risk of inconsistency in the event of misspecification errors.
C14|Consistent Pseudo‐Maximum Likelihood Estimators and Groups of Transformations|In a transformation model yt=c[a(xt,β),ut], where the errors ut are i.i.d. and independent of the explanatory variables xt, the parameters can be estimated by a pseudo‐maximum likelihood (PML) method, that is, by using a misspecified distribution of the errors, but the PML estimator of β is in general not consistent. We explain in this paper how to nest the initial model in an identified augmented model with more parameters in order to derive consistent PML estimators of appropriate functions of parameter β. The usefulness of the consistency result is illustrated by examples of systems of nonlinear equations, conditionally heteroscedastic models, stochastic volatility, or models with spatial interactions.
C14|Change Point Detection in the Conditional Correlation Structure of Multivariate Volatility Models|We propose semi-parametric CUSUM tests to detect a change point in the correlation structures of non--linear multivariate models with dynamically evolving volatilities. The asymptotic distributions of the proposed statistics are derived under mild conditions. We discuss the applicability of our method to the most often used models, including constant conditional correlation (CCC), dynamic conditional correlation (DCC), BEKK, corrected DCC and factor models. Our simulations show that, our tests have good size and power properties. Also, even though the near--unit root property distorts the size and power of tests, de--volatizing the data by means of appropriate multivariate volatility models can correct such distortions. We apply the semi--parametric CUSUM tests in the attempt to date the occurrence of financial contagion from the U.S. to emerging markets worldwide during the great recession.
C14|Performances management when modelling internal structure|The performances management is a key issue for public as well as private organizations. The core of the performances management in the DEA context are essentially the relative efficiency measurement for organizations considered as a “black box” that use inputs to produce two or more outputs. In reality, organizations/ production process are comprised of a number of divisions/stages which performs different functions/tasks interacting among them. For these reasons modelling internal structures of organizations/production process allow to discover the inefficiency of individual divisions/stages. In this paper we estimate the relative efficiency of a production process once modelling its internal structure with a network structure of three divisions/stages interrelated among them. To outline the differences in the performances management in the two cases (“black box” vs network structure) we compare they empirical cumulative distribution functions.
C14|Shadow price of patent stock as knowledge stock: Time and country heterogeneity|This study compares the shadow price (marginal cost) and shadow value (total cost) of patent stock (as knowledge stock) in each of 92 countries between 1992 and 2010. Two specifications are considered in the data envelopment analysis approach. One specification considers population, capital, patent stock, energy use (four inputs), greenhouse gas (undesirable output), and gross domestic product (desirable output). The other uses human capital and natural capital instead of population and energy use. Under these two specifications, respectively, the shadow price of the patent stock (on weighted average) for the whole period is −0.106 and −0.054 million US dollars per patent in the entire sample. Similarly, the shadow value of the patent stock (by the ratio of gross domestic product) in the entire sample is −5.8% and −2.9%, respectively. As the standing position of patent stock, the patent stock is less valuable than human capital and (produced) capital but more valuable than population, energy use, and natural capital. The patent stock also is likely to be valuable in developing countries. In addition, the shadow value of the patent stock is relatively high in certain large countries and nearly flat in most of the countries.
C14|Modeling the effect of competition using robust conditional nonparametric frontiers: Evidence from U.S. manufacturing sector|The study applies the probabilistic framework of nonparametric frontier estimation in order to model the effect of competitive conditions on sectors’ production efficiency levels. We utilize conditional Order-m robust frontiers modeling the dynamic effects of competitive conditions on a sample of 462 U.S. 6-digit manufacturing sectors over the period 1958-2009. The results derived from the time-dependent robust conditional estimators unveil a non-linear relationship between market competition and productive efficiency. Our findings suggest that for higher competitive conditions the effect is positive up to a certain threshold point after which the effect becomes negative.
C14|The spatial distribution of US cities|In this paper, we consider the distribution of bilateral distances between all pairs of cities to estimate K-densities using the methodology by Duranton and Overman (2005), identifying different spatial patterns. By using data from different definitions of US cities in 2010 (places, urban areas, and core-based statistical areas), we analyse the spatial distribution of cities, finding significant patterns of dispersion depending on the city size and city definition. Our results lend support to a hierarchical system of US cities in which the central cities of each subsystem are far away from each other.
C14|On the Examination of Competition in the Petroleum Industry: A Pooled Panel Threshold Analysis|This paper contributes to the literature since it tries to link the Exchange Rate Pass-Through (ERPT) with the “rockets and feathers” hypothesis using a panel of EU-28 countries. Allowing for the existence of an endogenous threshold variable our empirical findings indicate that the threshold model is better suited to this analysis than the baseline linear adjustment model. This is the case since the latter restricts the threshold to be centered around zero and the dynamic response to cumulative shocks cannot be properly identified. The empirical findings reveal that the threshold variable expressed by the trade-weighted dollar exchange rate index is statistically significant only in the sample above the threshold (high regime). This means that for the net EU exporting countries, fluctuations in the real effective exchange rate of the US against its major EU trading partners does affect the level of pre-tax retail gasoline prices with the relevant elasticity exceeding unity (complete ERPT). Moreover, all the statistical tests reject the null hypothesis that there is no significant threshold and thus an asymmetric adjustment gasoline mechanism prevails.
C14|An Information-Theoretic Approach to Estimating Willingness To Pay for River Recreation Site Attributes|This study applies an information theoretic econometric approach in the form of a new maximum likelihood-minimum power divergence (ML-MPD) semi-parametric binary response estimator to analyze dichotomous contingent valuation data. The ML-MPD method estimates the underlying behavioral decision process leading to a person’s willingness to pay for river recreation site attributes. Empirical choice probabilities, willingness to pay measures for recreation site attributes, and marginal effects of changes in some explanatory variables are estimated. For comparison purposes, a Logit model is also implemented. A Wald test of the symmetric logistic distribution underlying the Logit model is rejected at the 0.01 level in favor of the ML-MPD distribution model. Moreover, based on several goodness-of-fit measures we find that the ML-MPD is superior to the Logit model. Our results also demonstrate the potential for substantially overstating the precision of the estimates and associated inferences when the imposition of unknown structural information is not accounted explicitly for in the model. The ML-MPD model provides more intuitively reasonable and defensible results regarding the valuation of river recreation than the Logit model.
C14|A latent class analysis towards stability and changes in breadwinning patterns among coupled households|A latent class model is proposed to examine couples’ breadwinning typologies and explain the wage differentials according to the socio-demographic characteristics of the society with data collected through surveys. We derive an ordinal variable indicating the couple’s income provision-role type and suppose the existence of an underlying discrete latent variable to model the effect of covariates. We use a two-step maximum likelihood inference conducted to account for concomitant variables, informative sampling scheme and missing responses. The weighted log-likelihood is maximised through the Expectation-Maximization algorithm and information criteria are used to develop the model selection. Predictions are made on the basis of the maximum posterior probabilities. Disposing of data collected in Japan over thirty years we compare couples’ breadwinning patterns across time. We provide some evidence of the gender wage-gap and we show that it can be attributed to the fact that, especially in Japan, duties and responsibilities for the child care are supported exclusively by women.
C14|A review of more than one hundred Pareto-tail index estimators|This paper reviews more than one hundred Pareto (and equivalent) tail index estimators. It focuses on univariate estimators for nontruncated data. We discuss basic ideas of these estimators and provide their analytical expressions. As samples from heavy-tailed distributions are analysed by researchers from various fields of science, the paper provides nontechnical explanations of the methods, which could be understood by researchers with intermediate skills in statistics. We also discuss strengths and weaknesses of the estimators, if they are known. The paper can be viewed as a catalog or a reference book on Pareto-tail index estimators.
C14|Can we beat the Random Walk? The case of survey-based exchange rate forecasts in Chile|We examine the accuracy of survey-based expectations of the Chilean exchange rate relative to the US dollar. Our out-of-sample analysis reveals that survey-based forecasts outperform the Driftless Random Walk (DRW) in terms of Mean Squared Prediction Error at several forecasting horizons. This result holds true even when comparing the survey to a more competitive benchmark based on a refined information set. A similar result is found when precision is measured in terms of Directional Accuracy: survey-based forecasts outperform a “pure luck” benchmark at several forecasting horizons. Differing from the traditional “no predictability” result reported in the literature for many exchange rates, our findings suggest that the Chilean peso is indeed predictable.
C14|City size distribution and space|We study the US city size distribution over space. This paper makes two contributions to the empirical literature on city size distributions. First, this study uses data from different definitions of US cities in 2010 to study the distribution of cities in space, finding significant patterns of dispersion depending on city size. Second, the paper proposes a new distance-based approach to analyse the influence of distance on the city size distribution parameters, considering both the Pareto and lognormal distributions. By using all possible combinations of cities within a 300-mile radius, results indicate that the Pareto distribution cannot be rejected in most of the cases regardless of city size. Placebo regressions validate our results, thereby confirming the significant effect of geography on the Pareto exponent.
C14|Effect of aging on housing prices: evidence from a panel data|We empirically test the effect of ageing on housing prices. Our analysis shows that a decline in the fertility rate and an increase in longevity – the two main causes of an ageing population – have divergent effects on housing prices. This empirical finding helps us to reconcile a conflict which has lasted for 30 years in literature. We show that a decline in the fertility rate generally lowers housing prices because there are fewer workers in the population. At the same time, the workers and retirees react differently towards the impact of longer lifespans. In particular, the workers are urged to purchase more houses as a form of of saving and thus raise the prices, while the retirees tend to sell a greater fraction of the housing for extra funding. The conclusions correspond well with the Life Cycle Hypothesis and are drawn by using a semi-parametric method on an international panel data.
C14|Efficiency in BRICS Currency Markets Using Long-Spans of Data: Evidence from Model-Free Tests of Directional Predictability|We analyze the directional predictability in foreign exchange markets of Brazil, Russia, India, China and South Africa (BRICS) using the quantilogram, based on long-spans of monthly historical data, at times covering over a century. We find that the efficient market hypothesis (EMH) holds at the extreme phases of the currency markets (and around the median for India and South Africa). Since predictability holds at certain parts of the unconditional distribution of exchange rate returns, we find support for the Adaptive Market Hypothesis (AMH). AMH, based on the idea of bounded rationality, suggests that currency return predictability will be intermittent, due to changing market conditions and institutional factors.
C14|Convexity, Disposability and Returns to Scale in Production Analysis|Adequate modelling of undesirable outputs is a key aspect for any performance analysis of economic systems. A nonparametric approach assuming jointly weak disposability of desirable and undesirable outputs inspired by Shephard (1974) has gained substantial popularity in addressing this issue. Recently, researchers were offered an alternative that is to use multiple scaling factors (rather than a single one as in the Shephardâ€™s (1974) approach) when imposing weak disposability in practice. In this paper we discover new properties and relationships between the two approaches, which in turn sheds some new light on the problem and offers reconciling solutions.
C14|Aggregation of Individual Efficiency Measures and Productivity Indices|Here we consider the problem of aggregation of efficiency and productivity indices. We will summarize some of the existing results and will derive new results for aggregation of Hicks-Moorsteen productivity indices.
C14|Stochastic Frontier Analysis: Foundations and Advances|This chapter reviews some of the most important developments in the econometric estimation of productivity and efficiency surrounding the stochastic frontier model. We highlight endogeneity issues, recent advances in generalized panel data stochastic frontier models, nonparametric estimation of the frontier, quantile estimation and distribution free methods. An emphasis is placed on highlighting recent research and providing broad coverage, while details are left for further reading in the abundant (although not limited to) list of references provided.
C14|Measuring Productivity by Quadratic-mean-of-order-of-r Indexes|In this paper, we propose the quadratic-mean-of-order-of-r indexes of output, input and productivity and show that all index number formulae belonging to this family are superlative indexes. In turn, this helps by deriving a generalization of the well-known Diewertâ€™s theorem about equivalence of Fisher and Malmquist indexes. Our results also give new justifications for output and input comparison and productivity measurement via other interesting indexes such as the implicit Walsh index.
C14|Improving Finite Sample Approximation by Central Limit Theorems for DEA and FDH efficiency scores|We propose an improvement of the finite sample approximation of the central limit theorems (CLTs) that were recently derived for statistics involving production efficiency scores estimated via Data Envelopment Analysis (DEA) or Free Disposal Hull (FDH) approaches. The improvement is very easy to implement since it involves a simple correction of the already employed statistics without any additional computational burden and preserves the original asymptotic results such as consistency and asymptotic normality. The proposed approach persistently showed improvement in all the scenarios that we tried in various Monte-Carlo experiments, especially for relatively small samples or relatively large dimensions (measured by total number of inputs and outputs) of the underlying production model. This approach therefore is expected to be valuable (and at almost no additional computational costs) for practitioners wishing to perform statistical inference about production efficiency using DEA or FDH approaches.
C14|Econometric Analysis of Productivity: Theory and Implementation in R|Our chapter details a wide variety of approaches used in estimating productivity and efficiency based on methods developed to estimate frontier production using Stochastic Frontier Analysis (SFA) and Data Envelopment Analysis (DEA). The estimators utilize panel, single cross section, and time series data sets. The R programs include such approaches to estimate firm efficiency as the time invariant fixed effects, correlated random effects, and uncorrelated random effects panel stochastic frontier estimators, time varying fixed effects, correlated random effects, and uncorrelated random effects estimators, semi-parametric efficient panel frontier estimators, factor models for cross-sectional and time-varying efficiency, bootstrapping methods to develop confidence intervals for index number-based productivity estimates and their decompositions, DEA and Free Disposable Hull estimators. The chapter provides the professional researcher, analyst, statistician, and regulator with the most up to date efficiency modeling methods in the easily accessible open source programming language R.
C14|Forecasting of Recessions via Dynamic Probit for Time Series: Replication and Extension of Kauppi and Saikkonen (2008)|In this work we first replicate the results of the fully parametric dynamic probit model for forecasting US recessions from Kauppi and Saikkonen (2008) (which is in the spirit of Estrella and Mishkin (1995, 1998) and Dueker (1997)) and then contrast them to results from a nonparametric local-likelihood dynamic choice model for the same data. We then use expanded data to gain insights on whether these models could have warned the public about approach of the latest recession, associated with the Global Financial Crisis. Finally, we also apply both approaches to gain insights for 2018.
C14|Specification testing in random coefficient models|In this paper, we suggest and analyze a new class of specification tests for random coefficient models. These tests allow to assess the validity of central structural features of the model, in particular linearity in coefficients, generalizations of this notion like a known nonlinear functional relationship, or degeneracy of the distribution of a random coefficient, that is, whether a coefficient is fixed or random, including whether an associated variable can be omitted altogether. Our tests are nonparametric in nature, and use sieve estimators of the characteristic function. We provide formal power analysis against global as well as against local alternatives. Moreover, we perform a Monte Carlo simulation study, and apply the tests to analyze the degree of nonlinearity in a heterogeneous random coefficients demand model. While we find some evidence against the popular QUAIDS specification with random coefficients, it is not strong enough to reject the specification at the conventional significance level.
C14|Bayesian Parametric and Semiparametric Factor Models for Large Realized Covariance Matrices|This paper introduces a new factor structure suitable for modeling large realized covariance matrices with full likelihood based estimation. Parametric and nonparametric versions are introduced. Due to the computational advantages of our approach we can model the factor nonparametrically as a Dirichlet process mixture or as an infinite hidden Markov mixture which leads to an infinite mixture of inverse-Wishart distributions. Applications to 10 assets and 60 assets show the models perform well. By exploiting parallel computing the models can be estimated in a matter of a few minutes.
C14|Bayesian inference and prediction of a multiple-change-point panel model with nonparametric priors|Change point models using hierarchical priors have been very successful estimating the parameter values of short-lived regimes. However, hierarchical priors have been parametric which leads to shrinkage in the estimates of extraordinary regime parameters. We overcome this by modeling the hierarchical priors nonparametrically. We also extend the change point to a panel of change point processes where the prior shares in the probabilities of changing regimes. When applied to the returns from a panel of actively managed, US equity, mutual funds our multiple-change-point panel model finds mutual fund skill is not persistent but changes over time.
C14|Specification tests for non-Gaussian maximum likelihood estimators|We propose generalised DWH specification tests which simultaneously compare three or more likelihood-based estimators of conditional mean and variance parameters in multivariate conditionally heteroskedastic dynamic regression models. Our tests are useful for GARCH models and in many empirically relevant macro and finance applications involving VARs and multivariate regressions. To design powerful and reliable tests, we determine the rank deficiencies of the differences between the estimators' asymptotic covariance matrices under the null of correct specification, and take into account that some parameters remain consistently estimated under the alternative of distributional misspecification. Finally, we provide finite sample results through Monte Carlo simulations.
C14|Econometric Analysis of Productivity: Theory and Implementation in R|Our chapter details a wide variety of approaches used in estimating productivity and efficiency based on methods developed to estimate frontier production using Stochastic Frontier Analysis (SFA) and Data Envelopment Analysis (DEA). The estimators utilize panel, single cross section, and time series data sets. The R programs include such approaches to estimate firm efficiency as the time invariant fixed effects, correlated random effects, and uncorrelated random effects panel stochastic frontier estimators, time varying fixed effects, correlated random effects, and uncorrelated random effects estimators, semi-parametric efficient panel frontier estimators, factor models for cross-sectional and time-varying efficiency, bootstrapping methods to develop confidence intervals for index number-based productivity estimates and their decompositions, DEA and Free Disposable Hull estimators. The chapter provides the professional researcher, analyst, statistician, and regulator with the most up to date efficiency modelling methods in the easily accessible open source programming language R.
C14|Non-structural Analysis of Productivity Growth for the Industrialized Countries: A Jackknife Model Averaging Approach|Various structural and non-structural models of productivity growth have been proposed in the literature. In either class of models, predictive measurements of productivity and efficiency are obtained. This paper examines the model averaging approaches of Hansen and Racine (2012), which can provide a vehicle to weight predictions (in the form of productivity and efficiency measurements) from different non-structural methods. We first describe the jackknife model averaging estimator proposed by Hansen and Racine (2012) and illustrate how to apply the technique to a set of competing stochastic frontier estimators. The derived method is then used to analyze productivity and efficiency dynamics in 25 highly-industrialized countries over the period 1990 to 2014. Through the empirical application, we show that the model averaging method provides relatively stable estimates, in comparison to standard model selection methods that simply select one model with the highest measure of goodness of fit.
C14|External imbalances and growth|The purpose of the paper is to investigate the role that unbalanced net foreign asset positions play in the growth path of the economies. In particular, the hypothesis to be tested is whether external imbalances may constrain growth in debtor countries. We analyze a large sample of countries using Lane and Milesi- Ferretti “External Wealth of Nations Dataset” and employing both parametric and nonparametric techniques. We find a preponderant positive relationship between the external position and growth, although the impact differs between countries and temporal periods.
C14|Linkages Between Oil Price Shocks and Stock Returns Revisited|In this paper, we revisit the debate on the relationship between oil price shocks and stock market returns by replicating the quantile-on-quantile (QQ) regression model for the US stock market in Sim and Zhou (2015, Journal of Banking and Finance), and extending it to 15 countries. The classification of these countries as oil importers or oil exporters depends on their net position in crude oil trade. Our results indicate that the finding by Sim and Zhou (2015) that large negative oil price shocks can bolster stock returns when markets are performing well is only partially supported by the three largest oil importers in our sample-China, Japan and India-during the period 1988:1-2007:12. However, when extending the study to more recent data (period 1988:1-2016:12), we find that China and India experience higher returns when markets perform well and there is a large positive oil price shock. Also, large positive oil price shocks often lead to higher stock market returns when markets perform well for both oil exporting countries-Canada, Russia, Norway-and moderately oil dependent countries-such as Malaysia, Philippines and Thailand. These findings highlight that the relationship between the distributions of oil price shocks and stock market returns is not stable over time in most countries studied. Furthermore, the asymmetric effect of oil price shocks observed in the US market by Sim and Zhou (2015) is less evident in most countries for both the baseline and extended periods.
C14|Volatility Estimation and Jump Detection for drift-diffusion Processes|Logarithms of prices of financial assets are conventionally assumed to follow drift-diffusion processes. While the drift term is typically ignored in the infill asymptotic theory and applications, the presence of nonzero drifts is an undeniable fact. The finite sample theory and extensive simulations provided in this paper reveal that the drift component has a nonnegligible impact on the estimation accuracy of volatility and leads to a dramatic power loss of a class of jump identification procedures. We propose an alternative construction of volatility estimators and jump tests and observe significant improvement of both in the presence of nonnegligible drift. As an illustration, we apply the new volatility estimators and jump tests, along with their original versions, to 21 years of 5-minute log-returns of the NASDAQ stock price index.
C14|“A new metric of consensus for Likert scales”|In this study we present a metric of consensus for Likert-type scales. The measure gives the level of agreement as the percentage of consensus among respondents. The proposed framework allows to design a positional indicator that gives the degree of agreement for each item and for any given number of reply options. In order to assess the performance of the proposed metric of consensus, in an iterated one-period ahead forecasting experiment we test whether the inclusion of the degree of agreement in consumers’ expectations regarding the evolution of unemployment improves out-of-sample forecast accuracy in eight European countries. We find evidence that the degree of agreement among consumers contains useful information to predict unemployment rates in most countries. The obtained results show the usefulness of consensus-based metrics to track the evolution of economic variables.
C14|Financial density forecasts: A comprehensive comparison of risk‐neutral and historical schemes|We investigate the forecasting ability of the most commonly used benchmarks in financial economics. We approach the usual caveats of probabilistic forecasts studies—small samples, limited models, and nonholistic validations—by performing a comprehensive comparison of 15 predictive schemes during a time period of over 21 years. All densities are evaluated in terms of their statistical consistency, local accuracy and forecasting errors. Using a new composite indicator, the integrated forecast score, we show that risk‐neutral densities outperform historical‐based predictions in terms of information content. We find that the variance gamma model generates the highest out‐of‐sample likelihood of observed prices and the lowest predictive errors, whereas the GARCH‐based GJR‐FHS delivers the most consistent forecasts across the entire density range. In contrast, lognormal densities, the Heston model, or the nonparametric Breeden–Litzenberger formula yield biased predictions and are rejected in statistical tests.
C14|Inference on a distribution from noisy draws| We consider a situation where a distribution is being estimated by the empirical distribution of noisy measurements. The measurements errors are allowed to be heteroskedastic and their variance may depend on the realization of the underlying random variable. We use an asymptotic embedding where the noise shrinks with the sample size to calculate the leading bias arising from the presence of noise. Conditions are obtained under which this bias is asymptotically non-negligible. Analytical and jackknife corrections for the empirical distribution are derived that recenter the limit distribution and yield con fidence intervals with correct coverage in large samples. Similar adjustments are presented for nonparametric estimators of the density and quantile function. Our approach can be connected to corrections for selection bias and shrinkage estimation. Simulation results confi rm the much improved sampling behavior of the corrected estimators. An empirical application to the estimation of a stochastic-frontier model is also provided.
C14|Difference-in-Differences with Multiple Time Periods and an Application on the Minimum Wage and Employment|"Difference-in-Differences (DID) is one of the most important and popular designs for evaluating causal effects of policy changes. In its standard format, there are two time periods and two groups: in the first period no one is treated, and in the second period a ""treatment group"" becomes treated, whereas a ""control group"" remains untreated. However, many em- pirical applications of the DID design have more than two periods and variation in treatment timing. In this article, we consider identification and estimation of treatment effect parameters using DID with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the ""parallel trends assumption"" holds potentially only after conditioning on observed covariates. We propose a simple two-step estimation strategy, establish the asymptotic prop- erties of the proposed estimators, and prove the validity of a computationally convenient bootstrap procedure. Furthermore we propose a semiparametric data-driven testing procedure to assess the credibility of the DID design in our context. Finally, we analyze the effect of the minimum wage on teen employment from 2001-2007. By using our proposed methods we confront the challenges related to variation in the timing of the state-level minimum wage policy changes. Open-source software is available for implementing the proposed methods."
C14|Time-dependent lead-lag relationship between the onshore and offshore Renminbi exchange rates|We employ the thermal optimal path method to explore both the long-term and short-term interaction patterns between the onshore CNY and offshore CNH exchange rates (2012–2015). For the daily data, the CNY and CNH exchange rates show a weak alternate lead-lag structure in most of the time periods. When CNY and CNH display a large disparity, the lead-lag relationship is uncertain and depends on the prevailing market factors. The minute-scale interaction pattern between the CNY and CNH exchange rates change over time according to different market situations. We find that US dollar appreciation is associated with a lead-lag relationship running from offshore to onshore, while a (contrarian) Renminbi appreciation is associated with a lead-lag relationship running from onshore to offshore. These results are robust with respect to different sub-sample analyses and variations of the key smoothing parameter of the TOP method.
C14|Panel Data Analysis with Heterogeneous Dynamics|This paper proposes the analysis of panel data whose dynamic structure is heterogeneous across individuals. Our aim is to estimate the cross-sectional distributions and/or some distributional features of the heterogeneous mean and autocovariances. We do not assume any specific model for the dynamics. Our proposed method is easy to implement. We first compute the sample mean and autocovariances for each individual and then estimate the parameter of interest based on the empirical distributions of the estimated mean and autocovariances. The asymptotic properties of the proposed estimators are investigated using double asymptotics under which both the cross-sectional sample size (N) and the length of the time series (T) tend to infinity. We prove the functional central limit theorem for the empirical process of the proposed distribution estimator. By using the functional delta method, we also derive the asymptotic distributions of the estimators for various parameters of interest. We show that the distribution estimator exhibits a bias whose order is proportional to 1/√T. Conversely, when the parameter of interest can be written as the expectation of a smooth function of the heterogeneous mean and/or autocovariances, the bias is of order 1/T and can be corrected by the jackknife method. The results of Monte Carlo simulations show that our asymptotic results are informative regarding the finitesample properties of the estimators. They also demonstrate that the proposed jackknife bias correction is successful.
C14|Varying Random Coefficient Models|This paper provides a new methodology to analyze unobserved heterogeneity when observed characteristics are modeled nonlinearly. The proposed model builds on varying random coefficients (VRC) that are determined by nonlinear functions of observed regressors and additively separable unobservables. This paper proposes a novel estimator of the VRC density based on weighted sieve minimum distance. The main example of sieve bases are Hermite functions which yield a numerically stable estimation procedure. This paper shows inference results that go beyond what has been shown in ordinary RC models. We provide in each case rates of convergence and also establish pointwise limit theory of linear functionals, where a prominent example is the density of potential outcomes. In addition, a multiplier bootstrap procedure is proposed to construct uniform confidence bands. A Monte Carlo study examines finite sample properties of the estimator and shows that it performs well even when the regressors associated to RC are far from being heavy tailed. Finally, the methodology is applied to analyze heterogeneity in income elasticity of demand for housing.
C14|Density Forecasts in Panel Data Models : A Semiparametric Bayesian Perspective|This paper constructs individual-specific density forecasts for a panel of firms or households using a dynamic linear model with common and heterogeneous coefficients and cross-sectional heteroskedasticity. The panel considered in this paper features a large cross-sectional dimension N but short time series T. Due to the short T, traditional methods have difficulty in disentangling the heterogeneous parameters from the shocks, which contaminates the estimates of the heterogeneous parameters. To tackle this problem, I assume that there is an underlying distribution of heterogeneous parameters, model this distribution nonparametrically allowing for correlation between heterogeneous parameters and initial conditions as well as individual-specific regressors, and then estimate this distribution by pooling the information from the whole cross-section together. Theoretically, I prove that both the estimated common parameters and the estimated distribution of the heterogeneous parameters achieve posterior consistency, and that the density forecasts asymptotically converge to the oracle forecast. Methodologically, I develop a simulation-based posterior sampling algorithm specifically addressing the nonparametric density estimation of unobserved heterogeneous parameters. Monte Carlo simulations and an application to young firm dynamics demonstrate improvements in density forecasts relative to alternative approaches.
C14|Day-ahead electricity price forecasting with high-dimensional structures: Univariate vs. multivariate modeling frameworks|We conduct an extensive empirical study on short-term electricity price forecasting (EPF) to address the long-standing question if the optimal model structure for EPF is univariate or multivariate. We provide evidence that despite a minor edge in predictive performance overall, the multivariate modeling framework does not uniformly outperform the univariate one across all 12 considered datasets, seasons of the year or hours of the day, and at times is outperformed by the latter. This is an indication that combining advanced structures or the corresponding forecasts from both modeling approaches can bring a further improvement in forecasting accuracy. We show that this indeed can be the case, even for a simple averaging scheme involving only two models. Finally, we also analyze variable selection for the best performing high-dimensional lasso-type models, thus provide guidelines to structuring better performing forecasting model designs.
C14|Asymptotic refinements of a misspecification-robust bootstrap for generalized method of moments estimators|I propose a nonparametric iid bootstrap that achieves asymptotic refinements for t tests and confidence intervals based on GMM estimators even when the model is misspecified. In addition, my bootstrap does not require recentering the moment function, which has been considered as critical for GMM. Regardless of model misspecification, the proposed bootstrap achieves the same sharp magnitude of refinements as the conventional bootstrap methods which establish asymptotic refinements by recentering in the absence of misspecification. The key idea is to link the misspecified bootstrap moment condition to the large sample theory of GMM under misspecification of Hall and Inoue (2003). Two examples are provided: combining data sets and invalid instrumental variables.
C14|Autoregressive Wild Bootstrap Inference for Nonparametric Trends|In this paper a modified wild bootstrap method is presented to construct pointwise confidence intervals around a nonparametric deterministic trend model. We derive the asymptotic distribution of a nonparametric kernel estimator of the trend function under general conditions, which allow for serial correlation and heteroskedasticity. Asymptotic validity of the bootstrap method is established and it is shown to work well in finite samples in an extensive simulation study. The bootstrap method has the potential of providing simultaneous confidence bands for the same models along the lines of Bühlmann (1998) and can be applied without further adjustments to missing data. We illustrate this by applying the proposed method to a time series of atmospheric ethane which can be used as an indicator of atmospheric pollution and transport.
C14|Characteristic-Sorted Portfolios: Estimation and Inference|Portfolio sorting is ubiquitous in the empirical finance literature, where it has been widely used to identify pricing anomalies. Despite its popularity, little attention has been paid to the statistical properties of the procedure. We develop a general framework for portfolio sorting by casting it as a nonparametric estimator. We present valid asymptotic inference methods, and a valid mean square error expansion of the estimator leading to an optimal choice for the number of portfolios. In practical settings, the optimal choice may be much larger than standard choices of five or ten. To illustrate the relevance of our results, we revisit the size and momentum anomalies.
C14|Control variables, discrete instruments, and identification of structural functions| Control variables provide an important means of controlling for endogeneity in econometric models with nonseparable and/or multidimensional heterogeneity. We allow for discrete instruments, giving identi cation results under a variety of restrictions on the way the endogenous variable and the control variables affect the outcome. We consider many structural objects of interest, such as average or quantile treatment effects. We illustrate our results with an empirical application to Engel curve estimation.
C14|Spanning Tests for Markowitz Stochastic Dominance|Using properties of the cdf of a random variable defined as a saddle-type point of a real valued continuous stochastic process, we derive first-order asymptotic properties of tests for stochastic spanning w.r.t. a stochastic dominance relation. First, we define the concept of Markowitz stochastic dominance spanning, and develop an analytical representation of the spanning property. Second, we construct a non-parametric test for spanning via the use of an empirical analogy. The method determines whether introducing new securities or relaxing investment constraints improves the investment opportunity set of investors driven by Markowitz stochastic dominance. In an application to standard data sets of historical stock market returns, we reject market portfolio Markowitz efficiency as well as two-fund separation. Hence there exists evidence that equity management through base assets can outperform the market, for investors with Markowitz type preferences.
C14|Complete Subset Averaging with Many Instruments|We propose a two-stage least squares (2SLS) estimator whose first stage is the equal-weight average over a complete subset with $k$ instruments among $K$ available, which we call the $\textit{complete subset averaging (CSA) 2SLS}$. The approximate mean squared error (MSE) is derived as a function of the subset size $k$ by the Nagar (1959) expansion. The CSA-2SLS estimator is obtained by choosing $k$ minimizing the sample counterpart of the approximate MSE. We show that this method achieves asymptotic optimality among the class of estimators with different subset sizes. A feature of equal-weight averaging is that all the instruments are used. To deal with averaging over irrelevant instruments, we generalize the approximate MSE under the presence of a possibly growing set of irrelevant instruments, which suggests to choose a smaller $k$ than otherwise. An extensive simulation experiment shows potentially huge improvement in the bias and the MSE by using the CSA-2SLS when instruments are correlated with each other and there exists large endogeneity. As an empirical illustration, we estimate the logistic demand function in Berry, Levinsohn, and Pakes (1995) and find the estimated coefficient value is better supported by economic theory than other IV estimators.
C14|On the Evolution of the United Kingdom Price Distributions|We propose a functional principal components method that accounts for stratified random sample weighting and time dependence in the observations to understand the evolution of distributions of monthly micro-level consumer prices for the United Kingdom (UK). We apply the method to publicly available monthly data on individual-good prices collected in retail stores by the UK Office for National Statistics for the construction of the UK Consumer Price Index from March 1996 to September 2015. In addition, we conduct Monte Carlo simulations to demonstrate the effectiveness of our methodology. Our method allows us to visualize the dynamics of the price distribution and uncovers interesting patterns during the sample period. Further, we demonstrate the efficacy of our methodology with an out-of-sample forecasting algorithm that exploits the time dependence of distributions. Our out-of-sample forecast compares favorably with the random walk forecast.
C14|A Look Inside the Box: Combining Aggregate and Marginal Distributions to Identify Joint Distributions|This paper proposes a method for estimating the joint distribution of two or more variables when only their marginal distributions and the distribution of their aggregates are observed. Nonparametric identification is achieved by modelling dependence using a latent common-factor structure. Multiple examples are given of data settings where multivariate samples from the joint distribution of interest are not readily available, but some aggregate measures are observed. In the application, intra-household distributions are recovered by combining individual-level and household-level survey data. I show that, for individuals living in couple relationships, personal cash-management practices are significantly influenced by the partner's use of cash and stored-value cards. This finding implies that, for some methods of payment at least, ignoring the partner's impact might lead to spurious regression results due to an omitted variable bias.
