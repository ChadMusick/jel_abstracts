C00|Mitigating misleading implications for policy: Treatment of outliers in a difference-indifferences framework|Applications of the difference-in-differences estimator in economics, banking and finance, and management commonly treat outliers using the winsorize method. However, failure to winsorize outliers in both the treatment and controls groups introduces volatility in estimated coefficients, significance levels, and standard errors. A faulty process can lead to an exogenous event realising a significant effect that proper process would fail to detect. In demonstration, we randomly generate placebo interventions in bank-level data and discuss how to detect and limit the problem.
C00|Energy Consumption in the GCC Countries: Evidence on Persistence|This paper examines the statistical properties of energy consumption in the GCC countries applying fractional integration methods to annual data from 1980 to 2014. The results indicate that both the raw and the logged series exhibit a (statistically significant) linear time trend in the case of Bahrain, Oman and Qatar, and the raw series only in the case of Saudi Arabia. Mean reversion (i.e., statistical evidence of d
C00|Managing, Inducing, and Preventing Regime Shifts: A Review of the Literature|How do economic agents manage expected shifts in regimes? How do they try to influence or prevent the arrival of such shifts? This paper provides a selective survey of the analysis of regime shifts from an economic view point, with particular emphasis on the use of the tech-niques of optimal control theory and differential games. The paper is organized as follows. Section 2 gives an overview of the concepts of regime shifts, thresholds, and tipping points. Section 3 shows how unknown tipping points affect the optimal current policy of decision makers, with or without ambiguity aversion. Section 4.s focus is on political regime shifts in a two-class economy: how the elite may try to prevent revolution by using policy instruments such as repression, redistribution, and gradual democratization. Section 5 reviews models of dynamic games in resource exploitation involving regime shifts and thresholds. Section 6 reviews some studies of regime shifts in industrial organization theory, with focus on R&D races, including efforts to sabotage rivals in order to prevent entry. Section 7 reviews games of regime shifts when players can manage a Big Push. Section 8 discusses some directions for future research.
C00|Social Confusion and Corruption: Investigating the Causes and Effects of a Breakdown of Ethics|While studies of transitions to market economies have long focused on the issue of corruption, the perspectives from which their analyses have been based have diverged. Accordingly, this paper employs a systematic review through testing 14 hypotheses from the perspectives of political and economic causes, as well as culture and values, based on 559 works from the literature on the subject. Its findings make it clear that the liberalization and privatization of ownership both expand and contract corruption; the effects of culture and values also should not be overlooked, while mostly rejecting the so-called “greasing-the-wheels” hypothesis.
C00|Social Confusion and Corruption: Investigating the Causes and Effects of a Breakdown of Ethics|"While studies of transitions to market economies have long focused on the issue of corruption, the perspectives from which their analyses have been based have diverged. Accordingly, this paper employs a systematic review through testing 14 hypotheses from the perspectives of political and economic causes, as well as culture and values, based on 558 works from the literature on the subject. Its findings make it clear that the liberalization and privatization of ownership both expand and contract corruption; the effects of culture and values also should not be overlooked, while mostly rejecting the so-called ""greasing-the-wheels"" hypothesis."
C00|An attitude of complexity: thirteen essays on the nature and construction of reality under the challenge of Zeno's Paradox|This book is about the construction of reality. The central aim of this study is to understand how gravity works and how it may be focused and manipulated. While I do not have an answer to this question, the discoveries along the way have been worth collecting into a single volume for future reference.
C00|Islanded Microgrid Operation Based on the Chaotic Crow Search Algorithm|This paper investigates the optimal operation of the islanded microgrid. In order to find the optimal solution and also provide a fast response, a new heuristic method, which is known as the chaotic crow search optimization algorithm is developed. To show the merit of the model, it is tested on the IEEE 30 bus test network.
C00|Savage's theorem with atoms|The famous theorem of Savage is based on the richness of the states space, by assuming a \textit{continuum} nature for this set. In order to fill the gap, this article considers Savage's theorem with discrete state space. The article points out the importance the existence of pair event in the existence of utility function and the subjective probability. Under the discrete states space, this can be ensured by the intuitive \textit{atom swarming} condition. Applications for the establishment of an inter-temporal evaluation \emph{\`a la } Koopman \cite{K60}, \cite{K72}, and for the configuration under \textit{unlikely atoms} of Mackenzie \cite{Mackenzie2018} are provided.
C00|New Essentials of Economic Theory|This paper develops economic theory tools and framework free from general equilibrium assumptions. We describe macroeconomics as system of economic agents under action risks. Economic and financial variables of agents, their expectations and transactions between agents define macroeconomic variables. Agents variables depend on transactions between agents and transactions are performed under agents expectations. Agents expectations are formed by economic variables, transactions, expectations of other agents, other factors that impact macroeconomic evolution. We use risk ratings of agents as their coordinates on economic space and approximate description of economic and financial variables, transactions and expectations of numerous separate agents by description of variables, transactions and expectations of aggregated agents as density functions on economic space. Motion of separate agents on economic space due to change of agents risk rating induce economic flows of variables, transactions and expectations and we describe their impact on economic evolution. We apply our model equations to description of business cycles, model wave propagation for disturbances of economic variables and transactions, model asset price fluctuations and argue hidden complexities of classical Black-Scholes-Merton option pricing.
C00|Human networks and toxic relationships|We devise a theoretical model to shed light on the dynamics leading to the so called toxic relationships. We want to investigate what policy interventions people could advocate to protect themselves and to reduce suffocant assuefaction so to escape to the trap of physical or psychological abuses either in family or at work. By assuming that the toxic partner's behavior is exogenous and that the main source of addiction is income or wealth, and solving a dynamical system of differential equations we find that an asympotically stable equilibrium with positive love is always possibile for enough high level of appealing unless subsides to reduces assuefaction are introduced. Also the existence of a third uncondicionally reciprocating part as a benckmark (which represents not only the real presence of another partner but also the support from family, friends and overall private organizations. These last may help victims of domestic abuses or private organizations by offering economic and psychological support as well as legal counseling to victims of bullying at workplace and placement offices which e ectively help to find soon another job)plays an important role in reducing the toxic partner's appealing. By solving our model we outline the condition for a best mixed policy where both monetary subsides and alternatives are at work
C00|Methods of Economic Theory: Variables, Transactions and Expectations as Functions of Risks|This paper develops methods and framework of economic theory free from general equilibrium tools and assumptions. We model macroeconomics as system of agents those perform transactions with other agents under action of numerous expectations. Agents expectations are formed by economic and financial variables, transactions, expectations of other agents, other factors that impact macro economy. We use risk ratings of agents as their coordinates on economic domain and approximate description of economic variables, transactions and expectations of numerous separate agents by density functions of variables, transactions and expectations of aggregated agents on economic domain. Motion of separate agents on economic domain due to change of agents risk rating produce economic flows of variables, transactions and expectations. These risk flows define dynamics of economic variables and disturb any supposed market equilibrium states all the time. Permanent evolution of market supply-demand states due to risk flows makes general equilibrium concept too doubtful. As example we apply our methods to model assets pricing and return fluctuations.
C00|Self- Supplied Microgrid Economic Scheduling Based on Modified Multiverse Evolutionary Algorithm|In this paper, a new evolutionary algorithm, known as the multiverse evolutionary algorithm (MEA) is developed for self-supplied microgrid operation. To show the effectiveness of the proposed method, it has been tested on the modified IEEE 33 bus test system. Results demonstrates the economic merit of the proposed technique.
C00|Savage's theorem with atoms|The famous theorem of Savage is based on the richness of the states space, by assuming a continuum nature for this set. In order to fill the gap, this article considers Savage's theorem with discrete state space. The article points out the importance the existence of pair event in the existence of utility function and the subjective probability. Under the discrete states space, this can be ensured by the intuitive atom swarming condition. Applications for the establishment of an inter-temporal evaluation a la Koopman, and for the configuration under unlikely atoms of Mackenzie Mackenzie2018 are provided.
C00|Arbitrage Trading Strategy in Gold Futures|"There appears to be an arbitrage trading strategy in the gold market where you are ""long"" gold overnight, between the London Fix each day. Holding gold price exposure in this way produced reliable profits between 2000 and 2010. In fact, these reliable profits resemble the returns seen with a theoretical example of an inefficient market where a Bollinger Band trading strategy extracts arbitrage profits from a price series with mean reversion."
C00|Costly Information Acquisition|We provide revealed preference characterizations for choices made under various forms of costly information acquisition. We examine nonseparable, multiplicative, and constrained costly information acquisition. In particular, this allows the possibility of unknown time delay for acquiring information. The techniques we use parallel the duality properties in the standard consumer problem.
C00|An Evaluation Of Agricultural Trade In India: A Special Study Of Selected Agricultural Commodities|India since ages has been known to be an agrarian country. Indian agriculture and allied activities consist of 54.6% of the population (census 2011) and contributes 17.4% to the country?s Gross Value Added for the year 2016-17 (Current prices). India?s agricultural export amounts to $33.87 billion as of 2017, and it is 10.5% of total exports of the country.The paper is classified in the various parts like introduction, objectives, review of literature, research methodology, growth rates in agricultural commodity trade, India?s agriculture trade, foreign trade policy by the government on agriculture, conclusion and suggestions.The growth in trade for agricultural commodity of India was analyzed by employing an exponential model of the form Yt = abteu. In the present research paper agricultural trade, the data has been collected from the secondary sources from the economic survey, annual reports from the agricultural ministry and so on at the same time has been analyzed and evaluated by using Carl Pearson?s co relation co efficient test. After the systematic analysis of the data there has been suggestions made by the researcher to improve agricultural trade to strengthen Indian economy
C00|Interdisciplinary Classification Studies|Classification of different kinds of objects is an important source of information in different areas of science. We show examples of classification diagrams of objects related to bioinformatics, medicine, and social science. In particular, we present a bioinformatics method designed by us and called 2D-Dynamic Representation of DNA/RNA Sequences [1,2]. In this non-standard approach the sequences are represented as point-masses in a 2D space. This mathematical method facilitates the creation of the classification diagrams in which different kinds of sequences are separated. Analogous diagrams we have obtained in social science - different groups of people are separated according to some, properly selected, criteria. In this case, a classifier is a kind of an answers to some questions. We have shown that this method can be a good tool for studies of the retirement threshold [3,4]. In the present work we also show some unpublished results related to the quality of life of the patients with voice disorders in inflammatory, neoplastic and neurological diseases of the larynx.[1] D. Panas, P. W??, D. Bieli?ska-W??, A. Nandy, S.C. Basak, 2D-Dynamic Representation of DNA/RNA Sequences as a Characterization Tool of the Zika Virus Genome, MATCH Commun. Math. Comput. Chem. 77 (2017) 321-332.[2] D. Panas, P. W??, D. Bieli?ska-W??, A. Nandy, S.C. Basak, An Application of the 2D-Dynamic Representation of DNA/RNA Sequences to the Prediction of Influenza A Virus Subtypes, MATCH Commun. Math. Comput. Chem. 80 (2018) 295-310.[3]. A. Bieli?ska, M. Majkowicz, D. Bieli?ska-W??, P. W??, Influence of the Education Level on Health of Elderly People, eTELEMED 2018, The Tenth International Conference on eHealth, Telemedicine, and Social Medicine, Rome, Italy, 2018, XPS IARIA Press, eTELEMED 2018 Proceedings, eds. Y. Murata et al., pp. 6-11.[4]. A. Bieli?ska, M. Majkowicz, D. Bieli?ska-W??, P. W??, Classification Studies in Various Areas of Science, in ?Numerical Methods and Applications?, eds. G. Nikolov et al., Lecture Notes in Computer Science vol. 11189, pp. 326?333, Springer, 2019.
C00|An Integrated Approach For Building A Sustainable City Development Assessment Model|In recent years, countries around the world have proposed the vision of future cities such as ?smart cities?, ?sustainable cities? and ?inclusive cities?. Cities have also proposed relevant policies and measures, and hope to create a livable place. However, the composition of the city is wide and complex, and the future city is not only able to consider only one aspect. The planning of the future city should incorporate the concept of smartness, suatainability and inclusiveness. With the application of novel technologies in the future, the effective use and management of urban resources can be achieved, and the social network of citizens tends to be fair and harmonious, which enables the city to move towards sustainable urban development. Based on the planning criteria of smart cities, sustainable cities, and inclusive cities, this study will construct an assessment framework that is consistent with future cities. Through the Fuzzy Delphi Method (FDM) and the Analytic Network Process (ANP), the future urban development evaluation model is constructed considering the priority of its evaluation criteria. Finally, an empirical analysis of the future urban assessment model for Taichung City, Taiwan will be conducted to verify the suitability of the model and propose further planning strategies.
C00|A quantum framework for economic science: New directions|The current paper explores the cutting-edge applications of quantum field theory and quantum information theory modelling in different areas of economic science, namely, in the behavioural modelling of agents under market uncertainty, and mathematical modelling of asset or option prices and firm theory. The paper then provides a brief discussion into a possible extension of the extant literature of quantum-like modelling based on scattering theory and statistical field theory. A statistical theory of firm based on Feynman path integral technique is also proposed very recently. The collage of new initiatives as described in the current paper will hopefully ignite still newer ideas.
C00|Spurious Seasonality Detection: A Non-Parametric Test Proposal|This paper offers a general and comprehensive definition of the day-of-the-week effect. Using symbolic dynamics, we develop a unique test based on ordinal patterns in order to detect it. This test uncovers the fact that the so-called “day-of-the-week” effect is partly an artifact of the hidden correlation structure of the data. We present simulations based on artificial time series as well. While time series generated with long memory are prone to exhibit daily seasonality, pure white noise signals exhibit no pattern preference. Since ours is a non-parametric test, it requires no assumptions about the distribution of returns, so that it could be a practical alternative to conventional econometric tests. We also made an exhaustive application of the here-proposed technique to 83 stock indexes around the world. Finally, the paper highlights the relevance of symbolic analysis in economic time series studies.
C00|Making No-Arbitrage Discounting-Invariant: A New FTAP Beyond NFLVR and NUPBR|In general multi-asset models of financial markets, the classic no-arbitrage concepts NFLVR and NUPBR have a serious shortcoming — they depend crucially on the way prices are discounted. To avoid this unnatural economic behaviour, we introduce a new idea for defining “absence of arbitrage”. It rests on the new notion of strongly index weight maximal strategies, which allows us to generalise both NFLVR (by dynamic index weight efficiency) and NUPBR (by dynamic index weight viability). These new no-arbitrage concepts do not change when we look at discounted or undiscounted prices, and they can be used in open-ended models under very weak assumptions on asset prices. We establish corresponding versions of the FTAP, i.e., dual characterisations of our concepts in terms of martingale properties. A key new feature is that as one expects, “properly anticipated prices fluctuate randomly”, but with an endogenous discounting process which is not a priori chosen exogenously. We also illustrate our results by a wide range of examples. In particular, we show that the classic Black–Scholes model on [0,1) is arbitrage-free in our sense if and only if its parameters satisfy m−r ε {0, σ²} or, equivalently, either bond-discounted or stock-discounted prices are martingales.
C00|Large Financial Markets, Discounting, and No Asymptotic Arbitrage|For a large financial market (which is a sequence of usual, “small” financial markets), we introduce and study a concept of no asymptotic arbitrage (of the first kind) which is invariant under discounting. We give two dual characterisations of this property in terms of (1) martingale-like properties for each small market plus (2) a contiguity property of suitably chosen “generalised martingale measures” along the sequence of small markets. Our results extend the work of Rokhlin and of Klein/Schachermayer and Kabanov/Kramkov to a discounting-invariant framework. We also show how a market on [0,∞) can be viewed as a large financial market and how no asymptotic arbitrage, both classic and in our new sense, then relates to no-arbitrage properties directly on [0,∞).
C00|Texas Service Sector Outlook Survey: Survey Methodology and Performance|The Texas Service Sector Outlook Survey (TSSOS) and Texas Retail Outlook Survey (TROS) are monthly surveys of service sector and retail firms in Texas conducted by the Federal Reserve Bank of Dallas. TSSOS and TROS track the Texas private services sector, including general service businesses, retailers and wholesalers. The surveys provide invaluable information on regional economic conditions—information that Dallas Fed economists and the Bank president use in the formulation of monetary policy. This paper describes the survey’s methodology and analyzes the explanatory and predictive power of TSSOS and TROS indexes with regard to Texas employment growth. Regression analysis shows that several TSSOS and TROS indexes help explain monthly variation in Texas employment. In addition, most TSSOS and TROS indexes are also useful in forecasting Texas employment growth.
C00|Statistical Inference on the Canadian Middle Class|Conventional wisdom says that the middle classes in many developed countries have recently suffered losses, in terms of both the share of the total population belonging to the middle class, and also their share in total income. Here, distribution-free methods are developed for inference on these shares, by means of deriving expressions for their asymptotic variances of sample estimates, and the covariance of the estimates. Asymptotic inference can be undertaken based on asymptotic normality. Bootstrap inference can be expected to be more reliable, and appropriate bootstrap procedures are proposed. As an illustration, samples of individual earnings drawn from Canadian census data are used to test various hypotheses about the middle-class shares, and confidence intervals for them are computed. It is found that, for the earlier censuses, sample sizes are large enough for asymptotic and bootstrap inference to be almost identical, but that, in the twenty-first century, the bootstrap fails on account of a strange phenomenon whereby many presumably different incomes in the data are rounded to one and the same value. Another difference between the centuries is the appearance of heavy right-hand tails in the income distributions of both men and women.
C00|The Wall’s Impact in the Occupied West Bank: A Bayesian Approach to Poverty Dynamics Using Repeated Cross-Sections|In 2002, the Israeli government decided to build a wall inside the occupied West Bank. The wall had a marked effect on the access to land and water resources as well as to the Israeli labour market. It is difficult to include the effect of the wall in an econometric model explaining poverty dynamics as the wall was built in the richer region of the West Bank. So a diff-in-diff strategy is needed. Using a Bayesian approach, we treat our two-period repeated cross-section data set as an incomplete data problem, explaining the income-to-needs ratio as a function of time invariant exogenous variables. This allows us to provide inference results on poverty dynamics. We then build a conditional regression model including a wall variable and state dependence to see how the wall modified the initial results on poverty dynamics. We find that the wall has increased the probability of poverty persistence by 58 percentage points and the probability of poverty entry by 18 percentage points.
C00|Recent Developments in Macro-Econometric Modeling: Theory and Applications|Developments in macro-econometrics have been evolving since the aftermath of the Second World War.[...]
C00|Econometrics and Income Inequality|It is well-known that, after decades of non-interest in the theme, economics has experienced a proper surge in inequality research in recent years. [...]
C00|Cooperative game-theoretic features of cost sharing in location-routing|This article studies several variants of the location-routing problem using a cooperative game-theoretic framework. The authors derive characteristics in terms of subadditivity, convexity, and non-emptiness of the core. Moreover, for some of the game variants, it is shown that for facility opening costs substantially larger than the costs associated with routing, the core is always non-empty. The theoretical results are supported by numerical experiments aimed at illustrating the properties and deriving insights. Among others, it is observed that, while in general it is not possible to guarantee core allocations, in a huge majority of cases the core is non-empty.
C00|Sample statistics as convincing evidence: A tax fraud case|This report deals with the analysis of data used by tax officers to support their claim of tax fraud at a pizzeria. The possibilities of embezzlement under study are overreporting of take-away sales and underreporting of cash payments. Several modelling approaches are explored, ranging from simple well-known methods to presumably more precise tools. More specifically, we contrast common methods based on normal assumptions and models based on Gamma-assumptions. For the latter, both maximum likelihood and Bayesian approaches are covered. Several criteria for the choice of method in practice are discussed, among them, how easy the method is to understand, justify and communicate to the parties. Some dilemmas present itself: the choice of statistical method, its role in building the evidence, the choice of risk factor, the application of legal principles like “clear and convincing evidence” and “beyond reasonable doubt”. The insights gained may be useful for both tax officers and defenders of the taxpayer, as well as for expert witnesses.
C00|What is wrong with IRV?|Struggles over the single-seat preferential election method IRV, Instant Runoff Voting, (a.k.a. AV, Alternative Vote or RCV, Ranked-Choice Voting) go on in many arenas: legislatures, courts, websites, and scholarly journals. Monotonicity failures, i.e. elections (preference distributions) that may allow the startling tactical voting of Pushover or its reverse, has come to the forefront. An analysis of 3-candidate elections concludes that monotonicity failures, while not rare, are hard to predict and risky to exploit; it also explains the scarcity of evidence for effects on election results. A more unfortunate possibility is the No-Show accident; the number of ballots with preference order XYZ grows beyond a critical size and cause Z to win instead of Y. An analysis concludes that this must happen often enough to justify a modification of the rules. Pictograms and constellation diagrams are visualization tools that organize the set of possible elections efficiently for the analysis, which obtains explicit classification of elections where Pushover or a No-Show accident may occur or may already have occurred, and of bounds for the number of voters that must be involved. The analysis takes place in close contact with two frameworks for preferential election methods, one mathematical and one legal/political; these frameworks are themes for two survey sections.
C00|Estimation of the linear fractional stable motion|In this paper we investigate the parametric inference for the linear fractional stable motion in high and low frequency setting. The symmetric linear fractional stable motion is a three-parameter family, which constitutes a natural non-Gaussian analogue of the scaled fractional Brownian motion. It is fully characterised by the scaling parameter $\sigma>0$, the self-similarity parameter $H \in (0,1)$ and the stability index $\alpha \in (0,2)$ of the driving stable motion. The parametric estimation of the model is inspired by the limit theory for stationary increments L\'evy moving average processes that has been recently studied in \cite{BLP}. More specifically, we combine (negative) power variation statistics and empirical characteristic functions to obtain consistent estimates of $(\sigma, \alpha, H)$. We present the law of large numbers and some fully feasible weak limit theorems.
C00|How to measure the performance of a Collaborative Research Center|Abstract New Public Management helps universities and research institutions to perform in a highly competitive research environment. Evaluating publicly financed research improves transparency, helps in reflection and self-assessment, and provides information for strategic decision making. In this paper we provide empirical evidence using data from a Collaborative Research Center (CRC) on financial inputs and research output from 2005 to 2016. After selecting performance indicators suitable for a CRC, we describe main properties of the data using visualization techniques. To study the relationship between the dimensions of research performance, we use a time fixed effects panel data model and fixed effects Poisson model. With the help of year dummy variables, we show how the pattern of research productivity changes over time after controlling for staff and travel costs. The joint depiction of the time fixed effects and the research project’s life cycle allows a better understanding of the development of the number of discussion papers over time.
C00|Analysis of Publications of FEP and their Affinities|In this paper we analyse the scientific production of the researchers of the School of Economics and Management of the University of Porto (FEP). The titles of articles appearing in the bibliographic database Authenticus were used as the basis of this study. We have explored the representation in the form of a network visualised as graph, where each researcher is represented by a node. The nodes are connected by the level of similarity of the publications’ titles. To obtain this network we have used the software Affinity Miner. This software can also identify affinity groups that join researchers with similar publications. We have explored two methods for this purpose and determined that Louvain’s method produces the best results. As the original network has a rather high density of links, the visual analysis would be rather difficult if the network were not simplified. Simplification is done by removing the weakest links. Using a measure called modularity with component penalty, we determine an adequate value for the minimum weight of a link. The results obtained with this methodology are interesting. First, it can identify both central researchers of FEP and central researchers in each scientific group (e.g. in Economics, Management, etc.). The 9 affinity groups that were generated are also of interest. We note that the scientific group of Economics is divided into 3 rather cohesive subgroups. Two of those include mainly researchers from this scientific group, while the third one is a mix of researchers mainly from Economics and Management. All affinity groups are characterised by automatically generated keywords. Similar analysis can be done for all other scientific subgroups. In our opinion, the structures discovered are pertinent and provide interesting insights into how FEP is in fact organised.
C00|The reconstruction of capital theory: the true meaning of capital in a production function|The purpose of the present article is to explore the possibilities of a reconstruction of a Theory of Capital capable of taking into account the Reswitching phenomenon. In Section 1 a new measure of capital-time, for neoaustrian processes of production, is introduced. The main outcome of the use of this proposed new measure of capital is this: it can be shown that, even when Reswitching occurs, there is still always an inverse relationship between the rate of interest or profit and the quantity of capital-time. In Section 2 the results of Section 1 are extended for the case of two good technologies examples. In Section 3 a surrogate production process is introduced. By developing this surrogate production process it can be shown that in general there is an inverse relationship between the interest rate and the quantity of surrogate capital per man, the surrogate capital/output ratio, and between the interest rate and the newly defined steady-state consumption per capita. Section 4 presents further comments on the results of the previous sections. Section 5 introduces numerical examples.
C00|Updating Probabilities for a Mineral Exploration Project|This paper describes a mineral exploration project conducted by a junior mining company within a probabilistic framework. In particular, it shows how beliefs about a project can be updated in response to a series of new pieces of information about the project associated with exploration activities using subjective probabilities. Several alternative frameworks are briefly mentioned as ways to improve on this simplified model.
C00|Simulation Framework for Economic Modeling of Mineral Resources|This paper describes an approach to include uncertainty over the commodity price when modelling the economic attributes of a mine plan for a mineral resource. The approach starts with a method to generate price paths from a broad historical set to establish a set of price paths, where the NPV is calculated for each path to generate a distribution for the NPV. It goes on to describes how to use this distribution to compare different mine plans in a manner that is similar to stress testing.
C00|Dynamic Beta|The phrase “Dynamic Beta” is broad and this paper describes statistical procedure for estimating regression coefficients in a way that allows for variation across relevant subsets of the data. For example, the time axis. I describe an algorithm to structure the search for variation in sets of coefficient estimates and discuss the example of a single stock versus a stock index. In the end, I suggest that a human analyst has an important role for someone who has relevant skill in pattern recognition and subject area expertise.
C00|Evaluating the Kemess Stream sold by Centerra Gold in 2018|In a streaming contract, a mining company sells future revenues derived from a mine. Typically, the mining company sells the revenue for a secondary metal rather than the primary economic driver of the project. This paper considers a particular stream known as the “Kemess Stream”, where a public company called Centerra Gold Inc. sold future silver production from the Kemess Project to a private financial company. The Kemess Stream represents a typical streaming contract and provides an instructive example as a modelling exercise. This paper shows how to calculate the economics of the stream in detail based on economic reports published by Centerra Gold and discusses general principles reflected in the deal terms.
C00|Mining Pipe-Shaped Ore Deposits|This paper provides a rough comparison of two mine plans for a hypothetical, pipe-shaped ore body. The geometry for the ore body is based on stylized example of an auriferous tourmaline breccia pipes associated with porphyry deposits, which can have great vertical extent and relatively small surficial expression. I suppose the pipe outcrops on a hillside and can be accessed from the base of the hill, allowing the miner to enter the pipe at the midpoint of the vertical extent. Accessing the pipe in the middle allows the miner to either go up or downwards and this paper explores one mining method for each case. I calculate basic statistics associated with each method and compare the two models for mining method.
C00|The significance of faith proven by decision theory – Pascal's wager game is correct and refutes atheism completely|"Pascal's wager game showed that the atheistic view is always inferior compared to the theistic. The reason is that an infinitely high reward for the theist is always opposed to a finite payoff for the atheist, given that the existence of God cannot be excluded. However, an atheist can also obtain the infinite reward just by choosing strategy ""believe"", i.e. by pretending faith, but without really believing. As God may be demanding and grants the reward only those in firm faith, the inspection of faith by God is needed, which is included in the present analysis, i.e. God tests the faith. In addition, the infinitely high reward is removed from the payoffs, i.e. the believer goes out empty, and is even charged cost of faith. It shows that believing in God and worshipping is still the best option regardless, even if we believe that praying is in vain and has absolutely no value. The analysis also shows that the faith should even be so strong that we give our entire life to God and retain nothing for us since this is not only the safe rescue, but the rationally optimal choice of the extent of worship, and thus the unique salvation. Finally, all common criticisms to the wager game have been reinvestigated in detail; logic and mathematical analysis showed that they fail against the consideration of eternal punishment."
C00|Valuation Simulation for a Net Smelter Return Royalty on a Mining Project|This paper provides a rough calculation of the economic value of a hypothetical mining operation for a profitable of mineral deposit. I present simplifying assumptions for the deposit geometry and mine production constraints to calculate the value of a net smelter recovery royalty on the mine, which is due a small fraction of gross income from the mine. I calculate the value of the royalty with constant prices and conduct a simulation exercise where the metal price changes quarterly based on historical data for gold futures contracts.
C00|Рождение Макроэкономического Порядка Из Микроэкономического Хаоса<BR>[The birth of a macroeconomic order from microeconomic chaos]|В данной статье рассмотрена проблема самоорганизации экономических процессов в условиях совершенной конкуренции, основанной на взаимодействии индивидуальных и общественных экономических ценностей и рыночных цен. На основе диалектического анализа показано глубокое внутреннее единство проиозводства и потребления, спроса и предложения, полезности и затрат и др. категорий, которые обуславливают функциональную замкнутость и целостность экономической системы, как необходимое условие для понимания процесса формирования макроэкономического порядка из микроэкономического хаоса. На методологической основе диалектики дается новое понимание механизма саморегулирования рыночных процессов и экономической оптимизации. Предлагаемое теоретическое объяснение экономических процессов позволит создать более адекватные прикладные экономические модели и выработать эффективную экономическую политику. In this article, the problem of self-organization of economic processes in conditions of perfect competition, based on the interaction of individual and public economic values and market prices, is considered. On the basis of dialectical analysis, the deep internal unity of production and consumption, supply and demand, utility and costs, and other categories that determine the functional closeness and integrity of economic system as a necessary condition for to understand the formation process of a macroeconomic order from microeconomic chaos is shown. A new understanding of market processes' self-regulation mechanism and economic optimization on the methodological basis of dialectics is given. The proposed theoretical explanation of economic processes makes it possible to create more adequate applied economic models and develop an effective economic policy.
C00|Funding Options from the Market|Investors face many different versions of The Portfolio Problem. Consider, for example, holding shares and call options on a publicly-traded equity. The options are in-the-money and live. How best should the investor go about exercising those options? They could fund from capital or use the secondary market to fund the options, as follows. When market price is above strike price, it may be possible to sell shares into market in advance of exercising the call options. This operation can yield residual cash or shares. How much should an investor do this and when? This paper presents a specific numerical example where we trade out of options when the market price breaches a 2:1 ratio to strike price and provides descriptive statistics for investors’ wealth in simulation with standard Gaussian motion for share price and specific trading rule.
C00|A latent class analysis towards stability and changes in breadwinning patterns among coupled households|A latent class model is proposed to examine couples’ breadwinning typologies and explain the wage differentials according to the socio-demographic characteristics of the society with data collected through surveys. We derive an ordinal variable indicating the couple’s income provision-role type and suppose the existence of an underlying discrete latent variable to model the effect of covariates. We use a two-step maximum likelihood inference conducted to account for concomitant variables, informative sampling scheme and missing responses. The weighted log-likelihood is maximised through the Expectation-Maximization algorithm and information criteria are used to develop the model selection. Predictions are made on the basis of the maximum posterior probabilities. Disposing of data collected in Japan over thirty years we compare couples’ breadwinning patterns across time. We provide some evidence of the gender wage-gap and we show that it can be attributed to the fact that, especially in Japan, duties and responsibilities for the child care are supported exclusively by women.
C00|Can we beat the Random Walk? The case of survey-based exchange rate forecasts in Chile|We examine the accuracy of survey-based expectations of the Chilean exchange rate relative to the US dollar. Our out-of-sample analysis reveals that survey-based forecasts outperform the Driftless Random Walk (DRW) in terms of Mean Squared Prediction Error at several forecasting horizons. This result holds true even when comparing the survey to a more competitive benchmark based on a refined information set. A similar result is found when precision is measured in terms of Directional Accuracy: survey-based forecasts outperform a “pure luck” benchmark at several forecasting horizons. Differing from the traditional “no predictability” result reported in the literature for many exchange rates, our findings suggest that the Chilean peso is indeed predictable.
C00|A characterization of lexicographic preferences|This paper characterizes lexicographic preferences over alternatives that are identified by a finite number of attributes. We say two alternatives are 'totally different' if they are different with respect to every attribute. Our characterization is based on two key concepts: a weaker notion of continuity called 'mild continuity' (strict preference order between any two totally different alternatives is preserved around their small neighborhoods) and an 'unhappy set' (any alternative outside the set is preferred to all alternatives inside).
C00|Tres teorías demográficas, las evidencias disponibles y el paso de la descripción del cómo al entendimiento del porqué: Una aplicación y una crítica de tres hipótesis poblacionales en base a los casos de España y de la India (1950-2020)<BR>[Three demographic theories, the disposable evidence and the step between description and comprehension: An application and a critique of three population hypotheses based on the cases of Spain and India (1950-2020)]|This work aims to expose three of the main demographic theories, Demographic, Epidemiologic and Sanitary Transition theories, and explore its explanatory capacity. The content is structured in four sections. In the first we briefly review these theories. Secondly, we elaborate a diagnosis of the demographic structure of India and Spain, through its contextualization in the Asian and European context, respectively, and the use of several indicators. Next, we value the relationship between the evidence on these case studies and the hypotheses exposed. Finally, we outline possible theoretical critics based on these two cases
C00|Green Concept Evaluation through Fuzzy AHP-PROMETHEE II|The demand for green products have dramatically increased because the importance and public awareness of the preservation of natural environment was taken into consideration much more last two decades. As a result of this, especially manufacturing companies have been forced to design more green products, resulting in a problem of how they incorporate environmental issues into their design and evaluate concept options. The need for the practical decision making tools to address this problem is rapidly evolving due to the fact that the problem turns into a multiple-criteria decision making (MCDM) problem in the presence of a set of green concept alternatives and criteria. Therefore; in this paper, the four popular MCDM methods in fuzzy environment are utilized to reflect the vagueness and uncertainty on the judgments of DMs, because the crisp pairwise comparison in these conventional MCDM methods seems to be insufficient and imprecise to capture the right judgments of DMs. Of these methods; as Fuzzy AHP is used to calculate criteria weights, the other method; Fuzzy PROMETHEE II is used to rank alternatives. Furthermore, the incorporation of fuzzy set theory into these methods is discussed on a real-life case study.
C00|Crossover Designs for drug development|In clinical trials, an issue of paramount importance is that of determining the best treatment for an ailment, from among a class of competing treatments. Crossover designs have been widely used in clinical trials for drug development and recent years have seen a surge in research on these designs. In crossover trials, different drugs are applied to each patient over a sequence of time periods, observations being taken at each period. However, since the same patient is exposed to a sequence of drugs over time, the observation taken at any particular time period is influenced by the effect of the drug applied at that period, called the direct effect of a drug, together with an effect of the drug applied in the immediately preceding period, called the carryover effect of the drug. The presence of these two types of drug effects makes the design and analysis of these experiments difficult. Moreover, an observation is also influenced by an effect of the time period and effect of the patient. So, the key issues here include (a) adequate modelling of the observations, (b) estimation of direct and carryover effects, (b) derivation of efficient or optimal design for inference and (c) construction of this efficient design for experimental use.There are results available for efficient estimation for direct and carryover effects separately. However, a designed experiment finally recommends a single treatment for use over longer time periods, and when this treatment is used, an effect of utmost importance is the total of the direct effect and carryover effect of the same treatment, or the total drug effect. However, no results are available in the literature for this total effect. In this paper we focus on this issue and we develop a rigorous framework for studying the total effects under a non-circular model. Next, we derive the best design for use in this context. Some numerical results are also presented.
C00|Comparative Analysis on effectiveness of classroom teaching and web-based teaching: Teaching Quantitative methods & Techniques|As Accounting, Business and Management programs in various B-schools extend their online education offerings to reach more time- and place-bound students, and as accredited institutions become interested in documenting teaching and learning effectiveness, the degree to which online students are successful as compared to their classroom counterparts is of interest to teaching faculty and others charged with assessment. By comparing student performance measures and assessments of learning experience from both online and traditional sections of a required Quantitative methods & techniques course taught by the same instructor, this paper provides evidence that student performance as measured by grade is independent of the mode of instruction. Persistence in an online environment may be more challenging in Quantitative methods classes than in other subject classes. Furthermore, participation may be less aggressive, and the quality and quantity of interaction may be affected in online classes.
C00|Old McDonald's had a brand?: How traditional brands lose their breath|Traditional brands have to review implementation of previously used concepts and techniques of brand value building and management. The reason of such a managerial decision consists in the evolution of the market and buying decision stereotypes. So far, branding theory do not know how to explain that traditionally valuable brands lose their value sharply despite their position in global brand value rankings have not indicated it. The danger of this situation is really high. So, the aim of this paper is to verify through the cluster analysis based on secondary data from Interbrand ranking our hypothesis that the scissors between actual brand value of traditional and modern brands are opening more and more. And at the same time to predict which brands will have to face the threat of brand value decrease.
C00|Quantitative methods in the triangulation process|The following article presents an analysis of selected quantitative methods in the research process. The author highlighted the importance of quantitative analysis in the research process, which requires to collect a great number of data, as well as to analyse and interpret them. Selected research methods were stipulated: SPACE (Strategic Position and Action Evaluation), network methods and the Delphi method. An analysis of the use of triangulation method in order to enhance the reliability of research results was carried out.The Author attempted to answer the following research questions: What research methods should be employed in the research processes within the area of economics? Is the methodological triangulation a prerequisite in research processes? Does the triangulation enhance the reliability of employed quantitative methods in research processes?
C00|On sampling from a rectangular grid|The problem of estimating the population total of a variable of interest by sampling from a finite population with units arranged in a rectangular array is considered. As an alternative to simple random sampling, a two-step sampling procedure is proposed. The procedure first chooses some rows (columns) by simple random sampling without replacement, then some columns (rows) by simple random sampling without replacement and uses the sample consisting of the units corresponding to the intersection of rows and columns selected. An unbiased estimator of the population total, using this step-wise sampling procedure, is proposed. The variance of the estimator is derived and further, an unbiased estimator for that variance is obtained.
C00|Inference under a new exponential-exponential loss capturing specified penalties for over- and under-estimation|Asymmetric loss functions have gained enormous importance over the years, with particular relevance to situations where over- and under-estimation of the parameter of interest are considered not of equal consequence. In particular, the linear-exponential (LINEX) loss has been studied and used quite extensively in classical and Bayesian inference. While LINEX loss nicely captures whether over- or under-estimation has a more serious impact, it falls short of incorporating any prior knowledge about the relative penalty for over- vis-à-vis that for under-estimation. Thus, if such prior knowledge is available as happens in many practical situations, notably in finance, medicine and reliability theory, among others, then there is a pressing need for devising a loss function that accounts for this information and hence is more realistic than the LINEX loss. More specifically, suppose the ground realities in a given situation demand that over-estimation needs to be penalized k times the penalty of under-estimation, where k is known. Clearly, over-estimation gets more penalized than under-estimation if k > 1 and it is the other way round if k
C00|Adaptive estimation using records data under asymmetric loss, with applications|We consider a scenario where data are accessible in terms of record values, as can happen in a wide range of practical situations. Examples include the hottest day ever, the lowest stock market figure, auction prices of an item in bidding, etc. Such data can be analyzed as record values from a sequence of observations, an upper or lower record value being one that is larger or smaller, respectively, than all previous observations. The literature on classical theory of records and its several variants is quite rich. A significant literature also exists in reliability theory and associated areas. Not much work has, however, been done so far using records data when over and under estimation of the parameter of interest attract unequal penalties, even though there is a compelling need for considering such an asymmetric loss function whenever the consequences of over and under estimation are not identical. This can happen in such diverse fields of application as real estate management, accounting, reliability analysis, and so on.From the above perspective, we consider the estimation problem based on records data for the scale parameter of an exponential family of distributions under an asymmetric linear-exponential loss function. With a view to controlling the associated risk, we also aim at ensuring a pre-assigned upper bound on it. In the absence of a known and fixed sample size solution to this problem, we consider an adaptive sampling methodology ? for example, a one at a time purely sequential sampling rule. We suggest various estimators of the scale parameter and compare their performances to address the admissibility and other related issues. Monte-Carlo simulations lend strong support to our theory and methodology.
C00|Analyse scientométrique de la crise économique|En s’appuyant sur les techniques de cartographie, en se basant sur l’analyse textuelle et l’analyse des réseaux de citations nous avons analysé le développement de la thématique de la crise économique. L’objectif est de montrer dans un premier temps les courants de pensée et les auteurs influents. Dans un second temps comprendre son évolution à travers le temps. Pour ce faire, nous avons extrait de l’interface du WOS en ligne l’ensemble des publications contenant le mot « crisis » dans les catégories disciplinaires « economics » et « business & finance ». Notre requête renvoie plus de 24000 publications. Nos résultats nous ont permis de montrer les différents courants de pensée dominant l’analyse de la crise économique ainsi que les auteurs les plus influents. L’analyse textuelle des termes présents dans les titres, résumé et mots-clés montre des changements majeurs dans la façon dont les économistes traitent le sujet. Désormais, une bonne partie des publications traitant la thématique de la crise économique cherche non pas à traiter les conséquences ou à proposer des solutions, mais plutôt prévoir l’avènement des crises à travers l’analyse des différents risques qui conduiraient à une crise. Nous avons montré également que cette thématique est très fortement dominée par la finance tant au niveau microéconomique que macroéconomique.
C00|Tre grupper skatteytere i søkelyset: Har de ulike kjennetegn?|This report analyzes data for three groups of taxpayers in scrutiny of the tax authorities: Taxpayers who, after a tax amnesty, have voluntary come forward with previous unreported taxable income or wealth abroad, taxpayers where there exist automatic abroad control scheme and taxpayers who have been assigned penalty tax. A number of potential key characteristics are selected, and the groups are compared against one another and with a control group of ordinary taxpayers, with the objective to uncover differences between the groups. Three different methods of analysis are used: Simple categorization, correspondence analysis and classification trees. They are of explorative nature and suitable for graphical presentation of results. A comparison of results, and some advantages and disadvantages of the three methods are discussed, in relation to the ambition level: Find the characteristics that separates the groups, or more, establish rules for classification of individuals with unknown membership.
C00|Creaming - and the depletion of resources: A Bayesian data analysis|This paper considers sampling in proportion to size from a partly unknown distribution. The applied context is the exploration for undiscovered resources, like oil accumulations in different deposits, where the most promising deposits are likely to be drilled first, based on some geologic size indicators (“creaming”). A Log-normal size model with exponentially decaying creaming factor turns out to have nice analytical features in this context, and fits well available data, as demonstrated in Lillestøl and Sinding-Larsen (2017). This paper is a Bayesian follow-up, which provides posterior parameter densities and predictive densities of future discoveries, in the case of uninformative prior distributions. The theory is applied to the prediction of remaining petroleum accumulations to be found on the mature part of the Norwegian Continental Shelf.
C00|Central limit theorems for functionals of large sample covariance matrix and mean vector in matrix‐variate location mixture of normal distributions|In this paper, we consider the asymptotic distributions of functionals of the sample covariance matrix and the sample mean vector obtained under the assumption that the matrix of observations has a matrix‐variate location mixture of normal distributions. The central limit theorem is derived for the product of the sample covariance matrix and the sample mean vector. Moreover, we consider the product of the inverse sample covariance matrix and the mean vector for which the central limit theorem is established as well. All results are obtained under the large‐dimensional asymptotic regime, where the dimension p and the sample size n approach infinity such that p/n→c ∈ [0, + ∞) when the sample covariance matrix does not need to be invertible and p/n→c ∈ [0,1) otherwise.
C00|On the product of a singular Wishart matrix and a singular Gaussian vector in high dimensions|In this paper we consider the product of a singular Wishart random matrix and a singular normal random vector. A very useful stochastic representation is derived for this product, in using which the characteristic function of the product and its asymptotic distribution under the double asymptotic regime are established. The application of obtained stochastic representation speeds up the simulation studies where the product of a singular Wishart random matrix and a singular normal random vector is present. We further document a good performance of the derived asymptotic distribution within a numerical illustration. Finally, several important properties of the singular Wishart distribution are provided.
C00|Testing Missing at Random using Instrumental Variables|This paper proposes a test for missing at random (MAR). The MAR assumption is shown to be testable given instrumental variables which are independent of response given potential outcomes. A nonparametric testing procedure based on integrated squared distance is proposed. The statistic’s asymptotic distribution under the MAR hypothesis is derived. In particular, our results can be applied to testing missing completely at random (MCAR). A Monte Carlo study examines finite sample performance of our test statistic. An empirical illustration analyzes the nonresponse mechanism in labor income questions.
C00|Adaptive weights clustering of research papers|The JEL classification system is a standard way of assigning key topics to economic articles in order to make them more easily retrievable in the bulk of nowadays massive literature. Usually the JEL (Journal of Economic Literature) is picked by the author(s) bearing the risk of suboptimal assignment. Using the database of a Collaborative Research Center from Humboldt-Universit¨at zu Berlin and Xiamen University, China we employ a new adaptive clustering technique to identify interpretable JEL (sub)clusters. The proposed Adaptive Weights Clustering (AWC) is available on www.quantlet.de and is based on the idea of locally weighting each point (document, abstract) in terms of cluster membership. Comparison with k-means or CLUTO reveals excellent performance of AWC.
C00|Pricing Green Financial Products|With increasing wind power penetration more and more volatile and weather dependent energy is fed into the German electricity system. To manage the risk of windless days and transfer revenue risk from wind turbine owners to investors wind power derivatives were introduced. These insurance-like securities (ILS) allow to hedge the risk of unstable wind power production on exchanges like Nasdaq and European Energy Exchange. These products have been priced before using risk neutral pricing techniques. We present a modern and powerful methodology to model weather derivatives with very skewed underlyings incorporating techniques from extreme event modelling to tune seasonal volatility and compare transformed Gaussian and non-Gaussian CARMA(p; q) models. Our results indicate that the transformed Gaussian CARMA(p; q) model is preferred over the non-Gaussian alternative with Lévy increments. Out-of-sample backtesting results show good performance wrt burn analysis employing smooth Market Price of Risk (MPR) estimates based on NASDAQ weekly and monthly German wind power futures prices and German wind power utilisation as underlying. A seasonal MPR of a smile-shape is observed, with positive values in times of high volatility, e.g. winter months, and negative values, in times of low volatility and production, e.g. in summer months. We conclude that producers pay premiums to insure stable revenue steams, while investors pay premiums when weather risk is high.
C00|The systemic risk of central SIFIs|Systemic risk quantification in the current literature is concentrated on market-based methods such as CoVaR(Adrian and Brunnermeier (2016)). Although it is easily implemented, the interactions among the variables of interest and their joint distribution are less addressed. To quantify systemic risk in a system-wide perspective, we propose a network-based factor copula approach to study systemic risk in a network of systemically important financial institutions (SIFIs). The factor copula model offers a variety of dependencies/tail dependencies conditional on the chosen factor; thus constructing conditional network. Given the network, we identify the most “connected” SIFI as the central SIFI, and demonstrate that its systemic risk exceeds that of non-central SIFIs. Our identification of central SIFIs shows a coincidence with the bucket approach proposed by the Basel Committee on Banking Supervision, but places more emphasis on modeling the interplay among SIFIs in order to generate systemwide quantifications. The network defined by the tail dependence matrix is preferable to that defined by the Pearson correlation matrix since it confirms that the identified central SIFI through it severely impacts the system. This study contributes to quantifying and ranking the systemic importance of SIFIs.
C00|Das deutsche Arbeitsmarktwunder: Eine Bilanz|Dem deutschen Arbeitsmarkt ging es noch nie seit der Wiedervereinigung so gut wie heute. Die nachhaltige Entwicklung seit 2005 ist auf einen entscheidenden Treiber zurückzuführen: die Umverteilung eines beinahe gleichbleibenden Arbeitsstundenvolumens auf mehr Beschäftigte durch die massive Ausweitung der Teilzeitarbeit. Die Lohnzurückhaltung der Tarifparteien war dabei eine notwendige, jedoch nicht hinreichende Bedingung für diesen Erfolg. Die Kovarianz von Lohn und Erwerbsindikatoren deutet allerdings darauf hin, dass die Arbeitsmarktreformen der sogenannten Agenda 2010 die erwerbsfähige Bevölkerung ab 2005 zur Teilnahme am Arbeitsmarkt aktiviert haben. Insbesondere die Reform der Arbeitslosenunterstützung hat die Ausweitung des Arbeitsangebots im unteren Lohnsegment ermöglicht und bewerkstelligt, dass die sozialversicherungspflichtige Teil- und Vollzeitarbeit zunahm. Ein Rückbau der Reformen könnte diesen Erfolg gefährden.
C00|Realized volatility of CO2 futures|The EU Emission Trading System (EU ETS) was created to reduce the CO2 and other greenhouse gas emissions at the lowest economic cost. In reality market participants are faced with considerable uncertainty due to price changes and require price and volatility estimates and forecasts for appropriate risk management, asset allocation and volatility trading. Although the simplest approach to estimate volatility is to use the historical standard deviation, realized volatility is a more accurate measure for volatility, since it is based on intraday data. Besides the stylized facts commonly observed in financial time series, we observe long-memory properties in the realized volatility series, which motivates the use of Heterogeneous Autoregressive (HAR) class models. Therefore, we propose to model and forecast the realized volatility of the EU ETS futures with HAR class models. The HAR models outperform benchmark models such as the standard long-memory ARFIMA model in terms of model fit, in-sample and out-of-sample forecasting. The analysis is based on intraday data (May 2007-April 2012) for futures on CO2 certificates for the second EU-ETS trading period (expiry December 2008-2012). The estimation results of the models allow to explain the volatility drivers in the market and volatility structure, according to the Heterogeneous Market Hypothesis as well as the observed asymmetries. We see that both speculators with short investment horizons as well as traders taking long-term hedging positions are active in the market. In a simulation study we test the suitability of the HAR model for option pricing and conclude that the HAR model is capable of mimicking the long-term volatility structure in the futures market and can be used for short-term and long-term option pricing.
C00|Dynamic Semiparametric Factor Model with a Common Break|For change-point analysis of high dimensional time series, we consider a semiparametric model with dynamic structural break factors. The observations are described by a few low dimensional factors with time-invariate loading functions of covariates. The unknown structural break in time models the regime switching e ects introduced by exogenous shocks. In particular, the factors are assumed to be nonstationary and follow a Vector Autoregression (VAR) process with a structural break. In addition, to account for the known spatial discrepancies, we introduce discrete loading functions. We study the theoretical properties of the estimates of the loading functions and the factors. Moreover, we provide both the consistency and the asymptotic convergence results for making inference on the common breakpoint in time. The estimation precision is evaluated via a simulation study. Finally we present two empirical illustrations on modeling the dynamics of the minimum wage policy in China and analyzing a limit order book dataset.
C00|A plausible Decision Heuristics Model: Fallibility of human judgment as an endogenous problem|This study meditates about mental heuristic rules as a representation of bounded rationality in individual decision making. The heuristic process presented here represents simultaneously limited computational capacity, the capacity to determine relevant information in complex contexts around beliefs, and time as an endogenous part of decision. The mathematical model of this heuristic rule correlates to the fallibility of the agent depending on the relative outcome of the alternatives in exogenous terms; the availability of only part of the information regarding the alternatives concert by beliefs; and the amount of time the decision maker is willing to spend on a decision based on previous experience and knowing that there is a tradeoff between time and fallibility. The resulting mathematical model can be applied to many disciplines like such as opinion models, game theory, the comparison of systems of distribution of authority, and fields that utilize the technique of agent-based models (ABM) that use individual behavior to study the macroscopic results of interactions.
C00|Say it again Sam: the information content of corporate conference calls|Abstract This paper examines information-content of corporate conference calls. It studies the determinants, and the consequences, of information production. To facilitate this study, I develop a novel measure of information-content which analyzes every word choice made by management during both sessions of the call. In a sample of S&P 1500 firms from 2001 to 2012, this new measure of information-content explains cumulative abnormal returns. It shows how CEOs produce (suppress) information during the conference call. It shows how analyst participation in the call improves information production. It shows that a differential value is placed on information conditioned on the market segment of the firm. I contrast the effectiveness of this new measure to that of the conventional methodologies of tone and word-counting. I provide evidence that this new information-content measure is better suited to conference calls than are the other two.
C00|Comparing redistributive efficiency of tax-benefit systems in Europe|In empirical analysis, the Kakwani index is the most frequently used indicator for comparing progressivity across countries and over time. The Kakwani is often assumed to measure to what extent a policy design is targeted to the poor. It has, however, a major drawback: it is not defined for net tax incidence—that is, the whole system of taxes and benefits. Moreover, it is defined over different intervals for different pre-tax income distributions and different average tax rates. This paper proposes an index based on the concept of relative redistributive efficiency that is not affected by these drawbacks. The Redistributive Efficiency index was compared to the Kakwani index for taxes/benefits in EU countries by using Euromod baselines. In addition, the Redistributive Efficiency index was computed on the whole tax-benefit system; that is, taxes and benefits were evaluated together. Only Ireland and the UK combine high levels of redistributive efficiency with a relevant amount of tax revenues and social expenditures. They obviously obtain very high redistribution, above 15 points. Most of the countries considered show an intermediate level of redistribution (between 7 and 12 points), but with a different mix. A group of Central and Northern European countries plus Slovenia and Hungary combine medium levels of redistributive efficiency and medium size, while some Southern European countries (Spain and Portugal) and new members compensate a rather low amount of transfer and taxes with quite high levels of efficiency. The remaining new member states and Southern EU countries show a very low level of redistribution, below 7 points. Interestingly, they vary in the level of tax burden and of resources devoted to benefits but all of them show a poor Redistributive Efficiency. This suggests that low Redistributive Efficiency plays a key role in explaining why certain countries perform a limited amount of redistribution.
C00|Comparing redistributive efficiency of tax-benefit systems in Europe|In empirical analysis, the Kakwani index is the most frequently used indicator for comparing progressivity across countries and over time. The Kakwani is often assumed to measure to what extent a policy design is targeted to the poor. It has, however, a major drawback: it is not defined for net tax incidence—that is, the whole system of taxes and benefits. Moreover, it is defined over different intervals for different pre-tax income distributions and different average tax rates. This paper proposes an index based on the concept of relative redistributive efficiency that is not affected by these drawbacks. The Redistributive Efficiency index was compared to the Kakwani index for taxes/benefits in EU countries by using Euromod baselines. In addition, the Redistributive Efficiency index was computed on the whole tax-benefit system; that is, taxes and benefits were evaluated together. Only Ireland and the UK combine high levels of redistributive efficiency with a relevant amount of tax revenues and social expenditures. They obviously obtain very high redistribution, above 15 points. Most of the countries considered show an intermediate level of redistribution (between 7 and 12 points), but with a different mix. A group of Central and Northern European countries plus Slovenia and Hungary combine medium levels of redistributive efficiency and medium size, while some Southern European countries (Spain and Portugal) and new members compensate a rather low amount of transfer and taxes with quite high levels of efficiency. The remaining new member states and Southern EU countries show a very low level of redistribution, below 7 points. Interestingly, they vary in the level of tax burden and of resources devoted to benefits but all of them show a poor Redistributive Efficiency. This suggests that low Redistributive Efficiency plays a key role in explaining why certain countries perform a limited amount of redistribution.
C00|Effectiveness and Challenges of Recruitment process outsourcing (RPO) in the Indian Hotel Sector|The present study tries of evaluate the effectiveness and challenges faced by adopting RPO practice in the India Hotel sector. The research objectives are driven from the purpose of the research which deals with distinct issues related to RPO and various perspectives of utilizing RPO. These objectives guided in identifying factors influencing the rationale for outsourcing the recruitment processes. The need for RPO has gained significance with the impact of organizational structure, stringent project deadlines, insufficient time for internal HR department for recruiting personnel, inefficiencies in integrating all stages of recruitment and expansion of geographies. The study is developed on building a theoretical framework which is prepared through analysing previous literature about outsourcing, and their utilization and efficiency in organizations. Mixed method approach has been followed to analyse with better validity and exploration of the context related to the effectiveness of RPO in the Indian hotel sector. The key findings indicated that the culture was positive influence on the usage of RPO in the organizations, in terms of adopting RPO during expansion of geographies and globalization. Also, the organizational structure was the major challenge in adopting these practices along with monitoring the outsourcing activities, lack of communication and inefficient HR practices.
C00|Introducing the Net Present Value Profile|The Net Present Value is an important statistic in the evaluation of investment opportunities. Analysts often consider the sensitivity of NPV to different parameters in an economic model, but always seem to consider the NPV of a project at the single instant before the project is launched. This short note introduces the idea of a NPV Profile, which shows how the NPV of a project changes over the life of that project. The note calculates the NPV Profile for a simplified cash flow model and briefly discusses how this statistic can be used to consider returns in possible takeover scenarios.
C00|Example of a Rising NPV Profile for a Mining Project|This paper describes a situation where the NPV Profile for a mining project is rising in the initial periods of production. This is an interesting case because mining projects are typically characterized by decreasing NPV Profiles caused by declining reserves and ever-approaching end to mine life. In this example, the NPV Profile is increasing alongside mine production levels in the first half of the project life. The parameters for this model are based on simplifying assumptions for the potential production profile of Anaconda Mining, a publicly-traded mining company.
C00|Financial Inclusion and Economic Growth in WAEMU: A Multiscale Heterogeneity Panel Causality Approach|This paper examines the causal relationship between Financial Inclusion and economic growth in the West African Economic and Monetary Union (WAEMU) from 2006 to 2015. We combined the heterogeneity panel causality test proposed by Dimitrescu and Hurlin (2012) with the Maximal Overlap Discrete Wavelet Transform (MODWT) to analyze the bi-directional causality at different time scales. We used two Financial Inclusion indicators: the overall rate of demographic penetration of financial services and the overall rate of use of financial services. Our results show that at scale 1 (2-4 years), there is no causality between economic growth and Financial Inclusion indicators. However, at scale 2 (4-8 years), we found a bi-directional causality between economic growth and Financial Inclusion. Policymakers should, therefore, while promoting Financial Inclusion reforms that are beneficial to Financial Inclusion, make more efficient the levers favoring macroeconomic growth, which also seems to be a decisive factor of Financial Inclusion.
C00|On the diversification benefit of reinsurance portfolios|In this paper we compare the diversification benefit of portfolios containing excess-of-loss treaties and portfolios containing quota-share treaties, when the risk measure is the (excess) Value-at-Risk or the (excess) Expected Shortfall. In a first section we introduce the set-up under which we perform our investigations. Then we show that when the losses are continuous, independent, bounded, the cover unlimited and when the risk measure is the Expected Shortfall at a level alpha close to 1, a portfolio of n excess-of-loss treaties diversifies better than a comparable portfolio of n quota-share treaties. This result extends to the other risk measures under additional assumptions. We further provide evidence that the boundedness assumption is not crucial by deriving analytical formulas in the case of treaties with i.i.d. exponentially distributed original losses. Finally we perform the comparison in the more general setting of arbitrary continuous joint loss distributions and observe in that case that a finite cover leads to opposite results, i.e. a portfolio of n quota-share treaties diversifies better than a comparable portfolio of n excess-of-loss treaties at high quantile levels.
C00|Sub-economic impulse and consciousness with quantum chromodynamic modeling|The word “sub-economics” follows the sense of “sub-atomic” (Yang, 2012). The latter is about the smallest in matter while the former is about the deepest in mind. Both are difficult to observe, but science must zoom in to understand them. Sub-economic dynamics studies the underlying human impulse and consciousness that may affect individual behavior in the market. It touches on the elementary mental structure of the sub-economic world. However, coding such an elementary structure or modeling its dynamics is not only a necessity, but also a challenge. Quantum chromodynamics (QCD; Zee, 2010) is about strong interactions of quarks and gluons. It touches on the elementary material structure of the physical world. QCD is applied as a conceptual and instrumental tool in the development of sub-economic modeling. The results show a nearly perfect isomorphism between the two elementary structures. By utilizing gauge field theory, sub-economic dynamics shares the same gauge symmetric group, SU(3), with QCD.
C00|The Relativity Theory of General Economic Equilibrium|"Abstract Purpose. The purpose of this paper is to propose a new approach to the understanding of self-regulation mechanism of decentralized economic system. Design/methodology/approach. As a result of the dialectical analysis of fundamental economic categories of market economy it appears as the form of a complex, non-linear, functionally closed and causally open system of economic actions. These systems have a number of unique properties that are well studied by second-order cybernetics. This allows in the study of economic processes the unique research and development of this science to be involved in the interdisciplinary format. Findings. The self-organization of a market economy is carried out through the recursive processes. Recursive processes in the economic system, as well as in other complex nonlinear dynamical systems, generate ""eigenvalues"" (""fixed points""). These ""eigenvalues"" are the equilibrium prices to which through the recursive processes tend the actual market prices, thus providing a tendency of the system to the general equilibrium. However, due to constant influence on the system of random external factors, the general equilibrium is never achieved. Research limitations/implications. On the base of the created model the hidden relationships among the gross profit, gross saving, gross investment and gross consumption in debt, as well as the relationships among the other economic parameters are revealed. This is important for adequate understanding of economic reproduction, tendency to general equilibrium, genesis of economic cycles, etc. Practical implications. The proposed understanding of self-regulation mechanism of decentralized economic system will help to improve the applied economic models and to develop the effective economic policy. Originality/value. The original interpretation of economic self-regulation mechanism of market economy is given. The ―Symmetrical model‖ of general economic equilibrium, which shows how economic forces arise, where they are directed and how interact with each other, which provide the homeostasis of a decentralized economic system, is proposed. This model shows the attractor of a real disequilibrium economy."
C00|Introduction to Michel Husson's 'Value and price: a critique of neo-Ricardian claims'|"This is a prepublication version of the published article of the same name and should be cited as ""Freeman, A. 2018. Introduction to Michel Husson's 'Value and Price: a critique of neo-Ricardian Claims' Capital and Class Vol 42, Issue 3, 2018, pp 509-516"" Michel Husson originally published this landmark article in French as Manuel Perez (1980). It thus offers a new generation of Marx scholars a resource which academic Marxism has rejected, except for a minority tradition in which this article played a foundational role: the opportunity to understand, and grapple with, Marx’s own economics. This introduction aims to explain, to such new readers, the key role which Husson’s article played in advancing our understanding of Marx’s theory of value. It appeared nine years after Paul Samuelson (1971) pronounced Marx’s value theory a failure, and three years after English Marxist Ian Steedman (1977) formally endorsed this verdict. Husson set out the first, and in many ways the most comprehensive concise rebuttal of these claims."
C00|Leadership Styles and Job Satisfaction|Low compensation in the retail sector is adversely affecting employee satisfaction and turnover. Leadership style is important for motivating employees and increasing their satisfaction level. This study has examined the effect of transformational and transactional leadership styles on job satisfaction in selected retail outlets of Slough, United Kingdom. The adapted questionnaire was administered to the employees of the retail outlets. The sample size was 270 and the response rate was 85%. The study found that transformational leadership style has a positive effect on job satisfaction, whereas transactional leadership style has an insignificant effect on job satisfaction. Therefore, it can be argued that the transformational leadership style is more effective in the retail sector of Slough, United Kingdom.
C00|Causal analysis in marketing: a customer satisfaction problem|In recent years, shopping streets are declining in aid of shopping centers. Consequently, to maximize loyalty and competitiveness in an increasingly competitive market it is essential to understand the distinctions characterizing the customers who choose shopping centers from those who opt for shopping centers. To analyse this differentiation, I use the log-linear causal models but, since these have not a complete causal theory, I use a new causal analysis to remedy this problem (Gheno, 2016). Starting from a complex model, I come to a simpler model to understand the different behaviors of the two types of customers. The data analysis shows that shopping centers customers are more driven by the emotions than the more rational and concrete ones who choose shopping centers.
C00|Sequential Probability Ration Tests : Conservative and Robust|In practice, most computers generate simulation outputs sequentially, so it is attractive to analyze these outputs through sequential statistical methods such as sequential probability ratio tests (SPRTs). We investigate several SPRTs for choosing between two hypothesized values for the mean output (response). One SPRT is published in Wald (1945), and allows general distribution types. For a normal (Gaussian) distribution this SPRT assumes a known variance, but in our modified SPRT we estimate the variance. Another SPRT is published in Hall (1962), and assumes a normal distribution with an unknown variance estimated from a pilot sample. We also investigate a modification, replacing this pilot-sample estimator by a fully sequential estimator. We present a sequence of Monte Carlo experiments for quantifying the performance of these SPRTs. In experiment #1 the simulation outputs are normal. This experiment suggests that Wald (1945)’s SPRT with estimated variance gives significantly high error rates. Hall (1962)’s original and modified SPRTs are conservative; i.e., the actual error rates are much smaller than the prespecified (nominal) rates. The most efficient SPRT is our modified Hall (1962) SPRT. In experiment #2 we examine the robustness of the various SPRTs in case of nonnormal output. If we know that the output has a specific nonnormal distribution such as the exponential distribution, then we may also apply Wald (1945)’s original SPRT. Throughout our investigation we pay special attention to the design and analysis of these experiments.
C00|Dependency between Risks and the Insurer’s Economic Capital: A Copula-based GARCH Model|Copulas can be a useful tool to capture heavy-tailed dependence between risks in estimating economic capital. This paper provides a procedure of combining copula with GARCH model to construct a multivariate distribution. The copula-based GARCH model using a skewed student’s t-distribution controls for the issues of skewness, heavy tails, volatility clustering and conditional dependencies contained in the financial time series data. Using the sample of U.S. property liability insurance industry, we perform Monte Carlo simulation to estimate the insurer’s economic capital measured by Value-at-Risk (VaR) and Expected Shortfall (ES). The result indicates that the choice of dependence structure and business mix between asset classes and liability lines has a significant impact on the resulting capital requirements and diversification benefits. We find the incremental diversification benefit in terms of a reduction in the total capital requirement from the joint modeling of underwriting risk and market risk compared to the modeling of market risk only.
C00|Income Inequality in Israel: A Distinctive Evolution|The level of disposable income inequality in Israel has increased noticeably since the mid-1980s and today it is above most developed countries. In contrast, market income inequality, which hit a record level in 2002, has reversed its course since then and has shown a sharp decline in subsequent years, and it is now below the OECD average. This paper offers tentative explanations for the inverted U-shape evolution of market income inequality in Israel in the last 25 years, which is distinctive in view of most developed countries’ experience. In addition, this article addresses the unique combination of income inequality in Israel which has one of the highest levels of disposable income inequality but is ranked below the OECD average measure of market income inequality.
C00|When Inequality Matters for Macro and Macro Matters for Inequality|We develop an efficient and easy to use computational method for solving a wide class of general equilibrium heterogeneous agent models with aggregate shocks together with an open source suite of codes that implement our algorithms in an easy to use toolbox. Our method extends standard linearization techniques and is designed to work in cases when inequality matters for the dynamics of macroeconomic aggregates. We present two applications that analyze a two asset incomplete markets model parameterized to match the distribution of income, wealth, and marginal propensities to consume. First, we show that our model is consistent with two key features of aggregate consumption dynamics that are difficult to match with representative agent models: (1) the sensitivity of aggregate consumption to predictable changes in aggregate income, and (2) the relative smoothness of aggregate consumption. Second, we extend the model to feature capital-skill complementarity and show how factor-specific productivity shocks shape dynamics of income and consumption inequality.
C00|When Inequality Matters for Macro and Macro Matters for Inequality|We develop an efficient and easy to use computational method for solving a wide class of general equilibrium heterogeneous agent models with aggregate shocks together with an open source suite of codes that implement our algorithms in an easy to use toolbox. Our method extends standard linearization techniques and is designed to work in cases when inequality matters for the dynamics of macroeconomic aggregates. We present two applications that analyze a two asset incomplete markets model parameterized to match the distribution of income, wealth, and marginal propensities to consume. First, we show that our model is consistent with two key features of aggregate consumption dynamics that are difficult to match with representative agent models: (1) the sensitivity of aggregate consumption to predictable changes in aggregate income, and (2) the relative smoothness of aggregate consumption. Second, we extend the model to feature capital-skill complementarity and show how factor-specific productivity shocks shape dynamics of income and consumption inequality.<br><small>(This abstract was borrowed from another version of this item.)</small>
C00|Screening for Bid-rigging – Does it Work?|This paper proposes a method to detect bid-rigging by applying mutually reinforcing screens to a road construction procurement data set from Switzerland in which no prior information about collusion was available. The screening method is particularly suited to deal with the problem of partial collusion, i.e. collusion which does not involve all firms and/or all contracts in a specific data set, implying that many of the classical markers discussed in the corresponding literature will fail to identify bid-rigging. In addition to presenting new screens for collusion, it is shown how benchmarks and the combination of different screens may be used to identify subsets of suspicious contracts and firms. The discussed screening method succeeds in isolating a group of “suspicious” firms exhibiting the characteristics of a local bid-rigging cartel with cover bids and a – more or less pronounced – bid rotation scheme. Based on these findings the Swiss Competition Commission (COMCO) opened an investigation and sanctioned the identified “suspicious” firms for bid-rigging in 2016.
C00|A normalized value for information purchases|Consider agents who are heterogeneous in their preferences and wealth levels. These agents may acquire information prior to choosing an investment that has a property of no-arbitrage, and each piece of information bears a corresponding cost. We associate a numeric index to each information purchase (information-cost pair). This index describes the normalized value of the information purchase: it is the risk-aversion level of the unique CARA agent who is indifferent between accepting and rejecting the purchase, and it is characterized by a “duality” principle that states that agents with a stronger preference for information should engage more often in information purchases. No agent more risk-averse than the index finds it profitable to acquire the information, whereas all agents less risk-averse than the index do. Given an empirically measured range of degrees of risk aversion in a competitive economy with no-arbitrage investments, our model therefore comes close to describing an inverse demand for information, by predicting what pieces of information are acquired by agents and which ones are not. Among several desirable properties, the normalized-value formula induces a complete ranking of information structures that extends Blackwell's classic ordering.
C00|Efficient Estimation Using The Characteristic Function|The method of moments proposed by Carrasco and Florens (2000) permits to fully exploit the information contained in the characteristic function and yields an estimator which is asymptotically as efficient as the maximum likelihood estimator. However, this estimation procedure depends on a regularization or tuning parameter å that needs to be selected. The aim of the present paper is to provide a way to optimally choose å by minimizing the approximate mean square error (AMSE) of the estimator. Following an approach similar to that of Newey and Smith (2004), we derive a higher-order expansion of the estimator from which we characterize the finite sample dependence of the AMSE on å. We provide a data-driven procedure for selecting the regularization parameter that relies on parametric bootstrap. We show that this procedure delivers a root T consistent estimator of å. Moreover, the data-driven selection of the regularization parameter preserves the consistency, asymptotic normality and efficiency of the CGMM estimator. Simulation experiments based on a CIR model show the relevance of the proposed approach.<br><small>(This abstract was borrowed from another version of this item.)</small>
C00|Crude oil price behaviour before and after military conflicts and geopolitical events|Crude oil price behaviour depends on all the events that have the potential to disrupt the flow of oil. We understand that these causes could be geopolitical issues and/or military conflicts in/with the producer countries and a problem relating to demand and supply. In the paper we first investigate the statistical properties of the real oil prices as well as its log-transformation, along with the absolute and squared returns values. Then, we also address the following issue: Does the crude oil price behave in the same way before and after a military conflict or geopolitical problem in the producer countries? To answer this question we analyse the real oil prices of West Texas Intermediate (WTI) before and after the different military conflicts and political events that occurred after World War II. For this purpose we use techniques based on unit roots and fractional integration. The empirical results provide evidence of persistence and breaks in the oil prices series and stationary long memory in the absolute returns. However, we do not observe significant differences before and after the conflict and geopolitical events.
C00|Quantitative wave model of macro-finance|This paper presents macro-finance as ensemble of economic agents and suggests use risk ratings of economic agents as their coordinates on economic space. Financial variables of separate economic agents are defined as functions of time and coordinates on economic space. Aggregations of financial variables of separate economic agents with coordinates near point x on economic space define macro-financial variables as function of x. Hydrodynamic-like equations describe evolution and mutual dependence between macro-financial variables. As example, for simple model of mutual dependence between macro-financial Demand on Investment and Interest Rate we derive hydrodynamic-like equations in a closed form. Perturbations of macro financial variables can generate waves those propagate on economic space and we derive wave equations. Macro financial waves can propagate on economic space with exponential growth of amplitudes and cause time fluctuations of finance variables that may model financial and business cycles. Variety of macro financial waves on economic space gives new look on internal dynamics of macro finance and reveals hidden complexity of macro financial modeling and forecasting.
C00|Econometric tests to detect bid-rigging cartels: does it work?|This paper tests how well the method proposed by Bajari and Ye (2003) performs to detect bidrigging cartels. In the case investigated in this paper, the bid-rigging cartel rigged all contracts during the collusive period, and all firms participated to the bid-rigging cartel. The two econometric tests constructed by Bajari and Ye (2003) produce a high number of false negative results: the tests do not reject the null hypothesis of competition, although they should have rejected it. A robustness analysis replicates the econometric tests on two different sub-samples, composed solely by cover bids. On the first sub-sample, both tests produce again a high number of false negative results. However, on the second sub-sample, one test performs better to detect the bidrigging cartel. The paper interprets these results, discusses alternative methods, and concludes with recommendations for competition agencies.
C00|Simple Statistical Screens to Detect Bid Rigging|The paper applies simple statistical screens to a bid-rigging cartel in Switzerland, and shows how well the screens detect it by capturing the impact of collusion on the discrete distribution of the bids. In case of bid rigging, the support for the distribution of the bids decreases involving a lower variance, illustrated by the coefficient of variance and the kurtosis statistic. Furthermore, when firms rig bids without side-payment, the difference between the first and the second lowest bids increases whereas the difference between the losing bids decreases, involving a negatively skewed distribution of the bids, highlighted by the relative distance and the skewness statistic. Finally, the collusive interaction screen shows that the behaviour of firms changed radically between the cartel and post-cartel periods. Therefore, the simple statistical screens proposed in this paper purpose to screen large dataset and to detect bidrigging cartels by using only information on bids.
C00|Expert System for Bomb Factory Detection by Networks of Advance Sensors|(1) Background: Police forces and security administrations are nowadays considering Improvised explosives (IEs) as a major threat. The chemical substances used to prepare IEs are called precursors, and their presence could allow police forces to locate a bomb factory where the on-going manufacturing of IEs is carried out. (2) Methods: An expert system was developed and tested in handling signals from a network of sensors, allowing an early warning. The expert system allows the detection of one precursor based on the signal provided by a single sensor, the detection of one precursor based on the signal provided by more than one sensor, and the production of a global alarm level based on data fusion from all the sensors of the network. (3) Results: The expert system was tested in the Italian Air Force base of Pratica di Mare (Italy) and in the Swedish Defence Research Agency (FOI) in Grindsjön (Sweden). (4) Conclusion: The performance of the expert system was successfully evaluated under relevant environmental conditions. The approach used in the development of the expert system allows maximum flexibility in terms of integration of the response provided by any sensor, allowing to easily include in the network all possible new sensors.
C00|Electrochemical Sensor for Explosives Precursors’ Detection in Water|Although all countries are intensifying their efforts against terrorism and increasing their mutual cooperation, terrorist bombing is still one of the greatest threats to society. The discovery of hidden bomb factories is of primary importance in the prevention of terrorism activities. Criminals preparing improvised explosives (IE) use chemical substances called precursors. These compounds are released in the air and in the waste water during IE production. Tracking sources of precursors by analyzing air or wastewater can then be an important clue for bomb factories’ localization. We are reporting here a new multiplex electrochemical sensor dedicated to the on-site simultaneous detection of three explosive precursors, potentially used for improvised explosive device preparation (hereafter referenced as B01, B08, and B15, for security disclosure reasons and to avoid being detrimental to the security of the counter-explosive EU action). The electrochemical sensors were designed to be disposable and to combine ease of use and portability in a screen-printed eight-electrochemical cell array format. The working electrodes were modified with different electrodeposited metals: gold, palladium, and platinum. These different coatings giving selectivity to the multi-sensor through a “fingerprint”-like signal subsequently analyzed using partial least squares-discriminant analysis (PLS-DA). Results are given regarding the detection of the three compounds in a real environment and in the presence of potentially interfering species.
C00|About the Purification Route of Ionic Liquid Precursors|In this work a purification route of precursors for ionic liquids tailored to electrochemical energy storage systems is reported and described. The study was carried out on the N -butyl- N -methylpyrrolidinium bromide (PYR 14 Br) precursor, which represents the intermediate product of the synthesis process of the N -butyl- N -methylpyrrolidinium bis(trifluoromethanesulfonyl)imide (PYR 14 TFSI) hydrophobic ionic liquid. The target is to develop an easy and cost-effective approach for efficiently purifying several kinds of ionic liquid precursors and determining their purity content. The PYR 14 Br precursor was synthesized through an eco-friendly preparation procedure, which requires water as the only processing solvent, and purified through sorbent materials, such as activated charcoal and alumina. The effect of the treatment/nature/content of sorbents and processing temperature/time was investigated. The impurity content was detected by UV-VIS spectrophotometry measurements. Additionally, a correlation between the measured absorbance and the content of impurities within the precursor was obtained. The purity level of the precursor was seen to play a key role in the electrochemical performance of the ionic liquids.
C00|Acknowledgement to Reviewers of Challenges in 2016|The editors of Challenges would like to express their sincere gratitude to the following reviewers for assessing manuscripts in 2016.[...]
C00|Ice XVII as a Novel Material for Hydrogen Storage|Hydrogen storage is one of the most addressed issues in the green-economy field. The latest-discovered form of ice (XVII), obtained by application of an annealing treatment to a H 2 -filled ice sample in the C 0 -phase, could be inserted in the energy-storage context due to its surprising capacity of hydrogen physisorption, when exposed to even modest pressure (few mbars at temperature below 40 K), and desorption, when a thermal treatment is applied. In this work, we investigate quantitatively the adsorption properties of this simple material by means of spectroscopic and volumetric data, deriving its gravimetric and volumetric capacities as a function of the thermodynamic parameters, and calculating the usable capacity in isothermal conditions. The comparison of ice XVII with materials with a similar mechanism of hydrogen adsorption like metal-organic frameworks shows interesting performances of ice XVII in terms of hydrogen content, operating temperature and kinetics of adsorption-desorption. Any application of this material to realistic hydrogen tanks should take into account the thermodynamic limit of metastability of ice XVII, i.e., temperatures below about 130 K.
C00|New Studies of the Physical Properties of Metallic Amorphous Membranes for Hydrogen Purification|Amorphous metallic membranes display promising properties for hydrogen purification up to an ultrapure grade (purity > 99.999%). The hydrogen permeability through amorphous membranes has been widely studied in the literature. In this work we focus on two additional properties, which should be considered before possible application of such materials: the propensity to crystallize at high temperatures should be avoided, as the crystallized membranes can become brittle; the hydrogen solubility should be high, as solubility and permeability are proportional. We investigate the crystallization process and the hydrogen solubility of some membranes based on Ni, Nb, and Zr metals, as a function of Zr content, and with the addition of Ta or B. The boron doping does not significantly affect the crystallization temperature and the thermal stability of the membrane. However, the hydrogen solubility for p ~7 bar is as high as H/M ~0.31 at T = 440 °C and H/M ~0.27 at T = 485 °C. Moreover, the membrane does not pulverize even after repeated thermal cycles and hydrogenation processes up to 485 °C and 7 bar, and it retains its initial shape.
C00|Ionic Mobility and Phase Transitions in Perovskite Oxides for Energy Application|Perovskite oxides ﬁnd applications or are studied in many ﬁelds related to energy production, accumulation and saving. The most obvious application is oxygen or proton conductors in fuel cells (SOFCs), but the (anti)ferroelectric compositions may ﬁnd application in high energy capacitors for energy storage, efﬁcient electrocaloric cooling, and electromechanical energy harvesting. In SOFCs, the diffusion of O vacancies and other mobile ionic species, such as H+, are at the base of the functioning of the device, while in the other cases they constitute unwanted defects that reduce the performance and life-time of the device. Similarly, the (anti)ferroelectric phase transitions are a requisite for the use of some types of devices, but the accompanying domain walls can generate extended defects detrimental to the life of the material, and structural phase transformations should be avoided in SOFCs. All these phenomena can be studied by mechanical spectroscopy, the measurement of the complex elastic compliance as a function of temperature and frequency, which is the mechanical analogue of the dielectric susceptibility, but probes the elastic response and elastic dipoles instead of the dielectric response and electric dipoles. The two techniques can be combined to provide a comprehensive picture of the material properties. Examples are shown of the study of structural transitions and hopping and tunnelling processes of O vacancies and H in the ion conductor BaCe1-xYxO3-x and in SrTiO3-x, and of the aging and fatigue effects found in PZT at compositions where the ferro- and antiferroelectric states coexist.
C00|Tie-Up Cycles in Long-Term Mating. Part II: Fictional Narratives and the Social Cognition of Mating|In the first part of this paper, we have introduced a novel theoretical approach to mating dynamics, known as Tie-Up Theory (TU). In this second part, in the context of the bio-cultural approach to literature, that assigns to fictional narratives an important valence of social cognition, we apply the conceptual tools presented in the first part to the analysis of mating-related interaction dynamics in some blockbuster Hollywood movies from WWII to today. The interaction dynamics envisioned by our theory accurately reflect, to a significant level of detail, the narrative development of the movies under exam from the viewpoint of the mating dynamics of the couple of main characters, accounting for the specific reasons that lead them to react to certain situations via certain behaviors, and for the reasons why such behaviors lead to certain outcomes. Our analysis seems thus to bring some further legitimacy to the bio-cultural foundation of the narrative structure of the movies that we analyze, and moreover to the idea that it is possible to ‘inquire’ characters about their choices according to the narratological-experimental lines suggested by some proponents of the bio-cultural approach.
C00|A Study of the Conformers of the (Nonafluorobutanesulfonyl)imide Ion by Means of Infrared Spectroscopy and Density Functional Theory (DFT) Calculations|Pyrrolidinium-based ionic liquids with anions of the per(fluoroalkylsulfonyl)imide family are particularly interesting for their use as electrolytes in lithium batteries. These ions have several geometric isomers and the presence of different ion conformers and their distribution affects the ILs (Ionic liquids) physical and chemical properties. In the present work, we report the temperature dependence of the infrared spectra of the N -butyl- N -methyl-pyrrolidinium(trifluoromethanesulfonyl)(nonafluorobutanesulfonyl)imide (PYR 14 ‑ IM 14 ) ionic liquid; DFT (Density Functional Theory) calculations performed with different models provides indications about the IM 14 conformers and their vibrational spectra. Moreover the temperature dependence of the intensity of the lines identified as markers of different conformers provide indications about the conformers’ distribution and the difference of their enthalpy in the liquid phase.
C00|Hydrides as High Capacity Anodes in Lithium Cells: An Italian “Futuro in Ricerca di Base FIRB-2010” Project|Automotive and stationary energy storage are among the most recently-proposed and still unfulfilled applications for lithium ion devices. Higher energy, power and superior safety standards, well beyond the present state of the art, are actually required to extend the Li-ion battery market to these challenging fields, but such a goal can only be achieved by the development of new materials with improved performances. Focusing on the negative electrode materials, alloying and conversion chemistries have been widely explored in the last decade to circumvent the main weakness of the intercalation processes: the limitation in capacity to one or at most two lithium atoms per host formula unit. Among all of the many proposed conversion chemistries, hydrides have been proposed and investigated since 2008. In lithium cells, these materials undergo a conversion reaction that gives metallic nanoparticles surrounded by an amorphous matrix of LiH. Among all of the reported conversion materials, hydrides have outstanding theoretical properties and have been only marginally explored, thus making this class of materials an interesting playground for both fundamental and applied research. In this review, we illustrate the most relevant results achieved in the frame of the Italian National Research Project FIRB 2010 Futuro in Ricerca “Hydrides as high capacity anodes in lithium cells” and possible future perspectives of research for this class of materials in electrochemical energy storage devices.
C00|Case Studies of Energy Storage with Fuel Cells and Batteries for Stationary and Mobile Applications|In this paper, hydrogen coupled with fuel cells and lithium-ion batteries are considered as alternative energy storage methods. Their application on a stationary system (i.e., energy storage for a family house) and a mobile system (i.e., an unmanned aerial vehicle) will be investigated. The stationary systems, designed for off-grid applications, were sized for photovoltaic energy production in the area of Turin, Italy, to provide daily energy of 10.25 kWh. The mobile systems, to be used for high crane inspection, were sized to have a flying range of 120 min, one being equipped with a Li-ion battery and the other with a proton-exchange membrane fuel cell. The systems were compared from an economical point of view and a life cycle assessment was performed to identify the main contributors to the environmental impact. From a commercial point of view, the fuel cell and the electrolyzer, being niche products, result in being more expensive with respect to the Li-ion batteries. On the other hand, the life cycle assessment (LCA) results show the lower burdens of both technologies.
C00|Business Cycle Estimation with High-Pass and Band-Pass Local Polynomial Regression|Filters constructed on the basis of standard local polynomial regression (LPR) methods have been used in the literature to estimate the business cycle. We provide a frequency domain interpretation of the contrast filter obtained by the difference of a series and its long-run LPR component and show that it operates as a kind of high-pass filter, so that it provides a noisy estimate of the cycle. We alternatively propose band-pass local polynomial regression methods aimed at isolating the cyclical component. Results are compared to standard high-pass and band-pass filters. Procedures are illustrated using the US GDP series.
C00|A Note on Identification of Bivariate Copulas for Discrete Count Data|Copulas have enjoyed increased usage in many areas of econometrics, including applications with discrete outcomes. However, Genest and Nešlehová (2007) present evidence that copulas for discrete outcomes are not identified, particularly when those discrete outcomes follow count distributions. This paper confirms the Genest and Nešlehová result using a series of simulation exercises. The paper then proceeds to show that those identification concerns diminish if the model has a regression structure such that the exogenous variable(s) generates additional variation in the outcomes and thus more completely covers the outcome domain.
C00|Structural Breaks, Inflation and Interest Rates: Evidence from the G7 Countries|This study reconsiders the common unit root/co-integration approach to test for the Fisher effect for the economies of the G7 countries. We first show that nominal interest and inflation rates are better represented as I(0) variables. Later, we use the Bai–Perron procedure to show the existence of structural changes in the Fisher equation. After considering these breaks, we find very limited evidence of a total Fisher effect as the transmission coefficient of the expected inflation rates to nominal interest rates is very different than one.
C00|Testing for a Structural Break in a Spatial Panel Model|We consider the problem of testing for a structural break in the spatial lag parameter in a panel model (spatial autoregressive). We propose a likelihood ratio test of the null hypothesis of no break against the alternative hypothesis of a single break. The limiting distribution of the test is derived under the null when both the number of individual units N and the number of time periods T is large or N is ﬁxed and T is large. The asymptotic critical values of the test statistic can be obtained analytically. We also propose a break-date estimator that can be employed to determine the location of the break point following evidence against the null hypothesis. We present Monte Carlo evidence to show that the proposed procedure performs well in ﬁnite samples. Finally, we consider an empirical application of the test on budget spillovers and interdependence in ﬁscal policy within the U.S. states.
C00|Goodness-of-Fit Tests for Copulas of Multivariate Time Series|In this paper, we study the asymptotic behavior of the sequential empirical process and the sequential empirical copula process, both constructed from residuals of multivariate stochastic volatility models. Applications for the detection of structural changes and specification tests of the distribution of innovations are discussed. It is also shown that if the stochastic volatility matrices are diagonal, which is the case if the univariate time series are estimated separately instead of being jointly estimated, then the empirical copula process behaves as if the innovations were observed; a remarkable property. As a by-product, one also obtains the asymptotic behavior of rank-based measures of dependence applied to residuals of these time series models.
C00|Accuracy and Efficiency of Various GMM Inference Techniques in Dynamic Micro Panel Data Models|Studies employing Arellano-Bond and Blundell-Bond generalized method of moments (GMM) estimation for linear dynamic panel data models are growing exponentially in number. However, for researchers it is hard to make a reasoned choice between many different possible implementations of these estimators and associated tests. By simulation, the effects are examined in terms of many options regarding: (i) reducing, extending or modifying the set of instruments; (ii) specifying the weighting matrix in relation to the type of heteroskedasticity; (iii) using (robustified) 1-step or (corrected) 2-step variance estimators; (iv) employing 1-step or 2-step residuals in Sargan-Hansen overall or incremental overidentification restrictions tests. This is all done for models in which some regressors may be either strictly exogenous, predetermined or endogenous. Surprisingly, particular asymptotically optimal and relatively robust weighting matrices are found to be superior in finite samples to ostensibly more appropriate versions. Most of the variants of tests for overidentification and coefficient restrictions show serious deficiencies. The variance of the individual effects is shown to be a major determinant of the poor quality of most asymptotic approximations; therefore, the accurate estimation of this nuisance parameter is investigated. A modification of GMM is found to have some potential when the cross-sectional heteroskedasticity is pronounced and the time-series dimension of the sample is not too small. Finally, all techniques are employed to actual data and lead to insights which differ considerably from those published earlier.
C00|A Simple Test for Causality in Volatility|An early development in testing for causality (technically, Granger non-causality) in the conditional variance (or volatility) associated with financial returns was the portmanteau statistic for non-causality in the variance of Cheng and Ng (1996). A subsequent development was the Lagrange Multiplier (LM) test of non-causality in the conditional variance by Hafner and Herwartz (2006), who provided simulation results to show that their LM test was more powerful than the portmanteau statistic for sample sizes of 1000 and 4000 observations. While the LM test for causality proposed by Hafner and Herwartz (2006) is an interesting and useful development, it is nonetheless arbitrary. In particular, the specification on which the LM test is based does not rely on an underlying stochastic process, so the alternative hypothesis is also arbitrary, which can affect the power of the test. The purpose of the paper is to derive a simple test for causality in volatility that provides regularity conditions arising from the underlying stochastic process, namely a random coefficient autoregressive process, and a test for which the (quasi-) maximum likelihood estimates have valid asymptotic properties under the null hypothesis of non-causality. The simple test is intuitively appealing as it is based on an underlying stochastic process, is sympathetic to Granger’s (1969, 1988) notion of time series predictability, is easy to implement, and has a regularity condition that is not available in the LM test.
C00|Regime Switching Vine Copula Models for Global Equity and Volatility Indices|For nearly every major stock market there exist equity and implied volatility indices. These play important roles within finance: be it as a benchmark, a measure of general uncertainty or a way of investing or hedging. It is well known in the academic literature that correlations and higher moments between different indices tend to vary in time. However, to the best of our knowledge, no one has yet considered a global setup including both equity and implied volatility indices of various continents, and allowing for a changing dependence structure. We aim to close this gap by applying Markov-switching R -vine models to investigate the existence of different, global dependence regimes. In particular, we identify times of “normal” and “abnormal” states within a data set consisting of North-American, European and Asian indices. Our results confirm the existence of joint points in a time at which global regime switching between two different R -vine structures takes place.
C00|Consistency of Trend Break Point Estimator with Underspecified Break Number|This paper discusses the consistency of trend break point estimators when the number of breaks is underspecified. The consistency of break point estimators in a simple location model with level shifts has been well documented by researchers under various settings, including extensions such as allowing a time trend in the model. Despite the consistency of break point estimators of level shifts, there are few papers on the consistency of trend shift break point estimators in the presence of an underspecified break number. The simulation study and asymptotic analysis in this paper show that the trend shift break point estimator does not converge to the true break points when the break number is underspecified. In the case of two trend shifts, the inconsistency problem worsens if the magnitudes of the breaks are similar and the breaks are either both positive or both negative. The limiting distribution for the trend break point estimator is developed and closely approximates the finite sample performance.
C00|Fractional Unit Root Tests Allowing for a Structural Change in Trend under Both the Null and Alternative Hypotheses|This paper considers testing procedures for the null hypothesis of a unit root process against the alternative of a fractional process, called a fractional unit root test. We extend the Lagrange Multiplier (LM) tests of Robinson (1994) and Tanaka (1999), which are locally best invariant and uniformly most powerful, to allow for a slope change in trend with or without a concurrent level shift under both the null and alternative hypotheses. We show that the limit distribution of the proposed LM tests is standard normal. Finite sample simulation experiments show that the tests have good size and power. As an empirical analysis, we apply the tests to the Consumer Price Indices of the G7 countries.
C00|Between Institutions and Global Forces: Norwegian Wage Formation Since Industrialisation|This paper reviews the development of labour market institutions in Norway, shows how labour market regulation has been related to the macroeconomic development, and presents dynamic econometric models of nominal and real wages. Single equation and multi-equation models are reported. The econometric modelling uses a new data set with historical time series of wages and prices, unemployment and labour productivity. Impulse indicator saturation is used to achieve robust estimation of focus parameters, and the breaks are interpreted in the light of the historical overview. A relatively high degree of constancy of the key parameters of the wage setting equation is documented, over a considerably longer historical time period than earlier studies have done. The evidence is consistent with the view that the evolving system of collective labour market regulation over long periods has delivered a certain necessary level of coordination of wage and price setting. Nevertheless, there is also evidence that global forces have been at work for a long time, in a way that links real wages to productivity trends in the same way as in countries with very different institutions and macroeconomic development.
C00|Acknowledgement to Reviewers of Econometrics in 2016|The editors of Econometrics would like to express their sincere gratitude to the following reviewers for assessing manuscripts in 2016.[...]
C00|Endogeneity, Time-Varying Coefficients, and Incorrect vs. Correct Ways of Specifying the Error Terms of Econometric Models|Using the net effect of all relevant regressors omitted from a model to form its error term is incorrect because the coefficients and error term of such a model are non-unique. Non-unique coefficients cannot possess consistent estimators. Uniqueness can be achieved if; instead; one uses certain “sufficient sets” of (relevant) regressors omitted from each model to represent the error term. In this case; the unique coefficient on any non-constant regressor takes the form of the sum of a bias-free component and omitted-regressor biases. Measurement-error bias can also be incorporated into this sum. We show that if our procedures are followed; accurate estimation of bias-free components is possible.
C00|A Fast Algorithm for the Computation of HAC Covariance Matrix Estimators|This paper considers the algorithmic implementation of the heteroskedasticity and autocorrelation consistent (HAC) estimation problem for covariance matrices of parameter estimators. We introduce a new algorithm, mainly based on the fast Fourier transform, and show via computer simulation that our algorithm is up to 20 times faster than well-established alternative algorithms. The cumulative effect is substantial if the HAC estimation problem has to be solved repeatedly. Moreover, the bandwidth parameter has no impact on this performance. We provide a general description of the new algorithm as well as code for a reference implementation in R .
C00|A Potential Contradiction Between Economic Theory and Applied Finance|One of the basic premises that underlies economic theory in Finance is the assumption of declining marginal utility of income. This assumption imposes risk-aversion on the investors and is necessary requirement to an equilibrium capital markets. A popular method of analyzing empirical evidence among financial analysts is the Ordinary Least Squares Regression. This paper argues that in certain cases involving violation of the linearity assumption by the data, there may be a contradiction between the two approaches. In order to resolve the possibility of a contradiction one has to impose economic theory on the regression. The paper proposes the use of the Gini regression to bridge the gap between economic theory and regression.
C00|Pension Capital Investment in the Context of a Private Pension Fund|An analysis of the pension legislation has been performed, particularly in the field of restrictions on asset allocation into various funds, and a model for real investment profile has been proposed that secures minimal risk at the given profitability and satisfies legal requirements, as well as immunizes the sub-portfolio of risk-free securities from changes in the market interest rate. The numerical experiment has been carried out with and without regard to transaction expenses. It has been estimated that frequent renegotiation of the portfolio leads to a decrease in profits from investing as a result of transaction expenses.
C00|The Impact Of The Valuation Of Assets On The Company'S Profitability|"The research paper which has the title ""The impact of the valuation of assets on the companys profitability"" was achieved having as main objective to highlight how an enterprise valuation of assets directly influences the level of its profitability. This was achieved by dividing the relevant assets of agro-industrial companies in Dolj County, is the approach used in evaluating the income, income capitalization method is the method used. The date on which the assessment is held September 21, 2015, therefore this is the date of the assessment. We also emphasized the subjective nature of the evaluation in general and especially of the evaluation method based on capitalization of income, which also can lead to favorable or unfavorable change in the profitability of the company."
C00|Did The Danube Delta Pensions Managed To Overcome The Economic Crisis?|In the knowledge economy and competitiveness, economic and financial analysis of a company in every industry generally, the hospitality industry in particular, is an indispensable tool, which aims radiography entire business of the company, to diagnose strengths and weaknesses but also identify opportunities and threats coming from the market.One of the most important indicators measuring the economic performance of a company is turnover. This indicator enables research dimension, held in place within the industry, market position, economic entity's ability to initiate effective and efficient. Thus, the turnover reflects the degree of health of the company. In this paper we analysed in terms of economic and financial representative guesthouse in the Danube Delta to see the actual situation of tourism in the area. The main purpose was to see if the pensions of Delta managed to overcome the economic crisis global and national.
C00|Reflections and Comments on Randomness|This article is not directed to any purpose other than conveying the effort of understanding to the ones that are able to understand randomness.
C00|Crash course em matemática para economistas|This book is a set of ten chapters that briefly discuss topics normally presented in courses of mathematics for economists. These chapters are: 1 - Linear algebra; 2 - Differential calculus of many variables; 3 - Integral calculus; 4 - First order differential and difference equations; 5 - Systems of first order difference equations; 6 - Systems of first order differential equations; 7 - Elements of analysis; 8 - Unrestricted optimization; 9 - Restricted optimization; and 10 - Concave and quasi-concave functions. Knowledge of high school linear algebra and a previous elementary course of differential and integral calculus are required in order to better understand the concepts discussed in the book. Besides, familiarity with some of the basics concepts of microeconomics also helps.
C00|The Nature of Volatility Spillovers across the International Capital Markets|This paper studies the nature of volatility spillovers across countries from the perspective of network theory and by relying on data of US-listed ETFs. I use a Lasso-related technique to estimate the International Volatility Network (IVN) where the nodes correspond to large-cap international stock markets while the links account for significant volatility lead-lags. Also included in the analysis is the International TradeNetwork (ITN), whose links measure bilateral export-import flows thus, capturing fundamental interconnections between countries. I find that the IVN and the ITN resemble each other closely pointing out that volatility does not disseminate randomly but tends to spread across fundamentally related economies. I also note that the lagged volatility reactions embedded in the IVN are consistent with the notion of gradual diffusion of information across investors who are subject to limited attention and home bias. This hypothesis is formally tested by using as a direct proxy of investors’ attention the aggregate search frequency in Google. The empirical results support this intuition indicating that higher volatility surprises in key foreign markets predict higher domestic attention upon those markets in subsequent days. Once domestic attention is captured by such external shocks, it is contemporaneously transformed into higher domestic volatility.
C00|Reason-Based Choice And Context-Dependence: An Explanatory Framework|We introduce a “reason-based” framework for explaining and predicting individual choices. The key idea is that a decision-maker focuses on some but not all properties of the options and chooses an option whose “motivationally salient” properties he/she most prefers. Reason-based explanations can capture two kinds of contextdependent choice: (i) the motivationally salient properties may vary across choice contexts, and (ii) they may include “context-related” properties, not just “intrinsic” properties of the options. Our framework allows us to explain boundedly rational and sophisticated choice behaviour. Since properties can be recombined in new ways, it also offers resources for predicting choices in unobserved contexts.<br><small>(This abstract was borrowed from another version of this item.)</small>
C00|Multinomial VaR Backtests: A simple implicit approach to backtesting expected shortfall|Under the Fundamental Review of the Trading Book (FRTB) capital charges for the trading book are based on the coherent expected shortfall (ES) risk measure, which show greater sensitivity to tail risk. In this paper it is argued that backtesting of expected shortfall-or the trading book model from which it is calculated-can be based on a simultaneous multinomial test of value-at-risk (VaR) exceptions at different levels, an idea supported by an approximation of ES in terms of multiple quantiles of a distribution proposed in Emmer et al. (2015). By comparing Pearson, Nass and likelihood-ratio tests (LRTs) for different numbers of VaR levels N it is shown in a series of simulation experiments that multinomial tests with N ≥ 4 are much more powerful at detecting misspecifications of trading book loss models than standard bi-nomial exception tests corresponding to the case N = 1. Each test has its merits: Pearson offers simplicity; Nass is robust in its size properties to the choice of N ; the LRT is very powerful though slightly over-sized in small samples and more computationally burdensome. A traffic-light system for trading book models based on the multinomial test is proposed and the recommended procedure is applied to a real-data example spanning the 2008 financial crisis.
C00|Blanchard and Kahn’s (1980) solution for a linear rational expectations model with one state variable and one control variable: the correct formula|This note corrects Blanchard and Kahn’s (1980) solution for a linear dynamic rational expectations model with one state variable and one control variable.
C00|Gold price and stock markets nexus under mixed-copulas|This paper investigates the role of gold as a safe haven in international stock markets using various copula techniques to capture complex dependencies between stock markets and gold prices. It creates a new class of mix copulas from Clayton, Frank, Gumbel and Joe copulas. The paper employs parametric and nonparametric copulas to over 11years of daily data (1999–2010) from seven countries' to understand the nexus between international stock markets and gold prices. The results show that gold may be a safe haven asset during market crash for the case of Malaysia, Singapore, Thailand, the UK and the US markets but not for the Indonesian, Japanese and the Philippines markets. These results are of great interest for the investors and risk managers to comprehend portfolio diversification benefits and risk reductions during tranquil and downturn periods by including gold in their investment portfolios.
C00|Long term oil prices|In this paper we propose a model to estimate and simulate long term oil prices. Our model is based on properties of demand and supply for oil and it is able to reproduce historical real oil prices well. We use the model to estimate and simulate future real oil price scenarios. The results show that if we are not able to significantly increase demand elasticity, the yearly real oil price change can reach 12% in the years following the peak production level without taking a scarcity rent into account. Until peak production level is reached, the long term real oil price changes stemming from fundamental supply and demand changes are expected to be negative. Our simulation results based on an expected peak production year of 2020 and a scarcity rent of 3% suggest an expected real crude oil price of $169/bbl in 2040. For comparison the EIA outlook predicts a real oil price of $141/bbl for the same year. We also provide an on-line Appendix that allows the readers to change the assumptions underlying our analysis and see the results immediately.
C00|Almost stochastic dominance for risk averters and risk seeker|In this paper we first extend the theory of almost stochastic dominance (ASD) (for risk averters) to include the ASD for risk-seeking investors. We then study the relationship between ASD for risk seekers and ASD for risk averters. Recently, Tsetlin, et al.(2015) develop the theory of generalized ASD (GASD). We then briefly discuss the advantages and disadvantages of ASD and GASD.
C00|An order of asymmetry in copulas, and implications for risk management|We study symmetry properties of bivariate copulas. For this, we introduce an order of asymmetry, as well as measures of asymmetry which are monotone in that order. In an empirical study, we illustrate that asymmetric dependence structures do indeed occur in financial market data and discuss its relevance for financial risk management.
C00|Constant Market Share Analysis: A Note|Constant Market Share Analysis (CMSA) is a method which decomposes the variation of market shares of any trader country. The more recent version is proposed by Fagerberg and Sollie (1985) that avoids some limits deriving from previously specifications. After explicating how CMSA works, this note presents some applications to the Italian case. Two are the most important contribution of this paper. First, it includes a complete framework on CMSA. Second, the formal derivation of market share variation is proposed.
C00|A rationalist explanation of Russian risk-taking|Three seemingly unrelated topics of Russian politics are investigated. It is shown that under expected utility maximization the assumptions of an unbiased oil forward market and a risk-acceptant attitude (strictly convex utility function) are sufficient to explain Russia’s open position in oil and the bailout of Rosneft. The risk-acceptant attitude of the Russian leader also causes a shrunken bargaining range for the conflict in Ukraine, which can be enlarged by sanctions but not necessarily by the proliferation of weapons. This gives sanctions a clear edge over the proliferation of weapons.
C00|Jackknife Bias Reduction in the Presence of a Near-Unit Root|This paper considers the specification and performance of jackknife estimators of the autoregressive coefficient in a model with a near-unit root. The limit distributions of sub-sample estimators that are used in the construction of the jackknife estimator are derived, and the joint moment generating function (MGF) of two components of these distributions is obtained and its properties explored. The MGF can be used to derive the weights for an optimal jackknife estimator that removes fully the first-order finite sample bias from the estimator. The resulting jackknife estimator is shown to perform well in finite samples and, with a suitable choice of the number of sub-samples, is shown to reduce the overall finite sample root mean squared error, as well as bias. However, the optimal jackknife weights rely on knowledge of the near-unit root parameter and a quantity that is related to the long-run variance of the disturbance process, which are typically unknown in practice, and so, this dependence is characterised fully and a discussion provided of the issues that arise in practice in the most general settings.
C00|Non-Stationary Dynamic Factor Models for Large Datasets|We study a Large-Dimensional Non-Stationary Dynamic Factor Model where (1) the factors Ft are I (1) and singular, that is Ft has dimension r and is driven by q dynamic shocks with q less than r, (2) the idiosyncratic components are either I (0) or I (1). Under these assumption the factors Ft are cointegrated and modeled by a singular Error Correction Model. We provide conditions for consistent estimation, as both the cross-sectional size n, and the time dimension T, go to infinity, of the factors, the loadings, the shocks, the ECM coefficients and therefore the Impulse Response Functions. Finally, the numerical properties of our estimator are explored by means of a MonteCarlo exercise and of a real-data application, in which we study the effects of monetary policy and supply shocks on the US economy.
C00|Screening for bid-rigging - does it work?|This paper proposes a method to detect bid-rigging by applying mutually reinforcing screens to a road construction procurement data set from Switzerland in which no prior information about collusion was available. The screening method is particularly suited to deal with the problem of partial collusion, i.e. collusion which does not involve all firms and/or all contracts in a specific data set, implying that many of the classical markers discussed in the corresponding literature will fail to identify bid-rigging. In addition to presenting a new screen for collusion, it is shown how benchmarks and the combination of different screens may be used to identify subsets of suspicious contracts and firms in a data set. The discussed screening method succeeds in isolating a group of “suspicious” firms exhibiting the characteristics of a local bid-rigging cartel operating with cover bids and a – more or less pronounced – bid rotation scheme. Based on these findings the Swiss Competition Commission (ComCo) decided to open an investigation.
C00|Acknowledgement to Reviewers of Challenges in 2015|The editors of Challenges would like to express their sincere gratitude to the following reviewers for assessing manuscripts in 2015. [...]
C00|Flaws and Drawbacks in Present Regulation and the Need to Take Action and Support the Renewable Energies Policies in Italy|Renewable energies is an important sector that needs to be sustained and increased by the action of policies and economic support.[...]
C00|Alternative Energies and Fossil Fuels in the Bioeconomy Era: What is Needed in the Next Five Years for Real Change|Sustainable biomass feedstock is the key to sustainable biofuels.[...]
C00|Tie-Up Cycles in Long-Term Mating. Part I: Theory|In this paper, we propose a new approach to couple formation and dynamics that abridges findings from sexual strategies theory and attachment theory to develop a framework where the sexual and emotional aspects of mating are considered in their strategic interaction. Our approach presents several testable implications, some of which find interesting correspondences in the existing literature. Our main result is that, according to our approach, there are six typical dynamic interaction patterns that are more or less conducive to the formation of a stable couple, and that set out an interesting typology for the analysis of real (as well as fictional, as we will see in the second part of the paper) mating behaviors and dynamics.
C00|Potential for Reuse of E-Plastics through Processing by Compression Molding|The amounts of e-waste, consisting of metal (e-metals) and plastic (e-plastics) streams from electronic goods, are increasing in the United States and elsewhere. The e-metals waste streams are being recycled to a reasonable degree due to the value of precious metals. E-plastic waste streams currently are not recycled or reused to a significant extent. As a result, most e-plastics are disposed of by landfilling or thermal treatment, or sent overseas for alleged recycling or reuse, any of which could result in unsafe worker exposure and release into the environment. Two of the major barriers to e-plastics’ reuse or recycling are the mixed plastic content and the presence in the e-plastics of flame retardants (FR), of which two classes in particular, the brominated flame retardants (BFR) and organo-phosphorus flame retardants (OPFR), have associated health concerns. The major goal of this project is to investigate the possibility of direct reuse of e-plastics in compression molding. Preliminary data generated have identified a molding procedure that yields remanufactured e-plastics having a tensile strength of 29.3 MPa. This moderate strength level is suspected to be due to inclusions of plastic bits that did not melt and internal voids from out-gassing. Handheld X-ray fluorescence (XRF) was utilized to characterize elemental components in the e-plastics tested for compression molding. Several high “hits” for Br were found that could not be predicted visually. The preliminary XRF data for BFR and OPFR in this work are helpful for environmental and occupational hazard assessments of compression molding activities. Additionally, methods are suggested to characterize the metals, BFR, and OPFR content of the e-plastics using several different additional laboratory analytical techniques to determine the suitability for cost-effective and easy-to-use technologies.
C00|Challenges in Creating Evidence in Environmental Health Risk Assessments: The Example of Second-Hand Smoke|Public health interventions are directed to influence the (state of a) risk factor, either by behavioral or environmental changes. Therefore, environmental health risk assessments are highly relevant for public health decision making and policy development. The credibility of an environmental health risk assessment depends, to a large extent, on the strength of the scientific evidence on which it is based. In this article, the main challenges for assessing the impact of a potential adverse health effect from an environmental pollutant are described. Second-hand smoke (SHS) was chosen to illustrate the current state of evidence. The assessment of the impact of potential adverse health effects from environmental risk factors is dependent on several issues, such as the hypothesized health outcome, the nature of the exposure, the dose-response-relationship and the variability and susceptibility of the exposed population. The example of SHS exposure highlights the need for evidence-based public health. Several challenges in terms of study design, assessment methods, as well as data analysis and synthesis with respect to the stratification of results, and consideration of bias and confounding exist. Future research needs to take into account which methods and techniques will be used to generate evidence for population-level decisions.
C00|Practical Eco-Design and Eco-Innovation of Consumer Electronics—the Case of Mobile Phones|Annually, it is estimated that about 4 billion units of consumer electronics for mobile communications are produced worldwide. This could lead to various ecological imbalances unless the design and disposal of the products are handled optimally. To illustrate how industry looks at and responds to the increasing social awareness, this article describes how sustainability is successfully implemented in practice at a large Chinese company, developing and producing various kinds of electronic products used for communication. It also describes how a variety of eco-innovations and business models contribute to reducing the environmental impact; for example, through increased recovery and recycling. A new kind of eco-design procedure is presented along with a new methodology which shows how a mobile phone gradually becomes more sustainable from one generation to the next. The issues with and set-up of new eco-labeling schemes for mobile phones, eco-rating, is described in detail. The conclusion is that due to high competition between companies, the industry acts resourcefully and a lot is done to the save the ecological environment.
C00|Use of Bacteriocinogenic Cultures without Inhibiting Cheese Associated Nonstarter Lactic Acid Bacteria; A Trial with Lactobacillus plantarum|Bacteriocinogenic cultures can represent a natural way to increase the safety of cheeses made from raw milk, in which a relevant role in ripening and flavor formation is exerted by the nonstarter lactic acid bacteria (NSLAB). Since the latter can be inhibited by bacteriocin producers, this study evaluated to which extent a nisinogenic culture inoculated at low initial levels can affect the growth rate and peptide degradation activity of the nisin-sensitive cheese isolate Lactobacillus plantarum LZ by comparison with its isogenic variant, L. plantarum LZNI, with increased immunity to nisin. A growth delay of the nisin sensitive strain was observed only when its initial number was 100-fold lower than the nisin producer and nisin was added as an inducer of its own production. In this case, the amount of free α-amino groups was significantly different between cultures of L. plantarum LZ and LZNI only at Day 1. Reverse Phase High Performance Liquid Chromatography (RP-HPLC) highlighted a few differences between the peptide profiles of co-cultures L. plantarum LZ and LZNI. However, results showed that the bacteriocin producer did not dramatically influence the behavior of the sensitive NSLAB and that the evaluation of the effects on microbial contaminants in cheese is worthwhile.
C00|Bridging the Gap between Eco-Design and the Human Thinking System|Technological progress has enabled widespread adoption and use of consumer electronics, changing how global society lives and works. This progress has come with immense environmental cost, including extraction of scarce materials, consumption of fossil fuels, and growing e-waste challenges. Eco-design has emerged as a promising approach to reduce the environmental footprint of electronics by integrating sustainability-oriented decisions early in the product realization process. However, most approaches focus on the product itself, not on the consumer who ultimately decides how to purchase, use, maintain, and dispose of the device. This article presents a new framework to guide designers in developing products with features that encourage consumers to use them in an environmentally sustainable manner. The Sustainable Behavior Design (SBD) framework links common design concepts (ergonomic, emotional, preventative, and interaction design) with core aspects of the human thinking system to create features to make users aware of their behavior and decisions (reflective thinking) or result in sustainable behaviors even when users are unaware (automatic thinking). The SBD framework is demonstrated using a case study on a smartphone, a high demand product. The reimagined smartphone design integrates solutions addressing both automatic and reflective thinking systems, potentially reducing life cycle impacts by almost 30%.
C00|On Energy Resources, Climate Change, Fossil Fuels, and Drilling. Challenges in Tackling Problems by Governments in Europe and USA|This month in Italy a public consultation will decide (by majority, in case 50% of voters will participate) whether to renew the ongoing concessions between Italian government and drilling companies at the end of their contracts, even if the fossil fuels have not been extracted completely.[...]
C00|Fossil Fuels, Let’s Leave Them under Earth. Four Reasons to Vote “Yes” at the Italian Referendum on Drilling|The referendum that will be held on 17 April 2016 is calling Italians to express their willingness on an aspect of licensing the sea drilling activities: The end of the licenses to the offshore exploitation of fossil fuel resources within the 12 miles from the coast.
C00|Policies Supporting Renewable Energies Uses: The Next Big Challenge|The question in the referendum on 17 April is: “At the end of the concessions presently authorized for extraction plants in the sea, at a distance within 12 km from the coast of Italy, are you in favor of stopping the extraction, even if there is still some methane or oil to be extracted?”.[...]
C00|Modeling Autonomous Decision-Making on Energy and Environmental Management Using Petri-Net: The Case Study of a Community in Bandung, Indonesia|Autonomous decision-making in this study is defined as the process where decision-makers have the freedom and ability to find problems, select goals, and make decisions for achieving the selected problems/goals by themselves. Autonomous behavior is considered significant for achieving decision implementation, especially in the context of energy and environmental management, where multiple stakeholders are involved and each stakeholder holds valuable local information for making decisions. This paper aims to build a structured process in modeling the autonomous decision-making. A practical decision-making process in waste-to-energy conversion activities in a community in Bandung, Indonesia, is selected as a case study. The decision-making process here is considered as a discrete event system, which is then represented as a Petri-net model. First, the decision-making process in the case study is decomposed into discrete events or decision-making stages, and the stakeholders’ properties in each stage are extracted from the case study. Second, several stakeholder properties that indicate autonomous behavior are identified as autonomous properties. Third, presented is a method to develop the decision-making process as a Petri-net model. The model is utilized for identifying the critical points for verifying the performance of the derived Petri-net.
C00|Validation of a Miniaturized Spectrometer for Trace Detection of Explosives by Surface-Enhanced Raman Spectroscopy|Surface-enhanced Raman spectroscopy (SERS) measurements of some common military explosives were performed with a table-top micro-Raman system integrated with a Serstech R785 miniaturized device, comprising a spectrometer and detector for near-infrared (NIR) laser excitation (785 nm). R785 was tested as the main component of a miniaturized SERS detector, designed for in situ and stand-alone sensing of molecules released at low concentrations, as could happen in the case of traces of explosives found in an illegal bomb factory, where solid microparticles of explosives could be released in the air and then collected on the sensor’s surface, if placed near the factory, as a consequence of bomb preparation. SERS spectra were obtained, exciting samples in picogram quantities on specific substrates, starting from standard commercial solutions. The main vibrational features of each substance were clearly identified also in low quantities. The amount of the sampled substance was determined through the analysis of scanning electron microscope images, while the spectral resolution and the detector sensitivity were sufficiently high to clearly distinguish spectra belonging to different samples with an exposure time of 10 s. A principal component analysis procedure was applied to the experimental data to understand which are the main factors affecting spectra variation across different samples. The score plots for the first three principal components show that the examined explosive materials can be clearly classified on the basis of their SERS spectra.
C00|Putting Soil Security on the Policy Agenda: Need for a Familiar Framework|Soils generate agricultural, environmental, and socio-economic benefits that are vital to human life. The enormity of threats to global soil stocks raises the imperative for securing this vital resource. To contribute to the security framing and advancement of the soil security concept and discourse, this paper provides a working definition and proposes dimensions that can underpin the conceptualization of soil security. In this paper, soil security refers to safeguarding and improving the quality, quantity and functionality of soil stocks from critical and pervasive threats in order to guarantee the availability, access, and utilization of soils to sustainably generate productive goods and ecosystem services. The dimensions proposed are availability, accessibility, utilization, and stability, which are obviously similar to the dimensions of food security. Availability refers to the quality and spatial distribution of soils of a given category. Accessibility relates to the conditions or mechanisms by which actors negotiate and gain entitlements to occupy and use a given soil. Utilization deals with the use or purpose to which a given soil is put and the capacity to manage and generate optimal private and public benefits from the soil. Finally, stability refers to the governance mechanisms that safeguard and improve the first three dimensions. These dimensions, their interactions, and how they can be operationalized in a strategy to secure soils are presented and discussed.
C00|Soft Energy Paths Revisited: Politics and Practice in Energy Technology Transitions|This paper argues that current efforts to study and advocate for a change in energy technologies to reduce their climate and other environmental impacts often ignore the political, social, and bodily implications of energy technology choices. Framing renewable energy technologies exclusively in terms of their environmental benefits dismisses important questions about how energy infrastructures can be designed to correspond to democratic forms of socio-politics, forms of social organization that involve independence in terms of meeting energy needs, resilience in terms of adapting to change, participatory decision making and control, equitable distribution of knowledge and efficacy, and just distribution of ownership. Recognizing technological choices as political choices brings explicit attention to the kinds of socio-political restructuring that could be precipitated through a renewable energy technology transition. This paper argues that research on energy transitions should consider the political implications of technological choices, not just the environmental consequences. Further, emerging scholarship on energy practices suggests that social habits of energy usage are themselves political, in that they correspond to and reinforce particular arrangements of power. Acknowledging the embedded politics of technology, as the decades’ old concept of soft path technologies encourages, and integrating insights on the politics of technology with insights on technological practices, can improve future research on energy policy and public perceptions of energy systems. This paper extends insights regarding the socio-political implications of energy paths to consider how understandings of energy technologies as constellations of embedded bodily practices can help further develop our understanding of the consequences of energy technologies, consequences that move beyond environmental implications to the very habits and behaviors of patterned energy usage, which are themselves arguably political. This paper calls for future research that involves explicit examination of the relationship between technologies, socio-political distributions of power and access to energy resources, the social organization of energy practices, and options for energy transitions not just in terms of energy source, but also in terms of scale, design, and modes of ownership and control.
C00|Field Prototype of the ENEA Neutron Active Interrogation Device for the Detection of Dirty Bombs|The Italian National Agency for New Technologies, Energy, and Sustainable Economic Development (ENEA) Neutron Active Interrogation (NAI) device is a tool designed to improve CBRNE defense. It is designed to uncover radioactive and nuclear threats including those in the form of Improvised Explosive Devices (IEDs), the so-called “dirty bombs”. The NAI device, at its current development stage, allows to detect 6 g of 235 U hidden in a package. It is easily transportable, light in weight, and with a real-time response. Its working principle is based on two stages: (1) an “active” stage in which neutrons are emitted by a neutron generator to interact with the item under inspection, and (2) a “passive” stage in which secondary neutrons are detected originating a signal that, once processed, allows recognition of the offence. In particular, a clear indication of the potential threat is obtained by a dedicated software based on the Differential Die-Away Time Analysis method.
C00|Optimization of Plating Conditions for the Determination of Polonium Using Copper Foils|The technique of adsorption of polonium onto metal surfaces by spontaneous deposition has found applications in the analysis of environmental samples such as marine sediments, foodstuff, water, and tobacco since the 1960s. Silver foil has been preferred by many scientists but can become quite expensive for routine analysis. Deposition onto copper was first proposed in the 1970s, but has remained poorly studied. In the present study, the cost-effective and rapid optimum conditions necessary for the optimal recovery of Po from aqueous solutions during spontaneous deposition onto copper foils was evaluated while minimizing the deposition of Bi and Pb, which may interfere with subsequent analyses. A series of experiments was performed to determine adsorption yields for Po, Bi, and Pb to copper foils for a range of pH values from 1.0 to 5.5, with and without stable Bi and Pb carriers. Different methods for cleaning the copper foils were also compared. After initial measurements, Po, Bi, and Pb were desorbed from the disc in plating solutions without added activity. At higher pH values (3.0 and 5.5), less Bi was adsorbed to the copper foils, and subsequent desorption removed up to 99.1% of the plated Bi. The polonium yield remained fairly constant at all pH values and was unaffected by the desorption process. There was also no measureable increase in the polonium activity after 33 days, suggesting that Bi and Pb were not significantly co-deposited. All three cleaning methods performed well, whereas uncleaned foils in the same solution showed limited uptake. The use of copper foil under the optimum conditions described here could provide a valuable alternative to the use of silver in 210 Po analyses.
C00|Challenges in Specialty Coffee Processing and Quality Assurance|Coffee is an important crop that assures a sustainable economy to farmers in tropical regions. A dramatic concern for coffee production is currently represented by climate change, which threatens the survival of Coffea arabica cultivation worldwide and imposes modifications of the agronomic practices to prevent this risk. The quality of coffee beans depends on optimized protocols of cultivation, ripe berries collection, and removal of the outer fruit layers by dry or wet processes and moisture reduction. Storage and shipment represent two steps where bean quality needs to be preserved by preventing fungal contamination that may impact the final product and form mycotoxins, mainly ochratoxin A. In this review, we describe the challenges faced by the coffee industry to guarantee quality from production to roasting and brewing. An overview of novel technologies, such as the application of starter cultures in fermentation and the exploitation of industrial enzymes in accelerating the process of flavour development in coffee beans, is given. Moreover, the results of studies on microbial populations on coffee and the differences found in fungi, yeasts and bacteria composition among the investigations, are summarized. In particular, this review describes new attempts to contain the development of mycotoxigenic fungi, through the application of antagonistic microorganisms such as S. cerevisiae . The new wave of specialty coffees, i.e., those with a cupping score higher than 85/100, is also presented. It is shown how, through careful coffee production methods and controlled fermentation processes, coffee producers may increase their income by assuring high standards of quality and high added value for the coffee experience sector.
C00|Technical Problem Identification for the Failures of the Liberty Ships|The U.S. Liberty Ship Building Program in World War II set a record—a total of 2700 Liberty Ships were built in 6 years, in order to support the battle against Nazi-Germany. However, numerous vessels suffered sudden fracture, some of them being split in half. This paper demonstrates and investigation of the Liberty Ships failure and problems, which reveals that the failures are caused by a combination of three factors. The welds produced by largely unskilled work force contain crack type flaws. Beyond these cracks, another important reason for failure associated with welding is the hydrogen embitterment; most of the fractures initiate at deck square hatch corners where there is a stress concentration; and the ship steel has fairly poor Charpy-Impact tested fracture toughness. It has been admitted that, although the numerous catastrophic failures were a painful experience, the failures of the Liberty Ships caused significant progress in the study of fracture mechanics. Considering their effect, the Liberty Ships are still a success.
C00|A Linear Bayesian Updating Model for Probabilistic Spatial Classification|Categorical variables are common in spatial data analysis. Traditional analytical methods for deriving probabilities of class occurrence, such as kriging-family algorithms, have been hindered by the discrete characteristics of categorical fields. To solve the challenge, this study introduces the theoretical backgrounds of the linear Bayesian updating (LBU) model for spatial classification through an expert system. The main purpose of this paper is to present the solid theoretical foundations of the LBU approach. Since the LBU idea is originated from aggregating expert opinions and is not restricted to conditional independent assumption (CIA), it may prove to be reasonably adequate for analyzing complex geospatial data sets, such as remote sensing images or area-class maps.
C00|Innovations in Detection of Deliberate or Accidental Contamination with Biological Agents in Environment and Foods|In 2016, two special issues were launched and attended in this journal, “Challenges in New Technologies for Security” (http://www.mdpi.com/journal/challenges/special_issues/tech_ security) and “Food Microbiology: Technologies and processes, microbiology analysis methods, and antimicrobials” (http://www.mdpi.com/journal/challenges/special_issues/food-microbiology). In this editorial, I will review one of the topics of biological hazards detection, namely, the rapid analysis of biological agents (DNA and antigens of fungi, bacteria, and viruses) and their toxins.[...]
C00|Stand-Off Device for Plastic Debris Recognition in Post-Blast Scenarios|The fast analysis of crime scenes is a very critical issue for investigators that should collect, as much as possible, all and only meaningful evidence, and rapidly bring back to normality the involved area. With the scope to respond to the end user’s requirements, the project FORLAB (Forensic Laboratory for in-situ evidence analysis in a post blast scenario) has set, as its main goal, to develop a system of sensors for fast screening of post-blast scenes. In this frame, a new sensor based on laser induced fluorescence has been developed for standoff individuation and localization of plastic debris in post-blast scenarios. The system can scan large areas in short times (in some cases, minutes) providing real-time images of the scene where material discrimination is highlighted. In fact, the combined use of a laser source with a high repetition rate and of a signal collection setup based on a fixed intensified charged coupled device (ICCD) with a large field of view has allowed for the brief duration of the scanning process. In addition, dedicated software elaborates the fluorescence data obtained from the targets and retrieves a chemical characterization useful for material recognition.
C00|Early Warning of Biological Threats via Surface-Enhanced Raman Spectroscopy: A Case Study of Bacillus Spores|A study on the application of surface-enhanced Raman spectroscopy (SERS) in detecting biological threats is here reported. Simulants of deadly Bacillus anthracis endospores were used. This study proposes an automated device where SERS is used as a fast, pre-alarm technique of a two-stage sensor equipped with a real-time polymerase chain reaction (PCR). In order to check the potentialities of SERS in terms of sensitivity and specificity for on-site, real-time, automatic detection and identification of biological agents, two strains of genetically and harmless closely B. anthracis -related spores, Bacillus thuringiensis and Bacillus atrophaeus , were used as simulants. In order to assure the selectivity of the SERS substrate against B. thuringiensis spores, the substrate was functionalized by specific peptides. The obtained SERS measurements are classified as positive or negative hits by applying a special data evaluation based on the Euclidian distance between each spectrum and a reference spectrum of blank measurement. Principal component analysis (PCA) was applied for discriminating between different strains representing dangerous and harmless spores. The results show that the SERS sensor is capable of detecting a few tenths of spores in a few minutes, and is particularly sensitive and fast for this purpose. Post-process analysis of the spectra allowed for discrimination between the contaminated and uncontaminated SERS sensors and even between different strains of spores, although not as clearly. For this purpose, the use of a non-functionalized SERS substrate is suggested.
C00|Sequentially Adaptive Bayesian Learning for a Nonlinear Model of the Secular and Cyclical Behavior of US Real GDP|There is a one-to-one mapping between the conventional time series parameters of a third-order autoregression and the more interpretable parameters of secular half-life, cyclical half-life and cycle period. The latter parameterization is better suited to interpretation of results using both Bayesian and maximum likelihood methods and to expression of a substantive prior distribution using Bayesian methods. The paper demonstrates how to approach both problems using the sequentially adaptive Bayesian learning algorithm and sequentially adaptive Bayesian learning algorithm (SABL) software, which eliminates virtually of the substantial technical overhead required in conventional approaches and produces results quickly and reliably. The work utilizes methodological innovations in SABL including optimization of irregular and multimodal functions and production of the conventional maximum likelihood asymptotic variance matrix as a by-product.
C00|Parallelization Experience with Four Canonical Econometric Models Using ParMitISEM|This paper presents the parallel computing implementation of the MitISEM algorithm, labeled Parallel MitISEM . The basic MitISEM algorithm provides an automatic and flexible method to approximate a non-elliptical target density using adaptive mixtures of Student- t densities, where only a kernel of the target density is required. The approximation can be used as a candidate density in Importance Sampling or Metropolis Hastings methods for Bayesian inference on model parameters and probabilities. We present and discuss four canonical econometric models using a Graphics Processing Unit and a multi-core Central Processing Unit version of the MitISEM algorithm. The results show that the parallelization of the MitISEM algorithm on Graphics Processing Units and multi-core Central Processing Units is straightforward and fast to program using MATLAB. Moreover the speed performance of the Graphics Processing Unit version is much higher than the Central Processing Unit one.
C00|Evolutionary Sequential Monte Carlo Samplers for Change-Point Models|Sequential Monte Carlo (SMC) methods are widely used for non-linear filtering purposes. However, the SMC scope encompasses wider applications such as estimating static model parameters so much that it is becoming a serious alternative to Markov-Chain Monte-Carlo (MCMC) methods. Not only do SMC algorithms draw posterior distributions of static or dynamic parameters but additionally they provide an estimate of the marginal likelihood. The tempered and time (TNT) algorithm, developed in this paper, combines (off-line) tempered SMC inference with on-line SMC inference for drawing realizations from many sequential posterior distributions without experiencing a particle degeneracy problem. Furthermore, it introduces a new MCMC rejuvenation step that is generic, automated and well-suited for multi-modal distributions. As this update relies on the wide heuristic optimization literature, numerous extensions are readily available. The algorithm is notably appropriate for estimating change-point models. As an example, we compare several change-point GARCH models through their marginal log-likelihoods over time.
C00|Bayesian Nonparametric Measurement of Factor Betas and Clustering with Application to Hedge Fund Returns|We define a dynamic and self-adjusting mixture of Gaussian Graphical Models to cluster financial returns, and provide a new method for extraction of nonparametric estimates of dynamic alphas (excess return) and betas (to a choice set of explanatory factors) in a multivariate setting. This approach, as well as the outputs, has a dynamic, nonstationary and nonparametric form, which circumvents the problem of model risk and parametric assumptions that the Kalman filter and other widely used approaches rely on. The by-product of clusters, used for shrinkage and information borrowing, can be of use to determine relationships around specific events. This approach exhibits a smaller Root Mean Squared Error than traditionally used benchmarks in financial settings, which we illustrate through simulation. As an illustration, we use hedge fund index data, and find that our estimated alphas are, on average, 0.13% per month higher (1.6% per year) than alphas estimated through Ordinary Least Squares. The approach exhibits fast adaptation to abrupt changes in the parameters, as seen in our estimated alphas and betas, which exhibit high volatility, especially in periods which can be identified as times of stressful market events, a reflection of the dynamic positioning of hedge fund portfolio managers.
C00|Return and Risk of Pairs Trading Using a Simulation-Based Bayesian Procedure for Predicting Stable Ratios of Stock Prices|We investigate the direct connection between the uncertainty related to estimated stable ratios of stock prices and risk and return of two pairs trading strategies: a conditional statistical arbitrage method and an implicit arbitrage one. A simulation-based Bayesian procedure is introduced for predicting stable stock price ratios, defined in a cointegration model. Using this class of models and the proposed inferential technique, we are able to connect estimation and model uncertainty with risk and return of stock trading. In terms of methodology, we show the effect that using an encompassing prior, which is shown to be equivalent to a Jeffreys’ prior, has under an orthogonal normalization for the selection of pairs of cointegrated stock prices and further, its effect for the estimation and prediction of the spread between cointegrated stock prices. We distinguish between models with a normal and Student t distribution since the latter typically provides a better description of daily changes of prices on financial markets. As an empirical application, stocks are used that are ingredients of the Dow Jones Composite Average index. The results show that normalization has little effect on the selection of pairs of cointegrated stocks on the basis of Bayes factors. However, the results stress the importance of the orthogonal normalization for the estimation and prediction of the spread—the deviation from the equilibrium relationship—which leads to better results in terms of profit per capital engagement and risk than using a standard linear normalization.
C00|Timing Foreign Exchange Markets|To improve short-horizon exchange rate forecasts, we employ foreign exchange market risk factors as fundamentals, and Bayesian treed Gaussian process (BTGP) models to handle non-linear, time-varying relationships between these fundamentals and exchange rates. Forecasts from the BTGP model conditional on the carry and dollar factors dominate random walk forecasts on accuracy and economic criteria in the Meese-Rogoff setting. Superior market timing ability for large moves, more than directional accuracy, drives the BTGP’s success. We explain how, through a model averaging Monte Carlo scheme, the BTGP is able to simultaneously exploit smoothness and rough breaks in between-variable dynamics. Either feature in isolation is unable to consistently outperform benchmarks throughout the full span of time in our forecasting exercises. Trading strategies based on ex ante BTGP forecasts deliver the highest out-of-sample risk-adjusted returns for the median currency, as well as for both predictable, traded risk factors.
C00|The Evolving Transmission of Uncertainty Shocks in the United Kingdom|This paper investigates if the impact of uncertainty shocks on the U.K. economy has changed over time. To this end, we propose an extended time-varying VAR model that simultaneously allows the estimation of a measure of uncertainty and its time-varying impact on key macroeconomic and financial variables. We find that the impact of uncertainty shocks on these variables has declined over time. The timing of the change coincides with the introduction of inflation targeting in the U.K.
C00|Bayesian Calibration of Generalized Pools of Predictive Distributions|Decision-makers often consult different experts to build reliable forecasts on variables of interest. Combining more opinions and calibrating them to maximize the forecast accuracy is consequently a crucial issue in several economic problems. This paper applies a Bayesian beta mixture model to derive a combined and calibrated density function using random calibration functionals and random combination weights. In particular, it compares the application of linear, harmonic and logarithmic pooling in the Bayesian combination approach. The three combination schemes, i.e ., linear, harmonic and logarithmic, are studied in simulation examples with multimodal densities and an empirical application with a large database of stock data. All of the experiments show that in a beta mixture calibration framework, the three combination schemes are substantially equivalent, achieving calibration, and no clear preference for one of them appears. The financial application shows that the linear pooling together with beta mixture calibration achieves the best results in terms of calibrated forecast.
C00|Spatial Econometrics: A Rapidly Evolving Discipline|Spatial econometrics has a relatively short history in the scenario of the scientific thought. Indeed, the term “spatial econometrics” was introduced only forty years ago during the general address delivered by Jean Paelinck to the annual meeting of the Dutch Statistical Association in May 1974 (see [1]). [...]
C00|Forecasting Value-at-Risk under Different Distributional Assumptions|Financial asset returns are known to be conditionally heteroskedastic and generally non-normally distributed, fat-tailed and often skewed. These features must be taken into account to produce accurate forecasts of Value-at-Risk (VaR). We provide a comprehensive look at the problem by considering the impact that different distributional assumptions have on the accuracy of both univariate and multivariate GARCH models in out-of-sample VaR prediction. The set of analyzed distributions comprises the normal, Student, Multivariate Exponential Power and their corresponding skewed counterparts. The accuracy of the VaR forecasts is assessed by implementing standard statistical backtesting procedures used to rank the different specifications. The results show the importance of allowing for heavy-tails and skewness in the distributional assumption with the skew-Student outperforming the others across all tests and confidence levels.
C00|A Conditional Approach to Panel Data Models with Common Shocks|This paper studies the effects of common shocks on the OLS estimators of the slopes’ parameters in linear panel data models. The shocks are assumed to affect both the errors and some of the explanatory variables. In contrast to existing approaches, which rely on using results on martingale difference sequences, our method relies on conditional strong laws of large numbers and conditional central limit theorems for conditionally-heterogeneous random variables.
C00|Acknowledgement to Reviewers of Econometrics in 2015|The editors of Econometrics would like to express their sincere gratitude to the following reviewers for assessing manuscripts in 2015. [...]
C00|Functional-Coefficient Spatial Durbin Models with Nonparametric Spatial Weights: An Application to Economic Growth|This paper considers a functional-coefficient spatial Durbin model with nonparametric spatial weights. Applying the series approximation method, we estimate the unknown functional coefficients and spatial weighting functions via a nonparametric two-stage least squares (or 2SLS) estimation method. To further improve estimation accuracy, we also construct a second-step estimator of the unknown functional coefficients by a local linear regression approach. Some Monte Carlo simulation results are reported to assess the finite sample performance of our proposed estimators. We then apply the proposed model to re-examine national economic growth by augmenting the conventional Solow economic growth convergence model with unknown spatial interactive structures of the national economy, as well as country-specific Solow parameters, where the spatial weighting functions and Solow parameters are allowed to be a function of geographical distance and the countries’ openness to trade, respectively.
C00|Multiple Discrete Endogenous Variables in Weakly-Separable Triangular Models|We consider a model in which an outcome depends on two discrete treatment variables, where one treatment is given before the other. We formulate a three-equation triangular system with weak separability conditions. Without assuming assignment is random, we establish the identification of an average structural function using two-step matching. We also consider decomposing the effect of the first treatment into direct and indirect effects, which are shown to be identified by the proposed methodology. We allow for both of the treatment variables to be non-binary and do not appeal to an identification-at-infinity argument.
C00|Volatility Forecasting: Downside Risk, Jumps and Leverage Effect|We provide empirical evidence of volatility forecasting in relation to asymmetries present in the dynamics of both return and volatility processes. Using recently-developed methodologies to detect jumps from high frequency price data, we estimate the size of positive and negative jumps and propose a methodology to estimate the size of jumps in the quadratic variation. The leverage effect is separated into continuous and discontinuous effects, and past volatility is separated into “good” and “bad”, as well as into continuous and discontinuous risks. Using a long history of the S & P500 price index, we find that the continuous leverage effect lasts about one week, while the discontinuous leverage effect disappears after one day. “Good” and “bad” continuous risks both characterize the volatility persistence, while “bad” jump risk is much more informative than “good” jump risk in forecasting future volatility. The volatility forecasting model proposed is able to capture many empirical stylized facts while still remaining parsimonious in terms of the number of parameters to be estimated.
C00|Computational Complexity and Parallelization in Bayesian Econometric Analysis|Challenging statements have appeared in recent years in the literature on advances in computational procedures.[...]
C00|A Method for Measuring Treatment Effects on the Treated without Randomization|This paper contributes to the literature on the estimation of causal effects by providing an analytical formula for individual specific treatment effects and an empirical methodology that allows us to estimate these effects. We derive the formula from a general model with minimal restrictions, unknown functional form and true unobserved variables such that it is a credible model of the underlying real world relationship. Subsequently, we manipulate the model in order to put it in an estimable form. In contrast to other empirical methodologies, which derive average treatment effects, we derive an analytical formula that provides estimates of the treatment effects on each treated individual. We also provide an empirical example that illustrates our methodology.
C00|Recovering the Most Entropic Copulas from Preliminary Knowledge of Dependence|This paper provides a new approach to recover relative entropy measures of contemporaneous dependence from limited information by constructing the most entropic copula (MEC) and its canonical form, namely the most entropic canonical copula (MECC). The MECC can effectively be obtained by maximizing Shannon entropy to yield a proper copula such that known dependence structures of data (e.g., measures of association) are matched to their empirical counterparts. In fact the problem of maximizing the entropy of copulas is the dual to the problem of minimizing the Kullback-Leibler cross entropy (KLCE) of joint probability densities when the marginal probability densities are fixed. Our simulation study shows that the proposed MEC estimator can potentially outperform many other copula estimators in finite samples.
C00|Unit Root Tests: The Role of the Univariate Models Implied by Multivariate Time Series|In cointegration analysis, it is customary to test the hypothesis of unit roots separately for each single time series. In this note, we point out that this procedure may imply large size distortion of the unit root tests if the DGP is a VAR. It is well-known that univariate models implied by a VAR data generating process necessarily have a finite order MA component. This feature may explain why an MA component has often been found in univariate ARIMA models for economic time series. Thereby, it has important implications for unit root tests in univariate settings given the well-known size distortion of popular unit root test in the presence of a large negative coefficient in the MA component. In a small simulation experiment, considering several popular unit root tests and the ADF sieve bootstrap unit tests, we find that, besides the well known size distortion effect, there can be substantial differences in size distortion according to which univariate time series is tested for the presence of a unit root.
C00|Distribution of Budget Shares for Food: An Application of Quantile Regression to Food Security 1|This study examines, using quantile regression, the linkage between food security and efforts to enhance smallholder coffee producer incomes in Rwanda. Even though in Rwanda smallholder coffee producer incomes have increased, inhabitants these areas still experience stunting and wasting. This study examines whether the distribution of the income elasticity for food is the same for coffee and noncoffee growing provinces. We find that that the share of expenditures on food is statistically different in coffee growing and noncoffee growing provinces. Thus, the increase in expenditure on food is smaller for coffee growing provinces than noncoffee growing provinces.
C00|Building a Structural Model: Parameterization and Structurality|A specific concept of structural model is used as a background for discussing the structurality of its parameterization. Conditions for a structural model to be also causal are examined. Difficulties and pitfalls arising from the parameterization are analyzed. In particular, pitfalls when considering alternative parameterizations of a same model are shown to have lead to ungrounded conclusions in the literature. Discussions of observationally equivalent models related to different economic mechanisms are used to make clear the connection between an economically meaningful parameterization and an economically meaningful decomposition of a complex model. The design of economic policy is used for drawing some practical implications of the proposed analysis.
C00|Bayesian Bandwidth Selection for a Nonparametric Regression Model with Mixed Types of Regressors|This paper develops a sampling algorithm for bandwidth estimation in a nonparametric regression model with continuous and discrete regressors under an unknown error density. The error density is approximated by the kernel density estimator of the unobserved errors, while the regression function is estimated using the Nadaraya-Watson estimator admitting continuous and discrete regressors. We derive an approximate likelihood and posterior for bandwidth parameters, followed by a sampling algorithm. Simulation results show that the proposed approach typically leads to better accuracy of the resulting estimates than cross-validation, particularly for smaller sample sizes. This bandwidth estimation approach is applied to nonparametric regression model of the Australian All Ordinaries returns and the kernel density estimation of gross domestic product (GDP) growth rates among the organisation for economic co-operation and development (OECD) and non-OECD countries.
C00|Stable-GARCH Models for Financial Returns: Fast Estimation and Tests for Stability|A fast method for estimating the parameters of a stable-APARCH not requiring likelihood or iteration is proposed. Several powerful tests for the (asymmetric) stable Paretian distribution with tail index 1 < α < 2 are used for assessing the appropriateness of the stable assumption as the innovations process in stable-GARCH-type models for daily stock returns. Overall, there is strong evidence against the stable as the correct innovations assumption for all stocks and time periods, though for many stocks and windows of data, the stable hypothesis is not rejected.
C00|Removing Specification Errors from the Usual Formulation of Binary Choice Models|We develop a procedure for removing four major specification errors from the usual formulation of binary choice models. The model that results from this procedure is different from the conventional probit and logit models. This difference arises as a direct consequence of our relaxation of the usual assumption that omitted regressors constituting the error term of a latent linear regression model do not introduce omitted regressor biases into the coefficients of the included regressors.
C00|Continuous and Jump Betas: Implications for Portfolio Diversification|Using high-frequency data, we decompose the time-varying beta for stocks into beta for continuous systematic risk and beta for discontinuous systematic risk. Estimated discontinuous betas for S&P500 constituents between 2003 and 2011 generally exceed the corresponding continuous betas. We demonstrate how continuous and discontinuous betas decrease with portfolio diversification. Using an equiweighted broad market index, we assess the speed of convergence of continuous and discontinuous betas in portfolios of stocks as the number of holdings increase. We show that discontinuous risk dissipates faster with fewer stocks in a portfolio compared to its continuous counterpart.
C00|Testing Symmetry of Unknown Densities via Smoothing with the Generalized Gamma Kernels|This paper improves a kernel-smoothed test of symmetry through combining it with a new class of asymmetric kernels called the generalized gamma kernels. It is demonstrated that the improved test statistic has a normal limit under the null of symmetry and is consistent under the alternative. A test-oriented smoothing parameter selection method is also proposed to implement the test. Monte Carlo simulations indicate superior finite-sample performance of the test statistic. It is worth emphasizing that the performance is grounded on the first-order normal limit and a small number of observations, despite a nonparametric convergence rate and a sample-splitting procedure of the test.
C00|Evaluating Eigenvector Spatial Filter Corrections for Omitted Georeferenced Variables|The Ramsey regression equation specification error test (RESET) furnishes a diagnostic for omitted variables in a linear regression model specification ( i.e. , the null hypothesis is no omitted variables). Integer powers of fitted values from a regression analysis are introduced as additional covariates in a second regression analysis. The former regression model can be considered restricted, whereas the latter model can be considered unrestricted; this first model is nested within this second model. A RESET significance test is conducted with an F -test using the error sums of squares and the degrees of freedom for the two models. For georeferenced data, eigenvectors can be extracted from a modified spatial weights matrix, and included in a linear regression model specification to account for the presence of nonzero spatial autocorrelation. The intuition underlying this methodology is that these synthetic variates function as surrogates for omitted variables. Accordingly, a restricted regression model without eigenvectors should indicate an omitted variables problem, whereas an unrestricted regression model with eigenvectors should result in a failure to reject the RESET null hypothesis. This paper furnishes eleven empirical examples, covering a wide range of spatial attribute data types, that illustrate the effectiveness of eigenvector spatial filtering in addressing the omitted variables problem for georeferenced data as measured by the RESET.
C00|Estimation of Gini Index within Pre-Specified Error Bound|Gini index is a widely used measure of economic inequality. This article develops a theory and methodology for constructing a confidence interval for Gini index with a specified confidence coefficient and a specified width without assuming any specific distribution of the data. Fixed sample size methods cannot simultaneously achieve both specified confidence coefficient and fixed width. We develop a purely sequential procedure for interval estimation of Gini index with a specified confidence coefficient and a specified margin of error. Optimality properties of the proposed method, namely first order asymptotic efficiency and asymptotic consistency properties are proved under mild moment assumptions of the distribution of the data.
C00|Market Microstructure Effects on Firm Default Risk Evaluation|Default probability is a fundamental variable determining the credit worthiness of a firm and equity volatility estimation plays a key role in its evaluation. Assuming a structural credit risk modeling approach, we study the impact of choosing different non parametric equity volatility estimators on default probability evaluation, when market microstructure noise is considered. A general stochastic volatility framework with jumps for the underlying asset dynamics is defined inside a Merton-like structural model. To estimate the volatility risk component of a firm we use high-frequency equity data: market microstructure noise is introduced as a direct effect of observing noisy high-frequency equity prices. A Monte Carlo simulation analysis is conducted to (i) test the performance of alternative non-parametric equity volatility estimators in their capability of filtering out the microstructure noise and backing out the true unobservable asset volatility; (ii) study the effects of different non-parametric estimation techniques on default probability evaluation. The impact of the non-parametric volatility estimators on risk evaluation is not negligible: a sensitivity analysis defined for alternative values of the leverage parameter and average jumps size reveals that the characteristics of the dataset are crucial to determine which is the proper estimator to consider from a credit risk perspective.
C00|Measuring the Distance between Sets of ARMA Models|A distance between pairs of sets of autoregressive moving average (ARMA) processes is proposed. Its main properties are discussed. The paper also shows how the proposed distance finds application in time series analysis. In particular it can be used to evaluate the distance between portfolios of ARMA models or the distance between vector autoregressive (VAR) models.
C00|Econometrics Best Paper Award 2016|Econometrics has had a distinguished start publishing over 92 articles since 2013, with 76,475 downloads.[...]
C00|Jump Variation Estimation with Noisy High Frequency Financial Data via Wavelets|This paper develops a method to improve the estimation of jump variation using high frequency data with the existence of market microstructure noises. Accurate estimation of jump variation is in high demand, as it is an important component of volatility in finance for portfolio allocation, derivative pricing and risk management. The method has a two-step procedure with detection and estimation. In Step 1, we detect the jump locations by performing wavelet transformation on the observed noisy price processes. Since wavelet coefficients are significantly larger at the jump locations than the others, we calibrate the wavelet coefficients through a threshold and declare jump points if the absolute wavelet coefficients exceed the threshold. In Step 2 we estimate the jump variation by averaging noisy price processes at each side of a declared jump point and then taking the difference between the two averages of the jump point. Specifically, for each jump location detected in Step 1, we get two averages from the observed noisy price processes, one before the detected jump location and one after it, and then take their difference to estimate the jump variation. Theoretically, we show that the two-step procedure based on average realized volatility processes can achieve a convergence rate close to O P ( n − 4 / 9 ) , which is better than the convergence rate O P ( n − 1 / 4 ) for the procedure based on the original noisy process, where n is the sample size. Numerically, the method based on average realized volatility processes indeed performs better than that based on the price processes. Empirically, we study the distribution of jump variation using Dow Jones Industrial Average stocks and compare the results using the original price process and the average realized volatility processes.
C00|Special Issues of Econometrics: Celebrated Econometricians|Econometrics is pleased to announce the commissioning of a new series of Special Issues dedicated to celebrated econometricians of our time.[...]
C00|Nonparametric Regression with Common Shocks|This paper considers a nonparametric regression model for cross-sectional data in the presence of common shocks. Common shocks are allowed to be very general in nature; they do not need to be finite dimensional with a known (small) number of factors. I investigate the properties of the Nadaraya-Watson kernel estimator and determine how general the common shocks can be while still obtaining meaningful kernel estimates. Restrictions on the common shocks are necessary because kernel estimators typically manipulate conditional densities, and conditional densities do not necessarily exist in the present case. By appealing to disintegration theory, I provide sufficient conditions for the existence of such conditional densities and show that the estimator converges in probability to the Kolmogorov conditional expectation given the sigma-field generated by the common shocks. I also establish the rate of convergence and the asymptotic distribution of the kernel estimator.
C00|Generalized Fractional Processes with Long Memory and Time Dependent Volatility Revisited|In recent years, fractionally-differenced processes have received a great deal of attention due to their flexibility in financial applications with long-memory. This paper revisits the class of generalized fractionally-differenced processes generated by Gegenbauer polynomials and the ARMA structure (GARMA) with both the long-memory and time-dependent innovation variance. We establish the existence and uniqueness of second-order solutions. We also extend this family with innovations to follow GARCH and stochastic volatility (SV). Under certain regularity conditions, we give asymptotic results for the approximate maximum likelihood estimator for the GARMA-GARCH model. We discuss a Monte Carlo likelihood method for the GARMA-SV model and investigate finite sample properties via Monte Carlo experiments. Finally, we illustrate the usefulness of this approach using monthly inflation rates for France, Japan and the United States.
C00|Econometric Information Recovery in Behavioral Networks|In this paper, we suggest an approach to recovering behavior-related, preference-choice network information from observational data. We model the process as a self-organized behavior based random exponential network-graph system. To address the unknown nature of the sampling model in recovering behavior related network information, we use the Cressie-Read (CR) family of divergence measures and the corresponding information theoretic entropy basis, for estimation, inference, model evaluation, and prediction. Examples are included to clarify how entropy based information theoretic methods are directly applicable to recovering the behavioral network probabilities in this fundamentally underdetermined ill posed inverse recovery problem.
C00|Estimation of Dynamic Panel Data Models with Stochastic Volatility Using Particle Filters|Time-varying volatility is common in macroeconomic data and has been incorporated into macroeconomic models in recent work. Dynamic panel data models have become increasingly popular in macroeconomics to study common relationships across countries or regions. This paper estimates dynamic panel data models with stochastic volatility by maximizing an approximate likelihood obtained via Rao-Blackwellized particle filters. Monte Carlo studies reveal the good and stable performance of our particle filter-based estimator. When the volatility of volatility is high, or when regressors are absent but stochastic volatility exists, our approach can be better than the maximum likelihood estimator which neglects stochastic volatility and generalized method of moments (GMM) estimators.
C00|Editorial Announcement|I am pleased to announce that, following my retirement on the 30th September 2016, Marc Paolella will become Editor-in-Chief (EiC) of Econometrics.
C00|Oil Price and Economic Growth: A Long Story?|This study investigates changes in the relationship between oil prices and the US economy from a long-term perspective. Although neither of the two series (oil price and GDP growth rates) presents structural breaks in mean, we identify different volatility periods in both of them, separately. From a multivariate perspective, we do not observe a significant effect between changes in oil prices and GDP growth when considering the full period. However, we find a significant relationship in some subperiods by carrying out a rolling analysis and by investigating the presence of structural breaks in the multivariate framework. Finally, we obtain evidence, by means of a time-varying VAR, that the impact of the oil price shock on GDP growth has declined over time. We also observe that the negative effect is greater at the time of large oil price increases, supporting previous evidence of nonlinearity in the relationship.
