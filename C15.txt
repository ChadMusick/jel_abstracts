C15|Inference for Local Distributions at High Sampling Frequencies: A Bootstrap Approach|"We study inference for the local innovations of It^o semimartingales. Specifically, we construct a resampling procedure for the empirical CDF of high-frequency innovations that have been standardized using a nonparametric estimate of its stochastic scale (volatility) and truncated to rid the effect of ""large"" jumps. Our locally dependent wild bootstrap (LDWB) accommodate issues related to the stochastic scale and jumps as well as account for a special block-wise dependence structure induced by sampling errors. We show that the LDWB replicates first and second-order limit theory from the usual empirical process and the stochastic scale estimate, respectively, as well as an asymptotic bias. Moreover, we design the LDWB sufficiently general to establish asymptotic equivalence between it and and a nonparametric local block bootstrap, also introduced here, up to second-order distribution theory. Finally, we introduce LDWB-aided Kolmogorov-Smirnov tests for local Gaussianity as well as local von-Mises statistics, with and without bootstrap inference, and establish their asymptotic validity using the second-order distribution theory. The finite sample performance of CLT and LDWB-aided local Gaussianity tests are assessed in a simulation study as well as two empirical applications. Whereas the CLT test is oversized, even in large samples, the size of the LDWB tests are accurate, even in small samples. The empirical analysis verifies this pattern, in addition to providing new insights about the distributional properties of equity indices, commodities, exchange rates and popular macro finance variables."
C15|Edgeworth expansion for Euler approximation of continuous diffusion processes|In this paper we present the Edgeworth expansion for the Euler approximation scheme of a continuous diffusion process driven by a Brownian motion. Our methodology is based upon a recent work [22], which establishes Edgeworth expansions associated with asymptotic mixed normality using elements of Malliavin calculus. Potential applications of our theoretical results include higher order expansions for weak and strong approximation errors associated to the Euler scheme, and for studentized version of the error process.
C15|Asymptotic Theory And Wild Bootstrap Inference With Clustered Errors|We study asymptotic inference based on cluster-robust variance estimators for regression models with clustered errors, focusing on the wild cluster bootstrap and the ordinary wild bootstrap. We stateconditions under which both asymptotic and bootstrap tests and confidence intervals will be asymptotically valid. These conditions put limits on the rates at which the cluster sizes can increase as the number of clusters tends to infinity. To include power in the analysis, we allow the data to be generated under sequences of local alternatives. Under a somewhat stronger set of conditions, we also derive formal Edgeworth expansions for the asymptotic and bootstrap test statistics. Simulation experiments illustrate the theoretical results, and the Edgeworth expansions explain the overrejection of the asymptotic test and shed light on the choice of auxiliary distribution for the wild bootstrap.
C15|Steady-state growth|We compute steady-state economic growth - defined as the rate of growth that the economy would converge to in the absence of new shocks. This rate can be computed in real-time by means of a parsimonious time-varying parameter (TVP) VAR model. Our procedure offers a relatively agnostic estimation of benchmark equilibrium growth rates. Estimates show that the steady-state GDP growth rate in the case of the United States declined from just above 3% per year in the 1990s to 2.4% at present. Results for other six advanced economies and the euro area indicate that the steady-state growth rate, which is consistent with stable inflation and financial conditions, has been relatively stable since 2010 in most cases in spite of a recent slowdown in actual GDP growth rates.
C15|Multivariate Bayesian Predictive Synthesis in Macroeconomic Forecasting|We present new methodology and a case study in use of a class of Bayesian predictive synthesis (BPS) models for multivariate time series forecasting. This extends the foundational BPS framework to the multivariate setting, with detailed application in the topical and challenging context of multi-step macroeconomic forecasting in a monetary policy setting. BPS evaluates– sequentially and adaptively over time– varying forecast biases and facets of miscalibration of individual forecast densities for multiple time series, and– critically– their time-varying interdependencies. We define BPS methodology for a new class of dynamic multivariate latent factor models implied by BPS theory. Structured dynamic latent factor BPS is here motivated by the application context– sequential forecasting of multiple US macroeconomic time series with forecasts generated from several traditional econometric time series models. The case study highlights the potential of BPS to improve of forecasts of multiple series at multiple forecast horizons, and its use in learning dynamic relationships among forecasting models or agents.
C15|Multivariate Bayesian Predictive Synthesis in Macroeconomic Forecasting|We present new methodology and a case study in use of a class of Bayesian predictive synthesis (BPS) models for multivariate time series forecasting. This extends the foundational BPS framework to the multivariate setting, with detailed application in the topical and challenging context of multi-step macroeconomic forecasting in a monetary policy setting. BPS evaluates - sequentially and adaptively over time – varying forecast biases and facets of miscalibration of individual forecast densities for multiple time series, and – critically – their time-varying interdependencies. We define BPS methodology for a new class of dynamic multivariate latent factor models implied by BPS theory. Structured dynamic latent factor BPS is here motivated by the application context–sequential forecasting of multiple US macroeconomic time series with forecasts generated from several traditional econometric time series models. The case study highlights the potential of BPS to improve of forecasts of multiple series at multiple forecast horizons, and its use in learning dynamic relationships among forecasting models or agents.
C15|Density Forecasting|This paper reviews different methods to construct density forecasts and to aggregate forecasts from many sources. Density evaluation tools to measure the accuracy of density forecasts are reviewed and calibration methods for improving the accuracy of forecasts are presented. The manuscript provides some numerical simulation tools to approximate predictive densities with a focus on parallel computing on graphical process units. Some simple examples are proposed to illustrate the methods.
C15|The mirror does not lie: Endogenous ?scal limits for Slovakia|We study the interactions among ?scal policy, ?scal limits and the associated sovereign risk premium. The ?scal limit distribution, which measures the ability of the government to service its debt, arises endogenously from dynamic Laffer curves. We assume a feedback loop between the ?scal limit distribution and the risk premium and determine them simultaneously using and ef?cient iterative scheme. A nonlinear relationship between the sovereign risk premium and the level of government debt then emerges in equilibrium. The model is calibrated to Slovak data assuming steeply growing age-related transfers and volatile business cycle. We study the impact of various model parameters on the conditional (state-dependent) and unconditional distributions of the ?scal limit. Fiscal limit distributions obtained via Markov–Chain–Monte–Carlo regime switching algorithm depend on the rate of growth of government transfers, the degree of countercyclicality of policy, and the distribution of the underlying economic conditions. We ?nd that both distributions are considerably more heavy-tailed compared with those usually obtained in the literature for advanced economies, and are very sensitive to the size and rate of growth of transfers, the business cycle phase and the ?scal policy credibility. The main policy message is that the Maastricht debt limit of 60 percent of GDP is not safe enough for Slovakia. Furthermore, credible reforms reining in age-related spending and thus stabilising public ?nance in the long-run, should be a priority.
C15|A Performance Analysis of Some New Meta-Analysis Estimators Designed to Correct Publication Bias|Publication selection bias is widely recognized as a serious challenge to the validity of meta-analyses. This study analyses the performance of three new estimators designed to correct publication bias: the weighted average of the adequately powered (WAAP) estimator of Stanley et al. (2017), and two estimators proposed by Andrews & Kasy (2019), which we call AK1 and AK2. With respect to bias, we find that none of these is consistently superior to the commonly used PET-PEESE estimator. With respect to mean squared error, we find that Andrews & Kasey’s AK1 estimator does consistently better than other estimators except when publication bias is focused solely on the sign, as opposed to the significance, of an effect. With respect to coverage rates, we find that all the estimators perform consistently poorly, so that hypothesis tests about the mean true effect are unreliable. We also find that effect heterogeneity generally worsens estimator performance, and that its adverse impact compounds with greater heterogeneity. This is particularly of concern for meta-analyses in business and economics, where I2 values, a measure of heterogeneity, are often 90 percent or higher. Finally, we find that the type of simulation environment used in the Monte Carlo experiments significantly impacts estimator performance. A better understanding of what makes an “appropriate” simulation environment for analysing meta-analysis estimators would be a potentially productive subject for future research.
C15|Mostly Harmless Simulations? Using Monte Carlo Studies for Estimator Selection|We consider two recent suggestions for how to perform an empirically motivated Monte Carlo study to help select a treatment effect estimator under unconfoundedness. We show theoretically that neither is likely to be informative except under restrictive conditions that are unlikely to be satisfied in many contexts. To test empirical relevance, we also apply the approaches to a real-world setting where estimator performance is known. Both approaches are worse than random at selecting estimators which minimise absolute bias. They are better when selecting estimators that minimise mean squared error. However, using a simple bootstrap is at least as good and often better. For now researchers would be best advised to use a range of estimators and compare estimates for robustness.
C15|Sup-ADF-style bubble-detection methods under test|In this paper we analyze the performance of supremum augmented Dickey-Fuller (SADF), generalized SADF (GSADF), and backward SADF (BSADF) tests, as introduced by Phillips et al. (International Economic Review 56:1043-1078, 2015) for detecting and date-stamping financial bubbles. In Monte Carlo simulations, we show that the SADF and GSADF tests may reveal substantial size distortions under typical financial-market characteristics (like the empirically well-documented leverage effect). We consider the rational bubble specification suggested by Rotermann and Wilfl ing (Applied Economics Letters 25:1091-1096, 2018) that is able to generate realistic stock-price dynamics (in terms of level trajectories and volatility paths). Simulating stock-price trajectories that contain these parametric bubbles, we demonstrate that the SADF and GSADF tests can have extremely low power under a wide range of bubble-parameter constellations. In an empirical analysis, we use NASDAQ data covering a time-span of 45 years and find that the outcomes of the bubble date-stamping procedure (based on the BSADF test) are sensitive to the data-frequency chosen by the econometrician.
C15|Explorando los cambios de la pobreza en Argentina: 2003-2015|Este trabajo explora los cambios en la pobreza de ingresos en Argentina durante el período 2003-2015 utilizando diversas metodologías de descomposición. Los resultados sugieren que la mejora generalizada en el mercado laboral luego de la crisis fue el principal factor detrás de la fuerte caída de la pobreza en el período 2003-2007. En contraste, la reducción de la pobreza durante la etapa 2007-2011 está asociada al papel más activo que tomó la política social. Finalmente, durante el período 2011-2015 ningún factor contribuyó significativamente a la reducción de la pobreza. This paper explores the changes in income poverty in Argentina during the period 2003-2015 using various decomposition methodologies. The results suggest that the generalized improvement in the labor market after the crisis was the main factor behind the sharp fall in poverty in the period 2003-2007. In contrast, the reduction of poverty between 2007 and 2011 is associated with the most active role played by social policy. Finally, during the period 2011-2015, no factor could contribute significantly to the reduction of poverty
C15|On the Use of Spectral Value Decomposition for the Construction of Composite Indices|High dimensional composite index makes experts’ preferences in set-ting weights a hard task. In the literature, one of the approaches to derive weights from a data set is Principal Component or Factor Analysis that, although conceptually different, they are similar in results when FA is based on Spectral Value Decomposition and rotation is not performed. This works motivates theoretical reasons to derive the weights of the elementary indicators in a composite index when multiple components are retained in the analysis. By Monte Carlo simulation it offers, moreover, the best strategy to identify the number of components to retain.
C15|Variable Annuities: Underlying Risks and Sensitivities|This paper presents a quantitative model designed to understand the sensitivity of variable annuity (VA) contracts to market and actuarial assumptions and how these sensitivities make them a potentially important source of risk to insurance companies during times of stress. VA contracts often include long dated guarantees of market performance that expose the insurer to multiple nondiversifiable risks. Our modeling framework employs a Monte Carlo simulation of asset returns and policyholder behavior to derive fair prices for variable annuities in a risk neutral framework and to estimate sensitivities of reserve requirements under a real‐world probability measure. Simulated economic scenarios are applied to four hypothetical insurance company VA portfolios to assess the sensitivity of portfolio pricing and reserve levels to portfolio characteristics, modelling choices, and underlying economic assumptions. Additionally, a deterministic stress scenario, modeled on Japan beginning in the mid‐90s, is used to estimate the potential impact of a severe, but plausible, economic environment on the four hypothetical portfolios. The main findings of this exercise are: (1) interactions between market risk modeling assumptions and policyholder behavior modeling assumptions can significantly impact the estimated costs of providing guarantees, (2) estimated VA prices and reserve requirements are sensitive to market price discontinuities and multiple shocks to asset prices, (3) VA prices are very sensitive to assumptions related to interest rates, asset returns, and policyholder behavior, and (4) a drawn‐out period of low interest rates and asset underperformance, even if not accompanied by dramatic equity losses, is likely to result in significant losses in VA portfolios.
C15|Projections on the sustainability of the pension system in Romania|The social pension system is a matter of particular complexity for any country and national authorities. National and international statistical and forecasting institutions, as well as research institutions, assume the complexity of the pension system as a major scientific and professional challenge for identifying social protection phenomena and designing a sustainable pension system.In this context, the Institute of Financial Studies (ISF) in Bucharest takes over this rebellion of concerns, proposing a series of studies on the sustainability of the Romanian pension system. Fortunately, our intention is to support previous studies conducted by demographics and social protection teams, as well as projections freely provided by relevant bodies (Eurostat, US Census Bureau, INS, CNSP, CNPP, EFOR, etc.).In this first ISF study we summarize, in the first part, the specialized literature, especially Romanian, with approaches to the specificities of pensions in Romania compared to the European countries, in the second part, the demographic evolution and tendencies in Romania, in the third part and fourth, the projections on the number of pensioners versus the number of taxpayers and, respectively, the projection of the financial balance of the pension system in Romania. Our studyassumes, with appreciation, the updating of the many previous projections of the established institutions and the attempt to explain contextually demographic and social protection phenomena.
C15|Subsampling Sequential Monte Carlo for Static Bayesian Models|We show how to speed up Sequential Monte Carlo (SMC) for Bayesian inference in large data problems by data subsampling. SMC sequentially updates a cloud of particles through a sequence of distributions, beginning with a distribution that is easy to sample from such as the prior and ending with the posterior distribution. Each update of the particle cloud consists of three steps: reweighting, resampling, and moving. In the move step, each particle is moved using a Markov kernel and this is typically the most computation- ally expensive part, particularly when the dataset is large. It is crucial to have an efficient move step to ensure particle diversity. Our article makes two important contributions. First, in order to speed up the SMC computation, we use an approximately unbiased and efficient annealed likelihood estimator based on data subsampling. The subsampling approach is more memory effi- cient than the corresponding full data SMC, which is an advantage for parallel computation. Second, we use a Metropolis within Gibbs kernel with two con- ditional updates. A Hamiltonian Monte Carlo update makes distant moves for the model parameters, and a block pseudo-marginal proposal is used for the particles corresponding to the auxiliary variables for the data subsampling. We demonstrate the usefulness of the methodology for estimating three gen- eralized linear models and a generalized additive model with large datasets.
C15|Sensitivity Of Goodness Of Fit Indices To Lack Of Measurement Invariance With Categorical Indicators And Many Groups|Using Monte Carlo simulation experiments, this paper examines the performance of popular SEM goodness-of-fit indices, namely CFI, TLI, RMSEA, and SRMR, with respect to a specific task of measurement invariance testing with categorical data and many groups (10-50 groups). Study factors include the number of groups, the level of non-invariance in the data, and the absence/presence of model misspecifications other than non-invariance. In sum, the study design yields a total of 81 conditions. All simulated data sets are analyzed using two popular SEM estimators, MLR and WLSMV. The main contribution of this paper to the methodological literature on cross-cultural survey research is that it produces revised guidelines for evaluating the goodness of fit of invariance MGCFA models with many groups
C15|Inequality and Welfare Dynamics in the Russian Federation during 1994-2015|The Russian Federation offers the unique example of a leading centrally planned economy swiftly transforming itself into a market-oriented economy. This paper offers a comprehensive study of inequality and mobility patterns for Russia, using multiple rounds of the Russian Longitudinal Monitoring Surveys over the past two decades spanning this transition. The findings show rising income levels and decreasing inequality, with the latter being mostly caused by pro-poor growth rather than redistribution. The poorest tercile experienced a growth rate that was more than 10 times that of the richest tercile, leading to less long-term inequality than short-term inequality. The analysis also finds that switching from a part-time job to a full-time job, from a lower-skill job to a higher-skill job, or staying in the formal sector is statistically significantly associated with reduced downward mobility and increased income growth. However, a similar transition from the private sector to the public sector is negatively associated with income growth.
C15|Macroeconomic simulation comparison with a multivariate extension of the Markov Information Criterion|Comparison of macroeconomic simulation models, particularly agent-based models (ABMs), with more traditional approaches such as VAR and DSGE models has long been identified as an important yet problematic issue in the literature. This is due to the fact that many such simulations have been developed following the great recession with a clear aim to inform policy, yet the methodological tools required for validating these models on empirical data are still in their infancy. The paper aims to address this issue by developing and testing a comparison framework for macroeconomic simulation models based on a multivariate extension of the Markov Information Criterion (MIC) originally developed in Barde (2017). The MIC is designed to measure the informational distance between a set of models and some empirical data by mapping the simulated data to the markov transition matrix of the underlying data generating process, and is proven to perform optimally (i.e. the measurement is unbiased in expectation) for all models reducible to a markov process. As a result, not only can the MIC provide an accurate measure of distance solely on the basis of simulated data, but it can do it for a very wide class of data generating processes. The paper first presents the strategies adopted to address the computational challenges that arise from extending the methodology to multivariate settings and validates the extension on VAR and DGSE models. The paper then carries out a comparison of the benchmark ABM of Caiani et al. (2016) and the DGSE framework of Smets and Wouters (2007), which to our knowledge, is the first direct comparison between a macroeconomic ABM and a DGSE model.
C15|Benchmarked Risk Minimizing Hedging Strategies for Life Insurance Policies|Traditional life insurance policies offer no equity investment opportunities for the premium paid, and suffer from low returns over the long insurance terms. Modern equity-linked insurance policies offer equity investment opportunities exposed to equity market risk. To combine the low-risk of traditional policies with the high returns offered by equity-linked policies, we consider insurance policies under the benchmark approach (BA), where the policyholders’ funds are invested in the growth-optimal portfolio and the locally risk-free savings account. Under the BA, life insurance policies can be delivered at their minimal costs, lower than the classical actuarial theory predicts. Due to unhedgeable mortality risk, life insurance policies cannot be fully hedged. In this case benchmarked risk-minimization can be applied to obtain hedging strategies with minimally fluctuating profit and loss processes, where the fluctuations can further be reduced through diversification.
C15|Robustness of Support Vector Machines in Algorithmic Trading on Cryptocurrency Market|This study investigates the profitability of a algorithmic trading strategy based on training SVM model to identify cryptocurrencies with high or low predicted returns. A tail set is defined to be a group of coins whose volatility-adjusted returns are in the highest or lowest quantile. Each cryptocurrency is represented by a set of six technical features. SVM is trained on historical tail sets and tested on the current data. The classifier is chosen to be a nonlinear support vector machine. Portfolio is formed by ranking coins using SVM output. The highest ranked coins are used for long positions to be included in the portfolio for one reallocation period. The following metrics were used to estimate the portfolio profitability: %ARC (the annualized rate of change), %ASD (the annualized standard deviation of daily returns), MDD (the maximum drawdown coefficient), IR1, IR2 (the information ratio coefficients). The performance of the SVM portfolio is compared to the performance of the four benchmark strategies based on the values of the information ratio coefficient IR1 which quantifies the risk-weighted gain. The question on how sensitive the portfolio performance is to the parameters set in the SVM model is also addressed in this study.
C15|Does the inclusion of exposure to volatility into diversified portfolio improve the investment results? Portfolio construction from the perspective of a Polish investor|The main goal of this research is to analyse the investment benefits from an incorporation of the volatility exposure to the diversified portfolio from the perspective of a Polish investor. Volatility, treated as a new asset class, may improve the performance of the portfolio due to its negative correlation with most types of assets. This topic has been widely investigated for the United States and Europe whereas Polish market appears to be not heavily researched and this study may fill this gap. The research covers the period from October 2010 to July 2018 and is performed on the daily close prices. To construct the portfolios, the analysis uses the mean-variance framework and the naïve diversification approach. The comparison of risk-adjusted returns between investments with and without volatility exposure enables to answer the research question about an improvement of the results by the addition of a non-standard asset to the diversified portfolios. The VXX is considered as the proxy for volatility as it is the most popular ETN which follows the volatility index derivatives with the given maturity. To test the robustness of the results, the portfolios are constructed with a broad range of different parameters and assumptions imposed on the optimization procedure.
C15|Hybrid Investment Strategy Based on Momentum and Macroeconomic Approach|The purpose of this research is to test the potential returns and robustness of an automated investment strategy. The strategy is based on momentum and macroeconomic approach, that consists of the technical core – momentum, and the additional macro screening, which is used to determine whether investment signals generate relevant investment opportunities or just technical noise. In order to check whether the macroeconomic factor is the value added to the momentum strategy, the hybrid approach is tested and compared with the simple momentum and the macroeconomic strategy alone and then assessed on a risk-adjusted return basis. The main aim of this paper is to answer the question, whether an investor can gain surplus risk-adjusted returns from merging short-term momentum strategy with the long-term macroeconomic approach. Strategies are based on the data for the selected companies from the S&P500 index in the period ranging from 02/01/1990 to 31/12/2018.
C15|Bootstrap Methods for Inference in the Parks Model|The Parks (1967) estimator is a workhorse for panel data and seemingly unrelated regression equation systems because it allows the incorporation of serial correlation together with heteroskedasticity and cross-sectional correlation. It is efficient both asymptotically and in small samples. Kmenta and Gilbert (1970) and more recently Beck and Katz (1995) note that estimated standard errors are biased downward, often severely. Instead of fixing the Parks standard errors, Beck and Katz abandon the efficient estimator in favor of a Prais-Winston estimator together with “panel corrected standard errors” (PCSE), a procedure that only partially reduces the standard error bias. In this paper we develop both parametric and nonparametric bootstrap approaches to inference that avoid the need to use biased standard errors. We then illustrate the effectiveness of our procedures using Monte Carlo experiments that show that the bootstrap gives rejection probabilities close to the nominal level chosen by the researcher.
C15|Job duration and inequality|As suggested by recent empirical evidence, one of the causes behind the widespread rise of inequality experienced by OECD countries in the last few decades may have been the increased flexibility of labor markets. The authors explore this hypothesis through the analysis of a stock-flow consistent agent-based macroeconomic model able to reproduce with good statistical precision several empirical regularities. To this scope they employ three different sensitivity analysis techniques, which indicate that increasing job contract duration (i.e. decreasing flexibility) has the effect of reducing income and wealth inequality. However, the authors also find that this effect is diminished by tight monetary policy and low credit supply. This result suggests that the final outcome of structural reforms aimed at changing labor flexibility can depend on the macroeconomic environment in which these are implemented.
C15|A Simulation Study for Monotonic Dependence in the Presence of Outliers|This paper aims at examining the performance of a recently proposed measure of dependence – the Monotonic Dependence Coefficient – with respect to classical correlation measures like the Pearson’s product-moment and the Spearman’s rank-order correlation coefficients, using simulated outlier contaminated and non-contaminated datasets as well as a real dataset. The comparison aims at checking how and when these coefficients detect dependence relationships between two variables when outliers are present. Several scenarios are created, contemplating in particular multiple values for the coefficients, multiple outlier contamination percentages, various simulation data patterns, or a combination of these. The basic simulation dataset is generated from a bivariate standard normal distribution. Then, the contaminated data are generated from exponential, power-transformed and lognormal distributions. The main findings tend to favour the Spearman’s rank-order correlation coefficient for most of the scenarios, especially when the contamination is taken into account, whereas MDC performs better than the Spearman’s rank-order correlation coefficient in non-contaminated data.
C15|A Nonlinear Dynamic Factor Model of Health and Medical Treatment|Quantitative assessments of the relationship between health and medical treatment are of great importance to policy makers. However, simply looking at the raw correlation between health and medical care is unlikely to give the right answer because of endogeneity problems. We overcome these problems by formulating and estimating a tractable dynamic factor model of health and medical treatment where individual observed health outcomes are driven by the individual's latent health stock. The dynamics of latent health reflects both exogenous health depreciation and endogenous health investments. Our model allows us to investigate the effect of medical treatment on current health, as well as on future medical treatment and health outcomes. We estimate the model by maximum simulated likelihood and minimum distance methods using a rich longitudinal data set from Italy obtained by merging a number of administrative archives. These data contain detailed information on medical drug use, hospitalization, and mortality for a representative sample of elderly hypertensive patients. Our findings show that medical care consumption is highly correlated over time, and this relationship depends on both permanent and time-varying observed and unobserved heterogeneity. They also show that medical drug use significantly maintains future health levels and prevents transitions to worse health. These results suggest that policies aimed at increasing the awareness and the compliance of hypertensive patients help reduces cardiovascular risks and consequent hospitalization and mortality.
C15|Response surface regressions for critical value bounds and approximate p-values in equilibrium correction models|Single-equation conditional equilibrium correction models can be used to test for the existence of a level relationship among the variables of interest. The distributions of the respective test statistics are nonstandard under the null hypothesis of no such relationship and critical values need to be obtained with stochastic simulations. We compute more than 95 billion F -statistics and 57 billion t-statistics for a large number of specifications of the Pesaran, Shin, and Smith (2001, Journal of Applied Econometrics 16: 289Ð326) bounds test. Our large-scale simulations enable us to draw smooth density functions and to estimate response surface models that improve upon and substantially extend the set of available critical values for the bounds test. Besides covering the full range of possible sample sizes and lag orders, our approach notably allows for any number of variables in the long-run level relationship by exploiting the diminishing effect on the distributions of adding another variable to the model. The computation of approximate p-values enables a fine-grained statistical inference and allows us to quantify the finite-sample distortions from using asymptotic critical values. We find that the bounds test can be easily oversized by more than 5 percentage points in small samples.
C15|Keynesian Models, Detrending, and the Method of Moments|One important question in the Keynesian literature is whether we should detrend data when estimating the parameters of a Keynesian model using the moment method. It has been common in the literature to detrend data in the same way the model is detrended. Doing so works relatively well with linear models, in part because in such a case the information that disappears from the data after the detrending process is usually related to the parameters that also disappear from the detrended model. Unfortunately, in heavy non-linear Keynesian models, parameters rarely disappear from detrended models, but information does disappear from the detrended data. Using a simple real business cycle model, we show that both the moment method estimators of parameters and the estimated responses of endogenous variables to a technological shock can be seriously inaccurate when the data used in the estimation process are detrended. Using a dynamic stochastic general equilibrium model and U.S. data, we show that detrending the data before estimating the parameters may result in a seriously misleading response of endogeneous variables to monetary shocks. We suggest building the moment conditions using raw data, irrespective of the trend observed in the data.
C15|Integer-valued stochastic volatility|We propose a novel class of count time series models, the mixed Poisson integer-valued stochastic volatility models. The proposed specification, which can be considered as an integer-valued analogue of the discrete-time stochastic volatility model, encompasses a wide range of conditional distributions of counts. We study its probabilistic structure and develop an easily adaptable Markov chain Monte Carlo algorithm, based on the Griddy-Gibbs approach that can accommodate any conditional distribution that belongs to that class. We demonstrate that by considering the cases of Poisson and negative binomial distributions. The methodology is applied to simulated and real data.
C15|Optimal Control of the Parameters of the Production Line|The problem of optimal control of the parameters of the production flow line - stocks (work in process) and the rate of processing of objects of labour for a technological operation is considered. The article presents a mathematical formulation of the problem of controlling the parameters of a production line with restrictions on work in progress and the speed of machining parts for each technological operation. The control program is determined by the specified quality criteria. An example of the calculation of the optimal control for the production line parameters is presented.
C15|Распределенная Динамическая Pde-Модель Программного Управления Загрузкой Технологического Оборудования Производственной Линии<BR>[Distributed dynamic PDE-model of a program control by utilization of the technological equipment of production line]|В работе необходимо рассмотреть проектирование системы управления параметрами производственной линии для предприятия с поточным методом организации производства. Методика. Производственная ли-ния предприятия с поточным методом организации производства – это сложная динамическая распреде-ленная система. Технологический маршрут изготовления изделия для многих современных предприятий содержит несколько сотен технологических операций, в межоперационном заделе каждой из которых со-держатся тысячи изделий, ожидающих обработку. Технологические маршруты разных деталей одного вида изделий пересекаются. Это приводит к тому, что распределение предметов труда вдоль технологического маршрута оказывает значительное влияние на пропускную способность производственной линии. Для опи-сания таких систем введен новый класс моделей производственных линий (PDE-model). Модели этого клас-са используют уравнения в частных производных для описания поведения потоковых параметров произ-водственной линии. В данной статье построена PDE-модель производственной линии, потоковые парамет-ры которой зависят от величины коэффициента загрузки технологического оборудования для каждой опе-рации.
C15|New Approach to Estimating Gravity Models with Heteroscedasticity and Zero Trade Values|This paper proposes new estimation techniques for gravity models with zero trade values and heteroscedasticity. We revisit the standard PPML estimator and we propose an improved version. We also propose various Heckman estimators with different distributions of the residuals, nonlinear forms of both selection and measure equations, and various process of the variance. We add to the existent literature alternative estimation methods taking into account the non-linearity of both the variance and the selection equation. Moreover, because of the unavailability of pre-set package in the econometrics software (Stata, Eviews, Matlab, etc.) to perform the estimation of the above-mentioned Heckman versions, we had to code it in Matlab using a combination of fminsearch and fminunc functions. Using numerical gradient matrix G, we report standard errors based on the BHHH technique. The proposed new Heckman version could be used in other applications. Our results suggest that previous empirical studies might be overestimating the contribution of the GDP of both import and export countries in determining the bilateral trade.
C15|An alternative probabilistic frontier analysis to the measurement of eco-efficiency|This study applies a nonparametric time dependent conditional frontier model to estimate and evaluate the convergence in eco-efficiency of a group of 51 US states over the period 1990-2017. Specifically, we utilize a mixture of global and local pollutants (carbon dioxide CO2, sulphur dioxide SO2 and nitrogen oxides NOx) to capture the environmental damage caused by the anthropogenic activities. The empirical findings indicate divergence for the whole sample, while specific groups of convergence club regions are formulated dividing the US states into worst and best performers. Moreover, Our findings reveal significant convergence patterns between the US regions over the sample period.
C15|On Trust Dynamics of Economic Growth|Trust among individuals in society may have various economic and social implications. Though, worldwide data on economic growth rarely consider trust as an ingredient in manipulating economic outcomes. Thus, we include trust instigating from individual, affecting community and state thus, forming trust-based economy. In order to explore the relationship of trust with growth and its benefits implications, this study suggests a model which is validated by Markov process. Consequently, results indicate significant impact of trust on economic growth by achieving convergence in very few iterations in the case of trust-based economy. On the other hand, economy with lowest trust level shows delayed convergence and takes around 4 times more iterations to attain equilibrium. Additionally, socio-economic benefits are more visible in a trust-based economy.
C15|The calculation of Solvency Capital Requirement using Copulas|Our aim is to present an alternative methodology to the standard formula imposed to the insurance regulation (the European directive knows as Solvency II) for the calculus of the capital requirements. We want to demonstrate how this formula is now obsolete and how is possible to obtain lower capital requirement through the theory of the copulas, function that are gaining increasing importance in various economic areas. A lower capital requirement involves the advantage for the various insurance companies not to have unproductive capital that can therefore be used for the production of further profits. Indeed the standard formula is adequate only with some particular assumptions, otherwise it can overestimate the capital requirements that are actually needed as the standard formula underestimates the effect of diversification.
C15|Counter sanctions and well-being population of Russia: econometric analyses|This article examines the impact of counter-sanctions on the welfare of Russia’s population. We build a multiple-choice model and calculate the probability of being in a particular group of well-being based on the price (cost) of consumed counter-sanctions goods. The next step is the construction of a structural demand-supply system for estimating similar domestic good’s production elasticities. By knowing elasticity estimates we determine the price response to particular import closure. According to our estimates Russia's counter-sanctions led to an increase in poverty by 2.64 %.
C15|Дискретно-Событийная Модель Расчета Продолжительного Производственного Цикла Изготовления Партии Деталей<BR>[Discrete-Eventing Model Of Calculation Of The Duration Of The Production Cycle Of Manufacturing A Part Of Products]|The method of calculating the duration of the production cycle for manufacturing a batch of parts is considered. The production cycle of manufacturing the batch of parts is one of the main characteristics of the production system. It is used to calculate the important indicators of planning the production activity of the factory. At present, the task of calculating the duration of the production cycle for unsynchronized production lines remains relevant. The task takes on special relevance in the case when the processing time of a work item in a technological operation is a random quantity. The present work is devoted to the analysis of this case. To derive the equation of motion of labour objects for technological operations, a discrete-event model of the production process is used. The structure of the processing time of an object of labour on a technological operation is considered. The source of change in the value of inter-operational stocks at each technological operation is shown. The interrelation of the trajectories of the previous and after subjects of labour is analyzed. The equation of motion of a subject of labour on technological operations is recorded, taking into account the inter-operational stocks. Methods for its solution are proposed. Conditions for the applicability of the obtained results are considered. The analysis of the machine time spent on calculating the duration of the production cycle of manufacturing the batch of products for the factory of the semiconductor industry was carried out. Prospects for research have been determined.
C15|The optimal control problem for output material flow on a conveyor belt with input accumulating bunker|The article is devoted to the synthesis of optimal control of the conveyor belt with the accumulating input bunker. Much attention is given to the model of the conveyor belt with a constant speed of the belt. Simulation of the conveyor belt is carried out in the one-moment approximation using partial differential equations. The conveyor belt is represented as a distributed system. The used PDE-model of the conveyor belt allows determining the state of the flow parameters for a given technological position as a function of time. We consider the optimal control problem for flow parameters of the conveyor belt. The problem consists in ensuring the minimum deviation of the output material flow from a given target amount. The control is carried out by the material flow amount, which comes from the accumulating bunker into the conveyor belt input. In the synthesis of optimal control, we take into account the limitations on the size of the accumulating bunker, as well as on both max and min amounts of control. We construct optimal control of the material flow amount coming from the accumulating bunker. Also, we determine the conditions to switch control modes and estimate time period between the moments of the switching.
C15|Wild Bootstrap and Asymptotic Inference with Multiway Clustering|We study two cluster-robust variance estimators (CRVEs) for regression models with clustering in two dimensions and give conditions under which t-statistics based on each of them yield asymptotically valid inferences. In particular, one of the CRVEs requires stronger assumptions about the nature of the intra-cluster correlations. We then propose several wild bootstrap procedures and state conditions under which they are asymptotically valid for each type of t-statistic. Extensive simulations suggest that using certain bootstrap procedures with one of the t-statistics generally performs very well. An empirical example confirms that bootstrap inferences can differ substantially from conventional ones
C15|When and How to Deal with Clustered Errors in Regression Models|We discuss when and how to deal with possibly clustered errors in linear regression models. Specifically, we discuss situations in which a regression model may plausibly be treated as having error terms that are arbitrarily correlated within known clusters but uncorrelated across them. The methods we discuss include various covariance matrix estimators, possibly combined with various methods of obtaining critical values, several bootstrap procedures, and randomization inference. Special attention is given to models with few treated clusters and clusters that vary in size, where inference may be problematic. Two empirical examples and a simulation experiment illustrate the methods we discuss and the concerns we raise.
C15|Specification Tests for Temporal Heterogeneity in Spatial Panel Models with Fixed Effects|We propose score type tests for testing the existence of temporal heterogeneity in slope and spatial parameters in spatial panel data (SPD) models, allowing for the presence of individual-specific and/or time-specific fixed effects (or in general intercept heterogeneity). The SPD model with spatial lag effect is treated in detail by first considering the model with individual-specific effects only, and then extending it to the model with both individual and time specific effects. Two types of tests (naive and robust) are proposed, and their asymptotic properties are presented. These tests are then fully extended to an SPD model with both spatial lag and spatial error effects. Monte Carlo results show that the robust tests have much superior finite and large sample properties than the naive tests. Thus, the proposed robust tests provide reliable tools for identifying possible existence of temporal heterogeneity in regression and spatial coefficients. Empirical illustrations of the proposed tests are given.
C15|Maximum Likelihood Estimation for the Fractional Vasicek Model|This paper is concerned about the problem of estimating the drift parameters in the fractional Vasicek model from a continuous record of observations. Based on the Girsanov theorem for the fractional Brownian motion, the maximum likelihood (ML) method is used. The asymptotic theory for the ML estimates (MLE) is established in the stationary case, the explosive case, and the null recurrent case for the entire range of the Hurst parameter, providing a complete treatment of asymptotic analysis. It is shown that changing the sign of the persistence parameter will change the asymptotic theory for the MLE, including the rate of convergence and the limiting distribution. It is also found that the asymptotic theory depends on the value of the Hurst parameter.
C15|Estimation and Inference of Fractional Continuous-Time Model with Discrete-Sampled Data|This paper proposes a two-stage method for estimating parameters in a para-metric fractional continuous-time model based on discrete-sampled observations. In the ﬁrst stage, the Hurst parameter is estimated based on the ratio of two second-order diﬀerences of observations from diﬀerent time scales. In the second stage, the other parameters are estimated by the method of moments. All estimators have closed-form expressions and are easy to obtain. A large sample theory of the pro-posed estimators is derived under either the in-ﬁll asymptotic scheme or the double asymptotic scheme. Extensive simulations show that the proposed theory performs well in ﬁnite samples. Two empirical studies are carried out. The ﬁrst, based on the daily realized volatility of equities from 2011 to 2017, shows that the Hurst parameter is much lower than 0.5, which suggests that the realized volatility is too rough for continuous-time models driven by standard Brownian motion or fractional Brownian motion with Hurst parameter larger than 0.5. The second empirical study is of the daily realized volatility of exchange rates from 1986 to 1999. The estimate of the Hurst parameter is again much lower than 0.5. Moreover, the proposed frac-tional continuous-time model performs better than the autoregressive fractionally integrated moving average (ARFIMA) model out-of-sample.
C15|Estimation of Stochastic Frontier Panel Data Models with Spatial Inefficiency|"This paper proposes a stochastic frontier panel data model in which unit-specific inefficiencies are spatially correlated. In particular, this model has simultaneously three important features: i) the total inefficiency of a productive unit depends on its own inefficiency and on the inefficiency of its neighbors; ii) the spatially correlated and time varying inefficiency is disentangled from time invariant unobserved heterogeneity in a panel data model à la Greene (2005); iii) systematic differences in inefficiency can be explained using exogenous determinants. We propose to estimate both the ""true"" fixed- and random-effects variants of the model using a feasible simulated composite maximum likelihood approach. The finite sample behavior of the proposed estimators are investigated through a set of Monte Carlo experiments. Our simulation results suggest that the estimation approach is consistent, showing good finite sample properties especially in small samples."
C15|A nonlinear dynamic factor model of health and medical treatment|Quantitative assessments of the relationship between health and medical treatment are of great importance to policy makers.However, simply looking at the raw correlation between health and medical care is unlikely to give the right answer because of endogeneity problems. We overcome these problems by formulating and estimating a tractable dynamic factor model of health and medical treatment where individual observed health outcomes are driven by the individual’s latent health stock. The dynamics of latent health reflects both exogenous health depreciation and endogenous health investments. Our model allows us to investigate the effect of medical treatment on current health, as well as on future medical treatment and health outcomes. We estimate the model by maximum simulated likelihood and minimum distance methods using a rich longitudinal data set from Italy obtained by merging a number of administrative archives. These data contain detailed information on medical drug use, hospitalization, and mortality for a representative sample of elderly hypertensive patients. Our findings show that medical care consumption is highly correlated over time, and this relationship depends on both permanent and time-varying observed and unobserved heterogeneity. They also show that medical drug use significantly maintains future health levels and prevents transitions to worse health. These results suggest that policies aimed at increasing the awareness and the compliance of of hypertensive patients help reduce cardiovascular risks and consequent hospitalization and mortality.
C15|Factors Affecting the Adoption of Alternative Financing Methods for Startups by Africans in China|Alternative financing methods such as Crowdfunding, and Peer-to-Peer lending in the past few decades are becoming more sort after means of business start-up financing. On the other hand, traditional financing methods such as bank loans are now more difficult to acquire especially across African countries. Taking a sample of 410 Africans in China, this study examines the factors that affect the adoption of alternative financing methods for business start-ups. The results show that internet usage in Africa and China, length of residence in China, and the capital intensity of the type of the business to be started are all significant factors to the preference and adoption of alternative financing methods by Africans residing in China. Seeing as research into Africa?s adoption of alternative financing is lacking, this study, this study provides insight where Africans are concerned.
C15|Transformed Perturbation Solutions for Dynamic Stochastic General Equilibrium Models|This paper introduces a new solution method for Dynamic Stochastic General Equilibrium (DSGE) models that produces non explosive paths. The proposed solution method is as fast as standard perturbation methods and can be easily implemented in existing software packages like Dynare as it is obtained directly as a transformation of existing perturbation solutions proposed by Judd and Guu (1997) and Schmitt-Grohe and Uribe (2004), among others. The transformed perturbation method shares the same advantageous function approximation properties as standard higher order perturbation methods and, in contrast to those methods, generates stable sample paths that are stationary, geometrically ergodic and absolutely regular. Additionally, moments are shown to be bounded. The method is an alternative to the pruning method as proposed in Kim et al. (2008). The advantages of our approach are that, unlike pruning, it does not need to sacrifice accuracy around the steady state by ignoring higher order effects and it delivers a policy function. Moreover, the newly proposed solution is always more accurate globally than standard perturbation methods. We demonstrate the superior accuracy of our method in a range of examples.
C15|Forecast Density Combinations with Dynamic Learning for Large Data Sets in Economics and Finance|A flexible forecast density combination approach is introduced that can deal with large data sets. It extends the mixture of experts approach by allowing for model set incompleteness and dynamic learning of combination weights. A dimension reduction step is introduced using a sequential clustering mechanism that allocates the large set of forecast densities into a small number of subsets and the combination weights of the large set of densities are modelled as a dynamic factor model with a number of factors equal to the number of subsets. The forecast density combination is represented as a large finite mixture in nonlinear state space form. An efficient simulation-based Bayesian inferential procedure is proposed using parallel sequential clustering and filtering, implemented on graphics processing units. The approach is applied to track the Standard & Poor 500 index combining more than 7000 forecast densities based on 1856 US individual stocks that are are clustered in a relatively small subset. Substantial forecast and economic gains are obtained, in particular, in the tails using Value-at-Risk. Using a large macroeconomic data set of 142 series, similar forecast gains, including probabilities of recession, are obtained from multivariate forecast density combinations of US real GDP, Inflation, Treasury Bill yield and Employment. Evidence obtained on the dynamic patterns in the financial as well as macroeconomic clusters provide valuable signals useful for improved modelling and more effective economic and financial policies.
C15|Statistical Tests for Cross-Validation of Kriging Models|We derive new statistical tests for leave-one-out cross-validation of Kriging models. Graphically, we present these tests as scatterplots augmented with confi…dence intervals. We may wish to avoid extrapolation, which we de…fine as prediction of the output for a point that is a vertex of the convex hull of the given input combinations. Moreover, we may use bootstrapping to estimate the true variance of the Kriging predictor. The resulting tests (with or without extrapolation or bootstrapping) have type-I and type-II error probabilities, which we estimate through Monte Carlo experiments. To illustrate the application of our tests, we use an example with two inputs and the popular borehole example with eight inputs.
C15|Copula Multivariate GARCH Model with Constrained Hamiltonian Monte Carlo|The Copula Multivariate GARCH (CMGARCH) model is based on a dynamic copula function with time-varying parameters. It is particularly suited for modelling dynamic dependence of non-elliptically distributed financial returns series. The model allows for capturing more flexible dependence patterns than a multivariate GARCH model and also generalizes static copula dependence models. Nonetheless, the model is subject to a number of parameter constraints that ensure positivity of variances and covariance stationarity of the modeled stochastic processes. As such, the resulting distribution of parameters of interest is highly irregular, characterized by skewness, asymmetry, and truncation, hindering the applicability and accuracy of asymptotic inference. In this paper, we propose Bayesian analysis of the CMGARCH model based on Constrained Hamiltonian Monte Carlo (CHMC), which has been shown in other contexts to yield efficient inference on complicated constrained dependence structures. In the CMGARCH context, we contrast CHMC with traditional random-walk sampling used in the previous literature and highlight the benefits of CHMC for applied researchers. We estimate the posterior mean, median and Bayesian confidence intervals for the coefficients of tail dependence. The analysis is performed in an application to a recent portfolio of S&P500 financial asset returns.
C15|An improved approach for estimating large losses in insurance analytics and operational risk using the g-and-h distribution|In this paper, we study the estimation of parameters for g-and-h distributions. These distributions find applications in modeling highly skewed and fat-tailed data, like extreme losses in the banking and insurance sector. We first introduce two estimation methods: a numerical maximum likelihood technique, and an indirect inference approach with a bootstrap weighting scheme. In a realistic simulation study, we show that indirect inference is computationally more efficient and provides better estimates in case of extreme features of the data. Empirical illustrations on insurance and operational losses illustrate these findings.
C15|A Comparison of Parameter Estimation of Logistic Regression model by Maximum Likelihood, Ridge Regression, Markov Chain Monte Carlo Methods|The goal of this research is to estimate the parameter of logistic regression model. The coefficient parameter is evaluated by maximum likelihood, ridge regression, markov chain monte carlo methods. The logistic regression is considered the correlation between binary dependent variable and 2, 3, and 4 independent variables which is generated from normal distribution, contaminated normal distribution, and t distribution. The maximum likelihood estimator is estimated by differential the log likelihood function with respect to the coefficients. Ridge regression is to choose the unknown ridge parameter by cross-validation, so ridge estimator is evaluated on a form of maximum likelihood method by adding ridge parameter. The markov chain monte carlo estimator can approximate from Gibbs sampling algorithm by the posterior distribution based on a probability distribution and prior probability distribution. The performance of these method is compare by percentage of predicted accuracy value. The results are found that ridge regression are satisfied when the independent variables are simulated from normal distribution, and the maximum likelihood outperforms on the other distributions.
C15|The Effects Of Autocorrelation And Number Of Repeated Measures On Glmm Robustness With Ordinal Data|Longitudinal studies involving ordinal responses are widely conducted in many fields of the education, health and social sciences. In these cases, when units are observed over time, the possibility of auto-correlation between observations on the same subject exists. Therefore the assumption of independence which underlines the generalized linear models is violated. Generalized linear mixed models (GLMMs) accommodate repeated measures data for which the usual assumption of independent observations is untenable, and also accommodate a non-normally distributed dependent variable (i.e. multinomial distribution for ordinal data). Thus, GLMMs constitute a good technique for modelling correlated data and ordinal responses. In this study, for a split-plot design with two groups for the between-subjects factor and five response categories, we investigated empirical Type I error rates in GLMMs. To this end, we used a computer program developed by Wicklin to generate longitudinal ordinal data with SAS/IML. We manipulated the total sample size, the coefficient of variation of the group size, the number of repeated measures, and the values of autocorrelation coefficient. For each combination 5,000 replications were performed at a significance level of .05. The GLIMMIX procedure in SAS was used to fit the mixed-effects models for ordinal responses with multinomial distribution and the Kenward-Roger degrees of freedom adjustment for small samples. The results of simulations showed that the test is robust for group effect under all conditions analysed. For time and interaction effects, however, the robustness depends on the number of repeated measures and autocorrelations values. The test tends to be liberal with high autocorrelation, different values of autocorrelation in each group and large number of repeated measures. To sum up, GLMMs are a good analytical option for correlated ordinal outcomes with few repeated measures, low autocorrelation, and the same autocorrelation between groups.This research was supported by grant PSI2016-78737-P (AEI/FEDER, UE) from the Spanish Ministry of Economy, Industry and Competitiveness.
C15|Which Are The Most Common Distributions In Social, Health, And Education Sciences?|Statistical analysis is crucial for research and the choice of analytical technique should take into account the specific distribution of data. Although the data obtained from health, educational and social sciences research are often not normally distributed, there are very few studies detailing which distributions are most likely to represent data in these disciplines. The aim of the present study was to determine the frequency of appearance of the most common non-normal distributions in the health, educational and social sciences by means of a systematic review. The search was carried out in the Web of Science (WOS) database, from which we retrieved 984 abstracts of papers published between 2010 and 2015. In the final review, 148 papers from the area of health, 18 from education and 96 from the social sciences were included. The selection was performed independently by two reviewers. The inter-rater reliability for article selection and agreement regarding the type of distribution was high. The results showed that distributions from the exponential family are the most common non-normal distributions ? and more specifically, gamma as a continuous distribution and the negative binomial as a discrete distribution. In addition to identifying the most common distributions for real data these results will help researchers to decide which distributions should be used in simulation studies examining statistical procedures.This research was supported by grant PSI2016-78737-P (AEI/FEDER, UE) from the Spanish Ministry of Economy, Industry and Competitiveness.
C15|Agent-based model calibration using machine learning surrogates|Efficiently calibrating agent-based models (ABMs) to real data is an open challenge. This paper explicitly tackles parameter space exploration and calibration of ABMs by combining machine-learning and intelligent iterative sampling. The proposed approach “learns” a fast surrogate meta-model using a limited number of ABM evaluations and approximates the nonlinear relationship between ABM inputs (initial conditions and parameters) and outputs. Performance is evaluated on the Brock and Hommes (1998) asset pricing model and the “Islands” endogenous growth model Fagiolo and Dosi (2003). Results demonstrate that machine learning surrogates obtained using the proposed iterative learning procedure provide a quite accurate proxy of the true model and dramatically reduce the computation time necessary for large scale parameter space exploration and calibration.
C15|Assessing Distributional Properties of Forecast Errors|This paper considers the problem of assessing the distributional properties (normality and symmetry) of macroeconomic forecast errors of G7 countries for the purpose of fan-chart modelling. Test statistics based on a Cramer von-Mises distance are used with critical values obtained via a bootstrap. Our results indicate that the assumption of symmetry of the marginal distribution of forecast errors is reasonable whereas the assumption of normality is not.
C15|Bootstrap-Assisted Tests of Symmetry for Dependent Data|The paper considers the problem of testing for symmetry (about an unknown centre) of the marginal distribution of a strictly stationary and weakly dependent stochastic process. The possibility of using the autoregressive sieve bootstrap and stationary bootstrap procedures to obtain critical values and P-values for symmetry tests is explored. Bootstrap-assisted tests for symmetry are straightforward to implement and require no prior estimation of asymptotic variances. The small-sample properties of a wide variety of tests are investigated using Monte Carlo experiments. A bootstrap-assisted version of the triples test is found to have the best overall performance.
C15|Estimation of a Scale-Free Network Formation Model|Growing evidence suggests that many social and economic networks are scale free in that their degree distribution has a power-law tail. A common explanation for this phenomenon is a random network formation process with preferential attachment. For a general version of such a process, we develop the pseudo maximum likelihood and generalized method of moments estimators. We prove consistency of these estimators by establishing the law of large numbers for growing networks. Simulations suggest that these estimators are asymptotically normally distributed and outperform the commonly used non-linear least squares and Hill (1975) estimators in finite samples. We apply our estimation methodology to a co-authorship network.
C15|Inflation Dynamics in Turkey from a Bayesian Perspective|In this paper, we aim to contribute to the understanding of inflation dynamics in Turkey by estimating a Bayesian VAR (BVAR) model. Our identification strategy is based on a set of zero restrictions and use of exogenous control variables. Main results are as follows: (i) Pass-through from exchange rate to inflation is stronger than that from import prices. Moreover, exchange rate and import price shocks spread over inflation very quickly (most of the adjustment is complete within 9 months), particularly faster for the latter, with the estimates being highly precise (the dispersion around median responses are relatively narrow). (ii) Economic growth has a significant but lagged effect on inflation, yet with a greater uncertainty compared to exchange rate and import price pass-through. (iii) The degree of nominal wage pass-through on inflation is estimated to be close to the degree of exchange rate pass-through, albeit with a longer transmission and a greater uncertainty.
C15|Learning to Average Predictively over Good and Bad: Comment on: Using Stacking to Average Bayesian Predictive Distributions|We suggest to extend the stacking procedure for a combination of predictive densities, proposed by Yao et al in the journal Bayesian Analysis to a setting where dynamic learning occurs about features of predictive densities of possibly misspecified models. This improves the averaging process of good and bad model forecasts. We summarise how this learning is done in economics and finance using mixtures. We also show that our proposal can be extended to combining forecasts and policies. The technical tools necessary for the implementation refer to filtering methods from nonlinear time series and we show their connection with machine learning. We illustrate our suggestion using results from Basturk et al based on financial data about US portfolios from 1928 until 2015.
C15|Forecast density combinations of dynamic models and data driven portfolio strategies|A dynamic asset-allocation model is specified in probabilistic terms as a combination of return distributions resulting from multiple pairs of dynamic models and portfolio strategies based on momentum patterns in US industry returns. The nonlinear state space representation of the model allows efficient and robust simulation-based Bayesian inference using a novel non-linear filter. Combination weights can be cross-correlated and correlated over time using feedback mechanisms. Diagnostic analysis gives insight into model and strategy misspecification. Empirical results show that a smaller flexible model-strategy combination performs better in terms of expected return and risk than a larger basic model-strategy combination. Dynamic patterns in combination weights and diagnostic learning provide useful signals for improved modeling and policy, in particular, from a risk-management perspective.
C15|Prediction for Big Data through Kriging : Small Sequential and One-Shot Designs|"Kriging or Gaussian process (GP) modeling is an interpolation method that assumes the outputs (responses) are more correlated, the closer the inputs (ex- planatory or independent variables) are. A GP has unknown (hyper)parameters that must be estimated; the standard estimation method uses the ""maximum likelihood"" criterion. However, big data make it hard to compute the estimates of these GP parameters, and the resulting Kriging predictor and the variance of this predictor. To solve this problem, some authors select a relatively small subset from the big set of previously observed ""old"" data; their method is se- quential and depends on the variance of the Kriging predictor. The resulting designs turn out to be ""local""; i.e., most design points are concentrated around the point to be predicted. We develop three alternative one-shot methods that do not depend on GP parameters: (i) select a small subset such that this sub- set still covers the original input space–albeit coarser; (ii) select a subset with relatively many— but not all— combinations close to the new combination that is to be predicted, and (iii) select a subset with the nearest neighbors (NNs) of this new combination. To evaluate these designs, we compare their squared prediction errors in several numerical (Monte Carlo) experiments. These experi- ments show that our NN design is a viable alternative for the more sophisticated sequential designs."
C15|Hamiltonian Sequential Monte Carlo with Application to Consumer Choice Behavior|Practical use of nonparametric Bayesian methods requires the availability of efficient algorithms for implementation for posterior inference. The inherently serial nature of Markov Chain Monte Carlo (MCMC) imposes limitations on its efficiency and scalability. In recent years there has been a surge of research activity devoted to developing alternative implementation methods that target parallel computing environments. Sequential Monte Carlo (SMC), also known as a particle filter, has been gaining popularity due to its desirable properties. SMC uses a genetic mutation-selection sampling approach with a set of particles representing the posterior distribution of a stochastic process. We propose to enhance the performance of SMC by utilizing Hamiltonian transition dynamics in the particle transition phase, in place of random walk used in the previous literature. We call the resulting procedure Hamiltonian Sequential Monte Carlo (HSMC). Hamiltonian transition dynamics has been shown to yield superior mixing and convergence properties relative to random walk transition dynamics in the context of MCMC procedures. The rationale behind HSMC is to translate such gains to the SMC environment. We apply both SMC and HSMC to a panel discrete choice model with a nonparametric distribution of unobserved individual heterogeneity. We contrast both methods in terms of convergence properties and show the favorable performance of HSMC.
C15|Testing identifying assumptions in fuzzy regression discontinuity designs| We propose a new specification test for assessing the validity of fuzzy regression discontinuity designs (FRD-validity). We derive a new set of testable implications, characterized by a set of inequality restrictions on the joint distribution of observed outcomes and treatment status at the cut-off. We show that this new characterization exploits all the information in the data useful for detecting violations of FRD-validity. Our approach differs from, and complements existing approaches that test continuity of the distributions of running variables and baseline covariates at the cut-off since ours focuses on the distribution of the observed outcome and treatment status. We show that the proposed test has appealing statistical properties. It controls size in large sample uniformly over a large class of distributions, is consistent against all fixed alternatives, and has non-trivial power against some local alternatives. We apply our test to evaluate the validity of two FRD designs. The test does not reject the FRD-validity in the class size design studied by Angrist and Lavy (1999) and rejects in the insurance subsidy design for poor households in Colombia studied by Miller, Pinto, and Vera-Hernández (2013) for some outcome variables, while existing density tests suggest the opposite in each of the cases.
C15|Estimating Value-at-Risk for the g-and-h distribution: an indirect inference approach|TThe g-and-h distribution is a flexible model with desirable theoretical properties. Especially, it is able to handle well the complex behavior of loss data and it is suitable for VaR estimation when large skewness and kurtosis are at stake. However, parameter estimation is di cult, because the density cannot be written in closed form. In this paper we develop an indirect inference method using the skewed- t distribution as instrumental model. We show that the skewed-t is a well suited auxiliary model and study the numerical issues related to its implementation. A Monte Carlo analysis and an application to operational losses suggest that the indirect inference estimators of the parameters and of the VaR outperform the quantile-based estimators.
C15|Trading Volume, Illiquidity and Commonalities in FX Markets|We provide a unified model for foreign exchange (FX), trading volume, and volatility in a multi-currency environment. Tied by arbitrage conditions, FX rates are determined by common information and trader-specific components generating heterogeneous reservation prices thus inducing trading. Our model outlines new properties including volume-volatility relationships between direct and synthetic FX rates. It also provides a theoretical foundation for commonalities of volume, volatility, and illiquidity across currencies and time, and an intuitive closed-form solution for the price impact measure. Using unique (intraday) data representative for the global FX spot market, the empirical analysis validates our theoretical predictions.
C15|Are long-run output growth rates falling?|This paper studies the evolution of long-run output and labour productivity growth rates in the G-7 countries during the post-war period. We estimate the growth rates consistent with a constant unemployment rate using time-varying parameter models that incorporate both stochastic volatility and a Heckman-type two-step estimation procedure that deals with the possible endogeneity problem in the econometric models. The results show a significant decline in long-run growth rates that is not associated with the detrimental effects of the Great Recession, and that the rate of growth of labour productivity appears to be behind the slowdown in long-run GDP growth.
C15|A new baseline model for estimating willingness to pay from discrete choice models|We show a substantive problem exists with the widely-used ratio of coefficients approach to calculating willingness to pay (WTP) from discrete choice models. The correctly calculated standard error for WTP using this approach is shown to be undefined. This occurs because the cost parameter's standard error implies some possibility the true parameter value is arbitrarily close to zero. We propose a simple yet elegant way to overcome this problem by reparameterizing the (negative) cost variable's coefficient using an exponential transformation to enforce the theoretically correct positive coefficient. With it the confidence interval for WTP is now finite and well behaved.
C15|Momentum and contrarian effects on the cryptocurrency market|We report the results of investigation of the momentum and contrarian effects on cryptocurrency markets. The investigated investment strategies involve 100 (amongst over 1200 present as of date Nov 2017) cryptocurrencies with the largest market cap and average 14-day daily volume exceeding a given threshold value. Investment portfolios are constructed using different assumptions regarding the portfolio reallocation period, width of the ranking window, the number of cryptocurrencies in the portfolio, and the percent transaction costs. The performance is benchmarked against: (1) equally weighted and (2) market-cap weighted investments in all of the ranked assets, as well as against the buy and hold strategies based on (3) S&P500 index, and (4) Bitcoin price. Our results show a clear and significant dominance of the short-term contrarian effect over both momentum effect and the benchmark portfolios. The information ratio coefficient for the contrarian strategies often exceeds two-digit values depending on the assumed reallocation period and the width of the ranking window. Additionally, we observe a significant diversification potential for all cryptocurrency portfolios with relation to the S&P500 index.
C15|Why you should not invest in mining endeavour? The efficiency of BTC mining under current market conditions|The main aim of this paper is to analyse the efficiency of BTC mining under current market conditions. After thorough analysis of initial assumptions concerning the (1) price of mining machine and its effective amortization period, (2) difficulty and hash rate of BTC network, (3) BTC transaction fees and (4) energy costs, we have found that currently BTC mining is not profitable, except for some rare cases. The main reason of this phenomenon is the fast and unpredictable increase of difficulty of BTC network over time which results in decreasing participation of our mining machines in BTC network hash rate. The research is augmented with detailed sensitivity analysis of mining efficiency to initial parameters assumptions, which allows to observe that the conditions for BTC mining to be efficient and profitable are very challenging to meet.
C15|Machine learning in algorithmic trading strategy optimization - implementation and efficiency|The main aim of this paper was to formulate and analyze the machine learning methods, fitted to the strategy parameters optimization specificity. The most important problems are the sensitivity of a strategy performance to little parameter changes and numerous local extrema distributed over the solution space in an irregular way. The methods were designed for the purpose of significant shortening of the computation time, without a substantial loss of a strategy quality. The efficiency of methods was compared for three different pairs of assets in case of moving averages crossover system. The methods operated on the in sample data, containing 20 years of daily prices between 1998 and 2017. The problem was presented for three sets of two assets portfolios. In the first case, a strategy was trading on the SPX and DAX index futures, in the second on the AAPL and MSFT stocks and finally, in the third case on the HGF and CBF commodities futures. The major hypothesis verified in this thesis is that machine learning methods select strategies with evaluation criterion near to the highest one, but in significantly lower execution time than the Exhaustive Search.
C15|Modelling the spreading process of extreme risks via a simple agent-based model: Evidence from the China stock market|This paper focuses on investigating financial asset returns' extreme risks, which are defined as the negative log-returns over a certain threshold. A simple agent-based model is constructed to explain the behavior of the market traders when extreme risks occur. We consider both the volatility clustering and the heavy tail characteristics when constructing the model. Empirical study uses the China securities index 300 daily level data and applies the method of simulated moments to estimate the model parameters. The stationarity and ergodicity tests provide evidence that the proposed model is good for estimation and prediction. The goodness-of-fit measures show that our proposed model fits the empirical data well. Our estimated model performs well in out-of-sample Value-at-Risk prediction, which contributes to the risk management.
C15|To Impute or Not to Impute? A Review of Alternative Poverty Estimation Methods in the Context of Unavailable Consumption Data|There is an increasingly stronger demand for more frequent and accurate poverty estimates, despite the oftentimes unavailable household consumption data. We offer a review of alternative imputation methods that have been employed to provide poverty estimates in such contexts. These range from estimates on a nonmonetary basis, estimates for specific project targeting or tracking trends at the national level, to estimates at a more disaggregated level, as well as estimates of poverty dynamics. We provide a concise and accessible synthesis, which serves as an introduction to the literature. Our focus is on intuition and practical insights that highlight the nuanced differences between the existing methods rather than technical aspects.
C15|Are bootstrapped cointegration test findings unreliable?|Applied time series research often faces the challenge that (a) potentially relevant variables are unobservable, (b) it is fundamentally uncertain which covariates are relevant. Thus cointegration is often analyzed in partial systems, ignoring potential (stationary) covariates. By simulating hypothesized larger systems Benati (2015) found that a nominally significant cointegration outcome using a bootstrapped rank test (Cavaliere, Rahbek, and Taylor, 2012) in the bivariate sub-system might be due to test size distortions. In this note we review this issue systematically. Apart from revisiting the partial-system results we also investigate alternative bootstrap test approaches in the larger system. Throughout we follow the given application of a long-run Phillips curve (euro-area inflation and unemployment). The methods that include the covariates do not reject the null of no cointegration, but by simulation we find that they display very low power, such that the (bivariate) partial-system approach is still preferred. The size distortions of all approaches are only mild when a standard HP-filtered output gap measure is used among the covariates. The bivariate trace test p-value of 0.027 (heteroskedasticity-consistent wild bootstrap) therefore still suggests rejection of non-cointegration at the 5% but not at the 1% significance level. The earlier findings of considerable test size distortions can be replicated when instead an output gap measure with different longer-run developments is used. This detrimental effect of large borderline-stationary roots reflects an earlier insight from the literature (Cavaliere, Rahbek, and Taylor, 2015).
C15|Data Gaps, Data Incomparability, And Data Imputation: A Review Of Poverty Measurement Methods For Data‐Scarce Environments|Questions that often come up in contexts where household consumption data are unavailable or missing include: what are the best existing methods to obtain poverty estimates at a single snapshot in time? and over time? and what are the best available methods to study poverty dynamics? A variety of different techniques have been developed to tackle these questions, but unfortunately, they are presented in different forms and lack unified terminology. We offer a review of poverty imputation methods that address contexts ranging from completely missing and partially missing consumption data in cross‐sectional household surveys, to missing panel household data. We present the various existing methods under a common framework, with pedagogical discussion on their intuition. Empirical illustrations are provided using several rounds of household survey data from Vietnam. Furthermore, we also offer a practical guide with detailed instructions on computer programs that can be used to implement the reviewed techniques.
C15|To impute or not to impute ? a review of alternative poverty estimation methods in the context of unavailable consumption data|There is an increasingly stronger demand for more frequent and accurate poverty estimates, despite the oftentimes unavailable household consumption data. This paper offers a review of alternative imputation methods that have been employed to provide poverty estimates in such contexts. These range from estimates on a nonmonetary basis, estimates for specific project targeting or tracking trends at the national level, to estimates at a more disaggregated level, as well as estimates of poverty dynamics. The paper provides a concise and accessible synthesis, which serves as an introduction to the literature. The focus is on intuition and practical insights that highlight the nuanced differences between the existing methods rather than technical aspects.
C15|Exchange rates expectations and chaotic dynamics: A replication study|In this paper the author analyzes the behavior of exchange rates expectations for four currencies, by considering a re-calculation and an extension of Resende and Zeidan (Expectations and chaotic dynamics: empirical evidence on exchange rates, Economics Letters, 2008). Considering Lyapunov exponent-based tests results, they are not supportive of chaos in exchange rates expectations, although the so-called 0-1 test strongly supports the chaos hypothesis.
C15|Skewness-Adjusted Bootstrap Confidence Intervals and Confidence Bands for Impulse Response Functions|This article investigates the construction of skewness-adjusted confidence intervals and joint confidence bands for impulse response functions from vector autoregressive models. Three different implementations of the skewness adjustment are investigated. The methods are based on a bootstrap algorithm that adjusts mean and skewness of the bootstrap distribution of the autoregressive coefficients before the impulse response functions are computed. Using extensive Monte Carlo simulations, the methods are shown to improve the coverage accuracy in small and medium sized samples and for unit root processes for both known and unknown lag orders.
C15|Distribuciones no normales para la selección de activos en el mercado Colombiano|Modelos tradicionales para la construcción de portafolios son desarrollados sobre distribuciones normales. Un ejemplo de esto es la frontera eficiente propuesta por Markowitz, la cual solo incorpora los dos primeros momentos de la distribución (media-varianza) para la construcción de portafolios eficientes. Lo anterior requiere, al menos, que los portafolios posean una distribución elíptica. Adicionalmente, modelos de serie de tiempo se construyen frecuentemente sobre errores normales. Sin embargo, abundantes estudios rechazan la presencia de normalidad en mercados financieros e incluso literatura reciente ha encontrado que los retornos de índices accionarios colombianos exhiben distribuciones no normales. Considerando la evidencia literaria de distribuciones de los activos del mercado colombiano, modelos que no se restrinjan a distribuciones normales deberían generar mejores asignaciones de portafolio. El presente artículo demuestra que los retornos del mercado colombiano no siguen distribuciones normales y que, a través de la construcción de cópulas t y marginales logísticas e hiperbólicas generalizada, se supera ampliamente los métodos de estimación bajo normalidad. Adicionalmente, el estudio encuentra que la mayoría de activos mantienen una clase de distribución en el tiempo. Finalmente, se concluye que bajo distribuciones no-normales las fronteras eficientes no son necesariamente maximizadoras de utilidad y que el uso distribuciones no-normales generan retornos anuales entre 1% y 3% por encima de los retornos bajo distribuciones normales en el mercado accionario colombiano durante el periodo 2009-2016. Aunque el retorno per se no es el objetivo, el principal hallazgo consiste en que los portafolios construidos mediante supuestos normales, resultan ineficientes en un universo de distribuciones no-normales. Adicionalmente, se observa que los portafolios bajo distribuciones normales no se adecúan correctamente a los niveles de tolerancia por riesgo de los inversionistas, al no calibrar correctamente la distribución de resultados posibles.
C15|Does the choice of estimator matter for forecasting? A revisit|In this study, we further examine whether the choice of estimator matters for forecasting based on the conclusion of Westerlund and Narayan [WN, hereafter] (2012, 2015). A similar but small simulation study was conducted by WN (2012, 2015) to validate the need to account for salient features of predictors such as persistence, endogeneity and conditional heteroscedasticity in a forecast model. In addition to considering a more representative number of observations for high frequency, extensive replications and four competing estimators, we offer alternative functions for these effects and thereafter, we test whether the conclusion of WN (2012, 2015) will still hold. Our results further lend support to the WN (2012, 2015) findings and thus suggest that the choice of estimator matters for forecasting notwithstanding the alternative functions and scenarios considered in our study. Thus, pre-testing the predictors in a forecast model for the mentioned features is required to identify the appropriate estimator to apply.
C15|Prediction Based on Entrepreneurship-Prone Personality Proﬁles: Sometimes Worse Than the Toss of a Coin|The human personality predicts a wide range of activities and occupational choices—from musical sophistication to entrepreneurial careers. However, which method should be applied if information on personality traits is used for prediction and advice? In psychological research, group proﬁles are widely employed. In this contribution, we examine the performance of proﬁles using the example of career prediction and advice, involving a comparison of average trait scores of successful entrepreneurs with the traits of potential entrepreneurs. Based on a simple theoretical model estimated with SOEP data and analyzed with Monte Carlo methods, we show, for the ﬁrst time, that the choice of the comparison method matters substantially. We reveal that under certain conditions the performance of average proﬁles is inferior to the tossing of a coin. Alternative methods, such as directly estimating success probabilities, deliver better performance and are more robust.
C15|Prediction based on entrepreneurship-prone personality profiles: sometimes worse than the toss of a coin|Abstract The human personality predicts a wide range of activities and occupational choices—from musical sophistication to entrepreneurial careers. However, which method should be applied if information on personality traits is used for prediction and advice? In psychological research, group profiles are widely employed. In this contribution, we examine the performance of profiles using the example of career prediction and advice, involving a comparison of average trait scores of successful entrepreneurs with the traits of potential entrepreneurs. Based on a simple theoretical model estimated with GSOEP data and analyzed with Monte Carlo methods, we show, for the first time, that the choice of the comparison method matters substantially. We reveal that under certain conditions the performance of average profiles is inferior to the tossing of a coin. Alternative methods, such as directly estimating success probabilities, deliver better performance and are more robust.
C15|The evolving impact of global, region-specific and country-specific uncertainty|We build a dynamic factor model with time-varying parameters and stochastic volatility and use it to decompose the variance of a large set of financial and macroeconomic variables for 22 OECD countries spanning from 1960 onwards into contributions from country-specific uncertainty, region-specific uncertainty and uncertainty common to all countries. We find that common global uncertainty plays a primary role in explaining the volatility of inflation, interest rates and stock prices, although to a varying extent over time. Region-specific uncertainty drives most of the exchange rate volatility for all Euro Area countries and for countries in North-America and Oceania. All uncertainty estimates (global, regional, country-specific and idiosyncratic) play a non-negligible role for real economic activity, credit and money for most countries. We also find that all uncertainty measures display significant recurrent fluctuations, that the recent peaks in uncertainty found for most estimates around 2008/2009 are comparable to those seen in the mid-1970s and early 1980s, and that all uncertainty measures appear to be strongly countercyclical and positively correlated with inflation. JEL Classification: C15, C32, E32
C15|Econometric Analysis of Productivity: Theory and Implementation in R|Our chapter details a wide variety of approaches used in estimating productivity and efficiency based on methods developed to estimate frontier production using Stochastic Frontier Analysis (SFA) and Data Envelopment Analysis (DEA). The estimators utilize panel, single cross section, and time series data sets. The R programs include such approaches to estimate firm efficiency as the time invariant fixed effects, correlated random effects, and uncorrelated random effects panel stochastic frontier estimators, time varying fixed effects, correlated random effects, and uncorrelated random effects estimators, semi-parametric efficient panel frontier estimators, factor models for cross-sectional and time-varying efficiency, bootstrapping methods to develop confidence intervals for index number-based productivity estimates and their decompositions, DEA and Free Disposable Hull estimators. The chapter provides the professional researcher, analyst, statistician, and regulator with the most up to date efficiency modelling methods in the easily accessible open source programming language R.
C15|Skewness-Adjusted Bootstrap Confidence Intervals and Confidence Bands for Impulse Response Functions|This article investigates the construction of skewness-adjusted confidence intervals and joint confidence bands for impulse response functions from vector autoregressive models. Three different implementations of the skewness adjustment are investigated. The methods are based on a bootstrap algorithm that adjusts mean and skewness of the bootstrap distribution of the autoregressive coefficients before the impulse response functions are computed. Using extensive Monte Carlo simulations, the methods are shown to improve the coverage accuracy in small and medium sized samples and for unit root processes for both known and unknown lag orders.
C15|Asymptotic refinements of a misspecification-robust bootstrap for generalized method of moments estimators|I propose a nonparametric iid bootstrap that achieves asymptotic refinements for t tests and confidence intervals based on GMM estimators even when the model is misspecified. In addition, my bootstrap does not require recentering the moment function, which has been considered as critical for GMM. Regardless of model misspecification, the proposed bootstrap achieves the same sharp magnitude of refinements as the conventional bootstrap methods which establish asymptotic refinements by recentering in the absence of misspecification. The key idea is to link the misspecified bootstrap moment condition to the large sample theory of GMM under misspecification of Hall and Inoue (2003). Two examples are provided: combining data sets and invalid instrumental variables.
C15|Bootstrap Assisted Tests of Symmetry for Dependent Data|TThe paper considers the problem of testing for symmetry (about an unknown centre) of the marginal distribution of a strictly stationary and weakly dependent stochastic process. The possibility of using the autoregressive sieve bootstrap and stationary bootstrap procedures to obtain critical values and P-values for symmetry tests is explored. Bootstrap-assisted tests for symmetry are straightforward to implement and require no prior estimation of asymptotic variances. The small-sample properties of a wide variety of tests are investigated using Monte Carlo experiments. A bootstrap-assisted version of the triples test is found to have the best overall performance.
C15|State Correlation and Forecasting: A Bayesian Approach Using Unobserved Components Models|Implications to signal extraction that arise from specifying unobserved components (UC) models with correlated or orthogonal innovations have been well-investigated. In contrast, an analogous statement for forecasting evaluation cannot be made. This paper attempts to fill this gap in light of the recent resurgence of studies adopting UC models for forecasting purposes. In particular, four correlation structures are entertained: orthogonal, correlated, perfectly correlated innovations as well as a novel approach which combines features from two contrasting cases, namely, orthogonal and perfectly correlated innovations. Parameter space restrictions associated with different correlation structures and their connection with forecasting are discussed within a Bayesian framework. Introducing perfectly correlated innovations, however, reduces the covariance matrix rank. To accommodate that, a Markov Chain Monte Carlo sampler which builds upon properties of Toeplitz matrices and recent advances in precision-based algorithms is developed. Our results for several measures of U.S. inflation indicate that the correlation structure between state variables has important implications for forecasting performance as well as estimates of trend inflation.
C15|Bootstrapping Mean Squared Errors of Robust Small-Area Estimators: Application to the Method-of-Payments Data|This paper proposes a new bootstrap procedure for mean squared errors of robust small-area estimators. We formally prove the asymptotic validity of the proposed bootstrap method and examine its finite sample performance through Monte Carlo simulations. The results show that our procedure performs well and outperforms existing ones. We also apply our procedure to the estimation of the total volume and value of cash, debit card and credit card transactions in Canada as well as in its provinces and subgroups of households. In particular, we find that there is a significant average annual decline rate of 3.1 percent in the volume of cash transactions, and that this decline is relatively higher among high-income households living in heavily populated provinces. Our bootstrap estimator also provides indicators of quality useful in selecting the best small-area predictors from among several alternatives in practice.
C15|Asymmetric Risks to the Economic Outlook Arising from Financial System Vulnerabilities|When financial system vulnerabilities are elevated, they can give rise to asymmetric risks to the economic outlook. To illustrate this, I consider the economic outlook presented in the Bank of Canada’s October 2017 Monetary Policy Report in the context of two key financial system vulnerabilities: high levels of household indebtedness and housing market imbalances. Uncertainty on the profile of consumption by indebted households—and, therefore, risks to growth in gross domestic product (GDP)—arises from higher interest rates and from recent changes to the Office of the Superintendent of Financial Institutions’ B-20 mortgage underwriting guideline. I use non-linear Bayesian techniques to capture the potential amplification of negative shocks in a vulnerable environment. I find that the materialization of larger-than-expected impacts on consumption from higher interest rates and/or the tighter mortgage qualifying criteria would imply asymmetric risks to GDP growth.
C15|Wealth inequality in Italy: reconstruction of 1968-75 data and comparison with recent estimates|This paper provides a reconstruction of the joint distribution of Italian households’ income and wealth in the years ranging from 1968 to 1975. Exploiting the information available in some historical reports recently published by the Bank of Italy, the paper reconstructs synthetic microdata compatible with the aggregate results of sample surveys carried out in those years. In this way, inequality and poverty can be estimated by using the same statistical criteria that are used today, making an intertemporal comparison of the estimates possible. The concentration of household wealth shows a downward trend in the 1970s and ’80s, an increase in the years following the 1992-93 crisis and relative stability in the new century. In the period 1968-75 the concentration of wealth turns out to be greater than in recent years. The estimates of relative poverty show a decreasing trend until the 1990s and a subsequent increase; the upward trend of these indicators in recent years is steeper than that of the concentration indices. Migration flows have contributed significantly to the recent growth in the poverty indices.
C15|Estimating Non-Linear DSGEs with the Approximate Bayesian Computation: an application to the Zero Lower Bound|Estimation of non-linear DSGE models is still very limited due to high computational costs and identification issues arising from the non-linear solution of the models. Besides, the use of small sample amplifies those issues. This paper advocates for the use of Approximate Bayesian Computation (ABC), a set of Bayesian techniques based on moments matching. First, through Monte Carlo exercises, I assess the small sample performance of ABC estimators and run a comparison with the Limited Information Method (Kim, 2002), the state-of-the-art Bayesian method of moments used in DSGE literature. I find that ABC has a better small sample performance, due to the more efficient way through which the information provided by the moments is used to update the prior distribution. Second, ABC is tested on the estimation of a new-Keynesian model with a zero lower bound, a real life application where the occasionally binding constraint complicates the use of traditional method of moments.
C15|Time series with interdependent level and second moment: statistical testing and applications with Greek external trade and simulated data|This work aims to fill an existing gap in the literature regarding the statistical testing for the existence and the identification of the character of time-varying second moment in its dependence on a non-constant mean level in time series. To this end a new statistical testing procedure is introduced with some considerable advantages over the existing ones. Amongst others it is argued that the existing statistical tests are insufficient and sometimes lead to biased results. Further the effect of the application of this methodology on some crucial elements of time series modelling such as outlier detection and seasonal adjustment is examined, through case studies conducted on a comparative basis using both the new methodology and an established one. The severe consequences of the improper treatment of the type of time-varying second moment dealt with in this work are evidenced and emphasized. The data set comprises time series on monthly external trade statistics for Greece. Overall, the resulting empirical evidence favours the new approach. Further supporting evidence is provided by the application of the new methodology to simulated data.
C15|Mostly Harmless Simulations? On the Internal Validity of Empirical Monte Carlo Studies|In this paper we evaluate the premise from the recent literature on Monte Carlo studies that an empirically motivated simulation exercise is informative about the actual ranking of various estimators when applied to a particular problem. We consider two alternative designs and provide an empirical test for both of them. We conclude that a necessary condition for the simulations to be informative about the true ranking is that the treatment effect in simulations must be equal to the (unknown) true effect. This severely limits the usefulness of such procedures, since were the effect known, the procedure would not be necessary.
C15|Dynamic Effects of Monetary Policy Shocks on Macroeconomic Volatility|We use a simple New Keynesian model, with firm specific capital, non-zero steady-state inflation, long-run risks and Epstein-Zin preferences to study the volatility implications of a monetary policy shock. An unexpected increase in the policy rate by 150 basis points causes output and inflation volatility to rise around 10% above their steady-state standard deviations. VAR based empirical results support the model implications that contractionary shocks increase volatility. The volatility effects of the shock are driven by agents' concern about the (in) ability of the monetary authority to reverse deviations from the policy rule and the results are re-enforced by the presence of non-zero trend inflation.
C15|What Multiscale Approach Can Tell About the Nexus Between Exchange Rate and Stocks in the Major Emerging Markets?|This paper tries to answer which theory – the portfolio balance approach or the flow-oriented model, better explains the nexus between the national stock and exchange rate markets at different time-horizons in the major emerging markets of Europe and Asia. For that task we employ wavelet coherence and phase difference. Wavelet coherence results suggest that correlation between the two markets is not particularly strong throughout the observed period and at different wavelet scales, except in the period of World financial crisis (WFC). Phase difference in the Czech Republic, Turkey, Poland, Russia and South Korea are in anti-phase position during WFC in short run, which is in accordance with the portfolio-balance approach, whereby the stock market has the leading role. Also, phase difference at longer time-horizon indicate that an anti-phase situation is relatively common phenomenon in Poland, Russia, Turkey and South Korea. However, when we do calculations on real values, the results suggest that the real stock returns and the real exchange rate changes overwhelmingly behave in line with the flow-oriented model in all emerging markets, except for Poland. As for the Czech and Indian cases, phase differences indicate that the markets behave predominantly in accordance with the flow-oriented model at long-term horizon, regardless of whether nominal or real values are used.
C15|Inference in Bayesian Proxy-SVARs|Motivated by the increasing use of external instruments to identify structural vector autoregressions SVARs), we develop algorithms for exact finite sample inference in this class of time series models, commonly known as proxy SVARs. Our algorithms make independent draws from the normal-generalized-normal family of conjugate posterior distributions over the structural parameterization of a proxy-SVAR. Importantly, our techniques can handle the case of set identification and hence they can be used to relax the additional exclusion restrictions unrelated to the external instruments often imposed to facilitate inference when more than one instrument is used to identify more than one equation as in Mertens and Montiel-Olea (2018).
C15|A Class of Time-Varying Parameter Structural VARs for Inference under Exact or Set Identification|This paper develops a new class of structural vector autoregressions (SVARs) with time-varying parameters, which I call a drifting SVAR (DSVAR). The DSVAR is the first structural time-varying parameter model to allow for internally consistent probabilistic inference under exact—or set—identification, nesting the widely used SVAR framework as a special case. I prove that the DSVAR implies a reduced-form representation, from which structural inference can proceed similarly to the widely used two-step approach for SVARs: beginning with estimation of a reduced form and then choosing among observationally equivalent candidate structural parameters via the imposition of identifying restrictions. In a special case, the implied reduced form is a tractable known model for which I provide the first algorithm for Bayesian estimation of all free parameters. I demonstrate the framework in the context of Baumeister and Peersman’s (2013b) work on time variation in the elasticity of oil demand.
C15|Easy bootstrap-like estimation of asymptotic variances|The bootstrap is a convenient tool for calculating standard errors of the parameter estimates of complicated econometric models. Unfortunately, the bootstrap can be very time-consuming. In a recent paper, Honoré and Hu (2017), we propose a “Poor (Wo)man’s Bootstrap” based on one-dimensional estimators. In this paper, we propose a modified, simpler method and illustrate its potential for estimating asymptotic variances.
C15|Inference in Bayesian Proxy-SVARs|Motivated by the increasing use of external instruments to identify structural vector autoregressions (SVARs), we develop algorithms for exact finite sample inference in this class of time series models, commonly known as proxy-SVARs. Our algorithms make independent draws from the normal-generalized-normal family of conjugate posterior distributions over the structural parameterization of a proxy-SVAR. Importantly, our techniques can handle the case of set identification and hence they can be used to relax the additional exclusion restrictions unrelated to the external instruments often imposed to facilitate inference when more than one instrument are used to identify more than one equation, as in Mertens and Montiel-Olea (2018).
C15|Relationship between Foreign Exchange Rate and Stock Price of Commercial Banks in Romanian financial market|In the context of globalization and the financial crisis that the world traversed over the period 2007-2009, the Romanian capital market suffered extreme shocks (stock indices recording a decline of up to 90% while the national currency depreciated sharply against EUR and USD), which led to a significant increase in volatility in the national financial market. Considering that the financial sector was the trigger of the crisis and one of the most affected sector, we chose to analyze whether we can talk about the foreign exchange rate impact on price of the bank shares traded on the Bucharest Stock Exchange and vice versa (during March 2008 -June 2017), using correlation and VAR Granger Causality test. Frequency of data is daily. We also studied the evolution of the correlation between the banking sector (represented bythe shares of the banking companies traded on the Bucharest Stock Exchange) and the foreign exchange market during and after the financial crisis.Next, we analyzed volatility changes in this sector in the post-crisis period compared to the one recorded during the financial crisis. We have included the three Romanian banks: BRD-Groupe Societe Generale, Banca Transilvania, Patria Bank and two foreign banks traded on BSE: Erste Bank AG and Deutsche Bank and RON/EUR and RON/USD exchange rates.The results of the study showed that we can speak of a unidirectional causality running from the RON / EUR exchange rate to the prices of the Romanian banks included in the study (except for Patria Bank) and of a bidirectional causality for foreign banks Erste Bank and Deutsche Bank. During the crisis (as could be expected), we noticed an increase in volatility and market correlation and a slight decline once the effects of the crisis began to dissipate.
C15|Nonlinearities in the Real Exchange Rates: New Evidence from Developed and Developing Countries|This paper investigates nonlinearities in the dynamics of real exchange rates. We use Monte Carlo simulations to establish the size properties of the Terï¿½svirta-Anderson (1992) and the Terï¿½svirta (1994) test, when the dynamics of the real exchange rate is infl?uenced by an exogenous process. In addition, we examine the modifi?cation proposed by Ahmad, Lo and Mykhaylova (2013; Journal of International Economics) to show that the modifi?ed nonlinearity test performs much better than the original in both Monte Carlo exercises and in the actual data on 1431 bilateral real exchange rate series. Finally, we investigate the dynamics of the real exchange rate for both developed and developing countries using the modifi?ed test for the recent ?floating period. In general, the results fi?nds a greater incidence of nonlinear dynamics for developing country real exchange rates.
C15|Importance of Demand and Supply Shocks for Oil Price Variations|This paper studies the importance of demand and supply shocks in the oil market, and tries to explain the formation of the short-run oil price by applying an extended commodity storage model to the cyclical components of the price. First, I employ a multivariate method to extract the cyclical component of the oil price, world oil consumption, and global GDP. Next, I ﬁnd a large and positive eﬀect of global GDP shock on the oil price cycles in a VAR model. Then, I estimate the commodity storage model using a moment-matching method. All parameters are estimated signiﬁcantly, and the model shows good capability of reproducing the volatility and persistence of oil pricecycles. IﬁndthattheGDPshockgeneratesamuchmoremoderateeﬀectontheoil price cycles in the extended commodity storage model than the empirical evidence from the VAR analysis, and the production shock plays an important role for the variance of the cyclical component of the oil price.
C15|The influence of renewables on electricity price forecasting: a robust approach|In this paper a robust approach to modelling electricity spot prices is introduced. Differently from what has been recently done in the literature on electricity price forecasting, where the attention has been mainly drawn by the prediction of spikes, the focus of this contribution is on the robust estimation of nonlinear SETARX models (Self-Exciting Threshold Auto Regressive models with eXogenous regressors). In this way, parameters estimates are not, or very lightly, influenced by the presence of extreme observations and the large majority of prices, which are not spikes, could be better forecasted. A Monte Carlo study is carried out in order to select the best weighting function for Generalized M-estimators of SETAR processes. A robust procedure to select and estimate nonlinear processes for electricity prices is introduced, including robust tests for stationarity and nonlinearity and robust information criteria. The application of the procedure to the Italian electricity market reveals the forecasting superiority of the robust GM-estimator based on the polynomial weighting function respect to the non-robust Least Squares estimator. Finally, the introduction of external regressors in the robust estimation of SETARX processes contributes to the improvement of the forecasting ability of the model.
C15|Testing Identifying Assumptions In Fuzzy Regression Discontinuity Designs|We propose a new specification test for assessing the validity of fuzzy regression discontinuity designs (FRD-validity). We derive a new set of testable implications, characterized by a set of inequality restrictions on the joint distribution of observed outcomes and treatment status at the cut-off. We show that this new characterization exploits all the information in the data useful for detecting violations of FRD-validity. Our approach differs from, and complements existing approaches that test continuity of the distributions of running variables and baseline covariates at the cut-off since ours focuses on the distribution of the observed outcome and treatment status. We show that the proposed test has appealing statistical properties. It controls size in large sample uniformly over a large class of distributions, is consistent against all fixed alternatives, and has non-trivial power against some local alternatives. We apply our test to evaluate the validity of two FRD designs. The test does not reject the FRD-validity in the class size design studied by Angrist and Lavy (1999) and rejects in the insurance subsidy design for poor households in Colombia studied by Miller, Pinto, and Vera-HernÃ¡ndez (2013) for some outcome variables, while existing density tests suggest the opposite in each of the cases.
C15|Modeling systemic risk with Markov Switching Graphical SUR models|We propose a Markov Switching Graphical Seemingly Unrelated Regression (MS-GSUR) model to investigate time-varying systemic risk based on a range of multi-factor asset pricing models. Methodologically, we develop a Markov Chain Monte Carlo (MCMC) scheme in which latent states are identified on the basis of a novel weighted eigenvector centrality measure. An empirical application to the constituents of the S&P100 index shows that cross-firm connectivity significantly increased over the period 1999–2003 and during the financial crisis in 2008–2009. Finally, we provide evidence that firm-level centrality does not correlate with market values and it is instead positively linked to realized financial losses.
C15|Role of Bidding Method and Risk Allocation in the Performance of Public Private Partnership (PPP) Projects|This paper analyses the Indian PPP framework, including its bidding process and the standard concession agreement. The paper argues that the existing bidding method (i.e. premium/grant based method) can result in overvaluation of the projects due to optimism bias. When optimism about high traffic volumes do not materialize in the long run, projects could come under stress or fail, which is quite visible for Indian PPP road projects. This paper discusses an alternative bidding method called least present value of revenue (LPVR) and compares this method for the Indian PPP set-up with the help of Monte Carlo simulations by creating various real-life kind of scenarios. Results show that both methods have their own advantages depending on what was expected and what is actually realized. If expectations are low, then both methods gives more or less competitive results, but as expectations increase the LPVR method starts giving better results with a reasonably high certainty.
C15|Skewed logistic distribution for statistical temperature post-processing in mountainous areas|Non-homogeneous post-processing is often used to improve the predictive performance of probabilistic ensemble forecasts. A common quantity to develop, test, and demonstrate new methods is the near-surface air temperature frequently assumed to follow a Gaussian response distribution. However, Gaussian regression models with only few covariates are often not able to account for site-specific local features leading to strongly skewed residuals. This residual skewness remains even if many covariates are incorporated. Therefore, a simple refinement of the classical non-homogeneous Gaussian regression model is proposed to overcome this problem by assuming a skewed response distribution to account for possible skewness. This study shows a comprehensive analysis of the performance of non-homogeneous post-processing for 2m temperature for three different site types comparing Gaussian, logistic, and skewed logistic response distributions. Satisfying results for the skewed logistic distribution are found, especially for sites located in mountainous areas. Moreover, both alternative model assumptions but in particular the skewed response distribution, can improve on the classical Gaussian assumption with respect to overall performance, sharpness, and calibration of the probabilistic predictions.
C15|LASSO-Type Penalization in the Framework of Generalized Additive Models for Location, Scale and Shape|For numerous applications it is of interest to provide full probabilistic forecasts, which are able to assign probabilities to each predicted outcome. Therefore, attention is shifting constantly from conditional mean models to probabilistic distributional models capturing location, scale, shape (and other aspects) of the response distribution. One of the most established models for distributional regression is the generalized additive model for location, scale and shape (GAMLSS). In high dimensional data set-ups classical fitting procedures for the GAMLSS often become rather unstable and methods for variable selection are desirable. Therefore, we propose a regularization approach for high dimensional data set-ups in the framework for GAMLSS. It is designed for linear covariate effects and is based on L1 -type penalties. The following three penalization options are provided: the conventional least absolute shrinkage and selection operator (LASSO) for metric covariates, and both group and fused LASSO for categorical predictors. The methods are investigated both for simulated data and for two real data examples, namely Munich rent data and data on extreme operational losses from the Italian bank UniCredit.
C15|The resilience of EU Member States to the financial and economic crisis. What are the characteristics of resilient behaviour?|This study presents an empirical analysis of the resilience of European countries to the financial and economic crisis that started in 2007. The analysis addresses the following questions: Which countries showed a resilient behaviour during and after the crisis? Is resilience related only to the economic dimension? Has any of the EU countries been able to use the crisis as an opportunity and 'bounce forward'? Is it possible to identify any particular country characteristics linked to resilience? The analysis is based on the JRC conceptual framework for resilience (Manca et al., 2017) which places at its core the wellbeing of individuals, thus going beyond the merely economic growth perspective. The study carefully selects a number of key economic and social variables that aim to capture the resilience capacities of our society. Resilience is measured by investigating the dynamic response of these variables to the crisis in the short and medium run. In particular, we define four resilience indicators: the impact of the crisis, the recovery, the medium-run, and the ‘bouncing forward’. Results from a narrow exercise focusing on macroeconomic and financial variables confirm the validity of the proposed measurement approach: Germany appears to be among the most resilient countries; Ireland, after having been severely hit, shows a good absorptive capacity; Italy seems to be still struggling with the recovery, while Greece remains the most affected. After measuring resilience, we identify underlying country characteristics that may be associated with resilient behaviour. As such, these could indicate entry points for policies to increase countries' resilience to economic and financial shocks.
C15|Generalised Empirical Likelihood Kernel Block Bootstrapping|This article unveils how the kernel block bootstrap method of Parente and Smith (2018a,2018b) can be applied to make inferences on parameters of models de ned through moment restrictions. Bootstrap procedures that resort to generalised empirical likelihood implied probabilities to draw observations are also introduced. We prove the rst-order asymptotic validity of bootstrapped test statistics for overidentifying moment restrictions, parametric restrictions and additional moment restrictions. Resampling methods based on such probabilities were shown to be efficient by Brown and Newey (2002). A set of simulation experiments reveals that the statistical tests based on the proposed bootstrap methods perform better than those that rely on first-order asymptotic theory.
C15|Quasi-Maximum Likelihood and the Kernel Block Bootstrap for Nonlinear Dynamic Models|"This paper applies a novel bootstrap method, the kernelblockbootstrap, to quasi-maximum likelihood estimation of dynamic models with stationary strong mixing data. The method rst kernel weights the components comprising the quasi-log likelihood function in an appropriate way and then samples the resultant transformed components using the standard ""m out of n""bootstrap. We investigate the first order asymptotic properties of the KBB method for quasi-maximum likelihood demonstrating, in particular, its consistency and the rst-order asymptotic validity of the bootstrap approximation to the distribution of the quasi-maximum likelihood estimator. A set of simulation experiments for the mean regression model illustrates the efficacy of the kernel block bootstrap for quasi-maximum likelihood estimation."
C15|The impact of corruption on economic growth, a bootstrapping analysis|"In this paper we evaluate the impact of corruption in economic growth and if in less developed countries the hypothesis ""greasing the wheels” is valid. Using an unbalanced panel data with 2907 observations from 174 countries and 23 years between 1995 and 2017 (data from Transparency International and World Bank), we estimate using bootstrapping that the impact of corruption on growth is negative (an estimate of 0.025pp per CPI point) and that the hypothesis ""greasing the wheels” is not supported in the data. Our results are in accordance with the literature but are more robust because our database has much more observations."
C15|Two Distinct Seasonally Fractionally Differenced Periodic Processes|This article is devoted to study the e¤ects of the S-periodical fractional di¤erencing filter (1-L^S)^Dt . To put this e¤ect in evidence, we have derived the periodic auto-covariance functions of two distinct univariate seasonally fractionally di¤erenced periodic models. A multivariate representation of periodically correlated process is exploited to provide the exact and approximated expression auto-covariance of each models. The distinction between the models is clearly obvious through the expression of periodic auto-covariance function. Besides producing di¤erent autocovariance functions, the two models di¤er in their implications. In the first model, the seasons of the multivariate series are separately fractionally integrated. In the second model, however, the seasons for the univariate series are fractionally co-integrated. On the simulated sample, for each models, with the same parameters, the empirical periodic autocovariance are calculated and graphically represented for illustrating the results and support the comparison between the two models.
C15|Pricing Financial Derivatives Subject to Counterparty Risk and Credit Value Adjustment|This article presents a generic model for pricing financial derivatives subject to counterparty credit risk. Both unilateral and bilateral types of credit risks are considered. Our study shows that credit risk should be modeled as American style options in most cases, which require a backward induction valuation. To correct a common mistake in the literature, we emphasize that the market value of a defaultable derivative is actually a risky value rather than a risk-free value. Credit value adjustment (CVA) is also elaborated. A practical framework is developed for pricing defaultable derivatives and calculating their CVAs at a portfolio level.
C15|Analysis of dependencies between state tax behavior and macroeconomic indicators|Article examines the potential impact of macroeconomic parameters on tax behavior of governments, which can be regarded as integral part of more common problem of state’s economic behavior in tax policy area. We aimed to analyze and to reveal the interaction between base and derivative macroeconomic parameters, characterizing countries’ economic development and level of corporate taxation in order to conclude about effectiveness of state tax policy as well as about ways of its improving. Subject of study is the possible dependence of tax behavior of government institutions from macroeconomic indicators. In the framework of given study we used econometric methods. We made an analysis of eventual interdependence between value of corporate tax burden and certain macroeconomic indicators, representing evaluations of national economic systems: power, wealth, investment attractiveness and congenial investment climate. We revealed that corporate tax rate is used by governments not too actively, while the state tax behavior somewhere can be estimated as ineffective. Taking into account this fact as well as analysis of reasons causing such government behavior can facilitate the optimization of managerial decisions in the area of tax regulation. Also obtained results help to reveal instruments and motivation of economic agents’ behavior on macro-level. They can serve as base for further research concerning principles of behavior in economics in general as well as for examining more narrow areas such as “race to the bottom” problem.
C15|How to calibrate fiscal rules : a primer|This note provides guidance on how to calibrate fiscal rules; that is, how to determine the thresholds (ceiling, floor, or target) for specific fiscal aggregates constrained by rules. The note focuses, more specifically, on the calibration of the debt, balance, and expenditure rules.
C15|Asymmetry and Multiscale Dynamics in Macroeconomic Time Series Analysis|This thesis consists of three independent articles preceded by an introductory chapter. The first two articles focus on exchange rate dynamics in emerging market and developing economies, taking into account nonlinearities and asymmetries which are relevant for these countries and are potentially due to (i) transaction costs and other market frictions, and (ii) official intervention in the foreign exchange market. The third article is devoted to the analysis of the effects of monetary policy at different time horizons. The first article evaluates the purchasing power parity (PPP) theory in a panel of Sub-Saharan African countries. Unit root tests that are based on exponential smooth transition autoregressive (ESTAR) models are applied to account for nonlinearities and asymmetries in real exchange rate adjustment towards its equilibrium (mean) value. The results indicate empirical support for the PPP theory. The second article examines the relationship between current account adjustment and exchange rate flexibility in a panel of emerging market and developing economies. The purpose of this article is to (i) obtain a measure of exchange rate flexibility that considers autoregressive conditional heteroscedasticity and possible asymmetric responses of the exchange rate to shocks, and (ii) apply suitable dynamic panel data estimators to investigate this relationship. The results indicate that more flexible exchange rates are associated with faster current account adjustment. By means of wavelets the third article investigates the liquidity effect and the long-run neutrality of money at detailed timescales using time series data for Sweden and the US. The results indicate a significant liquidity effect at horizons of one to four years, but there is no evidence of monetary neutrality.
C15|Методика Классификационно-Регрессионного Анализа Районов По Показателям Сельского Хозяйства (На Примере Тульской Области)<BR>[Methods classification and regression analysis areas on indicators of agriculture (on the example of Tula region)]|The article offers a comprehensive approach to the analysis of agriculture at the meso level. This approach envisages implementation of two main stages: the classification of areas into several homogeneous groups and limiting the analysis of the factors determining the membership of an area to a certain class. The first stage envisages the construction of classification regression, and the second - the construction of the logit model multiple choice. This comprehensive approach increases the reliability of the analysis results, thereby contributing to strengthening the grounding of its results-based. The technique has been tested on data characterizing agriculture Tula region.
C15|Simulation of Spar Type Floating Offshore Wind Turbine Subjected to Misaligned Wind-Wave Loading Using Conservation of Momentum Method|Floating wind turbines are subjected to stochastic wind and wave loadings. Wind and wave loadings are not essentially aligned. The misalignment between wind and wave loadings affects the dynamical response of the floating wind turbines which needs to be studied. For this purpose, the nonlinear equations of motion of the spar-type floating wind turbine is derived using the Newton’s second law and conservation of angular momentum theory. The aerodynamic, hydrodynamic, mooring and buoyancy forces are determined and coupled with the system. The dynamic responses of the system are calculated and compared for different wind-wave misalignment angles. The simulation results demonstrate the importance of consideration of wind-wave misalignment angle on the dynamic response for the floating offshore wind turbine.
C15|Time-Varying Vector Autoregressions: Efficient Estimation, Random Inertia and Random Mean|Time-varying VAR models have become increasingly popular and are now widely used for policy analysis and forecast purposes. They constitute fundamental tools for the anticipation and analysis of economic crises, which represent rapid shifts in dynamic responses and shock volatility. Yet, despite their flexibility, time-varying VARs remain subject to a number of limitations. On the theoretical side, the conventional random walk assumption used for the dynamic parameters appears excessively restrictive. It also conceals the potential heterogeneities existing between the dynamic processes of different variables. On the application side, the standard two-pass procedure building on the Kalman filter proves excessively complicated and suffers from low efficiency. Based on these considerations, this paper contributes to the literature in four directions: i) it introduces a general time-varying VAR model which relaxes the standard random walk assumption and defines the dynamic parameters as general auto-regressive processes with variable- specific mean values and autoregressive coefficients. ii) it develops an estimation procedure for the model which is simple, transparent and efficient. The procedure requires no sophisticated Kalman filtering methods and reduces to a standard Gibbs sampling algorithm. iii) as an extension, it develops efficient procedures to estimate endogenously the mean values and autoregressive coefficients associated with each variable-specific autoregressive process. iv) through a case study of the Great Recession for four major economies (Canada, the Euro Area, Japan and the United States), it establishes that forecast accuracy can be significantly improved by using the proposed general time-varying model and its extensions in place of the traditional random walk specification.
C15|European Regional Productive Performance under a Metafrontier Framework. The role of patents and human capital on technology gap?|Assessing regional convergence is an important issue both at the national and at the supranational level, such as the level of European regions. Regional convergence and productivity growth are also principles of the European regional policy. This paper studies regional productivity convergence among 232 NUTS-2 European regions for the period 2003-2011. Despite the European regional policies implemented in the last two decades, the technology gap between European regions has only increased. The objective of this paper is to provide new evidence on production efficiency and the technology gap in European regions. We present a two-stage model of regional productive performance using a meta-frontier framework and a PVAR analysis. The main conclusion is that there exist significant differences in productive performance that confirm the North-South division in Europe. Finally, the results from the PVAR model provide robust evidence for the role played by human capital and innovation activity through patent realization in the technology gaps at the regional level in Europe.
C15|Re-examining the Foreign direct investment, Renewable energy consumption and Economic growth nexus: Evidence from a new Bootstrap ARDL test for Cointegration|This study re-examines the long-run relationship among foreign direct investment (FDI), renewable energy consumption (RE) and economic growth (GDP) for 9 Middle East and North Africa (MENA) countries over the period 1990–2015 using a newly developed cointegration test by McNown et al. (2018), the bootstrap autoregressive distributed lag (ARDL) which allows us to generate critical values for ARDL tests that are valid and appropriate for the specific data sets used and allow for endogeneity and feedback that may exist among the variables. In the long run analysis, we found evidence of cointegraion: (i) for Algeria, Armenia, Mauritania, and Tunisia when GDP is the dependant variable; (ii) for Egypt, Iran, Israel, Tunisia and Turkey when FDI is the dependent variable; and (iii) only for Iran, Morocco, and Tunisia when RE is the dependent variable. The short run Granger-causality analysis reveals varied nature of direction of causality between all variables and that is different among countries. This confirms that uniform policy recommendation relating to the causality between these variables may not work for these selected MENA countries.
C15|Family Ties and Children Obesity in Italy|This paper estimates the influence of overweight family members on weight outcomes of Italian children aged 6 to 14 years. We use a new dataset matching the 2012 cross sections of the Italian Multipurpose Household Survey and the Household Budget Survey. Endogenous peer groups within the family are accounted for using a set of instrumental variables. We find evidence of a strong, positive effect of both overweight adults and peer children in the family on children weight outcomes. The impact of overweight peer children in the household is larger than the impact of adults. These findings can help identifying the main factors driving the rise in Italian children obesity in the past few decades.
C15|Efectos de desbordamiento sobre los mercados financieros de Colombia. Identificación a través de la heterocedasticidad<BR>[Spillovers effects on financial markets of Colombia. Identification through heteroskedasticity]|La investigación cuantifica y analiza los efectos de los choques originados en los mercados estadounidenses sobre los principales mercados financieros colombianos. Para cumplir con este objetivo se emplea la información diaria de los mercados de dinero, bonos, acciones y tipo de cambio entre Colombia y EE.UU. durante el periodo de Enero 2003 y Enero 2015. La metodología utilizada es un modelo VAR estructural que emplea la heterocedasticidad que existe en los datos para la identificación y la estimación de los coeficientes de transmisión financiera. Se encuentra que los mercados estadounidenses generan efectos overall spillovers significativos sobre el mercado accionario colombiano. A su vez, los resultados reflejan la posición dominante del mercado de bonos estadounidenses como el motor de los efectos de desbordamiento.
C15|Testing Fractional Unit Roots with Non-linear Smooth Break Approximations using Fourier functions|In this paper we present a testing procedure for fractional orders of integration in the context of non-linear terms approximated by Fourier functions. The procedure is a natural extension of the linear method proposed in Robinson (1994) and similar to the one proposed in Cuestas and Gil-Alana (2016) based on Chebyshev polynomials in time. The test statistic has an asymptotic standard normal distribution and several Monte Carlo experiments conducted in the paper show that it performs well in finite samples. Various applications using real life time series, such as US unemployment rates, US GNP and Purchasing Power Parity (PPP) of G7 countries are presented at the end of the paper.
C15|Bayesian MCMC analysis of periodic asymmetric power GARCH models|A Bayesian MCMC estimate of a periodic asymmetric power GARCH (PAP-GARCH) model whose coefficients, power, and innovation distribution are periodic over time is proposed. The properties of the PAP-GARCH model such as periodic ergodicity, finiteness of moments and tail behaviors of the marginal distributions are first examined. Then, a Bayesian MCMC estimate based on Griddy-Gibbs sampling is proposed when the distribution of the innovation of the model is standard Gaussian or standardized Student with a periodic degree of freedom. Selecting the orders and the period of the PAP-GARCH model is carried out via the Deviance Information Criterion (DIC). The performance of the proposed Griddy-Gibbs estimate is evaluated through simulated and real data. In particular, applications to Bayesian volatility forecasting and Value-at-Risk estimation for daily returns on the S&P500 index are considered.
C15|Future developments in cyber risk assessment for the internet of things|This article is focused on the economic impact assessment of Internet of Things (IoT) and its associated cyber risks vectors and vertices – a reinterpretation of IoT verticals. We adapt to IoT both the Cyber Value at Risk model, a well-established model for measuring the maximum possible loss over a given time period, and the MicroMort model, a widely used model for predicting uncertainty through units of mortality risk. The resulting new IoT MicroMort for calculating IoT risk is tested and validated with real data from the BullGuard's IoT Scanner (over 310,000 scans) and the Garner report on IoT connected devices. Two calculations are developed, the current state of IoT cyber risk and the future forecasts of IoT cyber risk. Our work therefore advances the efforts of integrating cyber risk impact assessments and offer a better understanding of economic impact assessment for IoT cyber risk.
C15|Time-varying impact of uncertainty shocks on the US housing market|This paper investigates the impact of uncertainty shocks on the housing market of the United States using the time-varying parameter factor augmented vector autoregression (TVP-FAVAR). We use a comprehensive quarterly time-series dataset on real economic activity, price, and financial variables, besides housing market variables, covering the period 1963:Q1 to 2014:Q3. In addition to housing prices, we also consider variables related to home sales, permits and starts. In general, the results of the cumulative response of housing variables to a one standard deviation positive uncertainty shock at the one-, four-, eight-, and twelve-quarter-horizon tends to change over time, both in terms of sign and magnitude, with the uncertainty shock primarily negatively affecting the housing variables, in particular prices, permits and starts, in longer-runs (i.e., two- and three-years-ahead horizons).
C15|Fast and wild: Bootstrap inference in Stata using boottest|The wild bootstrap was originally developed for regression models with heteroskedasticity of unknown form. Over the past 30 years, it has been extended to models estimated by instrumental variables and maximum likelihood and to ones where the error terms are (perhaps multiway) clustered. Like boot- strap methods in general, the wild bootstrap is especially useful when conventional inference methods are unreliable because large-sample assumptions do not hold. For example, there may be few clusters, few treated clusters, or weak instruments. The package boottest can perform a wide variety of wild bootstrap tests, often at remarkable speed. It can also invert these tests to construct confidence sets. As a postestimation command, boottest works after linear estimation commands, in- cluding regress, cnsreg, ivregress, ivreg2, areg, and reghdfe, as well as many estimation commands based on maximum likelihood. Although it is designed to perform the wild cluster bootstrap, boottest can also perform the ordinary (non- clustered) version. Wrappers offer classical Wald, score/Lagrange multiplier, and Anderson–Rubin tests, optionally with (multiway) clustering. We review the main ideas of the wild cluster bootstrap, offer tips for use, explain why it is particularly amenable to computational optimization, state the syntax of boottest, artest, scoretest, and waldtest, and present several empirical examples.
C15|Improving Finite Sample Approximation by Central Limit Theorems for DEA and FDH efficiency scores|We propose an improvement of the finite sample approximation of the central limit theorems (CLTs) that were recently derived for statistics involving production efficiency scores estimated via Data Envelopment Analysis (DEA) or Free Disposal Hull (FDH) approaches. The improvement is very easy to implement since it involves a simple correction of the already employed statistics without any additional computational burden and preserves the original asymptotic results such as consistency and asymptotic normality. The proposed approach persistently showed improvement in all the scenarios that we tried in various Monte-Carlo experiments, especially for relatively small samples or relatively large dimensions (measured by total number of inputs and outputs) of the underlying production model. This approach therefore is expected to be valuable (and at almost no additional computational costs) for practitioners wishing to perform statistical inference about production efficiency using DEA or FDH approaches.
C15|Econometric Analysis of Productivity: Theory and Implementation in R|Our chapter details a wide variety of approaches used in estimating productivity and efficiency based on methods developed to estimate frontier production using Stochastic Frontier Analysis (SFA) and Data Envelopment Analysis (DEA). The estimators utilize panel, single cross section, and time series data sets. The R programs include such approaches to estimate firm efficiency as the time invariant fixed effects, correlated random effects, and uncorrelated random effects panel stochastic frontier estimators, time varying fixed effects, correlated random effects, and uncorrelated random effects estimators, semi-parametric efficient panel frontier estimators, factor models for cross-sectional and time-varying efficiency, bootstrapping methods to develop confidence intervals for index number-based productivity estimates and their decompositions, DEA and Free Disposable Hull estimators. The chapter provides the professional researcher, analyst, statistician, and regulator with the most up to date efficiency modeling methods in the easily accessible open source programming language R.
C15|The evolving impact of global, region-specific and country-specific uncertainty| We develop a dynamic factor model with time-varying parameters and stochastic volatility, estimate it with several variables for a large number of countries and decompose the variance of each variable in terms of contributions from uncertainty common to all countries (global uncertainty), region-specific uncertainty and country-specific uncertainty. Among other findings, the estimates suggest that global uncertainty plays a primary role in explaining the volatility of inflation, interest rates and stock prices, although to a varying extent over time, while all uncertainty components are found to play a non-negligible role for real economic activity, credit and money for most countries.
C15|Forecasting With High Dimensional Panel VARs|In this paper, we develop econometric methods for estimating large Bayesian timevarying parameter panel vector autoregressions (TVP-PVARs) and use these methods to forecast inflation for euro area countries. Large TVP-PVARs contain huge numbers of parameters which can lead to over-parameterization and computational concerns. To overcome these concerns, we use hierarchical priors which reduce the dimension of the parameter vector and allow for dynamic model averaging or selection over TVP-PVARs of different dimension and different priors. We use forgetting factor methods which greatly reduce the computational burden. Our empirical application shows substantial forecast improvements over plausible alternatives.
C15|Multivariate stochastic volatility with co-heteroscedasticity|This paper develops a new methodology that decomposes shocks into homoscedastic and heteroscedastic components. This specification implies there exist linear combinations of heteroscedastic variables that eliminate heteroscedasticity. That is, these linear combinations are homoscedastic; a property we call co-heteroscedasticity. The heteroscedastic part of the model uses a multivariate stochastic volatility inverse Wishart process. The resulting model is invariant to the ordering of the variables, which we show is important for impulse response analysis but is generally important for, e.g., volatility estimation and variance decompositions. The specification allows estimation in moderately high-dimensions. The computational strategy uses a novel particle filter algorithm, a reparameterization that substantially improves algorithmic convergence and an alternating-order particle Gibbs that reduces the amount of particles needed for accurate estimation. We provide two empirical applications; one to exchange rate data and another to a large Vector Autoregression (VAR) of US macroeconomic variables. We find strong evidence for co-heteroscedasticity and, in the second application, estimate the impact of monetary policy on the homoscedastic and heteroscedastic components of macroeconomic variables.
C15|Asymptotic theory for rough fractional Vasicek models|This paper extends the asymptotic theory for the fractional Vasicek model developed in Xiao and Yu (2018) from the case where H∈(1∕2,1) to where H∈(0,1∕2). It is found that the asymptotic theory of the persistence parameter (κ) critically depends on the sign of κ . Moreover, if κ>0, the asymptotic distribution for the estimator of κ is different when H∈(0,1∕2) from that when H∈(1∕2,1).
C15|Spatial Dynamic Panel Data Models with Correlated Random Effects|In this paper, M-estimation and inference methods are developed for spatial dynamic panel data models with correlated random effects, based on short panels. The unobserved individual-specific effects are assumed to be correlated with the observed time-varying regressors linearly or in a linearizable way, giving the so-called correlated random effects model, which allows the estimation of effects of time-invariant regressors. The unbiased estimating functions are obtained by adjusting the conditional quasi-scores given the initial observations, leading to M-estimators that are consistent, asymptotically normal, and free from the initial conditions except the process starting time. By decomposing the estimating functions into sums of terms uncorrelated given idiosyncratic errors, a hybrid method is developed for consistently estimating the variance-covariance matrix of the M-estimators, which again depends only on the process starting time. Monte Carlo results demonstrate that the proposed methods perform well in finite sample.
C15|The Determinants of Population Growth: Literature review and empirical analysis|We point out that the simple slice sampler generates chains that are correlation-free when the target distribution is centrally symmetric. This property explains several results in the literature about the relative performance of the simple and product slice samplers. We exploit it to improve two algorithms often used to circumvent the slice inversion problem, namely stepping out and multivariate sampling with hyperrectangles. In the general asymmetric case, we argue that symmetrizing the target distribution before simulating greatly enhances the efficiency of the simple slice sampler. To achieve symmetry we focus on the Box-Cox transformation with parameters chosen to minimize a measure of skewness. This strategy is illustrated with several sampling problems.
C15|Skewness-Adjusted Bootstrap Confidence Intervals and Confidence Bands for Impulse Response Functions|This article investigates the construction of skewness-adjusted confidence intervals and joint confidence bands for impulse response functions from vector autoregressive models. Three different implementations of the skewness adjustment are investigated. The methods are based on a bootstrap algorithm that adjusts mean and skewness of the bootstrap distribution of the autoregressive coefficients before the impulse response functions are computed. Using extensive Monte Carlo simulations, the methods are shown to improve the coverage accuracy in small and medium sized samples and for unit root processes for both known and unknown lag orders.
C15|A Correction/Update to â€œWhen Is It Justifiable to Ignore Variable Endogeneity In A Regression Model?â€|This note corrects an error -- pointed out in Kiviet (2016) -- in the Ashley and Parmeter (2015a) derivation of the asymptotic distribution of the OLS parameter estimator in the usual k-variate multiple regression model, but where some or all of the explanatory variables are endogenous. This sampling distribution lies at the heart of the Ashley and Parmeter (2015a) sensitivity analysis of a hypothesis test rejection p-value with respect to potential endogeneity in the explanatory variables in such regression models, so this correction is of practical importance. We also discuss the settings in which Kiviet's way of displaying univariate sensitivity analysis results is an improvement (and in what settings it is not), and we provide new analytic results for our sensitivity analysis in an important special case.
C15|On the financial connectedness of the commodity market: a replication of the Diebold and Yilmaz (2012) study|In this paper we replicate the Diebold and Yilmaz (2012) study on the connectedness of the Commodity market and three other financial markets: the stock market, the bond market, and the FX market. We show that both the row and the column normalization schemes of the Generalized Forecast Error Variance Decomposition, suggested by the authors, lead to inaccurate measures of net contribution to risk transmission, in terms of ranking and sign. We show that, considering data generating processes characterized by different degrees of comovement and persistence, a scalar based normalization of the Generalized Forecast Error Variance Decomposition yields consistent (free of sign and ranking errors) net spillovers.
C15|New method to detect convergence in simple multi-period market games with infinite large strategy spaces|We introduce a new methodology that enables the detection of onset of convergence towards Nash equilibria, in simple repeated-games with infinite large strategy spaces. The method works by constraining on a special and finite subset of strategies. We illustrate how the method can predict (in special time periods) with a high success rate the action of participants in a series of experiments
C15|"Fiscal Policy, as the ""Employer of Last Resort"": Impact of MGNREGS on Labour Force Participation Rates in India"|"We examine the impact of conditional fiscal transfers on public employment across gender in India taking the case of the Mahatma Gandhi National Rural Employment Guarantee Scheme (MGNREGS). The MGNREGS, as an ""employer of last resort"" fiscal policy, is a direct employment transfer, which guarantees to provide 100 days of paid work opportunities at a predetermined wage for public works in India through a self-selection criterion. Using unit record data of the latest 68th round of NSS Employment-Unemployment survey, we examined gender differential impacts of MGNREGS on labour force participation rates across States in India. The unit of analysis in our paper is not `household', but is one step ahead to capture the intra-household level of participating behaviour in the economic activity. The results, based on the survey enumerating 2,80,763 individuals in rural areas, revealed that there is a striking heterogeneity in the gender impacts of job guarantee programme across States of India. The probit estimates showed that MGNREGS job card holder's labour force participation rates were higher than the non-card holders and the result was more pronounced for women. The analysis of the time-use patterns and the unpaid care economy statistics of job guarantee card holders obtained from the unit records also shows that augmenting public investment in care economy infrastructure is significant for the job guarantee programme to function at its full potential in India."
C15|Multivariate Stochastic Volatility with Co-Heteroscedasticity|This paper develops a new methodology that decomposes shocks into homoscedastic and heteroscedastic components. This specification implies there exist linear combinations of heteroscedastic variables that eliminate heteroscedasticity. That is, these linear combinations are homoscedastic; a property we call co-heteroscedasticity. The heteroscedastic part of the model uses a multivariate stochastic volatility inverse Wishart process. The resulting model is invariant to the ordering of the variables, which we show is important for impulse response analysis but is generally important for, e.g., volatility estimation and variance decompositions. The specification allows estimation in moderately high-dimensions. The computational strategy uses a novel particle filter algorithm, a reparameterization that substantially improves algorithmic convergence and an alternating-order particle Gibbs that reduces the amount of particles needed for accurate estimation. We provide two empirical applications; one to exchange rate data and another to a large Vector Autoregression (VAR) of US macroeconomic variables. We find strong evidence for co-heteroscedasticity and, in the second application, estimate the impact of monetary policy on the homoscedastic and heteroscedastic components of macroeconomic variables.
C15|Confidence regions for entries of a large precision matrix|We consider the statistical inference for high-dimensional precision matrices. Specifically, we propose a data-driven procedure for constructing a class of simultaneous confidence regions for a subset of the entries of a large precision matrix. The confidence regions can be applied to test for specific structures of a precision matrix, and to recover its nonzero components. We first construct an estimator for the precision matrix via penalized node-wise regression. We then develop the Gaussian approximation to approximate the distribution of the maximum difference between the estimated and the true precision coefficients. A computationally feasible parametric bootstrap algorithm is developed to implement the proposed procedure. The theoretical justification is established under the setting which allows temporal dependence among observations. Therefore the proposed procedure is applicable to both independent and identically distributed data and time series data. Numerical results with both simulated and real data confirm the good performance of the proposed method.
C15|Role of bidding method and risk allocation in the performance of public private partnership (PPP) projects|The Public Private Partnership (PPP) model has recently gained a lot of attention in the infrastructure creation literature. Selection of the right private partner in PPP is important for the success of this model. The bidding method along with appropriate risk allocation plays a critical role in this regard. This paper analyses the Indian PPP framework, including its bidding process and the standard concession agreement. The paper argues that the existing bidding method (i.e. premium/grant based method) can result in overvaluation of the projects due to optimism bias. When optimism about high traffic volumes do not materialize in the long run, projects could come under stress or fail, which is quite visible for Indian PPP road projects. This paper discusses an alternative bidding method called least present value of revenue (LPVR) and compares this method for the Indian PPP set-up with the help of Monte Carlo simulations by creating various real-life kind of scenarios. Results show that both methods have their own advantages depending on what was expected and what is actually realized. If expectations are low, then both methods gives more or less competitive results, but as expectations increase the LPVR method starts giving better results with a reasonably high certainty.
C15|Forecasting with High‐Dimensional Panel VARs|This paper develops methods for estimating and forecasting in Bayesian panel vector autoregressions of large dimensions with time‐varying parameters and stochastic volatility. We exploit a hierarchical prior that takes into account possible pooling restrictions involving both VAR coefficients and the error covariance matrix, and propose a Bayesian dynamic learning procedure that controls for various sources of model uncertainty. We tackle computational concerns by means of a simulation‐free algorithm that relies on analytical approximations to the posterior. We use our methods to forecast inflation rates in the eurozone and show that these forecasts are superior to alternative methods for large vector autoregressions.
C15|Bayesian estimation of agent-based models|We consider Bayesian inference techniques for agent-based (AB) models, as an alternative to simulated minimum distance (SMD). Three computationally heavy steps are involved: (i) simulating the model, (ii) estimating the likelihood and (iii) sampling from the posterior distribution of the parameters. Computational complexity of AB models implies that efficient techniques have to be used with respect to points (ii) and (iii), possibly involving approximations. We first discuss non-parametric (kernel density) estimation of the likelihood, coupled with Markov chain Monte Carlo sampling schemes. We then turn to parametric approximations of the likelihood, which can be derived by observing the distribution of the simulation outcomes around the statistical equilibria, or by assuming a specific form for the distribution of external deviations in the data. Finally, we introduce Approximate Bayesian Computation techniques for likelihood-free estimation. These allow embedding SMD methods in a Bayesian framework, and are particularly suited when robust estimation is needed. These techniques are first tested in a simple price discovery model with one parameter, and then employed to estimate the behavioural macroeconomic model of De Grauwe (2012), with nine unknown parameters.
C15|Semiparametric Bayesian inference for time-varying parameter regression models with stochastic volatility|We develop a Bayesian semiparametric method to estimate a time-varying parameter regression model with stochastic volatility, where both the error distributions of the observations and parameter-driven dynamics are unspecified. We illustrate our methodology with an application to inflation.
C15|Discrete-response state space models with conditional heteroscedasticity: An application to forecasting the federal funds rate target|We propose a state space mixed model with stochastic volatility for ordinal-response time series data. For parameter estimation, we design an efficient Markov chain Monte Carlo algorithm. We illustrate our method with an empirical study on the federal funds rate target. The proposed model provides better forecasts than alternative specifications.
C15|Inference based on many conditional moment inequalities|We construct confidence sets for models defined by many conditional moment inequalities/equalities. The number of conditional moment restrictions can be up to infinitely many. To deal with the vast number of moment restrictions, we exploit the manageability (Pollard (1990)) of the class of moment functions. We verify this condition in five examples from the recent partial identification literature.
C15|Tests for conditional ellipticity in multivariate GARCH models|Tests are proposed for the assumption that the conditional distribution of a multivariate GARCH process is elliptic. These tests are of Kolmogorov–Smirnov and Cramér–von Mises-type and make use of the common geometry underlying the characteristic function of any spherically symmetric distribution. The asymptotic null distribution of the test statistics as well as the consistency of the tests is investigated under general conditions. It is shown that both the finite sample and the asymptotic null distribution depend on the unknown distribution of the Euclidean norm of the innovations. Therefore a conditional Monte Carlo procedure is used to actually carry out the tests. The validity of this resampling scheme is formally justified. Results on the behavior of the new tests in finite-samples are included along with comparisons with other tests.
C15|Bootstrapping integrated covariance matrix estimators in noisy jump–diffusion models with non-synchronous trading|We propose a bootstrap method for estimating the distribution (and functionals of it such as the variance) of various integrated covariance matrix estimators. In particular, we first adapt the wild blocks of blocks bootstrap method suggested for the pre-averaged realized volatility estimator to a general class of estimators of integrated covolatility. We then show the first-order asymptotic validity of this method in the multivariate context with a potential presence of jumps, dependent microstructure noise, irregularly spaced and non-synchronous data. Our results justify using the bootstrap to estimate the covariance matrix of a broad class of covolatility estimators. The bootstrap variance estimator is positive semi-definite by construction, an appealing feature that is not always shared by existing variance estimators of the integrated covariance estimator. As an application of our results, we also consider the bootstrap for regression coefficients. We show that the wild blocks of blocks bootstrap, appropriately centered, is able to mimic both the dependence and heterogeneity of the scores. We provide a proof of construction of bootstrap percentile and percentile-t intervals as well as variance estimates in this context. This contrasts the traditional pairs bootstrap which is not able to mimic the score heterogeneity even in the simple case where no microstructure noise is present. Our Monte Carlo simulations show that the wild blocks of blocks bootstrap improve the finite sample properties of the alternative approach based on the Gaussian approximation. We illustrate its practical use on high-frequency equity data.
C15|A local stable bootstrap for power variations of pure-jump semimartingales and activity index estimation|We provide a new resampling procedure–the local stable bootstrap–that is able to mimic the dependence properties of realized power variations for pure-jump semimartingales observed at different frequencies. This allows us to propose a bootstrap estimator and inference procedure for the activity index of the underlying process, β, as well as bootstrap tests for whether it obeys a jump-diffusion or a pure-jump process, that is, of the null hypothesis H0:β=2 against the alternative H1:β<2. We establish first-order asymptotic validity of the resulting bootstrap power variations, activity index estimator, and diffusion tests for H0. Moreover, the finite sample size and power properties of the proposed diffusion tests are compared to those of benchmark tests using Monte Carlo simulations. Unlike existing procedures, our bootstrap tests are correctly sized in general settings. Finally, we illustrate the use and properties of the new bootstrap diffusion tests using high-frequency data on three FX series, the S&P 500, and the VIX.
C15|Change point and trend analyses of annual expectile curves of tropical storms|Motivated by the conjectured existence of trends in the intensity of tropical storms, new inferential methodology to detect a trend in the annual pattern of environmental data is developed. It can be applied to any data which form a time series of functions. Other examples include annual temperature or daily pollution curves at specific locations. Within a framework of a functional regression model, two tests of significance of the slope function are derived. One of the tests relies on a Monte Carlo distribution to compute the critical values, the other is pivotal with the chi–square limit distribution. Full asymptotic justification of both tests is provided. Their finite sample properties are investigated by a simulation study. Applied to tropical storm data, these tests show that there is a significant trend in the shape of the annual pattern of upper wind speed levels of hurricanes.
C15|The performance of tests on endogeneity of subsets of explanatory variables scanned by simulation|Tests for classification as endogenous or predetermined of arbitrary subsets of regressors are formulated as significance tests in auxiliary IV regressions and their relationships with various more classic test procedures are examined and critically compared with statements in the literature. Then simulation experiments are designed by solving the data generating process parameters from salient econometric features, namely: degree of simultaneity and multicollinearity of regressors, and individual and joint strength of external instrumental variables. Next, for various test implementations, a wide class of relevant cases is scanned for flaws in performance regarding type I and II errors. Substantial size distortions occur, but these can be cured remarkably well through bootstrapping, except when instruments are relatively weak. The power of the subset tests is such that they establish an essential addition to the well-known classic full-set DWH tests in a data based classification of individual explanatory variables. This is also illustrated in an empirical example supplemented with hints for practitioners.
C15|A distance test of normality for a wide class of stationary processes|A distance test for normality of the one-dimensional marginal distribution of stationary fractionally integrated processes is considered. The test is implemented by using an autoregressive sieve bootstrap approximation to the null sampling distribution of the test statistic. The bootstrap-based test does not require knowledge of either the dependence parameter of the data or of the appropriate norming factor for the test statistic. The small-sample properties of the test are examined by means of Monte Carlo experiments. An application to real-world data is also presented.
C15|Meta-analytic cointegrating rank tests for dependent panels|Two new panel cointegrating rank tests which are robust to cross-sectional dependence are proposed. The dependence in the data generating process is modeled using unobserved common factors. The new tests are based on a meta-analytic approach, in which the p-values of the individual likelihood-ratio (LR) type test statistics computed from defactored data are combined into the panel statistics. A simulation study shows that the tests have reasonable size and power properties in finite samples. The application of the tests is illustrated by investigating the monetary exchange rate model for a panel data of 19 countries.
C15|Regression and Kriging metamodels with their experimental designs in simulation: A review|This article reviews the design and analysis of simulation experiments. It focusses on analysis via two types of metamodel (surrogate. emulator); namely, low-order polynomial regression, and Kriging (or Gaussian process). The metamodel type determines the design of the simulation experiment, which determines the input combinations of the simulation model. For example, a first-order polynomial regression metamodel should use a “resolution-III”design, whereas Kriging may use “Latin hypercube sampling”. More generally, polynomials of first or second order may use resolution III, IV, V, or “central composite” designs. Before applying either regression or Kriging metamodeling, the many inputs of a realistic simulation model can be screened via “sequential bifurcation”. Optimization of the simulated system may use either a sequence of low-order polynomials—known as “response surface methodology” (RSM)—or Kriging models fitted through sequential designs—including “efficient global optimization” (EGO). Finally, “robust”optimization accounts for uncertainty in some simulation inputs.
C15|Relevant states and memory in Markov chain bootstrapping and simulation|Markov chain theory is proving to be a powerful approach to bootstrap and simulate highly nonlinear time series. In this work, we provide a method to estimate the memory of a Markov chain (i.e. its order) and to identify its relevant states. In particular, the choice of memory lags and the aggregation of irrelevant states are obtained by looking for regularities in the transition probabilities. Our approach is based on an optimization model. More specifically, we consider two competing objectives that a researcher will in general pursue when dealing with bootstrapping and simulation: preserving the “structural” similarity between the original and the resampled series, and assuring a controlled diversification of the latter. A discussion based on information theory is developed to define the desirable properties for such optimal criteria. Two numerical tests are developed to verify the effectiveness of the proposed method.
C15|Effects of common factors on stock correlation networks and portfolio diversification|This study empirically investigates the effects of common factors on the connectivity of the network among stocks and on the distribution of the investment weights for stocks. The network is defined as a stock correlation network from the minimal spanning tree (MST), and portfolio is defined as an efficient portfolio from the Markowitz mean-variance (MV) optimization function (MVOF). For these research goals, we devise a method using the comparative correlation matrix (C-CM), which does not have the property of a single common factor included in the sample correlation matrix (S-CM). The results reveal that common factors clearly affect the changes of connectivity among stocks in the networks, and that their influence is much greater on stocks with many links to other stocks in the network. Further, common factors significantly affect the determination of the investment weight's distribution for stocks from the MVOF. In particular, among the common factors, a market factor plays a dominant role in both structuring the network among stocks and in constructing the well-diversified portfolio. In addition, the devised method of the C-CM without the property of the market factor in the S-CM plays a crucial role in constructing a more diversified portfolio with better out-of-sample performance in the future period. These results are robust in both the Korean and the U.S. stocks markets.
C15|FX technical trading rules can be profitable sometimes!|This paper investigates the profitability of technical trading rules in the foreign exchange market taking into account data snooping bias and transaction costs. A universe of 7650 trading rules is applied to six currencies quoted in U.S. dollars over the 1994:3–2014:12 period. The Barras, Scaillet, and Wermers (2010) false discovery rate method is employed to deal with data snooping and it detects almost all outperforming trading rules while keeping the proportion of false discoveries to a pre-specified level. The out-of-sample results reveal a large number of outperforming rules that are profitable over short periods based on the Sharpe ratio. However, they are not consistently profitable and so the overall results are more consistent with the adaptive markets hypothesis.
C15|Multiple-days-ahead value-at-risk and expected shortfall forecasting for stock indices, commodities and exchange rates: Inter-day versus intra-day data|In order to provide reliable Value-at-Risk (VaR) and Expected Shortfall (ES) forecasts, this paper attempts to investigate whether an inter-day or an intra-day model provides accurate predictions. We investigate the performance of inter-day and intra-day volatility models by estimating the AR(1)-GARCH(1,1)-skT and the AR(1)-HAR-RV-skT frameworks, respectively. This paper is based on the recommendations of the Basel Committee on Banking Supervision. Regarding the forecasting performances, the exploitation of intra-day information does not appear to improve the accuracy of the VaR and ES forecasts for the 10-steps-ahead and 20-steps-ahead for the 95%, 97.5% and 99% significance levels. On the contrary, the GARCH specification, based on the inter-day information set, is the superior model for forecasting the multiple-days-ahead VaR and ES measurements. The intra-day volatility model is not as appropriate as it was expected to be for each of the different asset classes; stock indices, commodities and exchange rates.
C15|Common and country specific economic uncertainty|We use a factor model with stochastic volatility to decompose the time-varying variance of macroeconomic and financial variables into contributions from country-specific uncertainty and uncertainty common to all countries. We find that the common component plays an important role in driving the time-varying volatility of nominal and financial variables. The cross-country co-movement in volatility of real and financial variables has increased over time with the common component becoming more important over the last decade. Simulations from a two-country DSGE model featuring Epstein-Zin preferences suggest that increased globalisation and trade openness may be the driving force behind the increased cross-country correlation in volatility.
C15|The Global Role of the U.S. Economy: Linkages, Policies and Spillovers|This paper analyzes the role of the United States in the global economy and examines the extent of global spillovers from changes in U.S. growth, monetary and fiscal policies, and uncertainty in its financial markets and economic policies. Developments in the U.S. economy, the world’s largest, have effects far beyond its shores. A surge in U.S. growth could provide a significant boost to the global economy. Tightening U.S. financial conditions—whether due to contractionary U.S. monetary policy or other reasons— could reverberate across global financial markets, with adverse effects on some emerging market and developing economies that rely heavily on external financing. In addition, lingering uncertainty about the course of U.S. economic policy could have an appreciably negative effect on global growth prospects. While the United States plays a critical role in the world economy, activity in the rest of the world is also important for the United States.
C15|Efficient simulation of clustering jumps with CIR intensity|We introduce a broad family of generalised self-exciting point processes with CIR-type intensities, and develop associated algorithms for their exact simulation. The underlying models are extensions of the classical Hawkes process, which already has numerous applications in modelling the arrival of events with clustering or contagion effect in finance, economics and many other fields. Interestingly, we find that the CIR-type intensity together with its point process can be sequentially decomposed into simple random variables, which immediately leads to a very efficient simulation scheme. Our algorithms are also pretty accurate and flexible. They can be easily extended to further incorporate externally-excited jumps, or, to a multidimensional framework. Some typical numerical examples and comparisons with other well known schemes are reported in detail. In addition, a simple application for modelling a portfolio loss process is presented.
C15|Computation of the Corrected Cornish-Fisher Expansion using the Response Surface Methodology: Application to V aR and CV aR| The Cornish-Fisher expansion is a simple way to determine quantiles of non- normal distributions. It is frequently used by practitioners and by academics in risk mana- gement, portfolio allocation, and asset liability management. It allows us to consider non- normality and, thus, moments higher than the second moment, using a formula in which terms in higher-order moments appear explicitly. This paper has two primary objectives. First, we resolve the classic confusion between the skewness and kurtosis coefficients of the formula and the actual skewness and kurtosis of the distribution when using the Cornish{ Fisher expansion. Second, we use the response surface approach to estimate a function for these two values. This helps to overcome the difficulties associated with using the Cornish{ Fisher expansion correctly to compute value at risk (V aR). In particular, it allows a direct computation of the quantiles. Our methodology has many practical applications in risk ma- nagement and asset allocation.
C15|Theory and application of an economic performance measure of risk|Homm and Pigorsch (2012a) use the Aumann and Serrano index to develop a new economic performance measure (EPM), which is well known to have advantages over other measures. In this paper, we extend the theory by constructing a one-sample confidence interval of EPM, and construct confidence intervals for the difference of EPMs for two independent samples. We also derive the asymptotic distribution for EPM and for the difference of two EPMs when the samples are independent. We conduct simulations to show the proposed theory performs well for one and two independent samples. The simulations show that the proposed approach is robust in the dependent case. The theory developed is used to construct both one-sample and two-sample confidence intervals of EPMs for Singapore and USA stock indices.
C15|Exogeneity Tests, Incomplete Models, Weak Identification and Non-Gaussian Distributions : Invariance and Finite-Sample Distributional Theory|We study the distribution of Durbin-Wu-Hausman (DWH) and Revankar-Hartley (RH) tests for exogeneity from a finite-sample viewpoint, under the null and alternative hypotheses. We consider linear structural models with possibly non-Gaussian errors, where structural parameters may not be identified and where reduced forms can be incompletely specified (or nonparametric). On level control, we characterize the null distributions of all the test statistics. Through conditioning and invariance arguments, we show that these distributions do not involve nuisance parameters. In particular, this applies to several test statistics for which no finite-sample distributional theory is yet available, such as the standard statistic proposed by Hausman (1978). The distributions of the test statistics may be non-standard – so corrections to usual asymptotic critical values are needed – but the characterizations are sufficiently explicit to yield finite-sample (Monte-Carlo) tests of the exogeneity hypothesis. The procedures so obtained are robust to weak identification, missing instruments or misspecified reduced forms, and can easily be adapted to allow for parametric non-Gaussian error distributions. We give a general invariance result (block triangular invariance) for exogeneity test statistics. This property yields a convenient exogeneity canonical form and a parsimonious reduction of the parameters on which power depends. In the extreme case where no structural parameter is identified, the distributions under the alternative hypothesis and the null hypothesis are identical, so the power function is flat, for all the exogeneity statistics. However, as soon as identification does not fail completely, this phenomenon typically disappears. We present simulation evidence which confirms the finite-sample theory. The theoretical results are illustrated with two empirical examples: the relation between trade and economic growth, and the widely studied problem of the return of education to earnings.
C15|Testing for Prospect and Markowitz stochastic dominance efficiency|We develop non-parametric tests for prospect stochastic dominance Efficiency (PSDE) and Markowitz stochastic dominance efficiency (MSDE) using block bootstrap resampling. Under the appropriate conditions we show that they are a symptotically conservative and consistent. We employ Monte Carlo experiments to assess the finite sample size and power of the tests. We use the tests to empirically establish whether the value-weighted market portfolio is the best choice of every individual with preferences exhibiting certain patterns of local attitudes to- wards risk. Our results indicate that we cannot reject the hypothesis of prospect stochastic dominance efficiency for the market portfolio. This is supportive of the claim that the par- ticular portfolio can be rationalized as the optimal choice for any S-shaped utility function. Instead,we reject the hypothesis forMarkowitz stochastic dominance,which could imply that there exist reverse S-shaped utility functions that do not rationalize the market portfolio.
C15|The Wild Bootstrap For Few (treated) Clusters|Inference based on cluster-robust standard errors in linear regression models, using either the Student's t distribution or the wild cluster bootstrap, is known to fail when the number of treated clusters is very small. We propose a family of new procedures calledthe subcluster wild bootstrap, which includes the ordinary wild bootstrap as a limiting case. In the case of pure treatment models, where all observations within clusters are either treated or not, the latter procedure can work remarkably well. The key requirement is that all cluster sizes, regardless of treatment, should be similar. Unfortunately, the analogue of this requirement is not likely to hold for difference-in-differences regressions. Our theoretical results are supported by extensive simulations and an empirical example.
C15|Validity Of Wild Bootstrap Inference With Clustered Errors|We study asymptotic inference based on cluster-robust variance estimators for regression models with clustered errors, focusing on the wild cluster bootstrap and the ordinary wild bootstrap. We stateconditions under which both asymptotic and bootstrap tests and confidence intervals will be asymptotically valid. These conditions put limits on the rates at which the cluster sizes can increase as the number of clusters tends to infinity. To include power in the analysis, we allow the data to be generated under sequences of local alternatives. Simulation experiments illustrate the theoretical results and show that all methods can work poorly in certain cases.
C15|Bootstrap And Asymptotic Inference With Multiway Clustering|We study a cluster-robust variance estimator (CRVE) for regression models with clustering in two dimensions that was proposed in Cameron, Gelback, and Miller (2011). We prove that this CRVE is consistent and yields valid inferences under precisely stated assumptions about moments and cluster sizes. We then propose several wild bootstrap procedures and prove that they are asymptotically valid. Simulations suggest that bootstrap inference tends to be much more accurate than inference based on the t distribution, especially when there are few clusters in at least one dimension. An empirical example confirms that bootstrap inferences can differ substantially from conventional ones.
C15|Pitfalls When Estimating Treatment Effects Using Clustered Data|Inference for estimates of treatment effects with clustered data requires great care when treatment is assigned at the group level. This is true for both pure treatment models anddifference-in-differences regressions. Even when the number of clusters is quite large, cluster-robust standard errors can be much too small if the number of treated (or control) clusters is small. Standard errors also tend to be too small when cluster sizes vary a lot, resulting in too many false positives. Bootstrap methods generally perform better than t-tests, but they can also yield very misleading inferences in some cases.
C15|Inference for Impulse Responses under Model Uncertainty|In many macroeconomic applications, impulse responses and their (bootstrap) confidence intervals are constructed by estimating a VAR model in levels - thus ignoring uncertainty regarding the true (unknown) cointegration rank. While it is well known that using a wrong cointegration rank leads to invalid (bootstrap) inference, we demonstrate that even if the rank is consistently estimated, ignoring uncertainty regarding the true rank can make inference highly unreliable for sample sizes encountered in macroeconomic applications. We investigate the effects of rank uncertainty in a simulation study, comparing several methods designed for handling model uncertainty. We propose a new method - Weighted Inference by Model Plausibility (WIMP) - that takes rank uncertainty into account in a fully data-driven way and outperforms all other methods considered in the simulation study. The WIMP method is shown to deliver intervals that are robust to rank uncertainty, yet allow for meaningful inference, approaching fixed rank intervals when evidence for a particular rank is strong. We study the potential ramifications of rank uncertainty on applied macroeconomic analysis by re-assessing the effects of fiscal policy shocks based on a variety of identification schemes that have been considered in the literature. We demonstrate how sensitive the results are to the treatment of the cointegration rank, and show how formally accounting for rank uncertainty can affect the conclusions.
C15|Causes and Effects of Negative Definite Covariance Matrices in Swamy Type Random Coefficient Models|In this paper, we investigate the causes and the finite-sample consequences of negative definite covariance matrices in Swamy type random coefficient models. Monte Carlo experiments reveal that the negative definiteness problem is less severe when the degree of coefficient dispersion is substantial, and the precision of the regression disturbances is high. The sample size also plays a crucial role. We then demonstrate that relying on the asymptotic properties of a biased but consistent estimator of the random coefficient covariance may lead to poor inference.
C15|Normality Tests for Dependent Data|The paper considers the problem of testing for normality of the one-dimensional marginal distribution of a strictly stationary and weakly dependent stochastic process. The possibility of using an autoregressive sieve bootstrap procedure to obtain critical values and P-values for normality tests is explored. The small-sample properties of a variety of tests are investigated in an extensive set of Monte Carlo experiments. The bootstrap version of the classical skewness–kurtosis test is shown to have the best overall performance in small samples.
C15|BIMic: the Bank of Italy microsimulation model for the Italian tax and benefit system|The paper presents BIMic, a static and non-behavioural microsimulation model developed at the Bank of Italy. BIMic reproduces the main features of the Italian tax and benefit system, such as social security contributions, personal income tax, property taxes, family allowances and some other social benefits. It aims to evaluate the budgetary impact and distributive effects of tax-benefit programmes. Such programmes may be actually operating at a given point in time or may be a counterfactual set. To illustrate a potential use of BIMic, this paper discusses the distributive impact of a recently approved legislative innovation regarding the additional transfer to pensioners (known as the quattordicesima ai pensionati).
C15|A goodness-of-fit test for Generalized Error Distribution|The Generalized Error Distribution is a widely used flexible family of symmetric probability distribution. Thanks to its properties, it is becoming more and more popular in many fields of science, and therefore it is important to determine whether a sample is drawn from a GED, usually done using a graphical approach. In this paper we present a new goodness-of-fit test for GED that performs well in detecting non-GED distribution when the alternative distribution is either skewed or a mixture. A comparison between well-known tests and this new procedure is performed through a simulation study. We have developed a function that performs the analysis described in this paper in the R environment. The computational time required to compute this procedure is negligible.
C15|Reevaluation of the capital charge in insurance after a large shock: empirical and theoretical views|Motivated by the recent introduction of regulatory stress tests in the Solvency II framework, we study the impact of the re-estimation of the tail risk and of loss absorbing capacities on post-stress solvency ratios. Our contribution is threefold. First, we build the first stylised model for re-estimated solvency ratio in insurance. Second, this leads us to solve a new theoretical problem in statistics: what is the asymptotic impact of a record on the re-estimation of tail quantiles and tail probabilities for classical extreme value estimators? Third, we quantify the impact of the re-estimation of tail quantiles and of loss absorbing capacities on real-world solvency ratios thanks to regulator data from Banque de France – ACPR. Our analysis sheds a first light on the role of the loss absorbing capacity and its paramount importance in the Solvency II capital charge computations. We conclude with a number of policy recommendations for insurance regulators.<br><small>(This abstract was borrowed from another version of this item.)</small>
C15|On the estimation of regime-switching Lévy models|The regime-switching Lévy model combines jump-diffusion under the form of a Lévy process, and Markov regime-switching where all parameters depend on the value of a continuous time Markov chain. We start by giving general stochastic results. Estimation is performed following a two-step procedure. The EM-algorithm is extended to this new class of jump-diffusion regime-switching models. Simulations are proposed, alongside an empirical application dedicated to the study of financial and commodity time series. When comparing the results with (i) non regime-switching models, and (ii) continuous regime-switching models (where the Lévy process is replaced by a classic Brownian motion), the Lévy regime-switching model outperforms other competitors.
C15|Meta-Analysis and Publication Bias: How Well Does the FAT-PET-PEESE Procedure Work?|This paper studies the performance of the FAT-PET-PEESE (FPP) procedure, a commonly employed approach for addressing publication bias in the economics and business meta-analysis literature. The FPP procedure is generally used for three purposes: (i) to test whether a sample of estimates suffers from publication bias, (ii) to test whether the estimates indicate that the effect of interest is statistically different from zero, and (iii) to obtain an estimate of the overall mean effect. Our findings indicate that the FPP procedure performs well in the basic but unrealistic environment of “fixed effects”, where all estimates are assumed to derive from a single population value and sampling error is the only reason for why studies produce different estimates. However, when we study its performance in more realistic data environments, where there is heterogeneity in the population effects across and within studies, the FPP procedure becomes unreliable for the first two purposes, and is less efficient than other estimators when estimating overall mean effect. Further, hypothesis tests about the overall, mean effect cannot be trusted.
C15|Mother's Time Allocation, Child Care and Child Cognitive Development|This paper analyzes the effects of maternal employment and non-parental child care on child cognitive development, taking into account the mother's time allocation between leisure and child-care time. I estimate a behavioral model, in which maternal labor supply, non-parental child care, goods expenditure and time allocation decisions are considered to be endogenous choices of the mother. The child cognitive development depends on maternal and non-parental child care and on the goods bought for the child. The model is estimated using US data from the Child Development Supplement and the Time Diary Section of the Panel Study of Income Dynamics. The results show that the productivity of mother's child-care time substantially differs by a mother's level of education. Moreover, the child-care time of college-educated mothers is more productive than non-parental child care. The simulation of maternity leave policies, mandating mothers not to work in the first two years of the child's life, reveals that the impact on the child's test score at age five is either positive or negative, depending on whether the leave is paid or not. The heterogeneous productivity of mothers' time leads to different allocation choices between child care and leisure: college-educated mothers re-allocate a larger fraction of their time out of work to child care than do the lower educated, while the opposite holds for leisure.
C15|Testing for Alpha in Linear Factor Pricing Models with a Large Number of Securities|This paper proposes a novel test of zero pricing errors for the linear factor pricing model when the number of securities, N, can be large relative to the time dimension, T, of the return series. The test is based on Student t tests of individual securities and has a number of advantages over the existing standardised Wald type tests. It allows for non-Gaussianity and general forms of weakly cross correlated errors. It does not require estimation of an invertible error covariance matrix, it is much faster to implement, and is valid even if N is much larger than T. Monte Carlo evidence shows that the proposed test performs remarkably well even when T = 60 and N = 5,000. The test is applied to monthly returns on securities in the S&P 500 at the end of each month in real time, using rolling windows of size 60. Statistically significant evidence against Sharpe-Lintner CAPM and Fama-French three factor models are found mainly during the recent financial crisis. Also we find a significant negative correlation between a twelve-months moving average p-values of the test and excess returns of long/short equity strategies (relative to the return on S&P 500) over the period November 1994 to June 2015, suggesting that abnormal profits are earned during episodes of market inefficiencies.
C15|The Career Costs of Children|We estimate a dynamic life cycle model of labor supply, fertility, and savings, incorporating occupational choices, with specific wage paths and skill atrophy that vary over the career. This allows us to understand the trade-off between occupational choice and desired fertility, as well as sorting both into the labor market and across occupations. We quantify the life cycle career costs associated with children, how they decompose into loss of skills during interruptions, lost earnings opportunities, and selection into more child-friendly occupations. We analyze the long-run effects of policies that encourage fertility and show that they are considerably smaller than short-run effects.
C15|Inference for Impulse Responses under Model Uncertainty|In many macroeconomic applications, confidence intervals for impulse responses are constructed by estimating VAR models in levels - ignoring cointegration rank uncertainty. We investigate the consequences of ignoring this uncertainty. We adapt several methods for handling model uncertainty and highlight their shortcomings. We propose a new method - Weighted-Inference-by-Model-Plausibility (WIMP) - that takes rank uncertainty into account in a data-driven way. In simulations the WIMP outperforms all other methods considered, delivering intervals that are robust to rank uncertainty, yet not overly conservative. We also study potential ramifications of rank uncertainty on applied macroeconomic analysis by re-assessing the effects of fiscal policy shocks.
C15|Estimating the impact of sericulture adoption on farmer income in Rwanda: an application of propensity score matching| The adoption of an agricultural technology is often seen as a way to overcome the constraints imposed by the existing resources and/or production methods. As a small landlocked country, Rwanda sought to develop the capability to produce silk, a high value-to-volume ratio product, as a means to overcome the constraints of high transportation cost of exports. Sericulture was also seen as a handy strategy to boost rural farmer income by putting previously less productive land to use for mulberry plantations. Because sericulture was not introduced randomly, this study relied on observational data and applied propensity score matching to estimate its income and poverty reduction effects in six rural districts. The results indicate that sericulture adoption had beneficial effects both on increasing income and reducing poverty. The strengthening of related skills development and the supporting infrastructure remains crucial for the sericulture to successfully diffuse and yield economic benefits commensurate with its potential.
C15|Joining the Incompatible: Exploiting Floristic Lists for the Sample-based Estimation of Species Richness|The lists of species obtained by purposive sampling by field ecologists are used to improve the sample-based estimation of species richness. The estimation criterion is a modification of the difference estimator in which the species inclusion probabilities are estimated by means of the species frequencies from incidence data. If the species list used to support the estimation is complete the estimator guesses the true richness without error. Moreover, contrary to the nonparametric estimators, our estimator provides values invariably greater than the number of species detected by the combination of sample-based and purposive surveys. A presumably asymptotically conservative estimator of the mean squared error is also provided. A simulation study based on two artificial populations is carried out in order to check the performance of the proposed estimator with respect to the familiar nonparametric estimators. Finally, the proposed estimator is adopted to estimate species richness in the Maremma Regional Park, Italy.
C15|Analytic Bias Correction for Maximum Likelihood Estimators When the Bias Function is Non-Constant|Recently, many papers have obtained analytic expressions for the biases of various maximum likelihood estimators, despite their lack of closed-form solution. These bias expressions have provided an attractive alternative to the bootstrap. Unless the bias function is “flat,” however, the expressions are being evaluated at the wrong point(s). We propose an “improved” analytic bias adjusted estimator, in which the bias expression is evaluated at a more appropriate point (at the bias adjusted estimator itself). Simulations illustrate that the improved analytic bias adjusted estimator can eliminate significantly more bias than the simple estimator which has been well established in the literature.
C15|Simulation error in maximum likelihood estimation of discrete choice models|Maximum simulated likelihood is the preferred estimator of most researchers who deal with discrete choice. It allows estimation of models such as mixed multinomial logit (MXL), generalized multinomial logit, or hybrid choice models, which have now become the state-of-practice in the microeconometric analysis of discrete choice data. All these models require simulation-based solving of multidimensional integrals, which can lead to several numerical problems. In this study, we focus on one of these problems – utilizing from 100 to 1,000,000 draws, we investigate the extent of the simulation bias resulting from using several different types of draws: (1) pseudo random numbers, (2) modified Latin hypercube sampling, (3) randomized scrambled Halton sequence, and (4) randomized scrambled Sobol sequence. Each estimation is repeated up to 1 000 times. The simulations use several artificial datasets based on an MXL data generating process with different numbers of individuals (400, 800, 1 200), different numbers of choice tasks per respondent (4, 8, 12), different number of attributes (5, 10), and different experimental designs (D-optimal, D-efficient for the MNL and D-efficient for the MXL model). Our large-scale simulation study allows for comparisons and drawing conclusions with respect to (1) how efficient different types of quasi Monte Carlo simulation methods are and (2) how many draws one should use to make sure the results are of “satisfying” quality – under different experimental conditions. Our study is the first to date to offer such a comprehensive comparison. Overall, we find that the number of the best-performing Sobol draws required for the desired precision exceeds 2 000 in some of the 5-attribute settings, and 20,000 in the case of some 10-attribute settings considered.
C15|Online Annex – Economic Challenges of Lagging Regions: Annex II – Econometric Analysis and Supplemental Tables|This report is an annex to wiiw Research Report 423, ‘Economic Challenges of Lagging Regions III Recent Investment Trends and Needs’. Based on spatial econometric methods, it provides estimates and simulations of the investment effects on economic development in the EU lagging regions. It also provides additional data related to the analysis in wiiw Research Report 423.
C15|Testing for Alpha in Linear Factor Pricing Models with a Large Number of Securities|This paper proposes a novel test of zero pricing errors for the linear factor pricing model when the number of securities, N, can be large relative to the time dimension, T, of the return series. The test is based on Student t tests of individual securities and has a number of advantages over the existing standardised Wald type tests. It allows for non-Gaussianity and general forms of weakly cross correlated errors. It does not require estimation of an invertible error covariance matrix, it is much faster to implement, and is valid even if N is much larger than T. Monte Carlo evidence shows that the proposed test performs remarkably well even when T = 60 and N = 5;000. The test is applied to monthly returns on securities in the S&P 500 at the end of each month in real time, using rolling windows of size 60. Statistically significant evidence against Sharpe-Lintner CAPM and Fama-French three factor models are found mainly during the recent financial crisis. Also we find a significant negative correlation between a twelve-months moving average p-values of the test and excess returns of long/short equity strategies (relative to the return on S&P 500) over the period November 1994 to June 2015, suggesting that abnormal profits are earned during episodes of market inefficiencies.
C15|M-PRESS-CreditRisk: A holistic micro- and macroprudential approach to capital requirements|M-PRESS-CreditRisk is a new top-down macro stress testing framework that can help supervisors gauge banks' capital adequacy related to credit risk. For the first time, it combines calibration of microprudential capital requirements and macroprudential buffers in a unified, coherent framework. Its core element is an advanced credit portfolio model - SystemicCreditRisk - built upon a rich, non-linear dependence structure for interconnected bank portfolios. Incorporating numerous sector/country-specific systematic factors, the model focuses on credit default concentration risk as a major source of large losses that may have systemic impact. A test run using a sample of 12 systemically important German banks provides measures for systemic credit risk and the banks' contributions to it in both baseline and stress scenarios. Capital requirements calibrated to the results combine elements of Pillar 1 and Pillar 2, whereas macroprudential buffers can internalize the system's tail risk. The maximum model-based combined requirements range between 6.3% and 27.2% of credit RWA depending on the bank. A comparison with the reported capital figures suggests that there appears to be enough capital in the banking system, but its distribution might be suboptimal from a systemic point of view as the capital level of a number of banks might need improvement.
C15|Estimation of agent-based models using sequential Monte Carlo methods|Estimation of agent-based models is currently an intense area of research. Recent contributions have to a large extent resorted to simulation-based methods mostly using some form of simulated method of moments estimation (SMM). There is, however, an entire branch of statistical methods that should appear promising, but has to our knowledge never been applied so far to estimate agent-based models in economics and finance: Markov chain Monte Carlo methods designed for state space models or models with latent variables. This later class of models seems particularly relevant as agent-based models typically consist of some latent and some observable variables since not all the characteristics of agents would mostly be observable. Indeed, one might often not only be interested in estimating the parameters of a model, but also to infer the time development of some latent variable. However, agent-based models when interpreted as latent variable models would be typically characterized by non-linear dynamics and non-Gaussian fluctuations and, thus, would require a computational approach to statistical inference. Here we resort to Sequential Monte Carlo (SMC) estimation based on a particle filter. This approach is used here to numerically approximate the conditional densities that enter into the likelihood function of the problem. With this approximation we simultaneously obtain parameter estimates and filtered state probabilities for the unobservable variable(s) that drive(s) the dynamics of the observable time series. In our examples, the observable series will be asset returns (or prices) while the unobservable variables will be some measure of agents' aggregate sentiment. We apply SMC to two selected agent-based models of speculative dynamics with somewhat different flavor. The empirical application to a selection of financial data includes an explicit comparison of the goodness-of-fit of both models.
C15|Machine learning to improve experimental design|This paper proposes a way of using observational pretest data for the design of experiments. In particular, this paper trains a random forest on the pretest data and stratifies the allocation of treatments to experimental units on the predicted dependent variables. This approach reduces much of the arbitrariness involved in defining strata directly on the basis of covariates. A simulation on 300 random samples drawn from six data sets shows that this algorithm is extremely effective in reducing the variance of the estimation compared to random allocation and to traditional ways of stratification. On average, this stratification approach requires half the sample size to estimate the treatment effect with the same precision as complete randomization. In more than 80% of all samples the estimated variance of the treatment estimator is lower and the estimated statistical power is higher than for standard designs such as complete randomization, conventional stratification or Mahalanobis matching.
C15|An econometric method for estimating population parameters from non‐random samples: An application to clinical case finding| The problem of sample selection complicates the process of drawing inference about populations. Selective sampling arises in many real world situations when agents such as doctors and customs officials search for targets with high values of a characteristic. We propose a new method for estimating population characteristics from these types of selected samples. We develop a model that captures key features of the agent's sampling decision. We use a generalized method of moments with instrumental variables and maximum likelihood to estimate the population prevalence of the characteristic of interest and the agents' accuracy in identifying targets. We apply this method to tuberculosis (TB), which is the leading infectious disease cause of death worldwide. We use a national database of TB test data from South Africa to examine testing for multidrug resistant TB (MDR‐TB). Approximately one quarter of MDR‐TB cases was undiagnosed between 2004 and 2010. The official estimate of 2.5% is therefore too low, and MDR‐TB prevalence is as high as 3.5%. Signal‐to‐noise ratios are estimated to be between 0.5 and 1. Our approach is widely applicable because of the availability of routinely collected data and abundance of potential instruments. Using routinely collected data to monitor population prevalence can guide evidence‐based policy making.
C15|A new semiparametric approach for mediation analyses|In economics it is very important to understand the mechanisms which determine the phenomena of interest. For example in marketing it is very important to analyze the factors which determine the future choices of the customer such that it is possible to influence the sales of a product. In most of the economic applications, to make this the researcher supposes the knowledge of the mathematical relations among the variables of interest and in particular these relations are supposed linear. In this work I propose a new estimation method for mediation models without supposing the form of the mathematical relations but aggregating many different equations. My new estimation method derives by Bauer?s work (2005) about the semiparametric approach for SEM (structural equation models). To apply my estimation method, I propose new formulas to calculate the direct, indirect and total effects. I apply my method both to simulated data and to marketing data.
C15|Stata programming of confidence regions for the ratio of two percentiles|In the wood industry, it is common practice to compare in terms of the ratio of the same strength properties for lumber of two different dimensions, grades or species. Because United States lumber standards are given in terms of population fifth percentile, and strength problems arised from the weaker fifth percentile rather than the stronger mean, the ratio should be expressed in terms of the fifth percentiles rather than the means of two strength distributions. Stata is an useful tool of handling the big data because the upper limit of number of observations in its SE version is 2,147,583,646. In this paper, an ado-file will be written for Stata programming of non parametric confidence regions for the ratio of two percentiles. In Stata, a scalar is used to save a value rather than a variable. The variable name is suggested to contain only small letters. A matrix will be used to save the Gaussian kernels contributed by all observations and the percentile when the confidence region is being constructed.
C15|Evaluating Investments in Portability and Interoperability between Software Service Platforms|Within a closed ecosystem, end-users cannot interoperate with other platforms or port their software and data easily without a cost for interface integration or data re-formatting. The customers of these closed software service platforms are locked-in. Potential customers, who are aware of this lock-in issue, are hesitant to adopt a closed software service platform, slowing down the wide deployment of the software service platform. This paper applies an economic perspective to investigate the value creation for providers and users at different levels of interoperability. For the analysis, a value creation model for software service platforms within a software service ecosystem has been developed. Simulations of the value creation model show that, even if investments in interoperability and portability are aimed at addressing user requirements, their impact also drives the providers’ profitability. Furthermore, emerging providers require investing more than market leading providers, as they have less power to set de facto standards. The simulation results also show that there is an optimal level of investments, with respect to profit and return on investments. Overall, from these results, platform providers cannot only obtain an understanding on how investments in interoperability and portability impact cost, enable cost-effective service integration, and create value, but also design new strategies for optimizing investments.
