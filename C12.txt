C12|Variance swap payoffs, risk premia and extreme market conditions|This paper estimates the Variance Risk Premium (VRP) directly from synthetic variance swap payoffs. Since variance swap payoffs are highly volatile, we extract the VRP by using signal extraction techniques based on a state-space representation of our model in combination with a simple economic constraint. Our approach, only requiring option implied volatilities and daily returns for the underlying, provides measurement error free estimates of the part of the VRP related to normal market conditions, and allows constructing variables indicating agents' expectations under extreme market conditions. The latter variables and the VRP generate different return predictability on the major US indices. A factor model is proposed to extract a market VRP which turns out to be priced when considering Fama and French portfolios.
C12|Inference for Local Distributions at High Sampling Frequencies: A Bootstrap Approach|"We study inference for the local innovations of It^o semimartingales. Specifically, we construct a resampling procedure for the empirical CDF of high-frequency innovations that have been standardized using a nonparametric estimate of its stochastic scale (volatility) and truncated to rid the effect of ""large"" jumps. Our locally dependent wild bootstrap (LDWB) accommodate issues related to the stochastic scale and jumps as well as account for a special block-wise dependence structure induced by sampling errors. We show that the LDWB replicates first and second-order limit theory from the usual empirical process and the stochastic scale estimate, respectively, as well as an asymptotic bias. Moreover, we design the LDWB sufficiently general to establish asymptotic equivalence between it and and a nonparametric local block bootstrap, also introduced here, up to second-order distribution theory. Finally, we introduce LDWB-aided Kolmogorov-Smirnov tests for local Gaussianity as well as local von-Mises statistics, with and without bootstrap inference, and establish their asymptotic validity using the second-order distribution theory. The finite sample performance of CLT and LDWB-aided local Gaussianity tests are assessed in a simulation study as well as two empirical applications. Whereas the CLT test is oversized, even in large samples, the size of the LDWB tests are accurate, even in small samples. The empirical analysis verifies this pattern, in addition to providing new insights about the distributional properties of equity indices, commodities, exchange rates and popular macro finance variables."
C12|Models with Multiplicative Decomposition of Conditional Variances and Correlations|Univariate and multivariate GARCH type models with multiplicative decomposition of the variance to short and long run components are surveyed. The latter component can be either deterministic or stochastic. Examples of both types are studied.
C12|Forecaster’s utility and forecasts coherence|I provide general frequentist framework to elicit the forecaster’s expected utility based on a Lagrange Multiplier-type test for the null of locality of the scoring rules associated to the probabilistic forecast. These are assumed to be observed transition variables in a nonlinear autoregressive model to ease the statistical inference. A simulation study reveals that the test behaves consistently with the requirements of the theoretical literature. The locality of the scoring rule is fundamental to set dating algorithms to measure and forecast probability of recession in US business cycle. An investigation of Bank of Norway’s forecasts on output growth leads us to conclude that forecasts are often suboptimal with respect to some simplistic benchmark if forecaster’s reward is not properly evaluated.
C12|Jury Theorems|We give a review of jury theorems, including Condorcet's (1785) classic theorem and several later refinements and departures. The review comes with a critique of jury theorems from a social-epistemology perspective. We assess the plausibility of the theorems' conclusions and premises and the potential of jury theorems to serve as formal arguments for the 'wisdom of crowds'. In particular, we argue (i) that there is a fundamental tension between voters' independence and voters' competence, hence between the two premises of typical jury theorems; (ii) that the (asymptotic) conclusion that 'huge groups are infallible', reached by many jury theorems, is an artifact of unjustified premises; and (iii) that the (non-asymptotic) conclusion that 'larger groups are more reliable', also reached by many jury theorems, is not an artifact and should be regarded as the more adequate formal rendition of the 'wisdom of crowds'.
C12|Interaction matrix selection in spatial autoregressive models with an application to growth theory|The interaction matrix, or spatial weight matrix, is the fundamental tool to model cross-sectional interdependence between observations in spatial autoregressive models. However, it is most of the time not derived from theory, as it should be ideally, but chosen on an ad hoc basis. In this paper, we propose a modified version of the J test to formally select the interaction matrix. Our methodology is based on the application of the robust against unknown heteroskedasticity GMM estimation method, developed by Lin and Lee (2010). We then implement the testing procedure developed by Hagemann (2012) to overcome the decision problem inherent to non-nested models tests. An application of the testing procedure is presented for the Schumpeterian growth model with worldwide interactions developed by Ertur and Koch (2011) using three different types of interaction matrices: genealogic distance, linguistic distance and bilateral trade flows. We find that the interaction matrix based on trade flows is the most adequate.
C12|Connecting Sharpe ratio and Student t-statistic, and beyond|Sharpe ratio is widely used in asset management to compare and benchmark funds and asset managers. It computes the ratio of the excess return over the strategy standard deviation. However, the elements to compute the Sharpe ratio, namely, the expected returns and the volatilities are unknown numbers and need to be estimated statistically. This means that the Sharpe ratio used by funds is subject to be error prone because of statistical estimation error. Lo (2002), Mertens (2002) derive explicit expressions for the statistical distribution of the Sharpe ratio using standard asymptotic theory under several sets of assumptions (independent normally distributed-and identically distributed returns). In this paper, we provide the exact distribution of the Sharpe ratio for independent normally distributed return. In this case, the Sharpe ratio statistic is up to a rescaling factor a non centered Student distribution whose characteristics have been widely studied by statisticians. The asymptotic behavior of our distribution provides the result of Lo (2002). We also illustrate the fact that the empirical Sharpe ratio is asymptotically optimal in the sense that it achieves the Cramer Rao bound. We then study the empirical SR under AR(1) assumptions and investigate the effect of compounding period on the Sharpe (computing the annual Sharpe with monthly data for instance). We finally provide general formula in this case of heteroscedasticity and autocorrelation. JEL classification: C12, G11.
C12|Mixed Causal-Noncausal Autoregressions: Bimodality Issues in Estimation and Unit Root Testing|This paper stresses the bimodality of the widely used Student's t likelihood function applied in modelling Mixed causal-noncausal AutoRegressions (MAR). It first shows that a local maximum is very often to be found in addition to the global Maximum Likelihood Estimator (MLE), and that standard estimation algorithms could end up in this local maximum. It then shows that the issue becomes more salient as the causal root of the process approaches unity from below. The consequences are important as the local maximum estimated roots are typically interchanged, attributing the noncausal one to the causal component and vice-versa, which severely changes the interpretation of the results. The properties of unit root tests based on this Student's t MLE of the backward root are obviously a ected as well. To circumvent this issues, this paper proposes an estimation strategy which i) increases noticeably the probability to end up in the global MLE and ii) retains the maximum relevant for the unit root test against a MAR stationary alternative. An application to Brent crude oil price illustrates the relevance of the proposed approach.
C12|Fertility Response to Climate Shocks|In communities highly dependent on rainfed agriculture for their livelihoods, the common oc- currence of climatic shocks such as droughts can lower the opportunity cost of having children, and raise fertility. Using longitudinal household data from Madagascar, we estimate the causal effect of drought occurrences on fertility, and explore the nature of potential mechanisms driving this effect. We exploit exogenous within-district year-to-year variation in rainfall deficits, and find that droughts occurring during the agricultural season significantly increase the number of children born to women living in agrarian communities. This effect is long lasting, as it is not reversed within four years fol- lowing the drought occurrence. Analyzing the mechanism, we find that droughts have no effect on common underlying factors of high fertility such as marriage timing and child mortality. Further- more, droughts have no significant effect on fertility if they occur during the non-agricultural season or in non-agrarian communities, and their positive effect in agrarian communities is mitigated by irrigation. These findings provide evidence that a low marginal price of having children is the main channel driving the fertility effect of drought in agrarian communities.
C12|Testing for breaks in the cointegrating relationship: On the stability of government bond markets' equilibrium|In this paper, test procedures for no fractional cointegration against possible breaks in the persistence structure of a fractional cointegrating relationship are introduced. The tests proposed are based on the supremum of the Hassler and Breitung (2006) test statistic for no cointegration over possible breakpoints in the long-run equilibrium. We show that the new tests correctly standardized converge to the supremum of a chisquared distribution, and that this convergence is uniform. An in-depth Monte Carlo analysis provides results on the finite sample performance of our tests. We then use the new procedures to investigate whether there was a dissolution of fractional cointegrating relationships between benchmark government bonds of ten EMU countries (Spain, Italy, Portugal, Ireland, Greece, Belgium, Austria, Finland, the Netherlands and France) and Germany with the beginning of the European debt crisis.
C12|On the Limit Theory of Mixed to Unity VARs: Panel Setting With Weakly Dependent Errors|In this paper we re-visit a recent theoretical idea introduced by Phillips and Lee (2015). They examine an empirically relevant situation when multiple time series under consideration exhibit different degrees of non-stationarity. By bridging the asymptotic theory of the local to unity and mildly explosive processes, they construct a Wald test for the commonality of the long-run behavior of two series. Therefore, a vector autoregressive (VAR) setup is natural. However, inference is complicated by the fact that the statistic is degenerate under the null and divergent under the alternative. This is true if the parameters of the data generating process are known and re-normalizing function can be constructed. If the parameters are unknown, as is in practice, the test statistic may be divergent even under the null. We solve this problem by converting the original setting of one vector time series in a panel setting with N individual vector series. We consider asymptotics with fixed N and large T and extend the results to sequential asymptotics when T passes to infinity before N. We show that the Wald test statistic converges to nuisance parameter-free Chi-squared distribution under the null hypothesis.
C12|Pitfalls of Two-Step Testing for Changes in the Error Variance and Coefficients of a Linear Regression Model|In empirical applications based on linear regression models, structural changes often occur in both the error variance and regression coefficients, possibly at different dates. A commonly applied method is to first test for changes in the coefficients (or in the error variance) and, conditional on the break dates found, test for changes in the variance (or in the coefficients). In this note, we provide evidence that such procedures have poor finite sample properties when the changes in the first step are not correctly accounted for. In doing so, we show that testing for changes in the coefficients (or in the variance) ignoring changes in the variance (or in the coefficients) induces size distortions and loss of power. Our results illustrate a need for a joint approach to test for structural changes in both the coefficients and the variance of the errors. We provide some evidence that the procedures suggested by Perron et al. (2019) provide tests with good size and power.
C12|Inference under covariate-adaptive randomization with multiple treatments|" This paper studies inference in randomized controlled trials with covariate-adaptive randomization when there are multiple treatments. More specifically, we study in this setting inference about the average effect of one or more treatments relative to other treatments or a control. As in Bugni et al. (2017), covariate-adaptive randomization refers to randomization schemes that first stratify according to baseline covariates and then assign treatment status so as to achieve ""balance"" within each stratum. In contrast to Bugni et al. (2017), however, we allow for the proportion of units being assigned to each of the treatments to vary across strata. We first study the properties of estimators derived from a ""fully saturated"" linear regression, i.e., a linear regression of the outcome on all interactions between indicators for each of the treatments and indicators for each of the strata. We show that tests based on these estimators using the usual heteroskedasticity-consistent estimator of the asymptotic variance are invalid in the sense that they may have limiting rejection probability under the null hypothesis strictly greater than the nominal level; on the other hand, tests based on these estimators and suitable estimators of the asymptotic variance that we provide are exact in the sense that they have limiting rejection probability under the null hypothesis equal to the nominal level. For the special case in which the target proportion of units being assigned to each of the treatments does not vary across strata, we additionally consider tests based on estimators derived from a linear regression with ""strata fixed effects,"" i.e., a linear regression of the outcome on indicators for each of the treatments and indicators for each of the strata. We show that tests based on these estimators using the usual heteroskedasticity-consistent estimator of the asymptotic variance are conservative in the sense that they have limiting rejection probability under the null hypothesis no greater than and typically strictly less than the nominal level, but tests based on these estimators and suitable estimators of the asymptotic variance that we provide are exact, thereby generalizing results in Bugni et al. (2017) for the case of a single treatment to multiple treatments. A simulation study illustrates the practical relevance of our theoretical results."
C12|What Do We Really Know about the Employment Effects of the UK's National Minimum Wage?|A substantial body of research on the UK's National Minimum Wage (NMW) has concluded that the the NMW has not had a detrimental effect on employment. This research has directly influenced, through the Low Pay Commission, the conduct of policy, including the subsequent introduction of the National Living Wage (NLW). We revisit this literature and offer a reassessment, motivated by two concerns. First, much of this literature employs difference-in-difference designs, even though there are significant challenges in conducting appropriate inference in such designs, and they can have very low power when inference is conducted appropriately. Second, the literature has focused on the binary outcome of statistical rejection of the null hypothesis, without attention to the range of (positive or negative) impacts on employment that are consistent with the data. In our re-analysis of the data, we conduct inference using recent suggestions for best practice and consider what magnitude of employment effects the data can and cannot rule out. We find that the data are consistent with both large negative and small positive impacts of the UK National Minimum Wage on employment. We conclude that the existing data, combined with difference-in-difference designs, in fact offered very little guidance to policy makers.
C12|Practical Significance, Meta-Analysis and the Credibility of Economics|Recently, there has been much discussion about replicability and credibility. By integrating the full research record, increasing statistical power, reducing bias and enhancing credibility, meta-analysis is widely regarded as 'best evidence'. Through Monte Carlo simulation, closely calibrated on the typical conditions found among 6,700 economics research papers, we find that large biases and high rates of false positives will often be found by conventional meta-analysis methods. Nonetheless, the routine application of meta-regression analysis and considerations of practical significance largely restore research credibility.
C12|Statistical Analysis and Evaluation of Macroeconomic Policies: A Selective Review|In this paper, we highlight some recent developments of a new route to evaluate macroeconomic policy effects, which are investigated under the framework with potential out- comes. First, this paper begins with a brief introduction of the basic model setup in modern econometric analysis of program evaluation. Secondly, primary attention goes to the focus on causal effect estimation of macroeconomic policy with single time series data together with some extensions to multiple time series data. Furthermore, we examine the connection of this new approach to traditional macroeconomic models for policy analysis and evaluation. Finally, we conclude by addressing some possible future research directions in statistics and econometrics.
C12|Testing Unconfoundedness Assumption Using Auxiliary Variables|In this paper, we propose an alternative test procedure for testing the conditional independence assumption which is an important identication condition commonly imposed in the literature of program analysis and policy evaluation. We transform the conditional independence test to a nonparametric conditional moment test using an auxiliary variable which is independent of the treatment assignment variable conditional on potential outcomes and observable covariates. The proposed test statistic is shown to have a limiting normal distribution under null hypotheses of conditional independence. Furthermore, the suggested method is shown to be valid under time series framework and thus the corresponding test statistic and its limiting distribution are also established. Monte Carlo simulations are conducted to examine the finite sample performances of the proposed test statistics. Finally, the proposed test method is applied to test the conditional independence in real examples: the 401(k) participation program and return to college education.
C12|Assessing predictive accuracy in panel data models with long-range dependence|This paper proposes tests of the null hypothesis that model-based forecasts are uninformative in panels, allowing for individual and interactive fixed effects that control for cross-sectional dependence, endogenous predictors, and both short-range and long-range dependence. We consider a Diebold-Mariano style test based on comparison of the model-based forecast and a nested nopredictability benchmark, an encompassing style test of the same null, and a test of pooled uninformativeness in the entire panel. A simulation study shows that the encompassing style test is reasonably sized in finite samples, whereas the Diebold-Mariano style test is oversized. Both tests have non-trivial local power. The methods are applied to the predictive relation between economic policy uncertainty and future stock market volatility in a multi-country analysis.
C12|Asymptotic theory for clustered samples|We provide a complete asymptotic distribution theory for clustered data with a large number of independent groups, generalizing the classic laws of large numbers, uniform laws, central limit theory, and clustered covariance matrix estimation. Our theory allows for clustered observations with heterogeneous and unbounded cluster sizes. Our conditions cleanly nest the classical results for i.n.i.d. observations, in the sense that our conditions specialize to the classical conditions under independent sampling. We use this theory to develop a full asymptotic distribution theory for estimation based on linear least-squares, 2SLS, nonlinear MLE, and nonlinear GMM.
C12|Deciding with judgment|Non sample information is hidden in frequentist statistics in the choice of the hypothesis to be tested and of the confidence level. Explicit treatment of these elements provides the connection between Bayesian and frequentist statistics. A frequentist decision maker starts from a judgmental decision and moves to the closest boundary of the confidence interval of the first order conditions, for a given loss function. This statistical decision rule does not perform worse than the judgmental decision with a probability equal to the confidence level. For any given prior, there is a mapping from the sample realization to the confidence level which makes Bayesian and frequentist decision rules equivalent. Frequentist decision rules can be interpreted as decisions under ambiguity. JEL Classification: C1, C11, C12, C13, D81
C12|Inference in Differences-in-Differences: How Much Should We Trust in Independent Clusters?|We analyze the conditions in which ignoring spatial correlation is problematic for inference in differences-in-differences (DID) models. Assuming that the spatial correlation structure follows a linear factor model, we show that inference ignoring such correlation remains reliable when either (i) the second moment of the difference between the pre- and post-treatment averages of common factors is low, or (ii) the distribution of factor loadings has the same expected values for treated and control groups, and do not exhibit significant spatial correlation. We present simulation results with real datasets that corroborate these conclusions. Our results provide important guidelines on how to minimize inference problems due to spatial correlation in DID applications.
C12|Matching Estimators with Few Treated and Many Control Observations|We analyze the properties of matching estimators when the number of treated observations is fixed while the number of treated observations is large. We show that, under standard assumptions, the nearest neighbor matching estimator for the average treatment effect on the treated is asymptotically unbiased, even though this estimator is not consistent. We also provide a test based on the theory of randomization tests under approximate symmetry developed in Canay et al. (2014) that is asymptotically valid when the number of control observations goes to infinity. This is important because large sample inferential techniques developed in Abadie and Imbens (2006) would not be valid in this setting.
C12|Influence functions for linear regression (with an application to regression adjustment)|Influence functions are useful, for example, because they provide an easy and flexible way to estimate standard errors. This paper contains a brief overview of influence functions in the context of linear regression and illustrates how their empirical counterparts can be computed in Stata, both for unweighted data and for weighted data. Influence functions for regression-adjustment estimators of average treatment effects are also covered.
C12|Testing for Correlation in Error-Component Models|This paper concerns linear models for grouped data with group-specific effects. We construct a test for the null of no within-group correlation beyond that induced by the group-specific effect. The approach tests against correlation of arbitrary form while allowing for (conditional) heteroskedasticity. Our setup covers models with exogenous, predetermined, or endogenous regressors. We provide theoretical results on size and power under asymptotics where the number of groups grows but their size is held fixed. In simulation experiments we find good size control and high power in a wide range of designs. We also find that our test is more powerful than the popular test developed by Arellano and Bond (1991), which uses only a subset of the information used by our procedure.
C12|Estimation and inference in semiparametric quantile factor models|We propose an estimation methodology for a semiparametric quantile factor panel model. We provide tools for inference that are robust to the existence of moments and to the form of weak cross-sectional dependence in the idiosyncratic error term. We apply our method to CRSP daily data.
C12|Your money and your life: risk attitudes over gains and losses|Prospect theory is the most influential descriptive alternative to the orthodox model of rational choice under risk and uncertainty, in terms of empirical analyses of some of its principal parameters and as a consideration in behavioural public policy. Yet the most distinctive implication of the theory – a fourfold predicted pattern of risk attitudes called the reflection effect – has been infrequently studied and with mixed results over money outcomes, and has never been completely tested over health outcomes. This article reports tests of reflection over money and health outcomes defined by life years gained from treatment. With open valuation exercises, the results suggest qualified support for the reflection effect over money outcomes and strong support over health outcomes. However, in pairwise choice questions, reflection was substantially ameliorated over life years, remaining significant only for treatments that offered short additional durations of life.
C12|CO2 Emissions, Energy Consumption and Economic Growth|The paper investigates the role of consumption of both renewable and sustainable energy, as well as alternative and nuclear energy, in mitigating the effects of carbon dioxide (CO2) emissions, based on the Environmental Kuznets Curve (EKC). The papers introduces a novel variable to capture trade openness, which appears to be a crucial factor in inter-regional co-operation and development, in order to evaluate its effect on the environment, The empirical analysis is based on a sample of nine signatories to the Comprehensive and Progressive Agreement for Trans-Pacific Partnership (CPTPP) for the period 1971-2014, which is based on data availability. The empirical analysis is based on several time series econometric methods, such as the cointegration test, two long run estimators, namely the fully modified ordinary least squares (FMOLS) and dynamic ordinary least squares (DOLS) methods, as well as the Granger causality test. There are several noteworthy empirical findings: it is possible to confirm the U-shaped EKC hypothesis for six countries, namely Australia, Canada, Chile, New Zealand, Peru and Vietnam; there is no evidence of the EKC for Mexico; a reverse-shaped EKC is observed for Japan and Malaysia, there are long run relationships among the variables, the adoption of either renewable energy, or alternative energy and nuclear energy, mitigates CO2 emissions, trade openness leads to more beneficial than harmful impacts in the long run, the Granger causality tests show more bi-directional-relationships between the variables in the long run, and the Granger causality tests show more uni-directional-relationships between the variables in the short run.
C12|New Misspecification Tests for Multinomial Logit Models|Misspecification tests for Multinomial Logit [MNL] models are known to have low power or large size distortion. We propose two new misspecification tests. Both use that preferences across binary pairs of alternatives can be described by independent binary logit models when MNL is true. The first test compares Composite Likelihood parameter estimates based on choice pairs with standard Maximum Likelihood estimates using a Hausman (1978) test. The second tests for overidentification in a GMM framework using more pairs than necessary. A Monte Carlo study shows that the GMM test is in general superior with respect to power and has correct size
C12|Temporal Aggregation of Seasonally Near-Integrated Processes|In this paper we investigate the implications that temporally aggregating, either by average sampling or systematic sampling, a seasonal process has on the integration properties of the resulting series at both the zero and seasonal frequencies. Our results extend the existing literature in three ways. First, they demonstrate the implications of temporal aggregation for a general seasonally integrated process with S seasons. Second, rather than only considering the aggregation of seasonal processes with exact unit roots at some or all of the zero and seasonal frequencies, we consider the case where these roots are local-to-unity (which includes exact unit roots as a special case) such that the original series is near-integrated at some or all of the zero and seasonal frequencies. These results show, among other things, that systematic sampling, although not average sampling, can impact on the non-seasonal unit root properties of the data; for example, even where an exact zero frequency unit root holds in the original data it need not necessarily hold in the systematically sampled data. Moreover, the systematically sampled data could be near-integrated at the zero frequency even where the original data is not. Third, the implications of aggregation on the deterministic kernel of the series are explored.
C12|Testing for Episodic Predictability in Stock Returns|Standard tests based on predictive regressions estimated over the full available sample data have tended to find little evidence of predictability in stock returns. Recent approaches based on the analysis of subsamples of the data have been considered, suggesting that predictability where it occurs might exist only within so-called “pockets of predictability” rather than across the entire sample. However, these methods are prone to the criticism that the sub-sample dates are endogenously determined such that the use of standard critical values appropriate for full sample tests will result in incorrectly sized tests leading to spurious findings of stock returns predictability. To avoid the problem of endogenously determined sample splits, we propose new tests derived from sequences of predictability statistics systematically calculated over sub-samples of the data. Specifically, we will base tests on the maximum of such statistics from sequences of forward and backward recursive, rolling, and double-recursive predictive sub-sample regressions. We develop our approach using the over-identified instrumental variable-based predictability test statistics of Breitung and Demetrescu (2015). This approach is based on partial-sum asymptotics and so, unlike many other popular approaches including, for example, those based on Bonferroni corrections, can be readily adapted to implementation over sequences of subsamples. We show that the limiting distributions of our proposed tests are robust to both the degree of persistence and endogeneity of the regressors in the predictive regression, but not to any heteroskedasticity present even if the sub-sample statistics are based on heteroskedasticity-robust standard errors. We therefore develop fixed regressor wild bootstrap implementations of the tests which we demonstrate to be first-order asymptotically valid. Finite sample behaviour against a variety of temporarily predictable processes is considered. An empirical application to US stock returns illustrates the usefulness of the new predictability testing methods we propose.
C12|Response surface regressions for critical value bounds and approximate p-values in equilibrium correction models|Single-equation conditional equilibrium correction models can be used to test for the existence of a level relationship among the variables of interest. The distributions of the respective test statistics are nonstandard under the null hypothesis of no such relationship and critical values need to be obtained with stochastic simulations. We compute more than 95 billion F -statistics and 57 billion t-statistics for a large number of specifications of the Pesaran, Shin, and Smith (2001, Journal of Applied Econometrics 16: 289Ð326) bounds test. Our large-scale simulations enable us to draw smooth density functions and to estimate response surface models that improve upon and substantially extend the set of available critical values for the bounds test. Besides covering the full range of possible sample sizes and lag orders, our approach notably allows for any number of variables in the long-run level relationship by exploiting the diminishing effect on the distributions of adding another variable to the model. The computation of approximate p-values enables a fine-grained statistical inference and allows us to quantify the finite-sample distortions from using asymptotic critical values. We find that the bounds test can be easily oversized by more than 5 percentage points in small samples.
C12|Tests of Conditional Predictive Ability: Some Simulation Evidence|In this note we provide simulation evidence on the size and power of tests of predictive ability described in Giacomini and White (2006). Our goals are modest but non-trivial. First, we establish that there exist data generating processes that satisfy the null hypotheses of equal finite-sample (un)conditional predictive ability. We then consider various parameterizations of these DGPs as a means of evaluating the size and power properties of the proposed tests. While some of our results reinforce those in Giacomini and White (2006), others do not. We recommend against using the fixed scheme when conducting these tests and provide evidence that very large bandwidths are sometimes required when estimating long-run variances.
C12|Off-Balance Sheet Activities, Inefficiency and Market Power of U.S. Banks|The Lerner index is a well-established measure of firms’ market power, but estimation and interpretation present several challenges, especially for banks. We estimate Lerner indices for U.S. banks for 2001-2016 while (i) accounting for banks’ off-balancesheet activities, (ii) estimating cost and profit functions nonparametrically to avoid mis-specification inherent in parametric estimation of translog functions on banking data, and (iii) allowing for cost and profit inefficiency that can otherwise bias index estimates. We find that banks have more market power than previous studies found, and that failure to account for off-balance-sheet activities or inefficiency can seriously bias estimates of market power.
C12|Tests of Conditional Predictive Ability: A Comment|We investigate a test of equal predictive ability delineated in Giacomini and White (2006; Econometrica). In contrast to a claim made in the paper, we show that their test statistic need not be asymptotically Normal when a fixed window of observations is used to estimate model parameters. An example is provided in which, instead, the test statistic diverges with probability one under the null. Simulations reinforce our analytical results.
C12|Dynamic specification tests for dynamic factor models|We derive computationally simple expressions for score tests of misspecification in parametric dynamic factor models using frequency domain techniques. We interpret those diagnostics as time domain moment tests which assess whether certain autocovariances of the smoothed latent variables match their theoretical values under the null of correct model specification. We also reinterpret reduced‐form residual tests as checking specific restrictions on structural parameters. Our Gaussian tests are robust to nonnormal, independent innovations. Monte Carlo exercises confirm the finite‐sample reliability and power of our proposals. Finally, we illustrate their empirical usefulness in an application that constructs a US coincident indicator.
C12|New Testing Approaches for Mean-Variance Predictability|We propose tests for smooth but persistent serial correlation in risk premia and volatilities that exploit the non-normality of financial returns. Our parametric tests are robust to distributional misspecification, while our semiparametric tests are as powerful as if we knew the true return distribution. Local power analyses confirm their gains over existing methods, while Monte Carlo exercises assess their finite sample reliability. We apply our tests to quarterly returns on the five Fama-French factors for international stocks, whose distributions are mostly symmetric and fat-tailed. Our results highlight noticeable differences across regions and factors and confirm the fragility of Gaussian tests.
C12|An F -type multiple testing approach for assessing randomness of linear mixed models|"In linear mixed models the assessing of the significance of all or a subset of the random effects is often of primary interest. Many techniques have been proposed for this purpose but none of them is completely satisfactory. One of the oldest methods for testing randomness is the F -test but it is often overlooked in modern applications due to poor statistical power and non-applicability in some important situations. In this work a two-step procedure is developed for generalizing an F -test and improving its statistical power. In the first step, by comparing two covariance matrices of a least squares statistics, we obtain a ""repeatable"" F -type test. In the second step, by changing the projected matrix which defines the least squares statistic we apply the test repeteadly to the same data in order to have a set of correlated statistics analyzed within a multiple testing approach. The resulting test is sufficiently general, easy to compute, with an exact distribution under the null and alternative hypothesis and, perhaps more importantly, with a strong increase of statistical power with respect to the F -test."
C12|Exact tests on returns to scale and comparisons of production frontiers in nonparametric models|When benchmarking production units by non-parametric methods like data envelopment analysis (DEA), an assumption has to be made about the returns to scale of the underlying technology. Moreover, it is often also relevant to compare the frontiers across samples of producers. Until now, no exact tests for examining returns to scale assumptions in DEA, or for test of equality of frontiers, have been available. The few existing tests are based on asymptotic theory relying on large sample sizes, whereas situations with relatively small samples are often encountered in practical applications. In this paper we propose three novel tests based on permutations. The tests are easily implementable from the algorithms provided, and give exact significance probabilities as they are not based on asymptotic properties. The first of the proposed tests is a test for the hypothesis of constant returns to scale in DEA. The others are tests for general frontier differences and whether the production possibility sets are, in fact, nested. The theoretical advantages of permutation tests are that they are appropriate for small samples and have the correct size. Simulation studies show that the proposed tests do, indeed, have the correct size and furthermore higher power than the existing alternative tests based on asymptotic theory.
C12|Identification-Robust Nonparametric Inference in a Linear IV Model|For a linear IV regression, we propose two new inference procedures on parameters of endogenous variables that are robust to any identification pattern, do not rely on a linear first-stage equation, and account for heteroskedasticity of unknown form. Building on Bierens (1982), we first propose an Integrated Conditional Moment (ICM) type statistic constructed by setting the parameters to the value under the null hypothesis. The ICM procedure tests at the same time the value of the coefficient and the specification of the model. We then adopt the conditionality principle used by Moreira (2003) to condition on a set of ICM statistics that informs on identification strength. Our two procedures uniformly control size irrespective of identification strength. They are powerful irrespective of the nonlinear form of the link between instruments and endogenous variables and are competitive with existing procedures in simulations and applications.
C12|Technological Innovation, Diffusion and Business Cycle Dynamics|In the U.S. economy, real output growth is forecastable with its own lag and the lagged consumption-output ratio. With technological progress modeled as a random walk, standard real-business-cycle (RBC) models fails to replicate this fact: these models generate equilibrium output dynamics, leading some researchers to question the empirical relevance of technology shocks for generating business cycle dynamics. In this paper, we develop a model economy that exhibits endogenous stochastic growth with aggregate fluctuations driven by technology shocks that are absorbed with some lag, reflecting the costly and time-consuming nature of innovation and diffusion. Our purpose is to compare the resulting business cycle dynamics with those predicted by standard RBC models, which implicitly assume the instantaneous diffusion of new technology. Unlike the standard RBC model, the environment studied here generates business cycle dynamics that better resemble observed patterns.<br><small>(This abstract was borrowed from another version of this item.)</small>
C12|Testing for Overconfidence Statistically: A Moment Inequality Approach|We propose an econometric procedure to test for the presence of overconfidence using data collected by ranking experiments. Our approach applies the techniques from the moment inequality literature. Although a ranking experiment is a typical way to collect data for the analysis of overconfidence, Benoit and Dubra (2011) show that a ranking experiment may generate data that indicate overconfidence even if participants are purely rational Bayesian updaters. Instead, the authors provide a set of inequalities that are consistent with purely rational Bayesian updaters. We propose the application of the tests of moment inequalities developed by Romano et al. (2014) to test such a set of inequalities. Then, we examine the data from Svenson (1981) on driving safety. Our results indicate the presence of overconfidence with respect to safety among US subjects tested by Svenson. However, other cases tested do not show evidence of overconfidence. We also apply our method to re-examine and confirm the results of Benoit et al. (2015).
C12|Yield Curve Dynamics and Fiscal Policy Shocks|We use an affine term structure model with time-varying macro trends and a vector autoregression model to investigate the response of the US Treasury yield curve to changes in fiscal policy. By accounting for the timing of the fiscal policy in the shock identification we can separate the effect of news about future increases in government spending from the effect of innovations in changes of current government expenditures. Further, we use the Baker, Bloom, and Davis (2016) uncertainty index dataset to explain the flight to quality type of events. By controlling for the low frequency movement in yields and the decomposition of yield to risk neutral rates and term premia we show that the news channel is driven by a cautious response of agents to an increase in projected future government spending and leads to a drop in yields. This result contrasts with shock into contemporaneous spending which has no significant impact on bond yields.
C12|A Doubly Corrected Robust Variance Estimator for Linear GMM|We propose a new finite sample corrected variance estimator for the linear generalized method of moments (GMM) including the one-step, two-step, and iterated estimators. Our formula additionally corrects for the over-identification bias in variance estimation on top of the commonly used finite sample correction of Windmeijer (2005) which corrects for the bias from estimating the efficient weight matrix, so is doubly corrected. Formal stochastic expansions are derived to show the proposed double correction estimates the variance of some higher-order terms in the expansion. In addition, the proposed double correction provides robustness to misspecification of the moment condition. In contrast, the conventional variance estimator and the Windmeijer correction are inconsistent under misspecification. That is, the proposed double correction formula provides a convenient way to obtain improved inference under correct specification and robustness against misspecification at the same time.
C12|Crude oil futures trading and uncertainty|This paper examines the effect of different dimensions of uncertainty on expectations of WTI crude oil futures momentum traders at a daily level. We consider two concepts of uncertainty and two momentum trading indicators based on technical analysis. In addition, we also use wavelet techniques to decompose crude oil futures prices into different frequencies accounting for investors' sentiment at various horizons. To allow for different effects on the propagation mechanism of uncertainty shocks, we apply a time-varying Bayesian VAR approach. Our findings indicate that both measures of uncertainty affect momentum trading on the crude oil futures market in several periods, especially during the great recession between 2007 and 2009. For the decomposed futures prices our results also show that the reaction to uncertainty differs substantially across frequencies. High frequencies exhibit a very short-lived reaction to uncertainty while low frequencies show a persistent reaction to uncertainty shocks.
C12|The impact of WTO accession on Chinese firms' product and labor market power|This paper examines the causal impact of domestic trade liberalization on Chinese firms' product and labor market power. To recover both market power measures, we identify a firm's regime of competitiveness, corresponding to a product market setting and a labor market setting, at any point in time. The identification is based on implementing the distance test, which takes the dependence between both settings into account. The product market setting is defined to be imperfectly competitive if the firm sets a price-cost markup, which is our model consistent measure of product market power. The labor market setting is defined to be imperfectly competitive if the firm either pays a wage markup or sets a wage markdown. Our model consistent measure of labor market power is either the workers' bargaining power during work-firm negotiations in the former or the wage elasticity of the firm's labor supply curve capturing its wage-setting power in the latter. To establish causal evidence of trade shocks on product and labor market power, we use China's WTO accession in 2001 as an identification strategy. Reducing tariffs on intermediate inputs decreases the likelihood of shifting firms away from an imperfectly competitive labor market setting. Reducing tariffs on final goods increases the likelihood of shifting firms away from setting wage markdowns. Trade liberalization via input tariff reductions increases a firm's price-cost markup but decreases the degree of wage-setting power that it possesses, conditional on the relevant product/labor market setting. Such joint responses of firms' pricing behavior in product and labor markets to trade policy changes are important for understanding the distributional consequences of trade shocks and the underlying drivers of increased inter-firm wage disparities.
C12|Adaptive Testing for Cointegration with Nonstationary Volatility|This paper generalises Boswijk and Zu (2018)'s adaptive unit root test for time series with nonstationary volatility to a multivariate context. Persistent changes in the innovation variance matrix of a vector autoregressive model lead to size distortions in conventional cointegration tests, which may be resolved using the wild bootstrap, as shown by Cavaliere et al. (2010, 2014). We show that it also leads to the possibility of constructing tests with higher power, by taking the time-varying volatilities and correlations into account in the formulation of the likelihood function and the resulting likelihood ratio test statistic. We find that under suitable conditions, adaptation with respect to the volatility process is possible, in the sense that nonparametric volatility matrix estimation does not lead to a loss of asymptotic local power relative to the case where the volatilities are observed. The asymptotic null distribution of the test is nonstandard and depends on the volatility process; we show that various bootstrap implementations may be used to conduct asymptotically valid inference. Monte Carlo simulations show that the resulting test has good size properties, and higher power than existing tests. Two empirical examples illustrate the applicability of the tests.
C12|Backtesting Value-at-Risk and Expected Shortfall in the Presence of Estimation Error|We investigate the effect of estimation error on backtests of (multi-period) expected shortfall (ES) forecasts. These backtests are based on first order conditions of a recently introduced family of jointly consistent loss functions for Value-at-Risk (VaR) and ES. We provide explicit expressions for the additional terms in the asymptotic covariance matrix that result from estimation error, and propose robust tests that account for it. Monte Carlo experiments show that the tests that ignore these terms suffer from size distortions, which are more pronounced for higher ratios of out-of-sample to in-sample observations. Robust versions of the backtests perform well, although this also depends on the choice of conditioning variables. In an application to VaR and ES forecasts for daily FTSE 100 index returns as generated by AR-GARCH, AR-GJR-GARCH, and AR-HEAVY models, we find that estimation error substantially impacts the outcome of the backtests.
C12|Identification-Robust Nonparametric Inference in a Linear IV Model|For a linear IV regression, we propose two new inference procedures on parameters of endogenous variables that are robust to any identification pattern, do not rely on a linear first-stage equation, and account for heteroskedasticity of unknown form. Building on Bierens (1982), we first propose an Integrated Conditional Moment (ICM) type statistic constructed by setting the parameters to the value under the null hypothesis. The ICM procedure tests at the same time the value of the coefficient and the specification of the model. We then adopt the conditionality principle used by Moreira (2003) to condition on a set of ICM statistics that informs on identification strength. Our two procedures uniformly control size irrespective of identification strength. They are powerful irrespective of the nonlinear form of the link between instruments and endogenous variables and are competitive with existing procedures in simulations and applications.
C12|Improving the representativeness of a simple random sample: an optimization model and its application to the Continuous Sample of Working Lives|This paper develops an optimization model for selecting a large subsample that improves the representativeness of a simple random sample previously obtained from a population larger than the population of interest. The problem formulation involves convex mixed-integer nonlinear programming (convex MINLP) and is therefore NP-hard. However, the solution is found by maximizing the “constant of proportionality” – in other words, maximizing the size of the subsample taken from a stratified random sample with proportional allocation – and restricting it to a p-value high enough to achieve a good fit to the population of interest using Pearson’s chi-square goodness-of-fit test. The beauty of the model is that it gives the user the freedom to choose between a larger subsample with a poorer fit and a smaller subsample with a better fit. The paper also applies the model to a real case: The Continuous Sample of Working Lives (CSWL), which is a set of anonymized microdata containing information on individuals from Spanish Social Security records. Several waves (2005-2017) are first examined without using the model and the conclusion is that they are not representative of the target population, which in this case is people receiving a pension income. The model is then applied and the results prove that it is possible to obtain a large dataset from the CSWL that (far) better represents the pensioner population for each of the waves analysed.
C12|Time-Series Momentum: A Monte-Carlo Approach|This paper develops a Monte-Carlo backtesting procedure for risk premia strategies and employs it to study Time-Series Momentum (TSM). Relying on time-series models, empirical residual distributions and copulas we overcome two key drawbacks of conventional backtesting procedures. We create 10,000 paths of different TSM strategies based on the S&P 500 and a cross-asset class futures portfolio. The simulations reveal a probability distribution which shows that strategies that outperform Buy-and-Hold in-sample using historical backtests may out-of-sample i) exhibit sizeable tail risks ii) underperform or outperform. Our results are robust to using different time-series models, time periods, asset classes, and risk measures.
C12|Testing for Attrition Bias in Field Experiments|We approach attrition in field experiments with baseline outcome data as an identification problem in a panel model. A systematic review of the literature indicates that there is no consensus on how to test for attrition bias. We establish identifying assumptions for treatment effects for both the respondent subpopulation and the study population. We then derive their sharp testable implications on the baseline outcome distribution and propose randomization procedures to test them. We demonstrate that the most commonly used test does not control size in general when internal validity holds. Simulations and applications illustrate the empirical relevance of our analysis.
C12|Time-Varying Cointegration and the Kalman Filter|We show that time-varying parameter state-space models estimated using the Kalman filter are particularly vulnerable to the problem of spurious regression, because the integrated error is transferred to the estimated state equation. We offer a simple yet effective methodology to reliably recover the instability in cointegrating vectors. In the process, the proposed methodology successfully distinguishes between the cases of no cointegration, fixed cointegration, and time-varying cointegration. We apply these proposed tests to elucidate the relationship between concentrations of greenhouse gases and global temperatures, an important relationship to both climate scientists and economists.
C12|Bilinear form test statistics for extremum estimation|This paper develops a set of test statistics based on bilinear forms in the context of the extremum estimation framework. We show that the proposed statistic converges to a conventional chi-square limit. A Monte Carlo experiment suggests that the test statistic works well in ?nite samples
C12|Can Economic Policy Uncertainty, Volume, Transaction Activity and Twitter Predict Bitcoin? Evidence from Time-Varying Granger Causality Tests|We examine the predictive power of economic policy uncertainty, volume, transaction activity, and Twitter on Bitcoin between 27 December 2013 and 11 February 2019 using the recently proposed time-varying Granger causality tests of Shi et al. (2018). First, of particular interest, we show that volume can only predict Bitcoin returns during two episodes (August 2016-January 2017 and May 2017-June 2017) based on a Wald test with a recursive evolving procedure under a homoskedasticity error assumption. However, volume cannot predict volatility under any speciﬁcations. Secondly, both US economic policy uncertainty and equity market uncertainty indices, which are used as proxies for policy uncertainty, have no effect on predicting Bitcoin returns. Thirdly, transaction activity also cannot predict Bitcoin returns. Lastly, the number of tweets about Bitcoin can Granger cause the volume of Bitcoin (for example, March 2015-August 2015 and January 2016-February 2019) but not returns or volatility.
C12|Asymptotic F Tests under Possibly Weak Identification|This paper develops asymptotic F tests robust to weak identification and temporal dependence. The test statistics are modified versions of the S statistic of Stock and Wright (2000) and the K statistic of Kleibergen (2005), both of which are based on the continuous updating generalized method of moments. In the former case, the modification involves only a multiplicative degree-of-freedom adjustment. In the latter case, the modification involves an additional multiplicative adjustment that uses a J statistic for testing overidentification. By adopting fixed-smoothing asymptotics, we show that both the modified S statistic and the modified K statistic are asymptotically F-distributed. The asymptotic F theory accounts for the estimation errors in the underlying heteroskedasticity and autocorrelation robust variance estimators, which the asymptotic chi-squared theory ignores. Monte Carlo simulations show that the F approximations are much more accurate than the corresponding chi-squared approximations in finite samples.
C12|An Asymptotic F Test for Uncorrelatedness in the Presence of Time Series Dependence|We propose a simple asymptotic F-distributed Portmanteau test for zero autocorrelations in an otherwise dependent time series. By employing the orthonormal series variance estimator of the variance matrix of sample autocovariances, our test statistic follows an F distribution asymptotically under fixed-smoothing asymptotics. The asymptotic F theory accounts for the estimation error in the underlying variance estimator, which the asymptotic chi-squared theory ignores. Monte Carlo simulations reveal that the F approximation is much more accurate than the corresponding chi-squared approximation in finite samples. Compared with the nonstandard test proposed by Lobato (2001), the asymptotic F test is as easy to use as the chi-squared test: There is no need to obtain critical values by simulations. Further, Monte Carlo simulations indicate that Lobato’s (2001) nonstandard test tends to be heavily undersized under the null and suﬀers from substantial power loss under the alternatives.
C12|A Real-time Density Forecast Evaluation of the ECB Survey of Professional Forecasters|We evaluate the real-time predictive ability of density forecasts from the European Central Bank’s Survey of Professional Forecasters (ECB SPF) using the Diebold and Mariano (1995) and West (1996) test. As the sample size for the ECB SPF is fairly small, we use fixed-b and fixed-m asymptotics to alleviate size distortions. We verify in an original Monte Carlo design that fixed-smoothing asymptotics delivers correctly sized tests in this framework. Empirical results indicate that ECB SPF density forecasts for unemployment and real GDP growth beat simple benchmarks at one-year horizon. ECB SPF density forecasts for inflation instead do not easily outperform simple benchmarks, as up to 2008 ECB SPF inflation expectations are close to the target. After 2008, we find that the predictive ability of the ECB SPF is more conspicuous for all variables, even though inflation expectations are still loosely anchored to the target.
C12|Asymmetric competition, risk, and return distribution|We propose a parsimonious statistical model of firm competition where structural differences in the strength of competitive pressure and the magnitude of return fluctuations above and below the system-wide benchmark translate into a skewed Subbotin or asymmetric exponential power (AEP) distribution of returns to capital. Empirical evidence from US data illustrates that the AEP distribution compares favorably to popular alternative models such as the symmetric or asymmetric Laplace density in terms of goodness of fit when entry and exit dynamics of markets are taken into account.
C12|Detecting structural differences in tail dependence of financial time series|An accurate assessment of tail inequalities and tail asymmetries of financial returns is key for risk management and portfolio allocation. We propose a new test procedure for detecting the full extent of such structural differences in the dependence of bivariate extreme returns. We decompose the testing problem into piecewise multiple comparisons of Cramér-von Mises distances of tail copulas. In this way, tail regions that cause differences in extreme dependence can be located and consequently be targeted by financial strategies. We derive the asymptotic properties of the test and provide a bootstrap approximation for finite samples. Moreover, we account for the multiplicity of the piecewise tail copula comparisons by adjusting individual p-values according to multiple testing techniques. Monte Carlo simulations demonstrate the test's superior finite-sample properties for common financial tail risk models, both in the i.i.d. and the sequentially dependent case. During the last 90 years in US stock markets, our test detects up to 20% more tail asymmetries than competing tests. This can be attributed to the presence of non-standard tail dependence structures. We also find evidence for diminishing tail asymmetries during every major financial crisis - except for the 2007-09 crisis - reflecting a risk-return trade-off for extreme returns.
C12|Multiple Testing and the Distributional Effects of Accountability Incentives in Education|Economic theory that underlies many empirical microeconomic applications predicts that treatment responses depend on individuals’ characteristics and location on the outcome distribution. Using data from a large-scale Pakistani school report card experiment, we consider tests for treatment effect heterogeneity that make corrections for multiple testing to avoid an overestimation of positive treatment effects. These tests uncover evidence of policy-relevant heterogeneous effects from information provision on child test scores. Further, our analysis reinforces the importance of preventing the inflation of false positive conclusions since over 65% of the estimated statistically significant quantile treatment effects become insignificant once these corrections are applied.
C12|Can a small New Keynesian model of the world economy with risk-pooling match the facts?|We ask whether a model of the US and Europe trading with the rest of the world can match the facts of world behaviour in a powerful indirect inference test. One version has uncovered interest parity (UIP), the other risk-pooling. Both pass the test but the most probable is risk-pooling. This is consistent with risk-pooling failing a number of single equation tests, as has been found in past work; we show that these tests will typically reject risk-pooling when it in fact prevails. World economic behaviour under risk-pooling shows much stronger spillovers than under UIP with opposite monetary responses to the exchange rate. We argue that the risk-pooling model therefore demands more attention from policy-makers.
C12|Asymptotic F Tests under Possibly Weak Identification|This paper develops asymptotic F tests robust to weak identification and temporal dependence. The test statistics are modified versions of the S statistic of Stock and Wright (2000) and the K statistic of Kleibergen (2005), both of which are based on the continuous updating generalized method of moments. In the former case, the modification involves only a multiplicative degree-of-freedom adjustment. In the latter case, the modification involves an additional multiplicative adjustment that uses a J statistic for testing overidentification. By adopting fixed-smoothing asymptotics, we show that both the modified S statistic and the modified K statistic are asymptotically F-distributed. The asymptotic F theory accounts for the estimation errors in the underlying heteroskedasticity and autocorrelation robust variance estimators, which the asymptotic chi-squared theory ignores. Monte Carlo simulations show that the F approximations are much more accurate than the corresponding chi-squared approximations in finite samples.
C12|Another Look at Cryptocurrency Bubbles|This paper deals with cryptocurrency bubbles. First, it points out that a number of recent papers on cryptocurrency bubbles are awed due to an insufficient consideration of the fundamental value of cryptocurrencies. As even fiat money is said to exhibit features of bubbles, the same applies to cryptocurrencies. Thus, any empirical investigation into either the presence of cryptocurrency bubbles or the fundamental value of cryptocurrencies is needless. Second, the paper conducts a short empirical analysis into the relationship of the prices of Etherum and Bitcoin. Evidence of explosive periods is found in the price of Etherum even if this price is expressed in terms of Bitcoin rather than US Dollars. These periods, however, are found to be in the first half of 2016 and 2017, respectively, but not during the price peak period of Bitcoin witnessed end of 2017 and beginning of 2018.
C12|The Fair Reward Problem: The Illusion of Success and How to Solve It|Humanity has been fascinated by the pursuit of fortune since time immemorial, and many successful outcomes benefit from strokes of luck. But success is subject to complexity, uncertainty, and change – and at times becoming increasingly unequally distributed. This leads to tension and confusion over to what extent people actually get what they deserve (i.e., fairness/meritocracy). Moreover, in many fields, humans are over-confident and pervasively confuse luck for skill (I win, it’s skill; I lose, it’s bad luck). In some fields, there is too much risk-taking; in others, not enough. Where success derives in large part from luck – and especially where bailouts skew the incentives (heads, I win; tails, you lose) – it follows that luck is rewarded too much. This incentivizes a culture of gambling, while downplaying the importance of productive effort. And, short term success is often rewarded, irrespective, and potentially at the detriment, of the long-term system fitness. However, much success is truly meritocratic, and the problem is to discern and reward based on merit. We call this the fair reward problem. To address this, we propose three different measures to assess merit: (i) raw outcome; (ii) risk-adjusted outcome, and (iii) prospective. We emphasize the need, in many cases, for the deductive prospective approach, which considers the potential of a system to adapt and mutate in novel futures. This is formalized within an evolutionary system, comprised of five processes, inter alia handling the exploration-exploitation trade-off. Several human endeavors – including finance, politics, and science – are analyzed through these lenses, and concrete solutions are proposed to support a prosperous and meritocratic society.
C12|Estimation of Large Dimensional Conditional Factor Models in Finance|This chapter provides an econometric methodology for inference in large-dimensional conditional factor models in finance. Changes in the business cycle and asset characteristics induce time variation in factor loadings and risk premia to be accounted for. The growing trend in the use of disaggregated data for individual securities motivates our focus on methodologies for a large number of assets. The beginning of the chapter outlines the concept of approximate factor structure in the presence of conditional information, and develops an arbitrage pricing theory for large-dimensional factor models in this framework. Then we distinguish between two different cases for inference depending on whether factors are observable or not. We focus on diagnosing model specification, estimating conditional risk premia, and testing asset pricing restrictions under increasing cross-sectional and time series dimensions. At the end of the chapter, we review some of the empirical findings and contrast analysis based on individual stocks and standard sets of portfolios. We also discuss the impact on computing time-varying cost of equity for a firm, and summarize differences between results for developed and emerging markets in an international setting.
C12|Determinantes de la productividad agrícola|Uno de los grandes desafíos que enfrenta el país es disminuir la desproporción en el desarrollo social y económico de las zonas urbanas y rurales. Para entender los retos detrás de este desafío es necesario tener una mirada comprensiva de las diferencias en las condiciones urbanas y rurales, tanto en términos de inclusión social, como productiva. En términos sociales los datos parecen mostrar la existencia de un círculo perverso que limita la acumulación de educación en las zonas rurales que, junto con unas bajas opciones de inserción productiva, conduce a un mercado de trabajo con unas condiciones precarias de empleabilidad, que se ve reflejado en bajos ingresos y pobreza. En cuanto al sector productivo, se observa que el acervo de condiciones naturales del país se encuentra en un nivel bajo de utilización, al mismo tiempo aquellos recursos que se utilizan no responden a su verdadera vocación productiva. En esa vía, se indago por los niveles de productividad agrícola y sus determinantes para varios cultivos (agroindustriales, frutales y cereales), teniendo en cuenta los diferentes niveles de la actividad agrícola: las características propias de los productores y de sus unidades productivas; de los aspectos institucionales con los que se cuenta (acceso a crédito, instrumentos de fomento, etc.), así como los aspectos las características geográficas (distancia a los centros principales de acopio, facilidad de acceso a los mercado, etc.).
C12|The War Next Door and the Reds are Coming: The Spanish Civil War and the Portuguese Stock Market|The Spanish Civil War (July 1936 to April 1939) was a key event that paved the way for World War II, unfolding with unprecedented violence and uncertainty over the its outcome. In this paper, we analyze the impact of the events of the Spanish Civil War on the Portuguese stock returns. Portugal is a particularly interesting case for analysis given its geographical exposure and historical ties to Spain as well as its political ties to the Nationalist side. Unlike previous studies of stock market responses to World War II outcomes, in our period of analysis the world at large was at peace, allowing for a clearer attribution of causation. We examine investors' reactions to news from the Spanish War using a panel of weekly returns for firms listed on the Lisbon Stock market, after classifying a series of important developments of the Spanish Civil War, classified according to its nature â?? military or political, and which contender emerged as favored â?? the Republicans, on the left, or the Nationalists, on the right. We run dynamic specifications with firm and month fixed effects, controlling for the reference interest rate in London, and events in Portugal. Our results reveal that Spanish Civil War events affect returns negatively, especially events that are military in nature. When we break down our sample into overseas firms â?? those whose most significant assets were located in Africa â?? and non-overseas firms, the latter present more significant effects from the event variables, especially from the Pro-Republican military events. Our findings are robust to the different specifications and suggest that both general uncertainty and partisan preferences affect Portuguese returns.
C12|Mixed Causal-Noncausal Autoregressions: Bimodality Issues in Estimation and Unit Root Testing <BR>[Modèles auto-régressifs non-causaux mixtes: Problèmes de bimodalité pour l'estimation et le test de racine unitaire]|This paper stresses the bimodality of the widely used Student's t likelihood function applied in modelling Mixed causal-noncausal AutoRegressions (MAR). It first shows that a local maximum is very often to be found in addition to the global Maximum Likelihood Estimator (MLE), and that standard estimation algorithms could end up in this local maximum. It then shows that the issue becomes more salient as the causal root of the process approaches unity from below. The consequences are important as the local maximum estimated roots are typically interchanged , attributing the noncausal one to the causal component and vice-versa, which severely changes the interpretation of the results. The properties of unit root tests based on this Student's t MLE of the backward root are obviously affected as well. To circumvent this issues, this paper proposes an estimation strategy which i) increases noticeably the probability to end up in the global MLE and ii) retains the maximum relevant for the unit root test against a MAR stationary alternative. An application to Brent crude oil price illustrates the relevance of the proposed approach. Keywords: Mixed autoregression, non-causal autoregression, maximum likelihood estimation, unit root test, Brent crude oil price.
C12|Testing Constancy in Varying Coefficient Models|This article proposes tests for constancy of coefficients in semi-varying coefficients models. The testing procedure resembles in spirit the union-intersection parameter stability tests in time series, where observations are sorted according to the explanatory variable responsible for the coefficients varying. The test can be applied to model specification checks of interactive effects in linear regression models. Because test statistics are not asymptotically pivotal, critical values and p-values are estimated using a bootstrap technique. The finite sample properties of the test are investigated by means of Monte Carlo experiments, where the new proposal is compared to existing tests based on smooth estimates of the unrestricted model. We also report an application to returns of education modeling
C12|Inference in Moment Inequality Models That Is Robust to Spurious Precision under Model Misspecification|Standard tests and confidence sets in the moment inequality literature are not robust to model misspecification in the sense that they exhibit spurious precision when the identified set is empty. This paper introduces tests and confidence sets that provide correct asymptotic inference for a pseudo-true parameter in such scenarios, and hence, do not suffer from spurious precision.
C12|Robust Tests for White Noise and Cross-Correlation|Commonly used tests to assess evidence for the absence of autocorrelation in a univariate time series or serial cross-correlation between time series rely on procedures whose validity holds for i.i.d. data. When the series are not i.i.d., the size of correlogram and cumulative Ljung-Box tests can be significantly distorted. This paper adapts standard correlogram and portmanteau tests to accommodate hidden dependence and non-stationarities involving heteroskedasticity, thereby uncoupling these tests from limiting assumptions that reduce their applicability in empirical work. To enhance the Ljung-Box test for non-i.i.d. data a new cumulative test is introduced. Asymptotic size of these tests is unaffected by hidden dependence and heteroskedasticity in the series. Related extensions are provided for testing cross-correlation at various lags in bivariate time series. Tests for the i.i.d. property of a time series are also developed. An extensive Monte Carlo study confirms good performance in both size and power for the new tests. Applications to real data reveal that standard tests frequently produce spurious evidence of serial correlation.
C12|Time-Varying Risk Premia in Large International Equity Markets|We use an estimation methodology tailored for large unbalanced panels of individual stock returns to address key economic questions about the factor structure, pricing performance of factor models, and time-variations in factor risk premia in international equity markets. We estimate factor models with time-varying factor exposures and risk premia at the individual stock level using 62,320 stocks in 46 countries over the 1985-2018 period. We consider market, size, value, momentum, profitability, and investment factors aggregated at the country, regional, and world level. We find that adding an excess country market factor to world or regional factors is sufficient to capture the factor structure for both developed and emerging markets. We do not reject asset pricing restriction tests for multifactor models in 74% to 91% of countries. Value and momentum premia show more variability over time and across countries than profitability and investment premia. The excess country market premium is statistically significant in many developed and emerging markets but economically larger in emerging markets.
C12|Exchange rate pass-through to import prices in Europe: A panel cointegration approach|This paper takes a panel cointegration approach to the estimation of short- and long-run exchange rate pass-through (ERPT) to import prices in the European countries. Although economic theory suggests a long-run relationship between import prices and exchange rate, in recent empirical studies its existence has either been overlooked, or it has proven dicult to establish. Resorting to novel tests for panel cointegration, we nd support for the equilibrium relationship hypothesis. Exchange rate pass-through elasticities, estimated by two di erent techniques for cointegrated panel regressions, give insight into the most recent development of the ERPT.
C12|Fertility response to climate shocks|In communities highly dependent on rainfed agriculture for their livelihoods, the common oc-currence of climatic shocks such as droughts can lower the opportunity cost of having children, and raise fertility. Using longitudinal household data from Madagascar, we estimate the causal effect of drought occurrences on fertility, and explore the nature of potential mechanisms driving this effect. We exploit exogenous within-district year-to-year variation in rainfall deficits, and find that droughts occurring during the agricultural season significantly increase the number of children born to women living in agrarian communities. This effect is long lasting, as it is not reversed within four years following the drought occurrence. Analyzing the mechanism, we find that droughts have no effect on common underlying factors of high fertility such as marriage timing and child mortality. Furthermore, droughts have no significant effect on fertility if they occur during the non-agricultural season or in non-agrarian communities, and their positive effect in agrarian communities is mitigated by irrigation. These findings provide evidence that a low opportunity cost of having children is the main channel driving the fertility effect of drought in agrarian communities.
C12|Can the South African Reserve Bank (SARB) protect the purchasing power of citizens? A new look at Fisher’s hypothesis|In this paper, we evaluate whether the South African Reserve Bank (SARB) has been successful at fulfilling it’s mandate of protecting the purchasing power of the country’s citizens. To this end, we use monthly data covering the post-inflation targeting era of 2002:01 to 2018:04 to re-examine Fisher’s hypothesis for the South African economy by testing for stationarity in real interest rates. Our study makes three noteworthy empirical contributions. Firstly, we use three measures of inflation in computing the real interest rate variable. Secondly, our inflation expectations variables are constructed in alignment with the inflation forecast horizons of 12 to 24 months as practiced by the SARB. Thirdly, we rely on the more powerful flexible Fourier unit root test in testing for integration properties of the real exchange rate. All-in-all, our findings highlight the Reserve Bank’s struggles in protecting the purchasing power of citizen’s for periods subsequent to the global financial crisis but not for periods before the crisis. Policy recommendations are also provided.
C12|Towards resolving the Purchasing Power Parity (PPP) ‘puzzle’ in Newly Industrialized Countries (NIC’s)|The Purchasing Power Parity (PPP) hypothesis represents one of the oldest existing economic doctrines and is plagued with empirical inconsistencies collectively labelled as ‘puzzles’. Our study resolves these ‘puzzles’ for 14 Newly Industrialized Countries (NIC) whose developmental strategies are impinged on the stability of real exchange rates which, in turn, validates the PPP hypothesis. We test for the stationarity of real exchange rates (RER’s) by applying an exponential smooth transition autoregressive unit root test augmented with a fractional frequency flexible Fourier form component (ESTAR-FFFFF) to capture heterogeneous smooth transition asymmetries and approximate unknown structural breaks in the time series. We find the RER’s in all 14 NIC’s are mean-reverting over monthly period of 1970:1-2018:12 which confirms the PPP hypothesis for these economies in the presence of exchange-rate regime shifts, oil and food shocks, financial crisis and other forms of asymmetries and structural breaks. Length: 29 pages
C12|Inference on winners| Many questions in econometrics can be cast as inference on a parameter selected through optimization. For example, researchers may be interested in the effectiveness of the best policy found in a randomized trial, or the best-performing investment strategy based on historical data. Such settings give rise to a winner's curse, where conventional estimates are biased and conventional confi dence intervals are unreliable. This paper develops optimal con fidence sets and median-unbiased estimators that are valid conditional on the parameter selected and so overcome this winner's curse. If one requires validity only on average over target parameters that might have been selected, we develop hybrid procedures that combine conditional and projection con fidence sets and offer further performance gains that are attractive relative to existing alternatives.
C12|Taming the Factor Zoo: A Test of New Factors|We propose a model-selection method to systematically evaluate the contribution to asset pricing of any new factor, above and beyond what a high-dimensional set of existing factors explains. Our methodology explicitly accounts for potential model-selection mistakes, unlike the standard approaches that assume perfect variable selection, which rarely occurs in practice and produces a bias due to the omitted variables. We apply our procedure to a set of factors recently discovered in the literature. While most of these new factors are found to be redundant relative to the existing factors, a few — such as profitability — have statistically significant explanatory power beyond the hundreds of factors proposed in the past. In addition, we show that our estimates and their significance are stable, whereas the model selected by simple LASSO is not.
C12|Administrative Data Linking and Statistical Power Problems in Randomized Experiments|The increasing availability of administrative data has led to a particularly exciting innovation in public policy research, that of the “low-cost” randomized trial in which administrative data are used to measure outcomes in lieu of costly primary data collection. Linking data from an experimental intervention to administrative records that track outcomes of interest typically requires matching datasets without a common unique identifier. In order to minimize mistaken linkages, researchers will often use “exact matching” (retaining an individual only if all their demographic variables match exactly in two or more datasets) in order to ensure that speculative matches do not lead to errors in an analytic dataset. We argue that when this approach is used to detect the presence of a binary outcome, this seemingly conservative approach leads to attenuated estimates of treatment effects, and critically, to underpowered experiments. For marginally powered studies, which are common in empirical social science, exact matching is particularly problematic. In this paper, we derive an analytic result for the consequences of linking errors on statistical power and show how the problem varies across different combinations of relevant inputs, including the matching error rate, the outcome density and the sample size. We conclude on an optimistic note by showing that machine learning-based probabilistic matching algorithms allow researchers to recover a considerable share of the statistical power that is lost to errors in data linking.
C12|Keynesian Models, Detrending, and the Method of Moments|One important question in the Keynesian literature is whether we should detrend data when estimating the parameters of a Keynesian model using the moment method. It has been common in the literature to detrend data in the same way the model is detrended. Doing so works relatively well with linear models, in part because in such a case the information that disappears from the data after the detrending process is usually related to the parameters that also disappear from the detrended model. Unfortunately, in heavy non-linear Keynesian models, parameters rarely disappear from detrended models, but information does disappear from the detrended data. Using a simple real business cycle model, we show that both the moment method estimators of parameters and the estimated responses of endogenous variables to a technological shock can be seriously inaccurate when the data used in the estimation process are detrended. Using a dynamic stochastic general equilibrium model and U.S. data, we show that detrending the data before estimating the parameters may result in a seriously misleading response of endogeneous variables to monetary shocks. We suggest building the moment conditions using raw data, irrespective of the trend observed in the data.
C12|Partial effects estimation for fixed-effects logit panel data models|We develop a multiple-step procedure for the estimation of point and average partial effects in fxed-effects logit panel data models that admit suffcient statistics for the incidental parameters. In these models, estimates of the individual effects are not directly available and have to be recovered by means of an additional step. We also derive a standard error formulation for the average partial effects. We study the finite-sample properties of the proposed estimator by simulation and provide an application based on unionised workers.
C12|Tests for conditional heteroscedasticity with functional data and goodness-of-fit tests for FGARCH models|Functional data objects that are derived from high-frequency financial data often exhibit volatility clustering characteristic of conditionally heteroscedastic time series. Versions of functional generalized autoregressive conditionally heteroscedastic (FGARCH) models have recently been proposed to describe such data, but so far basic diagnostic tests for these models are not available. We propose two portmanteau type tests to measure conditional heteroscedasticity in the squares of financial asset return curves. A complete asymptotic theory is provided for each test, and we further show how they can be applied to model residuals in order to evaluate the adequacy, and aid in order selection of FGARCH models. Simulation results show that both tests have good size and power to detect conditional heteroscedasticity and model mis-specification in finite samples. In an application, the proposed tests reveal that intra-day asset return curves exhibit conditional heteroscedasticity. Additionally, we found that this conditional heteroscedasticity cannot be explained by the magnitude of inter-daily returns alone, but that it can be adequately modeled by an FGARCH(1,1) model.
C12|Lognormal city size distribution and distance|This paper analyses whether the size distribution of nearby cities is lognormally distributed by using a distance-based approach. We use data from three different definitions of US cities in 2010, considering all possible combinations of cities within a 300-mile radius. The results indicate that support for the lognormal distribution decreases with distance, although the lognormal distribution cannot be rejected in most of the cases for distances below 100 miles.
C12|Inference in Differences-in-Differences: How Much Should We Trust in Independent Clusters?|We analyze the conditions in which ignoring spatial correlation is problematic for inference in differences-in-differences (DID) models. Assuming that the spatial correlation structure follows a linear factor model, we show that inference ignoring such correlation remains reliable when either (i) the second moment of the difference between the pre- and post-treatment averages of common factors is low, or (ii) the distribution of factor loadings has the same expected values for treated and control groups, and do not exhibit significant spatial correlation. We present simulations with real datasets that corroborate these conclusions. Our results provide important guidelines on how to minimize inference problems due to spatial correlation in DID applications.
C12|Superkurtosis|Very little is known on how traditional risk metrics behave in ultra high frequency trading (UHFT). We fi�ll this void �firstly by examining the existence of the intraday returns moments, and secondly by assessing the impact of their (non)existence in a risk management framework. We show that in the case of UHFT, the returns' third and fourth moments do not exist, which entails that traditional risk metrics are unable to judge capital adequacy adequately. Hence, the use of risk management techniques, such as VaR, by market participants who engage with UHFT impose serious threats to the stability of fi�nancial markets, given that capital ratios may be severely underestimated.
C12|A Residual-Based Cointegration test with a Fourier Approximation|This paper proposes a residual-based cointegration test in the presence of smooth structural changes approximated by a Fourier function. The test offers a simple way to accommodate unknown number and form of structural breaks and have good size and power properties in the presence of breaks.
C12|Cumulative analysis of dependence government tax behaviour on economy’s efficiency factors for totality the world countries|The article deals with an investigation of principles, factors, and conditions of the government tax behaviour by changing the tax rate. The research base is all countries in the world for which statistics are available. We define a set of potential indicators of the economic efficiency, based on GDP and FDI, nominal and per capita, as well as the ratio of FDI to GDP. By using the statistical analysis techniques we found a correlation between government behaviour and each of the selected indicators. In order to reduce the randomness of the results, we carry out cumulative testing of the hypothesis of independence of government tax behaviour from the efficiency of the economy for all possible partitions of the countries' totality with different interrelations of the countries’ sets behaviour with different economic efficiency levels. Based on the research, it can be argued that government tax behaviour, in general, is not maximizer behaviour. We argue that the factors GDP, FDI, and GDP per capita have the biggest impact on the government tax decisions. The obtained results allow to understand the principles of governments’ decision-making, and, therefore, to forecast in some way their behaviour in certain economic conditions. In particular, partitions accumulations can help identify behavioural trends. The present paper differs from previous studies both by the topic, studying the relations between government’s tax behavior and efficiency of countries' economies and by the approach to define this dependence, since the latest can be observed only when each variant of government’s tax reaction is analyzed separately.
C12|Mengukur Perkembangan Sektor Keuangan di Indonesia dan Faktor – Faktor yang Mempengaruhi<BR>[Assessing the Measurement and Determinants of Financial Sector Development in Indonesia]|In a number of occasions during the bad times where financial markets are under pressure, Indonesia often suffers the most compared to neighbors or peer countries. It indicates that there is something fundamental as the driving factors and the depth of the Indonesian financial sector may be the major factor. This research aims to investigate how deep and develop the Indonesian financial sector using multiple indicators and metrics. This research also investigates the causal relationship between financial sector development and the economic growth whether the Indonesian financial sector is supply-leading or demand-following. Moreover, this research attempts to identify the determinants of the financial sector development in Indonesia. The results show that the development of the Indonesian financial sector has been focused on the access aspect, while the development of its depth as well as efficiency is still limited. The depth level by the end of 2018 was even still lower than the level in the mid-1990s. The research's results also show that the financial sector development in Indonesia is demand-following or it develops as the economy grows. Lastly, the results show that the financial sector development in Indonesia is affected by multi-aspect factors ranging from macroeconomic factors and institutions to the openness levels either trade openness or financial openness. On those many structural aspects, Indonesia is inferior compared with many countries, so it is not a coincidence that the Indonesian financial sector is less developed compared to the many countries.
C12|Does U.K.’s Real GDP have a Unit Root? Evidence from a Multi-Century Perspective|We employ the nonlinear unit-root test recently developed by Omay et al. (2018), as well as other linear and nonlinear tests, to examine the stationarity of five multi-century historical U.K. series of real output compiled by the Bank of England (Thomas and Dimsdale, 2017). Three series span 1270 to 2016 and two series span 1700 to 2016. These datasets represent the longest span of historical real output data available and, thus, provide the environment for which unit-root tests are most powerful. A key feature of the Omay et al. (2018) test is its simulataneous allowance for two types of nonlinearity: time-dependent (structural breaks) nonlinearity and state-dependent (asymmetric adjustment) nonlinearity. The key finding of the test, contrary to what other more popular nonlinear unit-root tests suggest, provides strong evidence that the main structure of the five series is stationary with a sharp trend break and an asymmetric nonlinear adjustment. This finding is highly significant from the perspective of current macroeconomic debate because it refutes, for the historical U.K. series at least, the most stylized fact that real output follows a non-stationary process.
C12|How do Housing Returns in Emerging Countries Respond to Oil Shocks? A MIDAS Touch|In this study, we utilize the recent oil shock data of Baumeister and Hamilton (2019) to analyze how housing returns in China, India and Russia respond to different oil shocks. Given the available data for the relevant variables, the MIDAS approach which helps circumvent aggregation problem in the estimation process is employed. We also extend the MIDAS framework to account for nonlinearities in the model. Expectedly, the housing returns of the countries considered respond differently to the variants of oil shocks. More specifically, we find that the housing returns of India and China which are net oil-importing countries do not seem to possess oil risk hedging characteristics albeit with the converse for Russia which is a major net oil-exporter. We also find that modeling with the MIDAS framework offers better predictability than other variants with uniform frequency.
C12|Testing the White Noise Hypothesis in High-Frequency Housing Returns of the United States|In the pure time-series sense, weak-form of efficiency of the housing market would imply unpredictability of housing returns. Given this, utilizing a daily dataset of aggregate housing market returns of the United States, we test whether housing market returns are white noise using the blockwise wild bootstrap in a rolling-window framework. We investigate the dynamic evolution of housing market efficiency and find that the white noise hypothesis is accepted in most windows associated with non-crisis periods. However, for some periods before the burst of the housing market bubbles, and during the subprime mortgage crisis, European sovereign debt crisis and the Brexit, the white noise hypothesis is rejected, indicating that the housing market is inefficient in periods of turbulence. Our results have important implications for economic agents.
C12|Testing for Episodic Predictability in Stock Returns|Standard tests based on predictive regressions estimated over the full available sample data have tended to find little evidence of predictability in stock returns. Recent approaches based on the analysis of subsamples of the data have been considered, suggesting that predictability where it occurs might exist only within so-called 'pockets of predictability' rather than across the entire sample. However, these methods are prone to the criticism that the sub-sample dates are endogenously determined such that the use of standard critical values appropriate for full sample tests will result in incorrectly sized tests leading to spurious findings of stock returns predictability. To avoid the problem of endogenously-determined sample splits, we propose new tests derived from sequences of predictability statistics systematically calculated over sub-samples of the data. Specifically, we will base tests on the maximum of such statistics from sequences of forward and backward recursive, rolling, and double-recursive predictive sub-sample regressions. We develop our approach using the over-identified instrumental variable-based predictability test statistics of Breitung and Demetrescu (2015). This approach is based on partial-sum asymptotics and so, unlike many other popular approaches including, for example, those based on Bonferroni corrections, can be readily adapted to implementation over sequences of subsamples. We show that the limiting distributions of our proposed tests are robust to both the degree of persistence and endogeneity of the regressors in the predictive regression, but not to any heteroskedasticity present even if the sub-sample statistics are based on heteroskedasticity-robust standard errors. We therefore develop fixed regressor wild bootstrap implementations of the tests which we demonstrate to be first-order asymptotically valid. Finite sample behaviour against a variety of temporarily predictable processes is considered. An empirical application to US stock returns illustrates the usefulness of the new predictability testing methods we propose.
C12|A reexamination of inflation persistence dynamics in OECD countries: A new approach|This paper introduces a simple and easy to implement procedure to test for changes in persistence. The time-varying parameter that characterizes persistence changes under the alternative hypothesis is approximated by a parsimonious cosine function. The new test procedure is the minimum of a t-statistic, computed from a test regression that considers a set of reasonable values for a frequency term that is used to evaluate the time varying properties of persistence. The asymptotic distributions of the new tests are derived and critical values are provided. An indepth Monte Carlo analysis shows that the new procedure has important power gains when compared to the local GLS de-trended Dickey-Fuller (DFGLS) type tests introduced by Elliott et al. (1996) under various data generating processes with persistence changes. Moreover, an empirical application to OECD countries’ inflation series shows that for most countries analysed persistence was high in the first half of the sample and subsequently decreased. These results are compatible with modern macroeconomic theories that point to changes in inflation behavior in the early 1980s and also with recent empirical evidence against the I(1)-I(0) dichotomy.
C12|Testing for breaks in the cointegrating relationship: On the stability of government bond markets’ equilibrium|In this paper, test procedures for no fractional cointegration against possible breaks in the persistence structure of a fractional cointegrating relationship are introduced. The tests proposed are based on the supremum of the Hassler and Breitung (2006) test statistic for no cointegration over possible breakpoints in the long-run equilibrium. We show that the new tests correctly standardized converge to the supremum of a chi-squared distribution, and that this convergence is uniform. An in-depth Monte Carlo analysis provides results on the finite sample performance of our tests. We then use the new procedures to investigate whether there was a dissolution of fractional cointegrating relationships between benchmark government bonds of ten EMU countries (Spain, Italy, Portugal, Ireland, Greece, Belgium, Austria, Finland, the Netherlands and France) and Germany with the beginning of the European debt crisis.
C12|Inference in partially identiﬁed panel data models with interactive fixed eﬀects|This paper develops methods for statistical inferences in a partially identiﬁed nonparametric panel data model with endogeneity and interactive ﬁxed eﬀects. We consider the case where the number of cross-sectional units (N) is large and the number of time series periods (T).as well as the number of unobserved common factors (R) are ﬁxed. Under some normalization rules, wecan concentrateout thelarge dimen-sional parameter vector of factor loadings and specify a set of conditional moment restriction that are involved with only the ﬁnite dimensional factor parameters along with the inﬁnite dimensional nonpara-metric component. For a conjectured restriction on the parameter, we consider testing the null hypothesis that the restriction is satisﬁed by at least one element in the identiﬁed set and propose a test statistic based on a novel martingale diﬀerence divergence (MDD) measure for the distance between a conditional expectation object and zero. We derive the limiting distribution of the resultant test statistic under the null and show that it is divergent at rate-N under the global alternative based on the U-process theory. To obtain the critical values for our test, we propose a version of multiplier bootstrap and establish its asymptotic validity. Simulations demonstrate the ﬁnite sample properties of our inference procedure. We apply our method to study Engel curves for major nondurable expenditures in China by using a panel dataset from the China Family Panel Studies (CFPS).
C12|Improved Marginal Likelihood Estimation via Power Posteriors and Importance Sampling|The power-posterior method of Friel and Pettitt (2008) has been used to estimate the marginal likelihoods of competing Bayesian models. In this paper it is shown that the Bernstein-von Mises (BvM) theorem holds for the power posteriors under regularity conditions. Due to the BvM theorem, the power posteriors, when adjusted by the square root of the corresponding grid points, converge to the same normal distribution as the original posterior distribution, facilitating the implementation of importance sampling for the purpose of estimating the marginal likelihood. Unlike the power-posterior method that requires repeated posterior sampling from the power posteriors, the new method only requires the posterior output from the original posterior. Hence, it is computationally more efficient to implement. Moreover, it completely avoids the coding efforts associated with drawing samples from the power posteriors. Numerical efficiency of the proposed method is illustrated using two models in economics and finance.
C12|Macroeconomic indicators of determination on tax behaviour of OECD countries|The article deals with investigation of principles, factors and conditions of the government’s tax behavior, notably by means of changing the tax burden. We define a set of potential indicators of the economic efficiency, based on GDP and FDI, nominal and per capita, as well as the ratio of FDI to GDP. By using the statistical analysis techniques we found the statistical dependence between government’s behavior and each of the selected indicators. We argued that the factor GDP per capita has the biggest impact on the government’s tax decisions. Also it showed that the governments mostly act as satisfiers. The obtained results allow to understand the principles of governments’ decision-making, and, therefore, to forecast in some way their behavior in certain economic conditions. Moreover, it could help to understand the reasons why the “race to the bottom” situation appears. The present paper differs from previous studies both by the topic, studying the relations between government’s tax behavior and efficiency of countries economies and by the approach to define this dependence, since the latest can be observed only when each variant of government’s tax reaction is analyzed separately.
C12|Managing Strategic Change and ERP Implementation under Distinctive Learning Styles: Quantitative case of Burberry PLC|This paper examines the effective strategic change management and Enterprise Resource Planning implementation under distinctive learning styles namely; diverging, converging, assembling, and accommodating learning styles through case of Burberry brand at Bicester Village, Cheshire Oaks, and Chatham Place. Additionally, paper investigates the strategic changes and organizational factors in relation with the ability to adopt change and successful ERP implementation. Total 87 respondents were approached through snowball, purposive, and convenience sampling. Findings revealed that accommodating learning style is the most influential learning style that significant positively affects the ERP implementation process. Interestingly, all learning styles (diverging, converging, assimilating, and accommodating) have statistically significant correlation with organizational change process. Additionally, complexity is the most critical organizational component affecting employees' ability to accept changes.
C12|How sustainable are fiscal budgets in the Kingdom of Swaziland?|The recently experienced Swazi fiscal crisis of 2011 has facilitated the need for an academic probe into the sustainability of fiscal budgets in the Kingdom. Against the absence of empirical evidence evaluating the sustainability of Swazi fiscal budget, our study fills the hiatus by econometrically evaluating the sustainability of the fiscal budget of the Swazi economy between 1999 and 2016. Our empirical study depends on a combination of linear and asymmetric unit root and cointegration empirical procedures to attain this objective. In reviewing the obtained results, the evidence obtained from the linear econometric frameworks is inconclusive whereas the results from the more vigorous asymmetric models point to the unsustainability of Swazi fiscal budget over both the short and long-run. Important policy implications for Swazi fiscal policymakers are drawn from the analysis.
C12|Test der neoklassischen Produktionsfunktion<BR>[Testing the neoclassical production function]|The Cobb-Douglas production function has applications for firms, branches, industries, and for the macro-economies of nation-states as a whole. Throughout the course of his life, with the exception of his years as a senator, Paul H. Douglas consistently strived to gather all the supporting empirical evidence he could. Among others, he placed his hope in his former student, Paul A. Samuelson, and in Samuelson’s colleague Robert M. Solow, who pushed the production function forward “into new and more sophisticated fields.” Meanwhile, plenty of data on capital, labor, and output are available, but it seems as though no one is interested anymore in either the empirical evidence or the underlying logic of Solow’s sophisticated version of the old Cobb-Douglas theory. In this study, his method of segregating variations in output per head due to technical change from those due to changes in the availability of capital per head is applied to the (West) German economy. The temporal domain covers the development of the West German economy from 1950 to 1990 and after German unification, from 1991 to 2015. Even though it turns out that the underlying, true production functions are very similar, ex post prognoses of real wages with the help of the approximated first derivation of the complete production function are much better in the earlier than in the later period. This raises the question of whether factors are paid their marginal cost. Because the study is thought to serve as teaching material, certain mathematics concerning the production function are added. Several problems regarding the accurate testing of the function on the basis of Germany’s National Account dataset are discussed. Patterns that are consequences of the theory are confronted with empirical evidence resulting from 67 years of economic development. It might well be of political importance that the upward shift of the production function hypothetically caused by technical change has a tendency to fall.
C12|Important factors in a nations international competitiveness ranking|This paper analyses the importance of competitiveness factors in international competitiveness ranking of South Africa. In particular, the paper investigates the odds in favour of an improved, as opposed to a deteriorated, Overall international competitiveness ranking due to a change in selected competitiveness factors. The results show that the autonomous improvement in Overall international competitiveness ranking is statistically insignificant while the effect of a change in Government efficiency also has a statistically insignificant effect on the odds in favour of an improved Overall international competitiveness ranking. The results further show that a change in Economic performance, Business efficiency and Infrastructure increase the odds in favour of an improved Overall international competitiveness ranking. Finally, a change in Infrastructure has the biggest odds in favour of an improvement in Overall international competitiveness ranking compared to a change in Economic performance and Business efficiency.
C12|Dynamisation de la malédiction des ressources naturelles en Afrique sur les performances économiques : institution et guerre froide<BR>[Curse of Natural Resources and Economic Performance in Africa: Institution and Cold War]|This article articulates Acemoglu-Robinson's theory of inclusive and exclusive institutions to the theory of the effects of natural resources on the incentives of political elites to implement good institutions for development. If Africa is subject to the curse of natural resources it means that this continent has generally been organized on the basis of extractive institutions that have determined the conflicts between political elites, between border countries and between the great world powers. On this basis, this article proposes an original test over the period 1985-2010 for 30 African countries linking institutional quality to an indicator of dependence on natural resources and different control variables. The phenomenon of the curse is decreasing in Africa as we move away from the end of the Cold War.
C12|Conditions institutionnelles de la malédiction des ressources naturelles en Afrique sur les performances économiques<BR>[Institutional conditions of the natural resource curse in Africa on economic performance]|We show that if Africa is subject to the curse of natural resources it is because this continent has generally been organized since the European colonization on the basis of extractive institutions that determines the strong conflicts between the economic preferences of the political decision-makers and those of the rest of society. In particular, we show that the quality of institutions in African countries is fundamentally determined by historical factors. The main originality is that it uses as an instrumental variable, the institutional path dependence that ensures that there is a curse of natural resources only in countries where the extractive institutions of colonialism have been reproduced. We provide evidence that the overall impact of institutions and natural resource dependence on economic performance is critically dependent on past events as these determine the incentive structure and future institutional choices. The phenomenon of the curse is decreasing in Africa as we move away from the end of the Cold War.
C12|Robust analysis of convergence in per capita GDP in BRICS economies|Whilst the issue of whether or not per capita GDP adheres to the convergence theory continues to draw increasing attention within the academic paradigm, with very little consensus having been reached in the literature thus far. Our study contributes to the literature by examining the stationarity of per capita GDP for BRICS countries using annual data collected between 1971 and 2015. Considering that our sample covers a period underlying a number of crisis and structural breaks within and amongst the BRICS countries, we rely on a robust nonlinear unit root testing procedure which captures a series of unobserved structural breaks. Our results confirm on Brazil and China being the only two BRICS economies who present the most convincing evidence of per capita GDP converging back to it’s natural equilibrium after an economic shock, whilst Russia and South Africa provide less convincing evidence of convergence dynamics in the time series and India having the weakest convergence properties.
C12|Do both demand-following and supply-leading theories hold true in developing countries?|To overcome the limitations of the traditional approach which uses linear causality to examine whether the supply-leading and demand-following theories hold. As certain countries will be found not to follow the theory by using the traditional approach, this paper first suggests using all the proxies of financial development and economic growth as well as both multivariate and bivariate linear and nonlinear causality tests to analyze the relationship between financial development and economic growth. The multivariate nonlinear test not only takes into consideration both dependent and joint effects among variables, but is also able to detect a multivariate nonlinear deterministic process that cannot be detected by using any linear causality test. We find five more countries in which the supply-leading hypothesis and/or demand-following hypothesis hold true than with the traditional approach. However, there is still one country, Pakistan, for which no linear or nonlinear causality is found between its financial development and economic growth.
C12|Change Point Detection in the Conditional Correlation Structure of Multivariate Volatility Models|We propose semi-parametric CUSUM tests to detect a change point in the correlation structures of non--linear multivariate models with dynamically evolving volatilities. The asymptotic distributions of the proposed statistics are derived under mild conditions. We discuss the applicability of our method to the most often used models, including constant conditional correlation (CCC), dynamic conditional correlation (DCC), BEKK, corrected DCC and factor models. Our simulations show that, our tests have good size and power properties. Also, even though the near--unit root property distorts the size and power of tests, de--volatizing the data by means of appropriate multivariate volatility models can correct such distortions. We apply the semi--parametric CUSUM tests in the attempt to date the occurrence of financial contagion from the U.S. to emerging markets worldwide during the great recession.
C12|Concentration Based Inference in High Dimensional Generalized Regression Models (I: Statistical Guarantees)|"We develop simple and non-asymptotically justified methods for hypothesis testing about the coefficients ($\theta^{*}\in\mathbb{R}^{p}$) in the high dimensional generalized regression models where $p$ can exceed the sample size. Given a function $h:\,\mathbb{R}^{p}\mapsto\mathbb{R}^{m}$, we consider $H_{0}:\,h(\theta^{*})=\mathbf{0}_{m}$ against $H_{1}:\,h(\theta^{*})\neq\mathbf{0}_{m}$, where $m$ can be any integer in $\left[1,\,p\right]$ and $h$ can be nonlinear in $\theta^{*}$. Our test statistics is based on the sample ``quasi score'' vector evaluated at an estimate $\hat{\theta}_{\alpha}$ that satisfies $h(\hat{\theta}_{\alpha})=\mathbf{0}_{m}$, where $\alpha$ is the prespecified Type I error. By exploiting the concentration phenomenon in Lipschitz functions, the key component reflecting the dimension complexity in our non-asymptotic thresholds uses a Monte-Carlo approximation to mimic the expectation that is concentrated around and automatically captures the dependencies between the coordinates. We provide probabilistic guarantees in terms of the Type I and Type II errors for the quasi score test. Confidence regions are also constructed for the population quasi-score vector evaluated at $\theta^{*}$. The first set of our results are specific to the standard Gaussian linear regression models; the second set allow for reasonably flexible forms of non-Gaussian responses, heteroscedastic noise, and nonlinearity in the regression coefficients, while only requiring the correct specification of $\mathbb{E}\left(Y_{i}|X_{i}\right)$s. The novelty of our methods is that their validity does not rely on good behavior of $\left\Vert \hat{\theta}_{\alpha}-\theta^{*}\right\Vert _{2}$ (or even $n^{-1/2}\left\Vert X\left(\hat{\theta}_{\alpha}-\theta^{*}\right)\right\Vert _{2}$ in the linear regression case) nonasymptotically or asymptotically."
C12|A further look at Modified ML estimation of the panel AR(1) model with fixed effects and arbitrary initial conditions|"In this paper we consider two kinds of generalizations of Lancaster's (Review of Economic Studies, 2002) Modified ML estimator (MMLE) for the panel AR(1) model with fixed effects and arbitrary initial conditions and possibly covariates when the time dimension, T, is fixed. When the autoregressive parameter ρ=1, the limiting modified profile log-likelihood function for this model has a stationary point of inflection and ρ is first-order underidentified but second-order identified. We show that the generalized MMLEs exist w.p.a.1 and are uniquely defined w.p.1. and consistent for any value of ρ≥-1. When ρ=1, the rate of convergence of the MMLEs is N^{1/4}, where N is the cross-sectional dimension of the panel. We then develop an asymptotic theory for GMM estimators when one of the parameters is only second-order identified and use this to derive the limiting distributions of the MMLEs. They are generally asymmetric when ρ=1. One kind of generalized MMLE depends on a weight matrix W_{N} and we show that a suitable choice of W_{N} yields an asymptotically unbiased MMLE. We also show that Quasi LM tests that are based on the modified profile log-likelihood and use its expected rather than observed Hessian, with an additional modification for ρ=1, and confidence regions that are based on inverting these tests have correct asymptotic size in a uniform sense when |ρ|≤1. Finally, we investigate the finite sample properties of the MMLEs and the QLM test in a Monte Carlo study."
C12|The spatial distribution of US cities|In this paper, we consider the distribution of bilateral distances between all pairs of cities to estimate K-densities using the methodology by Duranton and Overman (2005), identifying different spatial patterns. By using data from different definitions of US cities in 2010 (places, urban areas, and core-based statistical areas), we analyse the spatial distribution of cities, finding significant patterns of dispersion depending on the city size and city definition. Our results lend support to a hierarchical system of US cities in which the central cities of each subsystem are far away from each other.
C12|Can we beat the Random Walk? The case of survey-based exchange rate forecasts in Chile|We examine the accuracy of survey-based expectations of the Chilean exchange rate relative to the US dollar. Our out-of-sample analysis reveals that survey-based forecasts outperform the Driftless Random Walk (DRW) in terms of Mean Squared Prediction Error at several forecasting horizons. This result holds true even when comparing the survey to a more competitive benchmark based on a refined information set. A similar result is found when precision is measured in terms of Directional Accuracy: survey-based forecasts outperform a “pure luck” benchmark at several forecasting horizons. Differing from the traditional “no predictability” result reported in the literature for many exchange rates, our findings suggest that the Chilean peso is indeed predictable.
C12|Testing Fractional Unit Roots with Non-linear Smooth Break Approximations using Fourier functions|In this paper we present a testing procedure for fractional orders of integration in the context of non-linear terms approximated by Fourier functions. The procedure is a natural extension of the linear method proposed in Robinson (1994) and similar to the one proposed in Cuestas and Gil-Alana (2016) based on Chebyshev polynomials in time. The test statistic has an asymptotic standard normal distribution and several Monte Carlo experiments conducted in the paper show that it performs well in finite samples. Various applications using real life time series, such as US unemployment rates, US GNP and Purchasing Power Parity (PPP) of G7 countries are presented at the end of the paper.
C12|Does Inflation Uncertainty Matter for Validity of Romer’s Hypothesis? Evidence from Nigeria|Romer (1993) posits openness to international restricts inflation. He offers an explanation based on time-inconsistency of monetary policy, however ensuing studies have raised questions on the validity of Romer’s assertion and its explanation. The aim of this paper was to estimate the effect of trade openness on inflation employing quantile regression analysis, contrary to traditional mean regression methods using annual data from Nigeria for the period 1970 to 2016. The paper also tested the hypothesis of whether inflation uncertainty influence the validity of Romer’s hypothesis for Nigeria. The study adopted two measures of openness – share of trade to GDP and KOF globalization index. The results of the study validate Romer’s hypothesis for both openness indexes that openness restrict inflation. With the inclusion of inflation uncertainty, the estimated impact of trade openness on inflation was quantitatively larger and the t-statistic on the interaction variable is significant in all quantiles except for the median quantile (0.50) and their coefficients are positive. The study concluded that in all distributions of inflation, inflation uncertainty reduces the ability of openness to trade in curbing inflation. Therefore, it recommends that policy maker should target and control inflation uncertainty when openness is employed as key policy instrument for controlling inflation.
C12|The Effect of Social Media on Employees’ Job Performance: The mediating Role of Organizational Structure|Social media is creating a drastic change at workplaces, and organizations are increasingly interested in adaption of it for their business processes. The aim of social media usage at workplace may differ but ultimate objective is to build social networks and sharing. This empirical research examined the effect of use of social media on employees’ job performance and the mediating effect of an organizational structure. Survey data gathered from 205 valid responses and analyzed by structural equation modelling technique. Results revealed that “use of social media” is positively correlated with “employees’ job performance”, while organizational structure has positive mediating effect.
C12|City size distribution and space|We study the US city size distribution over space. This paper makes two contributions to the empirical literature on city size distributions. First, this study uses data from different definitions of US cities in 2010 to study the distribution of cities in space, finding significant patterns of dispersion depending on city size. Second, the paper proposes a new distance-based approach to analyse the influence of distance on the city size distribution parameters, considering both the Pareto and lognormal distributions. By using all possible combinations of cities within a 300-mile radius, results indicate that the Pareto distribution cannot be rejected in most of the cases regardless of city size. Placebo regressions validate our results, thereby confirming the significant effect of geography on the Pareto exponent.
C12|Panel Modeling of Z-score: Evidence from Islamic and Conventional Saudi Banks|Several studies on the banking sector have shown that Islamic banks are more financially robust and stable compared to conventional banks, mostly in periods of financial crises. The aim of this research is to measure and compare the level of stability between Islamic and conventional banks in Saudi Arabia using quarterly data. The sample covers around two-thirds of banks operating in the Saudi stock market, and data comprises the last global financial crisis. The panel data model shows that Islamic banks relatively reduce the financial stability index; meanwhile, they contribute efficiently to enhance financial stability through the diversification of their assets. According to our findings Riyad Bank and SAMBA positively impact the financial stability, while Al-Rajhi bank has a positive but moderate role in enhancing the banking stability. As well, the Saudi banking sector exhibits a weak competitiveness which negatively impact the banking stability. Consequently, the limited presence of Islamic banks in the Saudi banking sector menaces any efforts to improve the financial stability.
C12|Panel Modeling of Z-score: Evidence from Islamic and Conventional Saudi Banks|Several studies on the banking sector have shown that Islamic banks are more financially robust and stable compared to conventional banks, mostly in periods of financial crises. The aim of this research is to measure and compare the level of stability between Islamic and conventional banks in Saudi Arabia using quarterly data. The sample covers around two-thirds of banks operating in the Saudi stock market, and data comprises the last global financial crisis. The panel data model shows that Islamic banks relatively reduce the financial stability index; meanwhile, they contribute efficiently to enhance financial stability through the diversification of their assets. According to our findings Riyad Bank and SAMBA positively impact the financial stability, while Al-Rajhi bank has a positive but moderate role in enhancing the banking stability. As well, the Saudi banking sector exhibits a weak competitiveness which negatively impact the banking stability. Consequently, the limited presence of Islamic banks in the Saudi banking sector menaces any efforts to improve the financial stability.
C12|Efficiency in BRICS Currency Markets Using Long-Spans of Data: Evidence from Model-Free Tests of Directional Predictability|We analyze the directional predictability in foreign exchange markets of Brazil, Russia, India, China and South Africa (BRICS) using the quantilogram, based on long-spans of monthly historical data, at times covering over a century. We find that the efficient market hypothesis (EMH) holds at the extreme phases of the currency markets (and around the median for India and South Africa). Since predictability holds at certain parts of the unconditional distribution of exchange rate returns, we find support for the Adaptive Market Hypothesis (AMH). AMH, based on the idea of bounded rationality, suggests that currency return predictability will be intermittent, due to changing market conditions and institutional factors.
C12|Time-varying causal relationship between stock market and unemployment in the United Kingdom: Historical evidence from 1855 to 2017|The influence of financial markets on the real economy, including that of stock market returns on unemployment, is a key focus in the literature. Using DCC-MGARCH tests, we analyse time-varying causality between stock market returns and unemployment in the UK using monthly data from 1855 to 2017. The tests reveal that there is significant evidence of information spillover between the stock market and the labour market. This information spillover was found to be significant in the direction of stock market returns to unemployment, insignificant in the opposite direction, and significant bi-directionally. The results were also found to be congruent to the macroeconomic history of the UK.
C12|Structural Changes in the Duration of Bull Markets and Business Cycle Dynamics|This paper tests for structural changes in the duration of bull regimes of adjusted market capitalization stock indexes comprehending 18 developed and emerging economies, using a novel approach introduced by Nicolau (2016); and investigates whether the structural changes detected in the bull markets' duration are connected to the business cycle. Interestingly, the results show that structural changes in the duration of bull market regimes seem to anticipate periods of economic recession. The results provide statistically significant evidence that decreases in bull markets duration do not occur independently from economic crises, as 13 out of the 18 markets considered in our sample verify such decreases at least 12 months prior to the occurrence of an economic crisis. Additionally, these structural changes seem to affect smaller companies first, and then the larger ones. The association between decreases in the bull market regimes' duration and economic crises is possibly a consequence of the financial markets' leading behavior over the economy, with these structural changes serving as proxies for decreasing confidence in the financial markets, which naturally affects economic stability.
C12|Testing the fractionally integrated hypothesis using M estimation: With an application to stock market volatility|A new class of tests for fractional integration in the time domain based on M estimation is developed. This approach offers more robust properties against non-Gaussian errors than least squares or other estimation principles. The asymptotic properties of the tests are discussed under fairly general assumptions, and for different estimation approaches based on direct optimization of the M loss-function and on iterated k-step and reweighted LS numeric algorithms. Monte Carlo simulations illustrate the good finite sample performance of the new tests and an application to daily volatility of several stock market indices shows the empirical relevance of the new tests.
C12|Adaptive Inference In Heteroskedastic Fractional Time Series Models|We consider estimation and inference in fractionally integrated time series models driven by shocks which can display conditional and unconditional heteroskedasticity of unknown form. Although the standard conditional sum-of-squares (CSS) estimator remains consistent and asymptotically normal in such cases, unconditional heteroskedasticity inflates its variance matrix by a scalar quantity, lambda>1, thereby inducing a loss in efficiency relative to the unconditionally homoskedastic case, lambda=1. We propose an adaptive version of the CSS estimator, based on non-parametric kernel-based estimation of the unconditional variance process. This eliminates the factor lambda from the variance matrix, thereby delivering the same asymptotic efficiency as that attained by the standard CSS estimator in the unconditionally homoskedastic case and, hence, asymptotic efficiency under Gaussianity. The asymptotic variance matrices of both the standard and adaptive CSS estimators depend on any conditional heteroskedasticity and/or weak parametric autocorrelation present in the shocks. Consequently, asymptotically pivotal inference can be achieved through the development of confidence regions or hypothesis tests using either heteroskedasticity robust standard errors and/or a wild bootstrap. Monte Carlo simulations and empirical applications are included to illustrate the practical usefulness of the methods proposed.
C12|Distributional Gains Of Near Higher Earners|This paper looks at changes in employment and relative wages of near higher earnings (NHE) workers between middle-class (MC) and higher earners (HE) in Canada over 2000-2015. An approach is also forwarded for evaluating these changes in terms of underlying demand and supply factors. It is found that the NHE behaves as a transition group between quite different patterns of change of the MC and HE groups, and that these changes have been recently attenuating. The MC group experienced a downward shift in employment demand, the HE group an upward shift in demand, and the NHE group an upward shift in supply of workers.
C12|Wild Bootstrap Randomization Inference For Few Treated Clusters|When there are few treated clusters in a pure treatment or difference-in-differences setting, t tests based on a cluster-robust variance estimator (CRVE) can severely over-reject. Although procedures based on the wild cluster bootstrap often work well when the number of treated clusters is not too small, they can either over-reject or under-reject seriously when it is. In a previous paper, we showed that procedures based on randomization inference (RI) can work well in such cases. However, RI can be impractical when the number of possible randomizations is small. We propose a bootstrap-based alternative to randomization inference, whichmitigates the discrete nature of RI P values in the few-clusters case. We also compare it to two other procedures. None of them works perfectly when the number of clusters is very small, but they can worksurprisingly well.
C12|Specification testing in random coefficient models|In this paper, we suggest and analyze a new class of specification tests for random coefficient models. These tests allow to assess the validity of central structural features of the model, in particular linearity in coefficients, generalizations of this notion like a known nonlinear functional relationship, or degeneracy of the distribution of a random coefficient, that is, whether a coefficient is fixed or random, including whether an associated variable can be omitted altogether. Our tests are nonparametric in nature, and use sieve estimators of the characteristic function. We provide formal power analysis against global as well as against local alternatives. Moreover, we perform a Monte Carlo simulation study, and apply the tests to analyze the degree of nonlinearity in a heterogeneous random coefficients demand model. While we find some evidence against the popular QUAIDS specification with random coefficients, it is not strong enough to reject the specification at the conventional significance level.
C12|Government Spending and the Term Structure of Interest Rates in a DSGE Model|We explore asset pricing implications of productive, wasteful and utility enhancing government expenditures in a New Keynesian macro-finance model with Epstein-Zin preferences. We decompose the pricing kernel into four underlying macroeconomic factors (consumption growth, inflation, time preference shocks, long run risks for consumption and leisure) and design novel method to quantify the contribution of each factor to bond prices. Our methodology extends the performance attribution analysis typically used in finance literature on portfolio analysis. Using this framework, we show that bonds can serve as an insurance vehicle against the fluctuations in investors wealth induced by government spending. Increase in uncertainty surrounding government spending rises the demand for bonds leading to decrease in yields over the whole maturity profile. Bonds insure investors by i) providing buffer against bad times, ii) hedging inflation risk and iii) hedging real risks by putting current consumption gains against future losses. In a special case where the central bank does not respond to changes in output bonds leverage inflation risk. Spending reversals strongly reduce the sensitivity of bond prices to changes in government spending.
C12|Specification tests for non-Gaussian maximum likelihood estimators|We propose generalised DWH specification tests which simultaneously compare three or more likelihood-based estimators of conditional mean and variance parameters in multivariate conditionally heteroskedastic dynamic regression models. Our tests are useful for GARCH models and in many empirically relevant macro and finance applications involving VARs and multivariate regressions. To design powerful and reliable tests, we determine the rank deficiencies of the differences between the estimators' asymptotic covariance matrices under the null of correct specification, and take into account that some parameters remain consistently estimated under the alternative of distributional misspecification. Finally, we provide finite sample results through Monte Carlo simulations.
C12|Integrated Deviance Information Criterion for Latent Variable Models|Deviance information criterion (DIC) has been widely used for Bayesian model comparison, especially after Markov chain Monte Carlo (MCMC) is used to estimate candidate models. This paper studies the problem of using DIC to compare latent variable models after the models are estimated by MCMC together with the data augmentation technique. Our contributions are twofold. First, we show that when MCMC is used with data augmentation, it undermines theoretical underpinnings of DIC. As a result, by treating latent variables as parameters, the widely used way of constructing DIC based on the conditional likelihood, although facilitating computation, should not be used. Second, we propose two versions of integrated DIC (IDIC) to compare latent variable models without treating latent variables as parameters. The large sample properties of IDIC are studied and an asymptotic justi fication of IDIC is provided. Some popular algorithms such as the EM, Kalman and particle filtering algorithms are introduced to compute IDIC for latent variable models. IDIC is illustrated using asset pricing models, dynamic factor models, and stochastic volatility models.
C12|A Posterior-Based Wald-Type Statistic for Hypothesis Testing|A new Wald-type statistic is proposed for hypothesis testing based on Bayesian posterior distributions. The new statistic can be explained as a posterior version of Wald test and have several nice properties. First, it is well-defi ned under improper prior distributions. Second, it avoids Jeffreys-Lindley's paradox. Third, under the null hypothesis and repeated sampling, it follows a x2 distribution asymptotically, offering an asymptotically pivotal test. Fourth, it only requires inverting the posterior covariance for the parameters of interest. Fifth and perhaps most importantly, when a random sample from the posterior distribution (such as an MCMC output) is available, the proposed statistic can be easily obtained as a by-product of posterior simulation. In addition, the numerical standard error of the estimated proposed statistic can be computed based on the random sample. The finite sample performance of the statistic is examined in Monte Carlo studies. The method is applied to two latent variable models used in microeconometrics and financial econometrics.
C12|Diagnostic Tests for Homoskedasticity in Spatial Cross-Sectional or Panel Models|We propose tests for homoskedasticity in spatial econometric models, based on joint or concentrated score functions and an Outer-Product-of-Martingale-Difference (OPMD) estimate of the variance of the joint or concentrated score functions. Versions of these tests robust against non-normality are also given. Asymptotic properties of the proposed tests are formally examined using a cross-section model and a panel model with fixed effects. Monte Carlo results show that the proposed tests based on the concentrated score function have good finite sample properties. Finally, the generality of the proposed approach in constructing tests for homoskedasticity is further demonstrated using a spatial dynamic panel data model with short panels.
C12|The Grid Bootstrap for Continuous Time Models|This paper considers the grid bootstrap for constructing confidence intervals for the persistence parameter in a class of continuous time models driven by a Levy process. Its asymptotic validity is established by assuming the sampling interval (h) shrinks to zero. Its improvement over the in-fill asymptotic theory is achieved by expanding the coefficient-based statistic around its in fill asymptotic distribution which is non-pivotal and depends on the initial condition. Monte Carlo studies show that the gird bootstrap method performs better than the in-fill asymptotic theory and much better than the longspan theory. Empirical applications to U.S. interest rate data highlight differences between the bootstrap confidence intervals and the confidence intervals obtained from the in-fill and long-span asymptotic distributions.
C12|Pc Complex: Pc Algorithm For Complex Survey Data|PC algorithm is one of the most known procedures for Bayesian networks structural learning. The structure is inferred carrying out several independence tests on a database and building a Bayesian network in agreement with the tests results. The PC algorithm is based on the assumption of independent and identically distributed observations. In practice, sample selection in surveys involves more complex sampling designs, then the standard test procedure is not valid even asymptotically. In order to avoid misleading results about the true causal structure the sample selection process must be taken into account in the structural learning process. In this paper, a modi ed version of the PC algorithm is proposed for inferring casual structure from complex survey data. It is based on resampling techniques for nite population. A simulation experiment showing the robustness with respect to departures from the assumptions and the good performance of the proposed algorithm is carried out.
C12|Detecting Co‐Movements in Non‐Causal Time Series|This paper introduces the notion of common non‐causal features and proposes tools to detect them in multivariate time series models. We argue that the existence of co‐movements might not be detected using the conventional stationary vector autoregressive (VAR) model as the common dynamics are present in the non‐causal (i.e. forward‐looking) component of the series. We show that the presence of a reduced rank structure allows to identify purely causal and non‐causal VAR processes of order P>1 even in the Gaussian likelihood framework. Hence, usual test statistics and canonical correlation analysis can be applied, where either lags or leads are used as instruments to determine whether the common features are present in either the backward‐ or forward‐looking dynamics of the series. The proposed definitions of co‐movements are also valid for the mixed causal—non‐causal VAR, with the exception that a non‐Gaussian maximum likelihood estimator is necessary. This means however that one loses the benefits of the simple tools proposed. An empirical analysis on Brent and West Texas Intermediate oil prices illustrates the findings. No short run co‐movements are found in a conventional causal VAR, but they are detected when considering a purely non‐causal VAR.
C12|Estimation and Inference in Adaptive Learning Models with Slowly Decreasing Gains|This paper develops techniques of estimation and inference in a prototypical macroeconomic adaptive learning model with slowly decreasing gains. A sequential three-step procedure based on a ‘super-consistent’ estimator of the rational expectations equilibrium parameter is proposed. It is shown that this procedure is asymptotically equivalent to first estimating the structural parameters jointly via ordinary least-squares (OLS) and then using the so-obtained estimates to form a plug-in estimator of the rational expectations equilibrium parameter. In spite of failing Grenander’s conditions for well-behaved data, a limiting normal distribution of the estimators centered at the true parameters is derived. Although this distribution is singular, it can nevertheless be used to draw inferences about joint restrictions by applying results from Andrews (1987) to show that Wald-type statistics remain valid when equipped with a pseudo-inverse. Monte-Carlo evidence confirms the accuracy of the asymptotic theory for the finite sample behaviour of estimators and test statistics discussed here.
C12|Consistent Estimation Of Models Defined By Conditional Moment Restrictions Under Minimal Identifying Conditions|For econometric models defined by conditional moment restrictions, it is well known that the popular estimation methods such as the generalized method of moments and generalized empirical likelihood based on an arbitrary finite number of unconditional moment restrictions implied by the conditional moment restrictions can render inconsistent estimates. To guarantee the estimation consistency, some additional assumptions on these unconditional moment restrictions have to be levied. This paper introduces a simple consistent estimation procedure without assuming identifying conditions on the implied unconditional moment restrictions. This procedure is based on a weighted L2 norm with a unique weighting function, where a full continuum of unconditional moment restrictions is employed. It is quite easy to implement for any dimension of conditioning variables, and no any user-chosen number is required. Furthermore statistical inference is straightforward since the proposed estimator is asymptotically normal. Monte Carlo simulations demonstrate that the new estimator has excellent finite sample properties and outperforms other competitors in the cases we consider.
C12|Testing for the Conditional Geometric Mixture Distribution|This study examines the mixture hypothesis of conditional geometric distributions using a likelihood ratio (LR) test statistic based on that used for unconditional geometric distributions. As such, we derive the null limit distribution of the LR test statistic and examine its power performance. In addition, we examine the interrelationship between the LR test statistics used to test the geometric and exponential mixture hypotheses. We also examine the performance of the LR test statistics under various conditions and confirm the main claims of the study using Monte Carlo simulations.
C12|How far can we forecast? Statistical tests of the predictive content|Forecasts are useless whenever the forecast error variance fails to be smaller than the unconditional variance of the target variable. This paper develops tests for the null hypothesis that forecasts become uninformative beyond some limiting forecast horizon h. Following Diebold and Mariano (DM, 1995) we propose a test based on the comparison of the mean-squared error of the forecast and the sample variance. We show that the resulting test does not possess a limiting normal distribution and suggest two simple modifications of the DM-type test with different limiting null distributions. Furthermore, a forecast encompassing test is developed that tends to better control the size of the test. In our empirical analysis, we apply our tests to macroeconomic forecasts from the survey of Consensus Economics. Our results suggest that forecasts of macroeconomic key variables are barely informative beyond 2-4 quarters ahead.
C12|Testing for cointegration with threshold adjustment in the presence of structural breaks|In this paper, we develop new threshold cointegration tests with SETAR and MTAR adjustment allowing for the presence of structural breaks in the equilibrium equation. We propose a simple procedure to simultaneously estimate the previously unknown breakpoint and test the null hypothesis of no cointegration. Thereby, we extend the well-known residual-based cointegration test with regime shift introduced by Gregory and Hansen (1996a) to include forms of nonlinear adjustment. We derive the asymptotic distribution of the test statistics and demonstrate the finite-sample performance of the tests in a series of Monte Carlo experiments. We find a substantial decrease of power of the conventional threshold cointegration tests caused by a shift in the slope coefficient of the equilibrium equation. The proposed tests perform superior in these situations. An application to the 'rockets and feathers' hypothesis of price adjustment in the US gasoline market provides empirical support for this methodology.
C12|Exchange rates expectations and chaotic dynamics: A replication study|In this paper the author analyzes the behavior of exchange rates expectations for four currencies, by considering a re-calculation and an extension of Resende and Zeidan (Expectations and chaotic dynamics: empirical evidence on exchange rates, Economics Letters, 2008). Considering Lyapunov exponent-based tests results, they are not supportive of chaos in exchange rates expectations, although the so-called 0-1 test strongly supports the chaos hypothesis.
C12|Some (Maybe) Unpleasant Arithmetic in Minimum Wage Evaluations: The Role of Power, Significance and Sample Size|In this paper, we discuss the importance of sample size in the evaluation of minimum wage effects. We first show which sample sizes are necessary to make reliable statements about the effects of minimum wages on binary outcomes, and second how to determine these sample sizes. This is particularly important when interpreting statistically insignificant effects, which could be due to (i) the absence of a true effect or (ii) lack of statistical power, which makes it impossible to detect an effect even though it exists. We illustrate this for the analysis of labour market transitions using two data sets which are particularly important in the minimum wage research for Germany, the Integrated Labour Market Biographies (IEB) and the Socio-Economic Panel (SOEP).
C12|Inference for structural impulse responses in SVAR-GARCH models|Conditional heteroskedasticity can be exploited to identify the structural vector autoregressions (SVAR) but the implications for inference on structural impulse responses have not been investigated in detail yet. We consider the conditionally heteroskedastic SVAR-GARCH model and propose a bootstrap-based inference procedure on structural impulse responses. We compare the finite-sample properties of our bootstrap method with those of two competing bootstrap methods via extensive Monte Carlo simulations. We also present a three-step estimation procedure of the parameters of the SVAR-GARCH model that promises numerical stability even in scenarios with small sample sizes and/or large dimensions.
C12|Robust performance hypothesis testing with smooth functions of population moments|Applied researchers often want to make inference for the difference of a given performance measure for two investment strategies. In this paper, we consider the class of performance measures that are smooth functions of population means of the underlying returns; this class is very rich and contains many performance measures of practical interest (such as the Sharpe ratio and the variance). Unfortunately, many of the inference procedures that have been suggested previously in the applied literature make unreasonable assumptions that do not apply to real-life return data, such as normality and independence over time. We will discuss inference procedures that are asymptotically valid under very general conditions, allowing for heavy tails and time dependence in the return data. In particular, we will promote a studentized time series bootstrap procedure. A simulation study demonstrates the improved finite-sample performance compared to existing procedures. Applications to real data are also provided.
C12|Measuring financial interdependence in asset returns with an application to euro zone equities|A general procedure is proposed to identify changes in asset return interdependence over time using entropy theory. The approach provides a decomposition of interdependence in terms of comoments including coskewness, cokurtosis and covolatility as well as more traditional measures based on second order moments such as correlations. A new diagnostic test of independence is also developed which incorporates these higher order comoments. The properties of the entropy interdependence measure are demonstrated using a number of simulation experiments, as well as applying the methodology to euro zone equity markets over the period 1990 to 2017.
C12|Illegal Drugs and Public Corruption: Crack Based Evidence from California|Do illegal drugs foster public corruption? To estimate the causal effect of drugs on public corruption in California, we adopt the synthetic control method and exploit the fact that crack cocaine markets emerged asynchronously across the United States. We focus on California because crack arrived here in 1981, before reaching any other state. Our results show that public corruption more than tripled in California in the first three years following the arrival of crack cocaine. We argue that this resulted from the particular characteristics of illegal drugs: a large trade-off between profits and law enforcement, due to a cheap technology and rigid demand. Such a trade-off fosters a convergence of interests between criminals and corrupted public officials resulting in a positive causal impact of illegal drugs on corruption.
C12|Likelihood Inference on Semiparametric Models: Average Derivative and Treatment Effect|Over the past few decades, much progress has been made in semiparametric modelling and estimation methods for econometric analysis. This paper is concerned with inference (i.e. confidence intervals and hypothesis testing) in semiparametric models. In contrast to the conventional approach based on tâ€ ratios, we advocate likelihoodâ€ based inference. In particular, we study two widely applied semiparametric problems, weighted average derivatives and treatment effects, and propose semiparametric empirical likelihood and jackknife empirical likelihood methods. We derive the limiting behaviour of these empirical likelihood statistics and investigate their finite sample performance through Monte Carlo simulation. Furthermore, we extend the (deleteâ€ 1) jackknife empirical likelihood toward the deleteâ€ d version with growing d and establish general asymptotic theory. This extension is crucial to deal with nonâ€ smooth objects, such as quantiles and quantile average derivatives or treatment effects, due to the wellâ€ known inconsistency phenomena of the jackknife under nonâ€ smoothness.
C12|Confidence regions for entries of a large precision matrix|We consider the statistical inference for high-dimensional precision matrices. Specifically, we propose a data-driven procedure for constructing a class of simultaneous confidence regions for a subset of the entries of a large precision matrix. The confidence regions can be applied to test for specific structures of a precision matrix, and to recover its nonzero components. We first construct an estimator for the precision matrix via penalized node-wise regression. We then develop the Gaussian approximation to approximate the distribution of the maximum difference between the estimated and the true precision coefficients. A computationally feasible parametric bootstrap algorithm is developed to implement the proposed procedure. The theoretical justification is established under the setting which allows temporal dependence among observations. Therefore the proposed procedure is applicable to both independent and identically distributed data and time series data. Numerical results with both simulated and real data confirm the good performance of the proposed method.
C12|“Generalized Measures of Correlation for Asymmetry, Nonlinearity, and Beyond”: Comment|This note comments on the Generalised Measure of Correlation (GMC) suggested by Zheng et al. (2012). The GMC concept was largely anticipated in a publication 115 years earlier, undertaken by Yule (1897), in the proceedings of the Royal Society. The note is directed at giving Yule (1897) credit for covering the foundations of the topic comprehensively.
C12|Testing for parameter instability in predictive regression models|We consider tests for structural change, based on the SupF and Cramer–von-Mises type statistics of Andrews (1993) and Nyblom (1989), respectively, in the slope and/or intercept parameters of a predictive regression model where the predictors display strong persistence. The SupF type tests are motivated by alternatives where the parameters display a small number of breaks at deterministic points in the sample, while the Cramer–von-Mises alternative is one where the coefficients are random and slowly evolve through time. In order to allow for an unknown degree of persistence in the predictors, and for both conditional and unconditional heteroskedasticity in the data, we implement the tests using a fixed regressor wild bootstrap procedure. The asymptotic validity of the bootstrap tests is established by showing that the asymptotic distributions of the bootstrap parameter constancy statistics, conditional on the data, coincide with those of the asymptotic null distributions of the corresponding statistics computed on the original data, conditional on the predictors. Monte Carlo simulations suggest that the bootstrap parameter stability tests work well in finite samples, with the tests based on the Cramer–von-Mises principle seemingly the most useful in practice. An empirical application to U.S. stock returns data demonstrates the practical usefulness of these methods.
C12|Mean group estimation in presence of weakly cross-correlated estimators|This paper extends the mean group (MG) estimator for random coefficient panel data models by allowing the underlying individual estimators to be weakly cross correlated. This can arise, for example, in panels with spatially correlated errors. We establish that the MG estimator is asymptotically correctly centered, and its asymptotic covariance matrix can be consistently estimated. In contrast with the homogeneous case, the random coefficient specification allows for correct inference even when nothing is known about the weak cross-sectional dependence of the errors.
C12|A Closer Look at the Behavior of Uncertainty and Disagreement: Micro Evidence from the Euro Area|This paper examines point and density forecasts of real GDP growth, inflation and unemployment from the European Central Bank’s Survey of Professional Forecasters. We present individual uncertainty measures and introduce individual point- and density-based measures of disagreement. The data indicate substantial heterogeneity and persistence in respondents’ uncertainty and disagreement, with uncertainty associated with prominent respondent effects and disagreement associated with prominent time effects. We also examine the co-movement between uncertainty and disagreement and find an economically insignificant relationship that is robust to changes in the volatility of the forecasting environment. This provides further evidence that disagreement is not a reliable proxy for uncertainty.
C12|The Effect of the Conservation Reserve Program on Rural Economies: Deriving a Statistical Verdict from a Null Finding|"This article suggests two methods for deriving a statistical verdict from a null finding,allowing economists to more confidently conclude when “not significant"" can in fact be interpreted as “no substantive effect."" The proposed methodology can be extended to a variety of empirical contexts where size and power matter. The example used to demonstrate the method is the Economic Research Service's 2004 Report to Congress that was charged with statistically identifying any unintended negative employment consequences of the Conservation Reserve Program (the Program). The report failed to identify a statistically significant negative long-term effect of the Program on employment growth, but the authors correctly cautioned that the verdict of “no negative employment effect"" was only valid if the econometric test was statistically powerful. We replicate the 2004 analysis and use new methods of statistical inference to resolve the two critical deficiencies that preclude estimation of statistical power by economists: 1) positing a compelling effect size, and 2) providing an estimate of the variability of an unobserved alternative distribution using simulation methods. We conclude that the test used in the report had high power for detecting employment effects of -1 percent or lower resulting from the Program, equivalent to job losses reducing a conservative estimate of environmental benefits by a third.<br><small>(This abstract was borrowed from another version of this item.)</small>"
C12|Robust inference in models identified via heteroskedasticity|Identification via heteroskedasticity exploits differences in variances across regimes to identify parameters in simultaneous equations. I study weak identification in such models, which arises when variances change very little or the variances of multiple shocks change close to proportionally. I show that this causes standard inference to become unreliable, outline two tests to detect weak identification, and establish conditions for the validity of nonconservative methods for robust inference on an empirically relevant subset of the parameter vector. I apply these tools to monetary policy shocks, identified using heteroskedasticity in high frequency data. I detect weak identification in daily data, causing standard inference methods to be invalid. However, using intraday data instead allows the shocks to be strongly identified.
C12|Specification tests for non-Gaussian maximum likelihood estimators|We propose generalised DWH specification tests which simultaneously compare three or more likelihood-based estimators of conditional mean and variance parameters in multivariate conditionally heteroskedastic dynamic regression models. Our tests are useful for Garch models and in many empirically relevant macro and finance applications involving Vars and multivariate regressions. To design powerful and reliable tests, we determine the rank deficiencies of the differences between the estimators' asymptotic covariance matrices under the null of correct specification, and take into account that some parameters remain consistently estimated under the alternative of distributional misspecification. Finally, we provide finite sample results through Monte Carlo simulations.
C12|Testing productivity change, frontier shift, and efficiency change|Inference about productivity change over time based on data envelopment (DEA) has focused primarily on the Malmquist index and is based on asymptotic properties of the index. In this paper we propose a novel set of significance tests for DEA based productivity change measures based on permutations and accounting for the inherent correlations when panel data are observed. The tests are easily implementable and give exact significance probabilities as they are not based on asymptotic properties. Tests are formulated both for the geometric means of the Malmquist index, and also of its components, i.e. the frontier shift index and the eciency change index, which together enable analysis of not only the presence of differences, but also gives an indication of whether the productivity change is due to shifts in the frontiers and/or changes in the efficiency distributions. Simulation results show the power of, and suggest how to interpret the results of, the proposed tests. Finally, the tests are illustrated using a data set from the literature.
C12|Instrument-based estimation with binarized treatments: Issues and tests for the exclusion restriction|When estimating local average and marginal treatment effects using instrumental variables (IV), multivalued endogenous treatments are frequently binarized based on a specific threshold in treatment support. However, such binarization introduces a violation of the IV exclusion if (i) the IV affects the multivalued treatment within support areas below and/or above the threshold and (ii) such IV-induced changes in the multivalued treatment affect the outcome. We discuss assumptions that satisfy the IV exclusion restriction with the binarized treatment and permit identifying the average effect of (i) the binarized treatment and (ii) unit-level increases in the original multivalued treatment among specific compliers. We derive testable implications of these assumptions and propose tests, which we apply to the estimation of the returns to (binary) college graduation instrumented by college proximity.
C12|On bootstrap implementation of likelihood ratio test for a unit root|In this paper we investigate a bootstrap implementation of the likelihood ratio test for a unit root recently proposed by Jansson and Nielsen (2012). We demonstrate that the likelihood ratio test shows poor finite sample properties under strongly autocorrelated errors, i.e. if the autoregressive or moving average roots are close to −1. The size distortions in these case are more pronounced in comparison to the bootstrap M and ADF tests. We found that the bootstrap version of likelihood ratio test (with autoregressive recolouring) demonstrates better performance than bootstrap M tests. Moreover, the bootstrap likelihood ratio test show better finite sample properties in comparison to the bootstrap ADF in some cases.
C12|Copula-based Tests for Nonclassical Measurement Error – The Case of Fractional Random Variables|This paper addresses measurement error (ME) of double bounded variables, of which fractional variables, defined on the interval [0,1], constitute a prominent example. The text discusses consequences of ME and suggests a specification test sensitive to ME of such variables. Given the latter’s bounded support, ME is not independent of the original error-free variate, a fact that invalidates classical ME assumptions as a framework for the test. This is circumvented with a score test of independence between the error-free variate and ME, under which the latter becomes degenerate at zero and their joint distribution, specified as a copula function, reduces to the original variable’s distribution. This procedure yields a specification test of the distribution of the error-free variable, valid under mild assumptions on the marginal distribution of ME and under departures from the specified copula. The test’s finite-sample behaviour is also evaluated through a set of simulation experiments.
C12|Order Invariant Tests for Proper Calibration of Multivariate Density Forecasts|Established tests for proper calibration of multivariate density forecasts based on Rosenblatt probability integral transforms can be manipulated by changing the order of variables in the forecasting model. We derive order invariant tests. The new tests are applicable to densities of arbitrary dimensions and can deal with parameter estimation uncertainty and dynamic misspecification. Monte Carlo simulations show that they often have superior power relative to established approaches. We use the tests to evaluate GARCH-based multivariate density forecasts for a vector of stock market returns.
C12|Does the Federal Constitutional Court Ruling Mean the German Financial Market is Efficient?|Following the landmark ruling by the German Federal Constitutional Court in Karlsruhe on 7th February 2014 in which they endorsed the efficient market hypothesis, we present evidence on the efficiency of the German financial market. Introducing a new variance bound test based on the Component-GARCH model of volatility to analyse the long- and short-runs effects on the efficiency of the German financial market, we test the price volatility of four markets: DAX stock index, German sovereign debt index as provided by Barclays and Bloomberg, Euro gold index by the World Gold Council and Euro currency index by the Bank of England. Our use of the Component-GARCH-T model highlight two key contributions, the first being the analysis of the efficiency of the market in the long and short runs. However, a more important contribution is the result of our variance bound test highlight the relatively strong acceptance of the efficient market hypothesis in both the short and long runs in all the observed financial markets. It must be stated our research is of importance to researches in both applied finance and portfolio management. The influencing question of what moves specific markets is crucial to market participants seeking market alpha for their investments strategies and portfolio optimisations.
C12|Testing for leverage effects in the returns of US equities|This article questions the empirical usefulness of leverage effects to forecast the dynamics of equity returns. In sample, we consistently find a significant but limited contribution of leverage effects over the past 25 years of S&P 500 returns. From an out-of-sample forecasting perspective and using a variety of different models, we find no statistical or economical value in using leverage effects, provided that an asymmetric and fat-tailed conditional distribution is used. This conclusion holds both at the index level and for 70% of the individual stocks constituents of the equity index.
C12|Moment-based tests under parameter uncertainty|This paper considers moment-based tests applied to estimated quantities. We propose a general class of transforms of moments to handle the parameter uncertainty problem. The construction requires only a linear correction that can be implemented in-sample and remains valid for some extended families of non-smooth moments. We reemphasize the attractiveness of working with robust moments, which lead to testing procedures that do not depend on the estimator. Furthermore, no correction is needed when considering the implied test statistic in the out-of-sample case. We apply our methodology to various examples with an emphasis on the backtesting of value-at-risk forecasts.
C12|Index futures volatility and trading activity: Measuring causality at a multiple horizon|Copeland (1976) and Shalen (1993) state that the causal relationship between trading activity variables, such as volume, open interest and volatility, the three most important factors for traders and portfolio managers, extends beyond one day. However, the literature on causality thus far concerns a one-day horizon. In this study, we provide a more powerful causality test by measuring the strength of the causal relationship over a multiple horizon. The robustness of the results is analysed by splitting the sample into two period pre and post 2008 crisis. Our findings may impact the designing of trading strategies.
C12|Subvector inference when the true parameter vector may be near or at the boundary|Extremum estimators are not asymptotically normally distributed when the estimator satisfies the restrictions on the parameter space – such as the non-negativity of a variance parameter – andthe true parameter vector is near or at the boundary. This possible lack of asymptotic normality makes it difficult to construct tests for testing subvector hypotheses that control asymptotic size in a uniform sense and have good local asymptotic power irrespective of whether the true parameter vector is at, near, or far from the boundary. We propose a novel estimator that is asymptotically normally distributed even when the true parameter vector is near or at the boundary and the objective function is not defined outside the parameter space. The proposed estimator allows the implementation of a new test based on the Conditional Likelihood Ratio statistic that is easy-to-implement, controls asymptotic size, and has good local asymptotic power properties. Furthermore, we show that the test enjoys certain asymptotic optimality properties when the parameter of interest is scalar. In an application of the random coefficients logit model (Berry, Levinsohn and Pakes, 1995) to the European car market, we find that, for most parameters, the new test leads to tighter confidence intervals than the two-sided t-test commonly used in practice.
C12|Volatility Estimation and Jump Detection for drift-diffusion Processes|Logarithms of prices of financial assets are conventionally assumed to follow drift-diffusion processes. While the drift term is typically ignored in the infill asymptotic theory and applications, the presence of nonzero drifts is an undeniable fact. The finite sample theory and extensive simulations provided in this paper reveal that the drift component has a nonnegligible impact on the estimation accuracy of volatility and leads to a dramatic power loss of a class of jump identification procedures. We propose an alternative construction of volatility estimators and jump tests and observe significant improvement of both in the presence of nonnegligible drift. As an illustration, we apply the new volatility estimators and jump tests, along with their original versions, to 21 years of 5-minute log-returns of the NASDAQ stock price index.
C12|Fixed-Bandwidth CUSUM Tests Under Long Memory|We propose a family of self-normalized CUSUM tests for structural change under long memory. The test statistics apply non-parametric kernel-based fixed-b and fixed-m long-run variance estimators and have well-defined limiting distributions that only depend on the long-memory parameter. A Monte Carlo simulation shows that these tests provide finite sample size control while outperforming competing procedures in terms of power.
C12|Testing for normality in truncated anthropometric samples|Anthropometric historical analysis depends on the assumption that human characteristics—such as height—are normally distributed. I propose and evaluate a metric entropy, based on nonparametrically estimated densities, as a statistic for a consistent test of normality. My first test applies to full distributions for which other tests already exist and performs similarly. A modified version applies to truncated samples for which no test has been previously devised. This second test exhibits correct size and high power against standard alternatives. In contrast to the distributional prior of Floud et al. (1990), the test rejects normality in large parts of their sample; the remaining data reveal a downward trend in height, not upward as they argue.
C12|High dimensional semiparametric moment restriction models|Moment restriction semiparametric models, where both the dimension of parameter and the number of restrictions are divergent and an unknown function is involved, are studied using the generalized method of moments (GMM) and sieve method dealing with the nonparametric parameter. The consistency and normality for the GMM estimators are established. Meanwhile, a new test statistic is proposed for overidentification issue. Numerical examples are used to verify the established theory.
C12|Permutation tests for equality of distributions of functional data| Economic data are often generated by stochastic processes that take place in continuous time, though observations may occur only at discrete times. For example, electricity and gas consumption take place in continuous time. Data generated by a continuous time stochastic process are called functional data. This paper is concerned with comparing two or more stochastic processes that generate functional data. The data may be produced by a randomized experiment in which there are multiple treatments. The paper presents a test of the hypothesis that the same stochastic process generates all the functional data. In contrast to existing methods, the test described here applies to both functional data and multiple treatments. The test is presented as a permutation test, which ensures that in a finite sample, the true and nominal probabilities of rejecting a correct null hypothesis are equal. The paper also presents the asymptotic distribution of the test statistic under alternative hypotheses. The results of Monte Carlo experiments and an application to an experiment on billing and pricing of natural gas illustrate the usefulness of the test.
C12|Testing continuity of a density via g -order statistics in the regression discontinuity design| In the regression discontinuity design (RDD), it is common practice to assess the credibility of the design by testing the continuity of the density of the running variable at the cut-off, e.g., McCrary (2008). In this paper we propose a new test for continuity of a density at a point based on the so-called g-order statistics, and study its properties under a novel asymptotic framework. The asymptotic framework is intended to approximate a small sample phenomenon: even though the total number n of observations may be large, the number of effective observations local to the cut-off is often small. Thus, while traditional asymptotics in RDD require a growing number of observations local to the cut-off as n ? 8, our framework allows for the number q of observations local to the cut-off to be fixed as n ? 8. The new test is easy to implement, asymptotically valid under weaker conditions than those used by competing methods, exhibits finite sample validity under stronger conditions than those needed for its asymptotic validity, and has favorable power properties against certain alternatives. In a simulation study, we find that the new test controls size remarkably well across designs. We finally apply our test to the design in Lee (2008), a well-known application of the RDD to study incumbency advantage.
C12|GEL-based inference with unconditional moment inequality restrictions| This paper studies the properties of generalised empirical likelihood (GEL) methods for the estimation of and inference on partially identifi ed parameters in models specifi ed by unconditional moment inequality constraints. The central result is, as in moment equality condition models, a large sample equivalence between the scaled optimised GEL objective function and that for generalised method of moments (GMM) with weight matrix equal to the inverse of the efficient GMM metric for moment equality restrictions. Consequently, the paper provides a generalisation of results in the extant literature for GMM for the non-diagonal GMM weight matrix setting. The paper demonstrates that GMM in such circumstances delivers a consistent estimator of the identi fied set, i.e., those parameter values that satisfy the moment inequalities, and derives the corresponding rate of convergence. Based on these results the consistency of and rate of convergence for the GEL estimator of the identifi ed set are obtained. A number of alternative equivalent GEL criteria are also considered and discussed. The paper proposes simple conservative consistent confi dence regions for the identi fied set and the true parameter vector based on both GMM with a non-diagonal weight matrix and GEL. A simulation study examines the efficacy of the non-diagonal GMM and GEL procedures proposed in the paper and compares them with the standard diagonal GMM method.
C12|Inference on winners| Many empirical questions can be cast as inference on a parameter selected through optimization. For example, researchers may be interested in the effectiveness of the best policy found in a randomized trial, or the best-performing investment strategy based on historical data. Such settings give rise to a winner’s curse, where conventional estimates are biased and conventional confidence intervals are unreliable. This paper develops optimal confidence sets and median-unbiased estimators that are valid conditional on the parameter selected and so overcome this winner’s curse. If one requires validity only on average over target parameters that might have been selected, we develop hybrid procedures that combine conditional and projection confidence sets to offer further performance gains relative to existing alternatives.
C12|Testing Identifying Assumptions In Fuzzy Regression Discontinuity Designs|We propose a new specification test for assessing the validity of fuzzy regression discontinuity designs (FRD-validity). We derive a new set of testable implications, characterized by a set of inequality restrictions on the joint distribution of observed outcomes and treatment status at the cut-off. We show that this new characterization exploits all the information in the data useful for detecting violations of FRD-validity. Our approach differs from, and complements existing approaches that test continuity of the distributions of running variables and baseline covariates at the cut-off since ours focuses on the distribution of the observed outcome and treatment status. We show that the proposed test has appealing statistical properties. It controls size in large sample uniformly over a large class of distributions, is consistent against all fixed alternatives, and has non-trivial power against some local alternatives. We apply our test to evaluate the validity of two FRD designs. The test does not reject the FRD-validity in the class size design studied by Angrist and Lavy (1999) and rejects in the insurance subsidy design for poor households in Colombia studied by Miller, Pinto, and Vera-HernÃ¡ndez (2013) for some outcome variables, while existing density tests suggest the opposite in each of the cases.
C12|Replication studies in economics—How many and which papers are chosen for replication, and why?|We investigate how often replication studies are published in empirical economics and what types of journal articles are replicated. We find that between 1974 and 2014 0.1% of publications in the top 50 economics journals were replication studies. We consider the results of published formal replication studies (whether they are negating or reinforcing) and their extent: Narrow replication studies are typically devoted to mere replication of prior work, while scientific replication studies provide a broader analysis. We find evidence that higher-impact articles and articles by authors from leading institutions are more likely to be replicated, whereas the replication probability is lower for articles that appeared in top 5 economics journals. Our analysis also suggests that mandatory data disclosure policies may have a positive effect on the incidence of replication.
C12|Replication studies in economics—How many and which papers are chosen for replication, and why?|We investigate how often replication studies are published in empirical economics and what types of journal articles are replicated. We find that between 1974 and 2014 0.1% of publications in the top 50 economics journals were replication studies. We consider the results of published formal replication studies (whether they are negating or reinforcing) and their extent: Narrow replication studies are typically devoted to mere replication of prior work, while scientific replication studies provide a broader analysis. We find evidence that higher-impact articles and articles by authors from leading institutions are more likely to be replicated, whereas the replication probability is lower for articles that appeared in top 5 economics journals. Our analysis also suggests that mandatory data disclosure policies may have a positive effect on the incidence of replication.
C12|Inference for the neighborhood inequality index|The neighborhood inequality (NI) index measures aspects of spatial inequality in the distribution of incomes within the city. The NI index is defi ned as a population average of the normalized income gap between each individual's income (observed at a given location in the city) and the incomes of the neighbors, living within a certain distance range from that individual. This paper provides minimum bounds for the NI index standard error and shows that unbiased estimators can be identifi ed under fairly common hypothesis in spatial statistics. These estimators are shown to depend exclusively on the variogram, a measure of spatial dependence in the data. Rich income data are then used to infer about trends of neighborhood inequality in Chicago, IL over the last 35 years. Results from a Monte Carlo study support the relevance of the standard error approximations.
C12|Regression Discontinuity and Heteroskedasticity Robust Standard Errors: Evidence from a Fixed-Bandwidth Approximation|In regression discontinuity designs (RD), for a given bandwidth, researchers can estimate standard errors based on different variance formulas obtained under different asymptotic frameworks. In the traditional approach the bandwidth shrinks to zero as sample size increases; alternatively, the bandwidth could be treated as fixed. The main theoretical results for RD rely on the former, while most applications in the literature treat the estimates as parametric, implementing the usual heteroskedasticity-robust standard errors. This paper develops the “fixed-bandwidth” alternative asymptotic theory for RD designs, which sheds light on the connection between both approaches. I provide alternative formulas (approximations) for the bias and variance of common RD estimators, and conditions under which both approximations are equivalent. Simulations document the improvements in test coverage that fixed-bandwidth approximations achieve relative to traditional approximations, especially when there is local heteroskedasticity. Feasible estimators of fixed-bandwidth standard errors are easy to implement and are akin to treating RD estimators as locally parametric, validating the common empirical practice of using heteroskedasticity-robust standard errors in RD settings. Bias mitigation approaches are discussed and a novel bootstrap higher-order bias correction procedure based on the fixed bandwidth asymptotics is suggested.
C12|Some (maybe) unpleasant arithmetic in minimum wage evaluations: The role of power, significance and sample size|In this paper, we discuss the importance of sample size in the evaluation of minimum wage effects. We first show which sample sizes are necessary to make reliable statements about the effects of minimum wages on binary outcomes, and second how to determine these sample sizes. This is particularly important when interpreting statistically insignificant effects, which could be due to (i) the absence of a true effect or (ii) lack of statistical power, which makes it impossible to detect an effect even though it exists. We illustrate this for the analysis of labour market transitions using two data sets which are particularly important in the minimum wage research for Germany, the Integrated Labour Market Biographies (IEB) and the Socio-Economic Panel (SOEP).
C12|Rationalizing Rational Expectations? Tests and Deviations|In this paper, we build a new test of rational expectations based on the marginal distributions of realizations and subjective beliefs. This test is widely applicable, including in the common situation where realizations and beliefs are observed in two different datasets that cannot be matched. We show that whether one can rationalize rational expectations is equivalent to the distribution of realizations being a mean-preserving spread of the distribution of beliefs. The null hypothesis can then be rewritten as a system of many moment inequality and equality constraints, for which tests have been recently developed in the literature. Next, we go beyond testing by defining and estimating the minimal deviations from rational expectations that can be rationalized by the data. In the context of structural models, we build on this concept to propose an easy-to-implement way to conduct a sensitivity analysis on the assumed form of expectations. Finally, we apply our framework to test for and quantify deviations from rational expectations about future earnings, and examine the consequences of such departures in the context of a life-cycle model of consumption.
C12|Testing localization of Thai automobile industries|The development of industrial clusters is crucially important for industries such as automobiles. However, it is still doubtful whether all parts suppliers should be localized, regardless of the parts categories. We tested the above hypotheses using data compiled from the Thailand Automotive Industry Directory 2014. First, the factors affecting the location of the Thai automobile industry were reviewed. Second, the kernel density of the bilateral distances between parts suppliers was estimated. Finally, hypothesis testing on the localization of parts suppliers was conducted. The study found that the automobile industry as a whole was significantly localized, and significant localization occurs only within 150 km, in terms of bilateral distance between firms.
C12|Unified Tests for a Dynamic Predictive Regression|Testing for predictability of asset returns has been a long history in economics and finance. Recently, based on a simple predictive regression, Kostakis, Magdalinos and Stamatogiannis (2015, Review of Financial Studies) derived a Wald type test based on the context of the extended instrumental variable (IVX) methodology for testing predictability of stock returns and Demetrescu (2014) showed that the local power of the standard IVX-based test could be improved in some cases when a lagged predicted variable is added to the predictive regression on purpose, which poses a general important question on whether a lagged predicted variable should be included in the model or not. This paper proposes novel robust procedures for testing both the existence of a lagged predicted variable and the predictability of asset returns in a predictive regression regardless of regressors being stationary or nearly integrated or unit root and the AR model for regressors with or without intercept. A simulation study confirms the good finite sample performance of the proposed tests before applying the proposed tests to some real datasets in finance to illustrate their practical usefulness.
C12|Frekvensbaserede versus bayesianske metoder i empirisk økonomi|"Indenfor økonomi og samfundsvidenskab har den klassiske frekvens-baserede analysemetode traditionelt været fremherskende, men de senere år er flere samfundsforskere begyndt at anvende bayesianske metoder i empirisk modellering. I denne artikel beskrives og sammenlignes de to metoder. Der argumenteres for, at vi i højere grad bør anvende den bayesianske tilgang. Den klassiske metode giver sandsynligheden for data, givet modellen (nulhypotesen), mens den bayesianske metode giver sandsynligheden for modellen, givet data. Anvendelse af ""p-værdien""i det klassiske hypotesetest fører til for mange ""falsk positive"" resultater. Den bayesianske metode er mere velegnet end den klassiske til analyse af de hypoteser økonomer arbejder med, hvor en model ikke tilstræbes at være ""sand"", men i stedet opfattes som en grov approksimation til virkeligheden."
C12|Randomization Inference for Difference-in-Differences with Few Treated Clusters|Inference using difference-in-differences with clustered data requires care. Previous research has shown that, when there are few treated clusters, t tests based on a cluster-robust variance estimator (CRVE) severely over-reject, different variants of the wild cluster bootstrap can over-reject or under-reject dramatically, and procedures based on randomization inference show promise. We demonstrate that randomization inference (RI) procedures based on estimated coefficients, such as the one proposed by Conley and Taber (2011), fail whenever the treated clusters are atypical. We propose an RI procedure based on t statistics which fails only when the treated clusters are atypical and few in number. We also propose a bootstrap-based alternative to randomization inference, which mitigates the discrete nature of RI P values when the number of clusters is small. Two empirical examples demonstrate that alternative procedures can yield dramatically different inferences.
C12|Volatility Estimation and Jump Detection for drift-diffusion Processes|Logarithms of prices of financial assets are conventionally assumed to follow drift-diffusion processes. While the drift term is typically ignored in the infill asymptotic theory and applications, the presence of nonzero drifts is an undeniable fact. The finite sample theory and extensive simulations provided in this paper reveal that the drift component has a nonnegligible impact on the estimation accuracy of volatility and leads to a dramatic power loss of a class of jump identification procedures. We propose an alternative construction of volatility estimators and jump tests and observe significant improvement of both in the presence of nonnegligible drift. As an illustration, we apply the new volatility estimators and jump tests, along with their original versions, to 21 years of 5-minute log-returns of the NASDAQ stock price index.
C12|Continuous Record Asymptotics for Structural Change Models|For a partial structural change in a linear regression model with a single break, we develop a continuous record asymptotic framework to build interference methods for the break date. We have T observations with a sampling frequency h over a fixed time horizon [0, N], and let T â†’ âˆž with h â†“ 0 wile keeping the time span N fixed. We impose very mild regularity conditions on an underlying continuous-time model assumed to generate the data. We consider the least-squares estimate of the break date and establish consistency and convergence rate. We providea limit theory for shrinking magnitudes of shifts and locally increasing variances. The asymptotic distribution corresponds to the location of the extremum of a function of the quadratic variation of the regressors and of a Gaussian centered martingale process over a certain time interval. We can account for the asymmetric informational content provided by the pre- and post-break regimes and show how the location of the break and shift magnitude are key ingredients in shaping the distribution. We consider a feasible version based on plug-in estimates, which provides a very good approximation to the finite sample distribution. We use the concept of Highest Density Region to construct confidence sets. Overall, our method is reliable and delivers accurate coverage probabilities and relatively short average length of the confidence sets. Importantly, it does so irrespective of the size of the break.
C12|Continuous Record Laplace-based Inference about the Break Date in Structural Change Models|Building upon the continuous record asymptotic framework recently introduced by Casini and Perron (2017a) for inference in structural change models, we propose a Laplace-based (quasi-Bayes) procedure for the construction of the estimate and confidence set for the date of a structural change. The procedure relies on a Laplace-type estimator defined by an integration-based rather than an optimization-based method. A transformation of the least-integration-based rather than an optimization-based method. A transformation of the least-squares criterion function is evaluated in order to derive a proper distribution, referred to as the Quasi-posterior. For a given choice of a loss function, the Laplace-type estimator is defined as the minimizer of the expected risk with the expectation taken under the Quasi-posterior. Besides providing an alternative estimate that is more precise-lower mean absolute error (MAE) and lower root-mean squared error (RMSE)-than the usual least-squares one, the Quasi-posterior distribution can be used to construct asymptotically valid inference using the concept of Highest Density Region. The resulting Laplace-based inferential procedure proposed is shown to have lower MAE and RMSE, and the confidence sets strike the best balance between empirical coverage rates and average lengths of the confidence sets relative to traditional long-span methods, whether the break size is small or large.
C12|Varying Random Coefficient Models|This paper provides a new methodology to analyze unobserved heterogeneity when observed characteristics are modeled nonlinearly. The proposed model builds on varying random coefficients (VRC) that are determined by nonlinear functions of observed regressors and additively separable unobservables. This paper proposes a novel estimator of the VRC density based on weighted sieve minimum distance. The main example of sieve bases are Hermite functions which yield a numerically stable estimation procedure. This paper shows inference results that go beyond what has been shown in ordinary RC models. We provide in each case rates of convergence and also establish pointwise limit theory of linear functionals, where a prominent example is the density of potential outcomes. In addition, a multiplier bootstrap procedure is proposed to construct uniform confidence bands. A Monte Carlo study examines finite sample properties of the estimator and shows that it performs well even when the regressors associated to RC are far from being heavy tailed. Finally, the methodology is applied to analyze heterogeneity in income elasticity of demand for housing.
C12|Characteristic-Sorted Portfolios: Estimation and Inference|Portfolio sorting is ubiquitous in the empirical finance literature, where it has been widely used to identify pricing anomalies. Despite its popularity, little attention has been paid to the statistical properties of the procedure. We develop a general framework for portfolio sorting by casting it as a nonparametric estimator. We present valid asymptotic inference methods, and a valid mean square error expansion of the estimator leading to an optimal choice for the number of portfolios. In practical settings, the optimal choice may be much larger than standard choices of five or ten. To illustrate the relevance of our results, we revisit the size and momentum anomalies.
C12|Spanning Tests for Markowitz Stochastic Dominance|Using properties of the cdf of a random variable defined as a saddle-type point of a real valued continuous stochastic process, we derive first-order asymptotic properties of tests for stochastic spanning w.r.t. a stochastic dominance relation. First, we define the concept of Markowitz stochastic dominance spanning, and develop an analytical representation of the spanning property. Second, we construct a non-parametric test for spanning via the use of an empirical analogy. The method determines whether introducing new securities or relaxing investment constraints improves the investment opportunity set of investors driven by Markowitz stochastic dominance. In an application to standard data sets of historical stock market returns, we reject market portfolio Markowitz efficiency as well as two-fund separation. Hence there exists evidence that equity management through base assets can outperform the market, for investors with Markowitz type preferences.
C12|Bootstrap Assisted Tests of Symmetry for Dependent Data|TThe paper considers the problem of testing for symmetry (about an unknown centre) of the marginal distribution of a strictly stationary and weakly dependent stochastic process. The possibility of using the autoregressive sieve bootstrap and stationary bootstrap procedures to obtain critical values and P-values for symmetry tests is explored. Bootstrap-assisted tests for symmetry are straightforward to implement and require no prior estimation of asymptotic variances. The small-sample properties of a wide variety of tests are investigated using Monte Carlo experiments. A bootstrap-assisted version of the triples test is found to have the best overall performance.
C12|Bitcoin Awareness and Usage in Canada: An Update|This note provides an update of the results of the 2017 Bitcoin Omnibus Survey (BTCOS) conducted by the Bank of Canada from December 12 to 15, 2017. The BTCOS was previously conducted in November and December 2016 and the results were reported in Henry et al. (2017, forthcoming). The 2017 survey took place in an interesting time, as Bitcoin prices were increasing and reached an all-time high on December 17, 2017. During this period, the level of awareness of Bitcoin increased from 64 per cent in the 2016 BTCOS to 85 per cent in the 2017 BTCOS, while ownership rose from 2.9 to 5.0 per cent, respectively. The main reason cited by survey participants for owning Bitcoin changed from transactional purposes in 2016 to investment purposes in 2017. Further, only about half of Bitcoin owners were found to regularly use Bitcoin to buy goods or services or to send money to other people.
C12|Diversification, integration and cryptocurrency market|We investigate the degree to which cryptocurrencies provide diversification benefits to an investor. We use a stochastic spanning methodology to construct optimal portfolios with and without cryptocurrencies, evaluating their comparative performance both in- and out-of-sample. Empirical analysis seems to indicate that the expanded investment universe with cryptocurrencies dominates the traditional one with stocks, bonds and cash, yielding potential diversification benefits and providing better investment opportunities for some risk averse investors. We further explain our results by documenting that cryptocurrency markets are segmented from the equity and bond markets.
C12|Testing Over- and Underidentification in Linear Models, with Applications to Dynamic Panel Data and Asset-Pricing Models|This paper develops the links between overidentification tests, underidentification tests, score tests and the Cragg-Donald (1993, 1997) and Kleibergen-Paap (2006) rank tests in linear instrumental variables (IV) models. This general framework shows that standard underidentification tests are (robust) score tests for overidentification in an auxiliary linear model, x_1 = X_2 δ + ε_1, where X = [x_1 X_2] are the endogenous explanatory variables in the original model, estimated by IV estimation methods using the same instruments as for the original model. This simple structure makes it possible to establish valid robust underidentification tests for linear IV models where these have not been proposed or used before, like clustered dynamic panel data models estimated by GMM. The framework also applies to general tests of rank, including the I test of Arellano, Hansen and Sentana (2012), and, outside the IV setting, for tests of rank of parameter matrices estimated by OLS. Invariant rank tests are based on LIML or continuously updated GMM estimators of the first-stage parameters. This insight leads to the proposal of a new two-step invariant asymptotically efficient GMM estimator, and a new iterated GMM estimator that converges to the continuously updated GMM estimator.
C12|Illegal drugs and public corruption: Crack based evidence from California|Do illegal drugs foster public corruption? To estimate the causal effect of drugs on public corruption in California, we adopt the synthetic control method and exploit the fact that crack cocaine markets emerged asynchronously across the United States. We focus on California because crack arrived here in 1981, before reaching any other state. Our results show that public corruption more than tripled in California in the first three years following the arrival of crack cocaine. We argue that this resulted from the particular characteristics of illegal drugs: a large trade-off between profits and law enforcement, due to a cheap technology and rigid demand. Such a trade-off fosters a convergence of interests between criminals and corrupted public officials resulting in a positive causal impact of illegal drugs on corruption.
C12|A coupled component GARCH model for intraday and overnight volatility|We propose a semi-parametric coupled component GARCH model for intraday and overnight volatility that allows the two periods to have different properties. To capture the very heavy tails of overnight returns, we adopt a dynamic conditional score model with t innovations. We propose a several step estimation procedure that captures the nonparametric slowly moving components by kernel estimation and the dynamic parameters by t maximum likelihood. We establish the consistency and asymptotic normality of our estimation procedures. We extend the modelling to the multivariate case. We apply our model to the study of the component stocks of the Dow Jones industrial average over the period 1991-2016. We show that actually overnight volatility has increased in importance during this period. In addition, our model provides better intraday volatility forecast since it takes account of the full dynamic consequences of the overnight shock and previous ones.
C12|Likelihood Corrections for Two-way Models|The use of panel data models with two-way fixed effects is widespread. Incidental-parameter bias, however, invalidates inference based on the (profile) likelihood. We consider modifications to the likelihood that yield asymptotically-unbiased estimators as well as test statistics that are size correct under rectangular-array asymptotics. The modifications are widely applicable and easy to implement. Through several examples we illustrate that the modifications can lead to dramatic improvements relative to maximum likelihood, both in terms of point estimation and inference.
C12|New More Powerful Likelihood Ratio Tests for Short Horizon Event Studies|"Short horizon Event Studies (ES) in financial research, are concerned with the effects of firm-specific or market-wide events such as, stock-splits, earnings announcements, mergers and acquisitions, derivatives introductions etc. on the underlying firms' stock prices. Though it has been around for half a century, and evidences abound about the phenomenon of Event Induced Variance (EIV), the methodological development in the ES literature, has mostly focused only on a shift in location of the expected abnormal returns.In this work, a random-effect model is proposed which explicitly accounts for the (empirically observed) cross-sectional variance of the (predicted) abnormal returns, along with another parameter accommodating for (another empirical phenomenon of) a change in post-event volatility. Under this model, the null hypothesis of ""no event effect"" also involves these additional variance parameters other than the usual mean. This necessitates development of new tests for this and other hypotheses of interests in ES, for which new Likelihood Ratio Tests (LRTs) are derived.As is standard in the ES literature, the specification and power behavior of the newly developed LRTs are compared with those of the existing ES tests, using real returns of 1231 stocks, that were listed in the National Stock Exchange, India between April 1998 and January 2016. 100,000 samples of sizes 5 and 50 are drawn to estimate and compare the probabilities of type-I error and power of the tests. The new LRTs are compared with both the popular and recent parametric and non-parametric ES tests in the literature. The powers are compared under both presence and absence of shift in location of the distribution of the abnormal returns, along with those of the two components of EIV. The newly developed LRTs are found to be adequately specified, and for more powerful than the existing ES tests in the literature, for a wide spectrum of alternatives."
C12|The Effects Of Autocorrelation And Number Of Repeated Measures On Glmm Robustness With Ordinal Data|Longitudinal studies involving ordinal responses are widely conducted in many fields of the education, health and social sciences. In these cases, when units are observed over time, the possibility of auto-correlation between observations on the same subject exists. Therefore the assumption of independence which underlines the generalized linear models is violated. Generalized linear mixed models (GLMMs) accommodate repeated measures data for which the usual assumption of independent observations is untenable, and also accommodate a non-normally distributed dependent variable (i.e. multinomial distribution for ordinal data). Thus, GLMMs constitute a good technique for modelling correlated data and ordinal responses. In this study, for a split-plot design with two groups for the between-subjects factor and five response categories, we investigated empirical Type I error rates in GLMMs. To this end, we used a computer program developed by Wicklin to generate longitudinal ordinal data with SAS/IML. We manipulated the total sample size, the coefficient of variation of the group size, the number of repeated measures, and the values of autocorrelation coefficient. For each combination 5,000 replications were performed at a significance level of .05. The GLIMMIX procedure in SAS was used to fit the mixed-effects models for ordinal responses with multinomial distribution and the Kenward-Roger degrees of freedom adjustment for small samples. The results of simulations showed that the test is robust for group effect under all conditions analysed. For time and interaction effects, however, the robustness depends on the number of repeated measures and autocorrelations values. The test tends to be liberal with high autocorrelation, different values of autocorrelation in each group and large number of repeated measures. To sum up, GLMMs are a good analytical option for correlated ordinal outcomes with few repeated measures, low autocorrelation, and the same autocorrelation between groups.This research was supported by grant PSI2016-78737-P (AEI/FEDER, UE) from the Spanish Ministry of Economy, Industry and Competitiveness.
C12|Does Herd Behaviour Exist In Turkish Stock Markets? The Case Of Borsa Istanbul|Herd describes how individuals in a group can act collectively without centralized direction. The herd behavior on stock markets implies that investors ignore their own ideas in stock trading decisions and trade in the direction of the market. It is important to detect the effect of herding behavior in markets to assess the validity of rational asset pricing models and diversification opportunities. This paper, the validity of herding has been researched at Borsa Istanbul by considering two different models developed by Christie and Huang (1995) and Chang, Cheng, and Khorana (2000). Research data consists of daily logarithmic stock returns for the period of 1998 ? 2016. Study has been diversified by dividing the period into two sub-periods, 1998-2005 and 2006- 2016. According to obtained results, the direction that herding behavior effect has been felt intensively at the first sub-period in rising market conditions. The effect has fallen at the second sub-period.
C12|Testing Identification Strength|We consider models defined by a set of moment restrictions that may be subject to weak identification. We propose a testing procedure to assess whether instruments are ”too weak” for standard (Gaussian) asymptotic theory to be reliable. Since the validity of standard asymptotics for GMM rests upon a Taylor expansion of the first order conditions, we distinguish two cases: (i) models that are either linear or separable in the parameters of interest (ii) general models that are neither linear nor separable. Our testing procedure is similar in both cases, but our null hypothesis of weak identification for a nonlinear model is broader than the popular one. Our test is straightforward to apply and allows to test the null hypothesis of weak identification of specific subvectors without assuming identification of the components not under test. In the linear case, it can be seen as a generalization of the popular first-stage F-test but allows us to fix its shortcomings in case of heteroskedasticity. In simulations, our test is well behaved when compared to contenders, both in terms of size and power. In particular, the focus on subvectors allows us to have power to reject the null of weak identification on some components of interest. This observation may explain why, when applied to the estimation of the Elasticity of Intertemporal Substitution, our test is the only one to find matching results for every country under the two symmetric popular specifications: the intercept parameter is always found strongly identified, whereas the slope parameter is always found weakly identified.
C12|Predictability Hidden by Anomalous Observations|Testing procedures for predictive regressions with lagged autoregressive variables imply a suboptimal inference in presence of small violations of ideal assumptions. We propose a novel testing framework resistant to such violations, which is consistent with nearly integrated regressors and applicable to multi-predictor settings, when the data may only approximately follow a predictive regression model. The Monte Carlo evidence demonstrates large improvements of our approach, while the empirical analysis produces a strong robust evidence of market return predictability hidden by anomalous observations, both in- and out-of-sample, using predictive variables such as the dividend yield or the volatility risk premium.
C12|Relative error accurate statistic based on nonparametric likelihood|This paper develops a new test statistic for parameters defined by moment conditions that exhibits desirable relative error properties for the approximation of tail area probabilities. Our statistic, called the tilted exponential tilting (TET) statistic, is constructed by estimating certain cumulant generating function under exponential tilting weights. We show that the asymptotic p-value of the TET statistic can provide an accurate approximation to the p-value of an infeasible saddlepoint statistic, which is asymptotically chi-squared distributed with a relative error of order n−1 both in normal and large deviation regions. Numerical results illustrate the accuracy of the proposed TET statistic. Our results cover both just- and over-identified moment condition models.
