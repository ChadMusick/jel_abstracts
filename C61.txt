C61|Capital Accumulation, Green Paradox, and Stranded Assets: An Endogenous Growth Perspective|The existing studies on Green Paradox and stranded assets focus on dirty exhaustible assets (fossil fuel reserves) and show that environmental regulations, by changing the costs of dirty inputs relative to clean ones, lead to replacements of the former by the latter and stranding of dirty assets due to perfect substitution. It, in turn, induces acceleration of dirty resource extractions and pollution emissions for fear of dirty assets becoming stranded - the Green Paradox effect. This paper uses an endogenous growth framework to revisit the problem of Green Paradox and stranded assets by taking a new perspective that focuses on capital accumulation with investment irreversibility. We show that if 1) direct irreversibility of investment does not rule out the indirect channel of converting dirty capital goods into clean ones through final goods allocations, and 2) interactions between dirty and clean capital as imperfect substitutes can generate reciprocal effects, then environmental regulation, through directing investment towards clean capital, does not necessarily leads to asset stranding of dirty capital. Accumulation of clean capital with a pollution-saving effect offsets the polluting impact of dirty one and leads to reversed Green Paradox. We further propose an endogenous growth mechanism through which the accumulation of both dirty and clean capital, as well as environmental improvement, can be sustained in the long run without converging to the steady state.
C61|Optimal Stopping Time, Consumption, Labour, and Portfolio Decision for a Pension Scheme|In this work we solve in a closed form the problem of an agent who wants to optimise the inter-temporal utility of both his consumption and leisure by choosing: (i) the optimal inter-temporal consumption, (ii) the optimal inter-temporal labour supply, (iii) the optimal share of wealth to invest in a risky asset, and (iv) the optimal retirement age. The wage of the agent is assumed to be stochastic and correlated with the risky asset on the financial market. The problem is split into two sub-problems: the optimal consumption, labour, and portfolio problem is solved first, and then the optimal stopping time is approached. The martingale method is used for the first problem, and it allows to solve it for any value of the stopping time which is just considered as a stochastic variable. The problem of the agent is solved by assuming that after retirement he received a utility that is proportional to the remaining human capital. Finally, a numerical simulation is presented for showing the behaviour over time of the optimal solution.
C61|Distributed Optimal Control Models in Environmental Economics: A Review|We review the most recent advances in distributed optimal control applied to environmental economics, covering in particular problems where the state dynamics are governed by partial differential equations (PDEs). This is a quite fresh application area of distributed optimal control, which has already suggested several new mathematical research lines due to the specificities of the environmental economics problems involved. We enhance the latter through a survey of the variety of themes and associated mathematical structures beared by this literature. We also provide a quick tour of the existing tools in the theory of distributed optimal control that have been applied so far in environmental economics.
C61|Modeling transfer profits as externalities in a cooperative game-theoretic model of natural gas networks|Existing cooperative game theoretic studies of bargaining power in gas pipeline systems are based on the so called characteristic function form (CFF). This approach is potentially misleading if some pipelines fall under regulated third party access (TPA). TPA, which is by now the norm in the EU, obliges the owner of a pipeline to transport gas for others, provided they pay a regulated transport fee. From a game theoretic perspective, this institutional setting creates so called “externalities”, the description of which requires partition function form (PFF) games. In this paper we propose a method to compute payoffs, reflecting the power structure, for a pipeline system with regulated TPA. The method is based on an iterative flow mechanism to determine gas flows and transport fees for individual players and uses the recursive core and the minimal claim function to convert the PPF game back into a CFF game, which can be solved by standard methods. We illustrate the approach with a simple stylized numerical example of the gas network in Central Eastern Europe with a focus on Ukraine's power index as a major transit country.
C61|Have Econometric Analyses of Happiness Data Been Futile? A Simple Truth about Happiness Scales|Econometric analyses in the happiness literature typically use subjective well-being (SWB) data to compare the mean of observed or latent happiness across samples. Recent critiques show that com-paring the mean of ordinal data is only valid under strong assumptions that are usually rejected by SWB data. This leads to an open question whether much of the empirical studies in the economics of happiness literature have been futile. In order to salvage some of the prior results and avoid future issues, we suggest regression analysis of SWB (and other ordinal data) should focus on the median ra-ther than the mean. Median comparisons using parametric models such as the ordered probit and logit can be readily carried out using familiar statistical softwares like STATA. We also show a previously as-sumed impractical task of estimating a semiparametric median ordered-response model is also possi-ble by using a novel constrained mixed integer optimization technique. We use GSS data to show the famous Easterlin Paradox from the happiness literature holds for the US independent of any paramet-ric assumption.
C61|Factor-Analysis-Based Directional Distance Function: The case of New Zealand hospitals|This paper develops a new factor-analysis-based (FAB) approach for choosing the optimal direction in a directional distance function (DDF) analysis. It has the combined merits of factor analysis and slacks-based measure (SBM) and incorporates the relative ease with which various input-output could be adjusted. This development relieves the dependency of price information that is normally unavailable in the provision of public goods. This new FAB-DDF model has been applied on a dataset containing all public hospitals in New Zealand (NZ) observed during 2011-2017. The empirical results indicate that the average reduction across different labor is in the range of 3-10 percent, and the corresponding figure for capital input is 25.7 percent. The case-adjusted inpatient-discharge and price-adjusted outpatient-visit are used as measures of desirable output, the average efficiencies are 92.7 percent and 99 percent respectively. Hospital readmission within 28 days of discharge is used as a measure for undesirable output, and the average efficiency score is 90 percent. These evidence support the suspicion that perverse incentives might exist under the National Health Targets abolished in 2018, which was a set of six indicators used in the last decade to evaluate the performance of local District Health Boards.
C61|Firm-level Investment Under Imperfect Capital Markets in Ukraine|This paper develops and estimates a model of firm-level fixed capital investment when firms face borrowing constraints. Dynamically optimal investment functions are derived for the firms with and without financial constraints. These policy functions are then used to construct the likelihood of observing each of the investment regimes in the data. Structural parameters are estimated using data from the Ukrainian manufacturing sector in 1993–1998. I provide empirical evidence of the role of market and ownership structure for firm-level investment behavior. I also discuss the effects of international trade exposure and involvement in non-monetary transactions on the probability of facing financial constraints and the resulting fixed capital accumulation path. Estimation results are used to illustrate the welfare implications of financial constraints in the Ukrainian manufacturing sector.
C61|ITER A quarterly indicator of regional economic activity in Italy|This work documents the construction of the new quarterly indicator of regional economic activity (Indicatore Trimestrale dell’Economia Regionale – ITER), which uses a parsimonious set of regional variables and combines them by means of temporal disaggregation techniques to obtain a quarterly index that is consistent with the official data on national and regional GDP and marked by a small lag compared with the reference period. The methodology was implemented to produce quarterly indicators for the economies of Italy’s four macro-areas in the period 1995-2017. With a view to assessing the performance of the quarterly indicator, a forecasting exercise was conducted regarding annual GDP growth in the four macro-areas for the period 2014-17. The forecasting performance of ITER is in line with that of the indicators developed by other national research institutions.
C61|Bayesian Comparative Statics|We study how information affects equilibria and welfare in games. For an agent, more precise information about an unknown state of the world leads to a mean-preserving spread of beliefs. We provide necessary and sufficient conditions to obtain either a non-increasing mean or a non-decreasing-mean spread of actions whenever information precision increases for at least one agent. We apply our Bayesian comparative statics framework to study informational externalities in strategic environments. In persuasion games, we derive sufficient conditions that lead to extremal disclosure of information. In oligopolistic markets, we characterize the incentives of firms to share information. In macroeconomic models, we show that information not only drives the amplitude of business cycles but also affects aggregate output.
C61|Market-implied systemic risk and shadow capital adequacy|This paper presents a forward-looking approach to measure systemic solvency risk using contingent claims analysis (CCA) as a theoretical foundation for determining an institution’s default risk based on the uncertainty in its asset value relative to promised debt payments over time. Default risk can be quantified as market-implied expected losses calculated from integrating equity market and balance sheet information in a structural default risk model. The expected losses of multiple banks and their non-parametric dependence structure define a multivariate distribution that generates portfolio-based estimates of the joint default risk using the aggregation technique of the Systemic CCA framework (Jobst and Gray, 2013). This market-implied valuation approach (‘shadow capital adequacy’) endogenises bank solvency as a probabilistic concept based on the perceived default risk (in contrast to accounting-based prudential measures of capital adequacy). The presented model adds to the literature of analytical tools estimating market-implied systemic risk by augmenting the CCA approach with a jump diffusion process of asset changes to inform a more comprehensive and flexible assessment of common vulnerabilities to tail risks of the four largest UK commercial banks.
C61|U.S. Macroeconomic Policy Evaluation in an Open Economy Context using Wavelet Decomposed Optimal Control Methods|It is widely recognized that the policy objectives of fiscal and monetary policymakers usually have different time horizons, and this feature may not be captured by traditional econometric techniques. In this paper, we first decompose U.S macroeconomic data using a time-frequency domain technique, namely discrete wavelet analysis. We then model the behavior of the U.S. economy over each wavelet frequency range and use our estimated parameters to construct a tracking model. To illustrate the usefulness of this approach, we simulate jointly optimal fiscal and monetary policy with different short-term targets: an inflation target, a money growth target, an interest rate target, and a real exchange rate target. The results determine the reaction in fiscal and monetary policy that is required to achieve an inflation target in a low inflation environment, and when both fiscal and monetary policy are concerned with meeting certain economic growth objectives. The combination of wavelet decomposition in an optimal control framework can also provide a new approach to macroeconomic forecasting.
C61|Production Efficiency of Nodal and Zonal Pricing in Imperfectly Competitive Electricity Markets|Electricity markets employ different congestion management methods to handle the limited transmission capacity of the power system. This paper compares production efficiency and other aspects of nodal and zonal pricing. We consider two types of zonal pricing: zonal pricing with Available Transmission Capacity (ATC) and zonal pricing with Flow-Based Market Coupling (FBMC). We develop a mathematical model to study the imperfect competition under zonal pricing with FBMC. Zonal pricing with FBMC is employed in two stages, a day-ahead market stage and a re-dispatch stage. We show that the optimality conditions and market clearing conditions can be reformulated as a mixed integer linear program (MILP), which is straightforward to implement. Zonal pricing with ATC and nodal pricing is used as our benchmarks. The imperfect competition under zonal pricing with ATC and nodal pricing are also formulated as MILP models. All MILP models are demonstrated on 6-node and the modified IEEE 24-node systems. Our numerical results show that the zonal pricing with ATC results in large production inefficiencies due to the inc-dec game. Improving the representation of the transmission network as in the zonal pricing with FBMC mitigates the inc-dec game.
C61|Can wholesale electricity prices support “subsidy-free” generation investment in Europe?|Using a Pan-European electricity dispatch model we find that with higher variable renewable energy (VRE) production wholesale power prices may no longer serve as a long-run signal for generation investment in 2025. If wind and solar are to be self-financing by 2025 under the current European market design, they would need to be operating in circumstances which combine lower capital cost with higher fossil fuel and/or carbon prices. In the absence of these conditions, long term subsidy mechanisms would need to continue in order to meet European renewable electricity targets. More VRE production will exacerbate the ‘missing money’ problem for conventional generation. Thus, closures of unprofitable fossil fuel generation would sharpen and increase energy-only prices but would put more pressure on ancillary services markets to support system stability. Thus, the question of the need for a market redesign to let the market guide investments in both renewables and conventional generation would seem to remain.
C61|Estimating Lifetimes and Stock Turnover Dynamics of Urban Residential Buildings in China|Building lifetime and stock turnover are both key determinants in modelling building energy and carbon. However in China, aside from anecdotal claims that urban residential buildings are generally short-lived, there are no recent official statistics, and empirical data are extremely limited. We present a system dynamics model where survival analysis is used to characterise the dynamic interplay between new construction, aging, and demolition of residential buildings in urban China. The uncertainties associated with building lifetime were represented using a Weibull distribution, whose shape and scale parameters were calibrated based on official statistics on floor area up to 2006. The calibrated Weibull lifetime distribution allowed us to estimate the dynamic stock turnover of Chinese urban residential buildings for 2007 to 2017. We find that the average lifetime of urban residential buildings was around 34 years, and the overall residential stock size reached 23.7 billion m 2 in 2017. The resultant age-specific sub-stocks provide a baseline for the overall stock, which—along with the calibrated Weibull lifetime distribution—can be used in further modelling and for analysis of policies to reduce the whole-life embodied and operational energy and CO 2 emissions in Chinese residential buildings.
C61|A Unit Commitment and Economic Dispatch Model of the GB Electricity Market – Formulation and Application to Hydro Pumped Storage|We present a well calibrated unit commitment and economic dispatch model of the GB electricity market and applied it to the economic analysis of the four existing hydro pumped storage (PS) stations in GB. We found that with more wind on the system PS arbitrage revenue increases: with every percentage point (p.p) increase in wind capacity the total PS arbitrage profit increases by 0.21 p.p.. However, under a range of wind capacity, the PS’ modelled revenue from price arbitrage is not enough to cover their ongoing fixed costs. Analysing the 2015-18 GB balancing and ancillary services data suggests that PS stations were not active in managing transmission constraints and in fact about 60% of constraint payments went to gas-fired units. However, the PS stations are active in provision of ancillary services such as fast reserve, response and other reserve services with a combined market share of at least 30% in 2018. Stacking up the modelled revenue from price arbitrage with the 2018 balancing and ancillary services revenues against the ongoing fixed costs suggests that the four existing PS stations are profitable. Most of the revenue comes from balancing and ancillary services markets – about 75% – whereas only 25% comes from price arbitrage. However, the revenues will not be enough to cover capex and opex of a new 600 MW PS station. The gap in financing will have to come from balancing and ancillary services market opportunities and less so from purely price arbitrage. Finally, we found that the marginal contribution of most of the existing PS stations to gas and coal plant profitability is negative, while from the system point of view, PS stations do contribute to minimizing the total operating cost.
C61|Seasonal Flexibility in the European Natural Gas Market|The paper focuses on a seasonal demand swing in the European gas market. We quantify and compare the role of different flexibility options (domestic production, pipeline and LNG imports, and gas storages) in covering European demand fluctuations in monthly resolution. We contribute to the existing literature focusing on seasonal flexibility by addressing the problem with a mathematical gas market optimisation model. Empirically, our paper provides valuable insights with regard to declining North Western European gas production. Furthermore, we focus our discussion on specific flexibility features of pipeline versus LNG supplies and gas imports versus storage dispatch. In terms of methodology, we develop a bottom-up market optimisation model and publish the complete source code (which is still uncommon for gas market models). Furthermore, we propose a new metric based on the coefficient of variation to quantify the importance of supply sources for seasonal flexibility provision.
C61|Applying Bayesian Model Averaging to Characterise Urban Residential Stock Turnover Dynamics|Building stock is a key determinant in building energy and China is the largest producer of CO2 emissions and the largest consumer of energy and building energy, so any effective energy and climate policy will need to address this key driver of energy use. However, official statistics on total floor area of urban residential stock in China only exist up to 2006. Previous studies estimating Chinese urban residential stock size and energy use made various questionable methodological assumptions and only produced deterministic results. We present a Bayesian approach to characterise the stock turnover dynamics and estimate stock size uncertainties. Firstly, a probabilistic dynamic building stock turnover model is developed to describe the building aging and demolition process governed by a hazard function specified by a parametric survival model. Secondly, using five candidate parametric survival models, the building stock turnover model is simulated through Markov Chain Monte Carlo (MCMC) to obtain posterior distributions of model-specific parameters, estimate marginal likelihood, and make predictions on stock size. Finally, Bayesian Model Averaging (BMA) is applied to create a model ensemble that combines the model-specific posterior predictive distributions of the stock evolution pathway in proportion to posterior model probabilities. This Bayesian modelling framework and its results in the form of probability distributions of annual total stock and age-specific substocks, can provide a solid basis for further modelling and analysis of policy trade-offs across embodied-versus-operational energy consumption and carbon emissions of buildings in the context of sector-wide transitions aimed at decarbonising buildings.
C61|Carbon-sensitive Meta-Productivity Growth and Technological Gap: An Empirical Analysis of Indian Thermal Power Sector|This paper measures carbon-sensitive efficiency and productivity growth in technologically heterogeneous coal-fired thermal power plants in India for the period of 2000 to 2013. It uses a unique data set of 56 plants, obtained petitioning the Right to Information Act 2005. We apply ‘within-MLE’ fixed effects stochastic frontier model to get consistent estimates of meta-directional output distance function. The thermal power plants are grouped in two categories: central sector and state sector. We find that the state sector plants have higher potential to simultaneously increase electricity generation and reduce carbon emission than the central sector plants. If all the state and central sectors plants were made to operate on the meta-frontier, reduction of 98 million tonnes of CO2 could have been achieved. Carbon-sensitive productivity growth in the central sector plants is higher than the plants in state sector, though in both the sectors productivity growth is governed by carbon-sensitive innovation effect. Commercialisation or autonomy in electricity generation also induces carbon-sensitive productivity growth and reduces carbon-sensitive productivity growth gap.
C61|Discounting in the Presence of Scarce Ecosystem Services|Discounting has to take account of ecosystem services in consumption and production. Previous literature focuses on the first aspect and shows the importance of the relative price effect, for given growth rates of consumption and ecosystem services. This paper focuses on intermediate ecosystem services in production and shows that for limited substitutability and a low growth rate of these ecosystem services, the growth rate of consumption, and thus the discount rate, declines towards a low value. Using a Ramsey optimal-growth framework, the paper distinguishes three cases. If ecosystem services can be easily substituted, then the discount rate converges to the usual value in the long term. Secondly, if ecosystem services can be easily substituted in production but not in consumption, the relative price effect is important. Finally, and most interestingly, if ecosystem services cannot be easily substituted in production, the discount rate declines towards a low value and the relative price effect is less important. Another part of the previous literature has shown that a declining discount rate is the result of introducing several forms of uncertainty, but this paper reaches that conclusion from an endogenous effect on the growth rate of the economy.
C61|Tenure Choice, Portfolio Structure and Long-Term Care - Optimal Risk Management in Retirement|We study the interplay between tenure decisions, stock market investment and the public social security system. Housing equity not only serves a dual purpose as a consumption good and as an asset, but also provides insurance to buffer various risks in retirement. Our life cycle model captures these links in order to explain why homeownership in Germany is so low. Our simulation results indicate that the public long-term care as well as the pension system reduce the homeownership rate in Germany by 10-15 percentage points.
C61|Power Markets in Transition: Decarbonization,Energy Efficiency, and Short-Term Demand Response|Energy efficiency and short-term demand response are key issues in the decarbonization of power markets. However, their interaction and combined impact on market prices as well as on the supply side, is yet to be understood. We develop a framework to implement investments in energy efficiency and short-term demand response in detailed partial equilibrium power market models. We quantify our results using the EU-REGEN model for the European power market and find that energy efficiency contributes, under a 80% emission reduction target, only 11% of carbon emission reductions. Intermittent renewable energies such as wind and solar power account for the major share of 53%. However, both energy efficiency and short-term demand response have their merits in reducing marginal abatement costs and additionally exhibit an subadditive effect, at least under a 80% climate policy.
C61|A Framework for Modeling the Dynamics of Power Markets – The EU-REGEN Model|The long-run development of power markets will be deeply affected by the gradual substitution of fossil fuel-based generation technologies by renewable energy technologies (RES). However, the intermittent supply of RES, in combination with the temporal non-homogeneity of electricity demand, limits the competitiveness of renewable energies (Joskow, 2011). We develop a partial-equilibrium model of the European power market that contributes with a framework for capturing the temporal and spatial variability of RES. Furthermore, we differentiate wind and solar technologies by different quality classes and contribute with a routine for using meteorological data to approximate the temporal availability of renewable energy technologies. The composite of all these RES features allows then for a detailed representation of RES and their implicit substitution elasticity with fossil fuel-based technologies. Our results for the long-run electricity generation path of the European power market show that, under an 80% CO2 emissions reduction scenario until 2050, renewable energy technologies become the main technologies that will meet the demand. The 2050 generation share of wind and solar power combined is around 40%. However, with the detailed depiction of their temporal and spatial characteristics, we identify that gas power is necessary as a complement to compensate for their intermittent supply, which requires in turn the utilization of carbon capture and storage to adhere to the climate target.
C61|Robust Desmoothed Real Estate Returns|This research starts from the observation that common desmoothing models are likely to generate some extreme returns. Such returns will distort risk measurement and hence can lead to investment decisions that are suboptimal relative to those that would be made if a transaction based index were available. Thus, we propose to improve the desmoothing models by incorporating a robust filter into the procedure. We report that in addition to properly treating for smoothing, the method prevents the occurrence of extreme values. As shown with U.S. data, our method leads to desmoothed series whose characteristics are akin to those of transaction-based indices.
C61|A Full Characterization of Best-Response Functions in the Lottery Colonel Blotto Game|We fully characterize best-response functions in Colonel Blotto games with lottery contest success functions.
C61|Unhedgeable Inflation Risk within Pension Schemes|Pension schemes generally aim to protect the purchasing power of their participants, but cannot completely do this when due to market incompleteness inflation risk cannot be fully hedged. Without a market price for inflation risk the value of a pension contract depends on the investor's risk appetite and inflation risk exposure. We develop a valuation framework to deal with two sources of unhedgeable inflation risk: the absence of instruments to hedge general consumer price inflation risk and differences in group-specific consumption bundles from the economy-wide bundle. We find that the absence of financial instruments to hedge inflation risks may reduce lifetime welfare by up to 6% of certainty-equivalent consumption for commonly assumed degrees of risk aversion. Regulators face a dilemma as young (workers) and old participants (retirees) have different capacities to absorb losses from unhedgeable inflation risks and as a consequence have a different risk appetite.
C61|Imposing Equilibrium Restrictions in the Estimation of Dynamic Discrete Games|Imposing equilibrium restrictions provides substantial gains in the estimation of dynamic discrete games. Estimation algorithms imposing these restrictions -- MPEC, NFXP, NPL, and variations -- have different merits and limitations. MPEC guarantees local convergence, but requires the computation of high-dimensional Jacobians. The NPL algorithm avoids the computation of these matrices, but -- in games -- may fail to converge to the consistent NPL estimator. We study the asymptotic properties of the NPL algorithm treating the iterative procedure as performed in finite samples. We find that there are always samples for which the algorithm fails to converge, and this introduces a selection bias. We also propose a spectral algorithm to compute the NPL estimator. This algorithm satisfies local convergence and avoids the computation of Jacobian matrices. We present simulation evidence illustrating our theoretical results and the good properties of the spectral algorithm.
C61|When the U.S. catches a cold, Canada sneezes: a lower-bound tale told by deep learning|"The Canadian economy was not initially hit by the 2007-2009 Great Recession but ended up having a prolonged episode of the effective lower bound (ELB) on nominal interest rates. To investigate the Canadian ELB experience, we build a ""baby"" ToTEM model -- a scaled-down version of the Terms of Trade Economic Model (ToTEM) of the Bank of Canada. Our model includes 49 nonlinear equations and 21 state variables. To solve such a high-dimensional model, we develop a projection deep learning algorithm -- a combination of unsupervised and supervised (deep) machine learning techniques. Our findings are as follows: The Canadian ELB episode was contaminated from abroad via large foreign demand shocks. Prolonged ELB episodes are easy to generate in open-economy models, unlike in closed-economy models. Nonlinearities associated with the ELB constraint have virtually no impact on the Canadian economy but other nonlinearities do, in particular, the degree of uncertainty and specific closing condition used to induce the model's stationarity."
C61|Capital Stock and Depreciation: Theory and an Empirical Application|There are many puzzles and unresolved problems in empirical economics that depend on the reliability of the productive capital series. Some macroeconomic topics and questions cannot be addressed correctly or answered with the available standard statistical measures of capital stock. We make an innovative contribution to the theory of capital together with an exercise that quantifies the depreciation rate and the capital stock for the U.S. economy. An intertemporal optimization model with adjustment and maintenance costs, gives us the algorithm and the corresponding economic estimation of capital deterioration and obsolescence. Our measures are based on profitability and the Tobin’s q ratio.
C61|Policy Effectiveness In Spatial Resource Wars: A Two-Region Model|"We develop a spatial resource model in continuous time in which two agents/players strategically exploit a mobile resource in a two-region setup. To counteract the overexploitation of the resource (the tragedy of commons) that occurs when players are free to choose where to fish/hunt/extract/harvest, the regulator can establish a series of spatially structured policies. We compare the equilibria in the case of a common resource with those that emerge when the regulator either creates a natural reserve, or assigns Territorial User Rights to the players. We show that, when the discount rate is close to its \critical value"", i.e. when technological and preference parameters dictate a low harvesting intensity/effort, the policies are ineffective in promoting the conservation of the resource and, in addition, they lead to a lower payoff for at least one of the players. Conversely, in a context of harsher harvesting intensity, the intervention can help to safeguard the resource, preventing extinction while also improving the welfare of both players."
C61|A Structural Model of a Multitasking Salesforce: Job Task Allocation and Incentive Plan Design|We develop the first structural model of a multitasking salesforce to address questions of job design and incentive compensation design. The model incorporates three novel features: (i) multitasking effort choice given a multidimensional incentive plan; (ii) salesperson’s private information about customers and (iii) dynamic intertemporal tradeoffs in effort choice across the tasks. The empirical application uses data from a micro nance bank where loan officers are jointly responsible and incentivized for both loan acquisition repayment but has broad relevance for salesforce management in CRM settings involving customer acquisition and retention. We extend two-step estimation methods used for unidimensional compensation plans for the multitasking model with private information and intertemporal incentives by combining flexible machine learning (random forest) for the inference of private information and the first-stage multitasking policy function estimation. Estimates reveal two latent segments of salespeople-a “hunter” segment that is more efficient in loan acquisition and a “farmer” segment that is more efficient in loan collection. We use counterfactuals to assess how (1) multi-tasking versus specialization in job design; (ii) performance combination across tasks (multiplicative versus additive); and (iii) job transfers that impact private information impact firm profits and specific segment behaviors.
C61|Dynamic Mechanisms with Verification|We consider a principal who allocates an indivisible object among a finite number of agents who arrive on-line, each of whom prefers to have the object than not. Each agent has access to private information about the principal's payoff if he receives the object. The decision to allocate the object to an agent must be made upon arrival of an agent and is irreversible. There are no monetary transfers but he principal can inspect agents' reports at a cost and punish them. A novelty of this paper is a reformulation of this dynamic problem as a compact linear program. Using the formulation we characterize the form of the optimal mechanism and reduce the dynamic version of the inspection problem with identical distributions to an instance of the secretary problem with one fewer secretary and a modified value distribution. This reduction also allows us to derive a prophet inequality for the dynamic version of the inspection problem.
C61|On the interaction between real economy and financial markets|We introduce a dynamical model describing the interaction between a three-sectors real economy and a financial market with four assets. Investors and financial mediators have heterogeneous beliefs. The model may be used to investigate interdependence within economic fluctuations and assets volatility.
C61|Model and measure the relative efficiency of a four-stage production process. An NDEA multiplier relational model under different systems of resource distribution preferences between sub-processes|Measuring the relative efficiency of a production process with the DEA considers the production process as a “black box” that uses inputs to transform them into outputs. In reality, many production processes are carried out by carrying out several interconnected activities that are usually grouped into phases that are in turn interconnected. For this reason, measuring the relative efficiency of a production process within the DEA technique requires shaping it as a network system (in others words to consider the production process as interconnected sub-process). In the case of network systems, the NDEA approach has developed many models to measure their relative efficiency: independent models, connected models and relational models. In particular, the relational model allows to measure at the same time both the efficiency of the system and the efficiency of the sub-process once the operations between the latter have been considered. In our opinion, many real production processes can be modelled as a network of four sub-processes that are differently interconnected with each other. In this paper we will model a production process as a network of four sub-processes with shared variables and fixed preferences about the allocation of system resources between them. To measure the relative efficiency of the process and its parts we will develop an input-oriented NDEA model in the multiplier version. To solve the model we will use virtual data under several resources allocation preference’s structure. Then we will conclude that 1) a production process with four interconnected sub-processes can represent a large number of real production processes, so the NDEA model developed here can potentially be used for many applications, 2) the resource allocation preference system inter-sub-process influences the measurement of relative efficiency.
C61|Long-Term Electricity Investments Accounting for Demand and Supply Side Flexibility|Short-term Electricity Demand Response (DR) is an emerging technology in Europe's Electricity markets that will introduce a new degree of flexibility. The objective of this work is to analyze to what extent the untapped DR potential can facilitate an optimal transition to an European low emission power system. The benefits of DR consists of a reduction in peak load consumption, which leads to reduction in capacity investments, production and consumption savings, reduced congestion phases, reliable integration of intermittent renewable resources and supply and demand flexibility. The capabilities of DR are studied in the European Model for Power Investment with (High Shares of) Renewable Energy (EMPIRE), which is an electricity sector model with a time span of 30 years ending in 2050. The model is two-stage stochastic that includes uncertainty at the operational level and energy economics dynamics at a strategic level. The main contribution of this article is designing the investment-operation DR module within the EMPIRE framework. It models several classes of shiftable and curtailable loads in residential, commercial and industrial sectors, including flexibility periods, operational costs and endogenous DR investments, for 31 European countries. The results show that DR capacity substitutes partially flexible supply side capacity from peak gas plants and battery storage, in addition to enabling more solar PV production.
C61|Analysis of Sustainable Procurement in SMEs in Developing Countries|The purpose of the paper is to integrate supply base consolidation, rationalization, and buyer’s perspective about its suppliers to reveal more insight to implement sustainable procurement in small and medium enterprises (SMEs) in developing countries like India. In this paper an attempt has been made to integrate Constrained Optimization of Frobenius Norm by Genetic Algorithm (COFGA) with traditional spend, and value risk analysis to consolidate and rationalize supply base w.r.t fifteen triple bottom line indicators (TBL). This paper shows that spend analysis is justified in crisp domain and becomes myopic in limited data environment. Spend analysis becomes more ineffective to deal imprecise and vague qualitative data. Integrated approach of multiple criteria decision analysis,spend analysis, and value risk analysis, thus, an alternative approach to give better insight to sustainable procurement in fuzzy environment. Finally, a case study is discussed to use proposed method.
C61|Long-Term Electricity Investments Accounting for Demand and Supply Side Flexibility|Short-term Electricity Demand Response (DR) is an emerging technology in Europe's Electricity markets that will introduce a new degree of exibility. The objective of this work is to analyze to what extent the untapped DR potential can facilitate an optimal transition to an European low emission power system. The beneffits of DR consists of a reduction in peak load consumption, which leads to reduction in capacity investments, production and consumption savings, reduced congestion phases, reliable integration of intermittent renewable resources and supply and demand exibility. The capabilities of DR are studied in the European Model for Power Investment with (High Shares of) Renewable Energy (EMPIRE), which is an electricity sector model with a time span of 30 years ending in 2050. The model is two-stage stochastic that includes uncertainty at the operational level and energy economics dynamics at a strategic level. The main contribution of this article is designing the investment-operation DR module within the EMPIRE framework. It models several classes of shiftable and curtailable loads in residential, commercial and industrial sectors, including exibility periods, operational costs and endogenous DR investments, for 31 European countries. The results show that DR capacity substitutes partially exible supply side capacity from peak gas plants and battery storage, in addition to enabling more solar PV production.
C61|Resolutions to flip-over credit risk and beyond|Abstract Given a risk outcome y over a rating system {R_i }_(i=1)^k for a portfolio, we show in this paper that the maximum likelihood estimates with monotonic constraints, when y is binary (the Bernoulli likelihood) or takes values in the interval 0≤y≤1 (the quasi-Bernoulli likelihood), are each given by the average of the observed outcomes for some consecutive rating indexes. These estimates are in average equal to the sample average risk over the portfolio and coincide with the estimates by least squares with the same monotonic constraints. These results are the exact solution of the corresponding constrained optimization. A non-parametric algorithm for the exact solution is proposed. For the least squares estimates, this algorithm is compared with “pool adjacent violators” algorithm for isotonic regression. The proposed approaches provide a resolution to flip-over credit risk and a tool to determine the fair risk scales over a rating system.
C61|Monotonic Estimation for the Survival Probability over a Risk-Rated Portfolio by Discrete-Time Hazard Rate Models|Monotonic estimation for the survival probability of a loan in a risk-rated portfolio is based on the observation arising, for example, from loan pricing that a loan with a lower credit risk rating is more likely to survive than a loan with a higher credit risk rating, given the same additional risk covariates. Two probit-type discrete-time hazard rate models that generate monotonic survival probabilities are proposed in this paper. The first model calculates the discrete-time hazard rate conditional on systematic risk factors. As for the Cox proportion hazard rate model, the model formulates the discrete-time hazard rate by including a baseline component. This baseline component can be estimated outside the model in the absence of model covariates using the long-run average discrete-time hazard rate. This results in a significant reduction in the number of parameters to be otherwise estimated inside the model. The second model is a general form model where loan level factors can be included. Parameter estimation algorithms are also proposed. The models and algorithms proposed in this paper can be used for loan pricing, stress testing, expected credit loss estimation, and modeling of the probability of default term structure.
C61|Monotonic Estimation for Probability Distribution and Multivariate Risk Scales by Constrained Minimum Generalized Cross-Entropy|Minimum cross-entropy estimation is an extension to the maximum likelihood estimation for multinomial probabilities. Given a probability distribution {r_i }_(i=1)^k, we show in this paper that the monotonic estimates {p_i }_(i=1)^k for the probability distribution by minimum cross-entropy are each given by the simple average of the given distribution values over some consecutive indexes. Results extend to the monotonic estimation for multivariate outcomes by generalized cross-entropy. These estimates are the exact solution for the corresponding constrained optimization and coincide with the monotonic estimates by least squares. A non-parametric algorithm for the exact solution is proposed. The algorithm is compared to the “pool adjacent violators” algorithm in least squares case for the isotonic regression problem. Applications to monotonic estimation of migration matrices and risk scales for multivariate outcomes are discussed.
C61|Saving and dissaving under Ramsey - Rawls criterion|This article studies an inter-temporal optimization problem using a criterion which is a combination between Ramsey and Rawls criteria. A detailed description of the saving behaviour through time is provided. The optimization problem under $\alpha-$\emph{maximin} criterion is also considered with optimal solution characterized.
C61|A simple characterization for sustained growth|This article considers an inter-temporal optimization problem in a fairly general form and give sufficient conditions ensuring the convergence to infinity of the economy. These conditions are easy to verify and can be applied for a large class of problems in literature. As examples, some applications for different economies are also given.
C61|A simple characterization for sustained growth|This article considers an inter-temporal optimization problem in a fairly general form and give sufficient conditions ensuring the convergence to infinity of the economy. These conditions are easy to verify and can be applied for a large class of problems in literature. As examples, some applications for different economies are also given.
C61|A simple characterization for sustained growth|This article considers an inter-temporal optimization problem in a fairly general form and give sufficient conditions ensuring the convergence to infinity of the economy. These conditions are easy to verify and can be applied for a large class of problems in literature. As examples, some applications for different economies are also given.
C61|The calculation of Solvency Capital Requirement using Copulas|Our aim is to present an alternative methodology to the standard formula imposed to the insurance regulation (the European directive knows as Solvency II) for the calculus of the capital requirements. We want to demonstrate how this formula is now obsolete and how is possible to obtain lower capital requirement through the theory of the copulas, function that are gaining increasing importance in various economic areas. A lower capital requirement involves the advantage for the various insurance companies not to have unproductive capital that can therefore be used for the production of further profits. Indeed the standard formula is adequate only with some particular assumptions, otherwise it can overestimate the capital requirements that are actually needed as the standard formula underestimates the effect of diversification.
C61|Passive Control of Spar Type Floating Wind Turbine using Effective Economic Optimal Design Values|This paper studies the performance of a passive linear tuned mass damper on controlling motion of a floating wind turbine. Controlled and uncontrolled analytical models of the spar FWT is established using Newton’s second law and conservation of angular momentum theory. The aerodynamic, hydrodynamic, mooring and buoyancy forces are determined and coupled with the system. For the controlled model, the TMDs are located in different locations and are tuned to different frequency ratios to reduce the motion of the FWT in different directions. The economic optimal design values are considered for the passive controller. The control performance is evaluated by the reduction of root mean square response in each degree of freedom. The results reveals that the linear TMD tuned to the frequency of pitch degree of freedom reduces the translational motion and rotational motion by 5% and 12% respectively. However, tuning the linear tuned mass damper to the frequency of surge degree of freedom provides 8% and 6% motion reduction in translational and rotational degrees of freedom. Also, it has been shown that installing the linear TMD inside the spar is more effective than installing the TMD inside the nacelle.
C61|Human networks and toxic relationships|We devise a theoretical model to shed light on the dynamics leading to the so called toxic relationships. We want to investigate what policy interventions people could advocate to protect themselves and to reduce suffocant assuefaction so to escape to the trap of physical or psychological abuses either in family or at work. By assuming that the toxic partner's behavior is exogenous and that the main source of addiction is income or wealth, and solving a dynamical system of differential equations we find that an asympotically stable equilibrium with positive love is always possibile for enough high level of appealing unless subsides to reduces assuefaction are introduced. Also the existence of a third uncondicionally reciprocating part as a benckmark (which represents not only the real presence of another partner but also the support from family, friends and overall private organizations. These last may help victims of domestic abuses or private organizations by offering economic and psychological support as well as legal counseling to victims of bullying at workplace and placement offices which e ectively help to find soon another job)plays an important role in reducing the toxic partner's appealing. By solving our model we outline the condition for a best mixed policy where both monetary subsides and alternatives are at work
C61|A tale of two Rawlsian criteria|This work considers optimisation problems under Rawls and maximin with multiple discount factors criteria. It proves that though these criteria are different, they have the same optimal value and solution.
C61|Forecasting with many predictors using message passing algorithms|Machine learning methods are becoming increasingly popular in economics, due to the increased availability of large datasets. In this paper I evaluate a recently proposed algorithm called Generalized Approximate Message Passing (GAMP) , which has been very popular in signal processing and compressive sensing. I show how this algorithm can be combined with Bayesian hierarchical shrinkage priors typically used in economic forecasting, resulting in computationally efficient schemes for estimating high-dimensional regression models. Using Monte Carlo simulations I establish that in certain scenarios GAMP can achieve estimation accuracy comparable to traditional Markov chain Monte Carlo methods, at a tiny fraction of the computing time. In a forecasting exercise involving a large set of orthogonal macroeconomic predictors, I show that Bayesian shrinkage estimators based on GAMP perform very well compared to a large set of alternatives.
C61|High-dimensional macroeconomic forecasting using message passing algorithms|This paper proposes two distinct contributions to econometric analysis of large information sets and structural instabilities. First, it treats a regression model with time-varying coeﬃcients, stochastic volatility and exogenous predictors, as an equivalent high-dimensional static regression problem with thousands of covariates. Inference in this speciﬁcation proceeds using Bayesian hierarchical priors that shrink the high-dimensional vector of coeﬃcients either towards zero or time-invariance. Second, it introduces the frameworks of factor graphs and message passing as a means of designing eﬃcient Bayesian estimation algorithms. In particular, a Generalized Approximate Message Passing (GAMP) algorithm is derived that has low algorithmic complexity and is trivially parallelizable. The result is a comprehensive methodology that can be used to estimate time-varying parameter regressions with arbitrarily large number of exogenous predictors. In a forecasting exercise for U.S. price inﬂation this methodology is shown to work very well.
C61|The Role of Electricity in Decarbonizing European Road Transport – Development and Assessment of an Integrated Multi-Sectoral Model|Despite regulation eﬀorts, CO2 emissions from European road transport have continued to rise. Increased use of electricity oﬀers a promising decarbonization option, both to fuel electric vehicles and run power-to-x systems producing synthetic fuels. To understand the economic implications of increased coupling of the road transport and electricity sectors, an integrated multi-sectoral partial-equilibrium investment and dispatch model is developed for the European electricity and road transport sectors, linked by an energy transformation module to endogenously account for, e.g., increasing electricity consumption and ﬂexibility provision from electric vehicles and power-to-x systems. The model is applied to analyze the eﬀects of sector-speciﬁc CO2 reduction targets on the vehicle, electricity and ptx technology mix as well as trade ﬂows of ptx fuels in European countries from 2020 to 2050. The results show that, by 2050, the fuel shares of electricity and ptx fuels in the European road transport sector reach 37% and 27%, respectively, creating an additional electricity demand of 1200 TWh in Europe. To assess the added value of the integrated modeling approach, an additional analysis is performed in which all endogenous ties between sectors are removed. The results show that by decoupling the two sectors, the total system costs may be signiﬁcantly overestimated and the production costs of ptx fuels may be inaccurately approximated, which may aﬀect the merit order of decarbonization options.
C61|How Does Climate Change Affect Optimal Allocation of Variable Renewable Energy?|Ongoing climate change aﬀects complex and long-lived infrastructures like electricity systems. Particularly for decarbonized electricity systems based on variable renewable energies, there is a variety of impact mechanisms working diﬀerently in size and direction. Main impacts for Europe include changes in wind and solar resources, hydro power, cooling water availability for thermoelectric generation and electricity demand. Hence, it is not only important to understand the total eﬀects, i.e., how much welfare may be gained when accounting for climate change impacts in all dimensions, but also to disentangle various eﬀects in terms of their marginal contribution to the potential welfare loss. This paper applies a two-stage modeling framework to assess RCP8.5 climate change impacts on the European electricity system. Thereby, the performance of two electricity system design strategies – one based on no anticipation of climate change and one anticipating impacts of climate change – is studied under a variety of climate change impacts. Impacts on wind and solar resources are found to cause the largest system eﬀects in 2100. Combined climate change impacts increase system costs of a system designed without climate change anticipation due to increased fuel and carbon permit costs. Applying a system design strategy with climate change anticipation increases the cost-optimal share of variable renewable energy based on additional wind oﬀshore capacity in 2100, at a reduction in nuclear, wind onshore and solar PV capacity. Compared to a no anticipation strategy, total system costs are reduced.
C61|The Reformed EU ETS - Intertemporal Emission Trading with Restricted Banking|With the increase of the linear reduction factor (LRF), the implementation of the market stability reserve (MSR) and the introduction of the cancellation mechanism (CM), the EU ETS changed fundamentally. We develop a discrete time model of the intertemporal allowance market that accurately depicts these reforms assuming that prices develop with the Hotelling rule as long as the TNAC is non-empty. A sensitivity analysis ensures the robustness of the model results regarding its input parameters. The accurate modelling of the EU ETS allows for a decomposition of the eﬀects of the individual amendments and the evaluation of the dynamic eﬃciency. The MSR shifts emissions to the future but is allowance preserving. The CM reduces the overall emission cap, increasing allowance prices in the long run, but does not signiﬁcantly impact the emission and price path in the short run. The increased LRF leads with 9 billion cancelled allowances to a stronger reduction than the CM and is therefore the main price driver of the reform.
C61|Interest rate hysteresis in macroeconomic investment under uncertainty|"The interest rate is generally considered as a monetary policy tool and, at the same time, via Tobin's q, as an important driver of macroeconomic investment. As an innovation, this paper derives the exact shape of the ""hysteretic"" impact of changes in the interest rate on macroeconomic investment under the scenarios of certainty and uncertainty. We capture the direct interest rate-hysteresis effects on investment and the capital stock and, explicitly, stochastic changes of the interest rate-investment hysteresis relationship. Starting with hysteresis effects on a microeconomic level of a single firm, we apply an explicit aggregation procedure to derive the interest rate-hysteresis effects on a macroeconomic level. Based on our simple model we are quite skeptical regarding the efficacy of the central bank in providing incentives for macroeconomic investment in times of low or even zero interest rates and high uncertainty. Only if the central bank implements monetary policy strategies such as ""forward guidance"" and is able to credibly commit to low interest rates also for the foreseeable future, our quite strong verdict may be of less relevance."
C61|A Comparison of Marital Matching in First and Second Marriages|Data from the U.S. Census Bureau reveal that in 2013, four-in-ten new marriages included at least one partner who had been married before, and two-in-ten new marriages were between people who had both previously stepped down the aisle. Although, the incidence of remarriage has increased, equilibrium sorting in second marriages has received far less attention in the literature compared to matching patterns in first marriages. Understanding the sorting process is useful since the quality of marital matches influences many economic and social outcomes such as marital stability, fertility, child welfare, income distribution within and across families and labor supply decisions. Furthermore, it is likely that these outcomes may change over time as the maturing process following the first marriage may alter the selection criteria and have a disparate effect on assortative mating patterns in subsequent marriages. Although, a considerable number of studies have examined the factors affecting the time and propensity to remarry, studies on the marital matching process in remarriages are few, not current, have small sample sizes and lack advanced empirical techniques. This paper constructs an empirical model of spouse selection based on Becker?s efficient marriage market hypothesis, in which optimal assignments of marriage partners are derived from maximizing the household output function. By specifying a marital production function and introducing the influence of multiple individual characteristics simultaneously in the matching technology, this paper creates a matching algorithm and uses the estimated parameters to not only assess patterns of assortative mating, but also to isolate factors that drive matching behaviors in first and second marriages.Using a nationally representative multi-cohort longitudinal data of newly admitted legal immigrants and their children to the United States, this paper finds support for Becker?s predictions of positive assortative mating on all observable traits. Results reveal that while the outcome of the process of mate selection is driven almost entirely by the economic assets of the male, his income, in the first marriage, the noneconomic assets of the male, his age, play a bigger role in the second marriage. Although the emphasis on female age increases with the second marriage, female age, a proxy, for reproductive potential, remains the main driver of the selection process. Overall, results indicate that the selection criteria changes more for women than for men with higher order of marriage. Finally, the equilibrium sorting indicates that the incidence of likes marrying likes increases significantly with the second marriage.
C61|An algorithm for construction of a portfolio with a fundamental criterion|The classical models for construction of investment portfolio do not take into account fundamental values of considered companies. In our approach we extend the portfolio choice by adding this dimension to the classical criteria of profitability and risk. It is assumed that an investor selects stock according to their attractiveness, measured by some fundamental values of companies. In this approach portfolios are assessed according to three criteria: their profitability, risk (measured by variance of returns) and fundamental value (measured by some indicators of fundamental value). In this article we consider earnings to price ratio as the measure of the fundamental value of a company. In the paper we consider an algorithm for constructing portfolios with fundamental criterion based on analytical solutions for appropriate optimization problems. In the optimization problem we consider minimizing variance with constrains on expected return and attractiveness of investment, measured with some indicators of fundamental values of companies in a portfolio. We also present empirical examples of calculating effective portfolios of stocks listed on the Warsaw Stock Exchange.
C61|Good Diversification is Never Wasted: How to Tilt Factor Portfolios with Sectors|Using large-cap exchange-traded funds (ETFs), this paper provides guidance onenhancing the performance of long-only factor portfolios through sector-based blending. The blending method builds ETF portfolios that optimize the factor exposure of sectors. We use the original factors of Fama and French asbenchmarks. The results show that blended portfolios combine the diversification benefits of sector investing with the risk premia of factor investing, and so constitute a promising extension of pure factor ETFs.
C61|Asset Dynamics, Liquidity, And Inequality In Decentralized Markets|The Kiyotaki and Wright model has exerted a considerable influence on the monetary search literature. We argue that the model also delivers important insights into a broader range of macroeconomic and development issues. The analysis studies how market frictions and the liquidity of assets affect the distribution of income. Experiments illustrate how the economy adjusts to shocks to asset returns and to the matching technology. They also deal with long‐run transition. An experiment interprets the reversal of fortune hypothesis as a situation in which an economy with a low‐return asset takes over a similar economy with a high‐return asset. (JEL C61, C63, E41, E27, D63)
C61|Production efficiency of nodal and zonal pricing in imperfectly competitive electricity markets|Electricity markets employ different congestion management methods to handle the limited transmission capacity of the power system. This paper compares production efficiency and other aspects of nodal and zonal pricing. We consider two types of zonal pricing: zonal pricing with Available Transmission Capacity (ATC) and zonal pricing with Flow-Based Market Coupling (FBMC).We develop a mathematical model to study the imperfect competition under zonal pricing with FBMC. Zonal pricing with FBMC is employed in two stages, a day-ahead market stage and a re-dispatch stage. We show that the optimality conditions and market clearing conditions can be reformulated as a mixed integer linear program (MILP), which is straightforward to implement. Zonal pricing with ATC and nodal pricing is used as our benchmarks. The imperfect competition under zonal pricing with ATC and nodal pricing are also formulated as MILP models. All MILP models are demonstrated on 6-node and the modified IEEE 24-node systems. Our numerical results show that the zonal pricing with ATC results in large production inefficiencies due to the incdec-game. Improving the representation of the transmission network as in the zonal pricing with FBMC mitigates the inc-dec game.
C61|Social Cost of Carbon under stochastic tipping points: when does risk play a role?|Carbon dioxide emissions impose a social cost on economies, owing to the damages they will cause in the future. In particular, emissions increase global temperature that may reach tipping points in the climate or economic system, triggering large economic shocks. Tipping points are uncertain by nature, they induce higher expected damages but also dispersion of possible damages, that is risk. Both dimensions increase the Social Cost of Carbon (SCC). However, the respective contributions of higher expected damages and risk have not been disentangled. We develop a simple method to compare how much expected damages explain the SCC, compared to the risk induced by a stochastic tipping point. We find that expected damages account for more than 90% of the SCC with productivity shocks lower than 10%, the high end of the range of damages commonly assumed in Integrated Assessment Models. It takes both high productivity shock and high risk aversion for risk to have a significant effect. Our results also shed light on the observation that risk aversion plays a modest role in determining the SCC (the ''risk aversion puzzle''): they suggest that too low levels of damages considered in previous studies could be responsible for the low influence of risk aversion.
C61|Multiscale Volatility Transmission and Portfolio Construction Between the Baltic Stock Markets|This paper investigates volatility transmission and portfolio construction between the three Baltic stock indices at different time-horizons. Methodologies used for this study encompass parametric EGARCH model and the three non-parametric approaches – wavelet coherence, wavelet correlation and phase difference. Wavelet coherence indicated that risk integration between the Baltic stock markets is not so strong, while wavelet correlations confirmed this contention more precisely. Additional analysis showed that low wavelet correlations are also present between the Baltic indices and the German DAX index. These findings may suggest that the selected indices could be useful for the construction of risk-minimizing portfolios. In order to confirm (discard) this assumption, we constructed wavelet-based two-asset portfolios. The results provided evidence that hedging opportunities exist when the Baltic indices are combined between themselves, but also when they are coupled with the DAX index. This is particularly true for the longer time-horizons.
C61|Optimal Stopping Time, Consumption, Labour, and Portfolio Decision for a Pension Scheme| In this work we solve in a closed form the problem of an agent who wants to optimise the inter-temporal utility of both his consumption and leisure by choosing: (i) the optimal inter-temporal consumption, (ii) the optimal inter-temporal labour supply, (iii) the optimal share of wealth to invest in a risky asset, and (iv) the optimal retirement age. The wage of the agent is assumed to be stochastic and correlated with the risky asset on the financial market. The problem is split into two sub-problems: the optimal consumption, labour, and portfolio problem is solved first, and then the optimal stopping time is approached. The martingale method is used for the first problem, and it allows to solve it for any value of the stopping time which is just considered as a stochastic variable. The problem of the agent is solved by assuming that after retirement he received a utility that is proportional to the remaining human capital. Finally, a numerical simulation is presented for showing the behaviour over time of the optimal solution.
C61|Goods-Market Frictions and International Trade|We add goods-market frictions to a general equilibrium dynamic model with heterogeneous exporting producers and identical importing retailers. Our tractable framework leads to endogenously unmatched producers, which attenuate welfare responses to foreign shocks but increase the trade elasticity relative to a model without search costs. Search frictions are quantitatively important in our calibration, attenuating welfare responses to tariffs by 40 percent and increasing the trade elasticity by 50 percent. Eliminating search costs raises welfare by 1 percent and increasing them by only a few dollars has the same effects on welfare and trade flows as a 10 percent tariff.
C61|Risk Management for Sovereign Debt Financing with Sustainability Conditions|We develop a model of debt sustainability analysis with optimal financing decisions in the presence of macroeconomic, financial and fiscal uncertainty. We define a coherent measure of refinancing risk, and trade off the risks of debt stock and flow dynamics, subject to debt sustainability constraints and endogenous risk and term premia. We optimize both static and dynamic financing strategies, compare them with several simple rules and consol financing to demonstrate economically significant effects of optimal financing, and show that the stock-flow tradeoff can be critical for sustainability. We quantify the minimum refinancing risk and the maximum rate of debt reduction that a sovereign can achieve given its economic fundamentals, and extend the model to identify optimal timing for debt flow adjustments that allow the sovereign to go beyond these limits. We put the model to the data on three real-world cases: a representative euro zone crisis country, a low-debt country (Netherlands) and a high-debt country (Italy). These applications illustrate the use of the model in informing diverse policy decisions on sustainable public finance. The model is part of the European Stability Mechanism toolkit to assess debt sustainability and repayment capacity of member states in the context of financial assistance.
C61|Housing Choices and Their Implications for Consumption Heterogeneity|No abstract is available for this item.
C61|Efficient Computation with Taste Shocks|Taste shocks result in nondegenerate choice probabilities, smooth policy functions, continuous demand correspondences, and reduced computational errors. They also cause significant computational cost when the number of choices is large. However, I show that, in many economic models, a numerically equivalent approximation may be obtained extremely efficiently. If the objective function has increasing differences (a condition closely tied to policy function monotonicity) or is concave in a discrete sense, the proposed algorithms are O(n log n) for n states and n choice--a drastic improvement over the naive algorithm's O(n2) cost. If both hold, the cost can be further reduced to O(n). Additionally, with increasing differences in two state variables, I propose an algorithm that in some cases is O(n2) even without concavity (in contrast to the O(n3) naive algorithm). I illustrate the usefulness of the proposed approach in an incomplete markets economy and a long-term sovereign debt model, the latter requiring taste shocks for convergence. For grid sizes of 500 points, the algorithms are up to 200 times faster than the naive approach.
C61|Industry Competitiveness Indicators|It can be argued that the competitiveness of an industry consists of two main parts: The production conditions and the utilization of these. The production conditions are largely determined by factors exogenous to the firms comprising the industry, including the economic environment, regulatory framework, etc. The utilization of the production conditions corresponds to the classic economic notion of structural efficiency. We here argue that it is crucial for policy analysis to be able to quantify each of these two aspects separately, since the production conditions are partly in the hands of the policy makers, whereas the utilization is mainly the responsibility of firm management. In this paper we define two new bilateral indicators; the Bilateral Industry Utilization (BIU) indicator, and the Bilateral Production Conditions (BPC) indicator. These are applied to a large data set of dairy farms across 19 European countries provided by the Farm Accountancy Data Network (FADN). With focus on the competitiveness of Danish dairy farms we show that dairy farms in most other countries have significantly better production conditions than those in Denmark while Sweden is the only country with significantly better utilization. Finally, we asses potential causes behind the differences and discuss possible remedies.
C61|Exact tests on returns to scale and comparisons of production frontiers in nonparametric models|When benchmarking production units by non-parametric methods like data envelopment analysis (DEA), an assumption has to be made about the returns to scale of the underlying technology. Moreover, it is often also relevant to compare the frontiers across samples of producers. Until now, no exact tests for examining returns to scale assumptions in DEA, or for test of equality of frontiers, have been available. The few existing tests are based on asymptotic theory relying on large sample sizes, whereas situations with relatively small samples are often encountered in practical applications. In this paper we propose three novel tests based on permutations. The tests are easily implementable from the algorithms provided, and give exact significance probabilities as they are not based on asymptotic properties. The first of the proposed tests is a test for the hypothesis of constant returns to scale in DEA. The others are tests for general frontier differences and whether the production possibility sets are, in fact, nested. The theoretical advantages of permutation tests are that they are appropriate for small samples and have the correct size. Simulation studies show that the proposed tests do, indeed, have the correct size and furthermore higher power than the existing alternative tests based on asymptotic theory.
C61|Benchmarking with uncertain data: a simulation study comparing alternative methods|We consider efficiency measurement methods in the presence of uncertain input and output data, and without the (empirically problematic) assumption of convexity of the production technology. In particular, we perform a simulation study in order to contrast two well-established methods, IDEA and Fuzzy DEA, with a recently suggested extension of Fuzzy DEA in the literature (dubbed the HB method). We demonstrate that the HB method has important advantages over the conventional methods, resulting in more accurate efficiency estimates and narrower bounds for the efficiency scores of individual Decision Making Units (DMUs): thereby providing more informative results that may lead to more effective decisions. The price is computational complexity. Although we show how to significantly speed up computational time compared to the original suggestion, the HB method remains the most computationally heavy method among those considered. This may limit the use of the method in cases where efficiency estimates have to be computed on the fly, as in interactive decision support systems based on large data sets.
C61|Firm Decisions under Jump-Diffusive Dynamics|We present a model of firm investment under uncertainty and partial irreversibility in which uncertainty is represented by a jump diffusion. This allows to represent both the continuous Gaussian volatility and the discontinuous uncertainty related to information arrival, sudden changes and large shocks. The model shows how both sources of uncertainty negatively impact the optimal investment and disinvestment policies, and how the presence oflarge negative jumps can drastically affect the firm’s ability to recover. Our results show that the standard Gaussian framework consistently underestimates the negative effect of uncertainty on firm investment decisions. We test these predictions on a panel dataset of UK firms: we first structurally estimate the uncertainty parameters using multinomial maximum likelihood and differential evolution techniques and subsequently study their impact on firm investment rates, validating our model predictions.
C61|Local bifurcations of three and four-dimensional systems: A tractable characterization with economic applications|We provide necessary and sufficient conditions to detect local bifurcations of three and four-dimensional dynamical systems in continuous time. We characterize not only the bifurcations of codimension one but also those of codimension two. For the sake of completeness, we give also the non-degeneracy conditions for each bifurcation. The added value of our methodology rests on its generality. To illustrate the tractability of our approach, we provide two analytical applications of dimension three and four to environmental economics, complemented with numerical simulations.
C61|Distributed Optimal Control Models in Environmental Economics: A Review|We review the most recent advances in distributed optimal control applied to environmental economics, covering in particular problems where the state dynamics are governed by partial differential equations (PDEs). This is a quite fresh application area of distributed optimal control, which has already suggested several new mathematical research lines due to the specificities of the environmental economics problems involved. We enhance the latter through a survey of the variety of themes and associated mathematical structures beared by this literature. We also provide a quick tour of the existing tools in the theory of distributed optimal control that have been applied so far in environmental economics
C61|Geographic environmental Kuznets curves: The optimal growth linear-quadratic case|We solve a linear-quadratic model of a spatio-temporal economy using a polluting one-input technology. Space is continuous and heterogenous: locations di er in productivity, nature self-cleaning technology and environmental awareness. The unique link between locations is transboundary pollution which is modelled as a PDE di usion equation. The spatio-temporal functional is quadratic in local consumption and linear in pollution. Using a dynamic programming method adapted to our in nite dimensional setting, we solve the associated optimal pollution. We show that optimal emissions will decrease at given location if and only if local productivity is larger than a threshold which depends both on the local pollution absorption capacity and environmental awareness. Furthermore, we numerically explore the relationship between the spatial optimal distributions of production and (asymptotic) pollution in order to uncover possible (geographic) Environmental Kuznets Curve cases.
C61|Just-noticeable difference as a behavioural foundation of the critical cost-efficiency index|Critical cost-efficiency index (or CCEI), proposed in Afriat (1973) and Varian (1990), is one of the most commonly used measures of departures from rationality. We show that this index is equivalent to a particular notion of the just-noticeable difference, that is, a measure of dissimilarity between alternatives that is sufficient for the agent to tell them apart. Therefore, we show that CCEI evaluates the consumer's cognitive inability to discriminate among options.
C61|Supermodular correspondences and comparison of multi-prior beliefs|Economic decisions often involve maximising an objective whose value is itself the outcome of another optimisation problem. This decision structure arises in multi-output production and choice under uncertainty with multi-prior beliefs. To analyse comparative statics in these models, we introduce a theory of Supermodular correspondences. In particular, we employ this theory to generalise the notion of first order stochastic dominance to multi-prior beliefs, allowing us to characterise conditions under which greater optimism leads to higher action.
C61|Demographic Developments and Their Macroeconomic Impacts|This study firstly introduces demographic concepts and reveals the different dynamics displayed by emerging and advanced economies. Then, the links between demographic developments and economic activity are discussed. We aim to support our arguments with quantitative findings obtained through a New-Keynesian general equilibrium model. The general equilibrium model, which is calibrated for Turkey, is simulated for the period until 2050 in accordance with changes in demographic exogenous variables. Through this simulation, the path that main macroeconomic variables follow is analyzed. The simulation exercise, conducted through the model which is de-trended with the increases in productivity and population, reveals that, compared to the initial steady state, production factor prices significantly change; real interest rate declines and real wage increases.
C61|Policy Distortions and Aggregate Productivity with Endogenous Establishment-Level Productivity|The large differences in income per capita across countries are mostly accounted for by differences in total factor productivity (TFP). What explains these differences in TFP across countries? Evidence suggests that the (mis)allocation of factors of production across heterogenous production units is an important factor. We study factor misallocation in a model with an endogenously determined distribution of establishment-level productivity. In this framework, policy distortions not only misallocate resources across a given set of productive units, but they also worsen the distribution of establishment-level productivity. We show that in our model, compared to the model with an exogenous distribution, the quantitative effect of policy distortions is substantially amplified. Whereas empirically-plausible policy distortions in our model generate TFP that is 14 percent that of a benchmark economy with no distortions, with an exogenous distribution the same policy distortions generate TFP that is 86 percent of the benchmark, a 6-fold amplification factor.
C61|Imposing equilibrium restrictions in the estimation of dynamic discrete games|Imposing equilibrium restrictions provides substantial gains in the estimation of dynamic discrete games. Estimation algorithms imposing these restrictions -- MPEC, NFXP, NPL, and variations -- have different merits and limitations. MPEC guarantees local convergence, but requires the computation of high-dimensional Jacobians. The NPL algorithm avoids the computation of these matrices, but -- in games -- may fail to converge to the consistent NPL estimator. We study the asymptotic properties of the NPL algorithm treating the iterative procedure as performed in finite samples. We find that there are always samples for which the algorithm fails to converge, and this introduces a selection bias. We also propose a spectral algorithm to compute the NPL estimator. This algorithm satisfies local convergence and avoids the computation of Jacobian matrices. We present simulation evidence illustrating our theoretical results and the good properties of the spectral algorithm.
C61|A dynamic model of effort choice in high school|I estimate a dynamic model of educational decisions that allows for observed and unobserved differences in initial ability. Each year students choose their level of effort by deciding over the academic level of their study program and the likelihood of end-of-year performance. Good performance is costly, but necessary to continue in the program. This replaces traditional approaches, which assume performance follows an exogenous law of motion. I use the model to investigate high school tracking policies and obtain the following results: (1) encouraging underperforming students to switch to less academic programs substantially reduces grade retention and dropout, (2) the resulting decrease in the number of college graduates is small and insignicant, and (3) a model that assumes performance is exogenous ignores a change in unobserved study effort, leading to large biases and falsely concluding there would be an important negative impact on graduation rates in higher education.
C61|Representation formulas for limit values of long run stochastic optimal controls|No abstract is available for this item.
C61|Static use of options in dynamic portfolio optimization under transaction costs and solvency constraints|We study a dynamic portfolio optimization problem where it is possible to invest in a risk-free bond, in a risky stock modeled by a lognormal diffusion and in call options written on the stock. The use of the options is limited to static strategies at the beginning of the investment period. The investor faces transaction costs with a fixed component and solvency constraints and the objective is to maximize the expected utility of the final wealth. We characterize the value function as a constrained viscosity solution of the associated quasi-variational inequality and we prove the local uniform convergence of a Markov chain approximation scheme to compute numerically the optimal solution. Because of transaction costs and solvency constraints the options cannot be pefectly replicated and despite the restriction to static policies our numerical results show that in most cases the investor will keep a significant part of his portfolio invested in options.
C61|Improving the representativeness of a simple random sample: an optimization model and its application to the Continuous Sample of Working Lives|This paper develops an optimization model for selecting a large subsample that improves the representativeness of a simple random sample previously obtained from a population larger than the population of interest. The problem formulation involves convex mixed-integer nonlinear programming (convex MINLP) and is therefore NP-hard. However, the solution is found by maximizing the “constant of proportionality” – in other words, maximizing the size of the subsample taken from a stratified random sample with proportional allocation – and restricting it to a p-value high enough to achieve a good fit to the population of interest using Pearson’s chi-square goodness-of-fit test. The beauty of the model is that it gives the user the freedom to choose between a larger subsample with a poorer fit and a smaller subsample with a better fit. The paper also applies the model to a real case: The Continuous Sample of Working Lives (CSWL), which is a set of anonymized microdata containing information on individuals from Spanish Social Security records. Several waves (2005-2017) are first examined without using the model and the conclusion is that they are not representative of the target population, which in this case is people receiving a pension income. The model is then applied and the results prove that it is possible to obtain a large dataset from the CSWL that (far) better represents the pensioner population for each of the waves analysed.
C61|Wheels and cycles: (sub)optimality and volatility of corrupted economies|"We consider a simple economy where production depends on labor supply and social capital. Networking increases the social capital (""greases the wheel"") but also the corruption level (""sands the wheel""). Corruption is a negative productive externality. We compare the market economy, where the negative externality is not taken in account by individuals, with a centralized economy, where the planner internalizes the negative effect. We highlight the possible existence of cycles in the market economy and optimal cycles in the planned one. We compare the centralized and the decentralized solutions in the short and in the long run."
C61|Persistent Exploitation with Intertemporal Reproducible Solution in Pre-industrial Economies|This paper presents an intertemporal model of pre-industrial economies defined with leisure preference to study the condition of the emergence and persistence of exploitation as unequal exchange of labor. We show that pure workers are exploited in any finite periods if there is positive real profit rate, even though labor allocation among agents tends to be equalized in the limit regardless of the saving behaviors. The so-called Fundamental Marxian Theorem and Profit-Exploitation Correspondence Principle are generalized in the intertemporal setting with exploitation in the whole life, and the Class-Exploitation Correspondence Principle is established with exploitation within period.
C61|Investment in farming under uncertainty and decoupled support: a real options approach|Under the current version of the Common Agricultural Policy (CAP), payments to EU farmers are decoupled from the production of agricultural commodities. In fact, farmers qualify for CAP support as soon as their land is maintained in good agricultural and environmental condition. In this paper, we study how decoupled payments influence the decision to invest in farming. We show that decoupling is implicitly providing a costless hedge against volatile farming profits. Consequently, a higher decoupled payment leads the potential farmer to hasten its investment but also results in a farm with lower productive capacity.
C61|Production Externalities and Investment Caps: a Welfare Analysis under Uncertainty|In markets where production has adverse externalities, policy makers may wish to increase welfare by imposing a cap on market entries. In this paper, we examine the implications that the cap has on the firms’ investment equilibrium policy and on social welfare in the presence of market uncertainty. In contrast with previous literature, we explicitly model the present externality and then let the social planner choose the cap level maximizing welfare. We find that: i) if the consideration of the option value triggers investment at price above the social marginal cost of production, then it is optimal to have no cap at all; ii) otherwise, the cap should be set on the current market quantity and a ban on further market entries should be announced.
C61|Optimal investment with vintage capital: equilibrium distributions|The paper concerns the study of equilibrium points, or steady states, of economic systems arising in modelling optimal investment with vintage capital, namely, systems where all key variables (capitals, investments, prices) are indexed not only by time τ but also by age s. Capital accumulation is hence described as a partial differential equation (briefly, PDE), and equilibrium points are in fact equilibrium distributions in the variable s of ages. Investments in frontier as well as non-frontier vintages are possible. Firstly a general method is developed to compute and study equilibrium points of a wide range of infinite dimensional, infinite horizon boundary control problems for linear PDEs with convex criterion, possibly applying to a wide variety of economic problems. Sufficient and necessary conditions for existence of equilibrium points are derived in this general context. In particular, for optimal investment with vintage capital, existence and uniqueness of a long run equilibrium distribution is proved for general concave revenues and convex investment costs, and analytic formulas are obtained for optimal controls and trajectories in the long run, definitely showing how effective the theoretical machinery of optimal control in infinite dimension is in computing explicitly equilibrium distributions, and suggesting that the same method can be applied in examples yielding the same abstract structure. To this extent, the results of this work constitutes a first crucial step towards a thorough understanding of the behaviour of optimal controls and trajectories in the long run.
C61|Robustness of Support Vector Machines in Algorithmic Trading on Cryptocurrency Market|This study investigates the profitability of a algorithmic trading strategy based on training SVM model to identify cryptocurrencies with high or low predicted returns. A tail set is defined to be a group of coins whose volatility-adjusted returns are in the highest or lowest quantile. Each cryptocurrency is represented by a set of six technical features. SVM is trained on historical tail sets and tested on the current data. The classifier is chosen to be a nonlinear support vector machine. Portfolio is formed by ranking coins using SVM output. The highest ranked coins are used for long positions to be included in the portfolio for one reallocation period. The following metrics were used to estimate the portfolio profitability: %ARC (the annualized rate of change), %ASD (the annualized standard deviation of daily returns), MDD (the maximum drawdown coefficient), IR1, IR2 (the information ratio coefficients). The performance of the SVM portfolio is compared to the performance of the four benchmark strategies based on the values of the information ratio coefficient IR1 which quantifies the risk-weighted gain. The question on how sensitive the portfolio performance is to the parameters set in the SVM model is also addressed in this study.
C61|Does the inclusion of exposure to volatility into diversified portfolio improve the investment results? Portfolio construction from the perspective of a Polish investor|The main goal of this research is to analyse the investment benefits from an incorporation of the volatility exposure to the diversified portfolio from the perspective of a Polish investor. Volatility, treated as a new asset class, may improve the performance of the portfolio due to its negative correlation with most types of assets. This topic has been widely investigated for the United States and Europe whereas Polish market appears to be not heavily researched and this study may fill this gap. The research covers the period from October 2010 to July 2018 and is performed on the daily close prices. To construct the portfolios, the analysis uses the mean-variance framework and the naïve diversification approach. The comparison of risk-adjusted returns between investments with and without volatility exposure enables to answer the research question about an improvement of the results by the addition of a non-standard asset to the diversified portfolios. The VXX is considered as the proxy for volatility as it is the most popular ETN which follows the volatility index derivatives with the given maturity. To test the robustness of the results, the portfolios are constructed with a broad range of different parameters and assumptions imposed on the optimization procedure.
C61|Hybrid Investment Strategy Based on Momentum and Macroeconomic Approach|The purpose of this research is to test the potential returns and robustness of an automated investment strategy. The strategy is based on momentum and macroeconomic approach, that consists of the technical core – momentum, and the additional macro screening, which is used to determine whether investment signals generate relevant investment opportunities or just technical noise. In order to check whether the macroeconomic factor is the value added to the momentum strategy, the hybrid approach is tested and compared with the simple momentum and the macroeconomic strategy alone and then assessed on a risk-adjusted return basis. The main aim of this paper is to answer the question, whether an investor can gain surplus risk-adjusted returns from merging short-term momentum strategy with the long-term macroeconomic approach. Strategies are based on the data for the selected companies from the S&P500 index in the period ranging from 02/01/1990 to 31/12/2018.
C61|Cost-Effective Clinical Trial Design: Application of a Bayesian Sequential Stopping Rule to the ProFHER Pragmatic Trial|We investigate value-based clinical trial design by applying a Bayesian decisiontheoretic model of a sequential experiment to data from the ProFHER pragmatic trial. In the first applied analysis of its kind to use research cost data, we show that the model’s stopping policy would have stopped the trial early, saving about 5% of the research budget (approximately £73,000). A bootstrap analysis based on generating resampled paths from the trial data suggests that the trial’s expected sample size could have been reduced by approximately 40%, saving an expected 15% of the budget, with 93% of resampled paths making a decision consistent with the result of the trial itself. Results show how substantial benefits to trial cost stewardship may be achieved by accounting for research costs in defining the trial’s stopping policy and active monitoring of trial data as it accumulates.
C61|Efficient Sequential Assignments with Randomly Arriving Multi-Item Demand Agents|A seller has several heterogeneous indivisible items like tickets to sell over time before a deadline. These items become worthless after the deadline. Buyers arrive sequentially and randomly and have their own private valuations over items. Each buyer may acquire more than one item. We formulate this as an incentive compatible revenue maximization problem and characterize optimal allocation policies and derive various properties.
C61|Interest Rate Hysteresis in Macroeconomic Investment under Uncertainty|The interest rate is generally considered as an important driver of macroeconomic investment. As an innovation, this paper derives the exact shape of the “hysteretic” impact of changes in the interest rate on macroeconomic investment under the scenarios of both certainty and uncertainty. We capture the direct interest rate-hysteresis on the investments and the capital stock and, explicitly, of stochastic changes on the interest rate-investment hysteresis. Starting with hysteresis effects on a microeconomic level of a single firm, we apply an explicit aggregation procedure to derive the interest rate hysteresis effects on a macroeconomic level. Based on our simple model we are able to obtain some conclusions about the efficacy of a central bank’s interest rate policy, e.g. in times of low or even zero interest rates and high uncertainty, in terms of stimulating macroeconomic investment.
C61|Nuclear Decommissioning after the German Nuclear Phase-Out: An Integrated View on New Regulations and Nuclear Logistics|With Germany’s nuclear phase-out, 23 reactors need to be dismantled in the near future. Initiated by the dire financial situation of the affected utilities in 2014, a major discourse on ensuring financial liability led to a redistribution of liabilities and finances, with the utilities remaining in charge of dismantling, while liability for interim and final storage now transferred to the public. This paper assesses whether the new regulation will ultimately be to the benefit of the public. It introduces a two-stage stochastic optimization framework which encompasses the different dismantling phases and resulting waste flows and storage levels of low- and intermediate-level waste (LLW and ILW) as well as the associated costs. Results show that storage risk – proclaimed as a major barrier to efficient decommissioning – is not a major driver for the optimal decommissioning schedule. However, a delay of ten years might now increase interim storage costs borne by the public by over 20%. By contrast, lacking knowledge and limited machinery is a major unaccounted cost driver, which might quickly eat-up the buffer currently included in utility funds in order to deal with dismantling uncertainties. Our analysis reveals the storage gate as the new crucial interface between utilities and the public storage provider.
C61|Global Futures of Energy, Climate, and Policy: Qualitative and Quantitative Foresight towards 2055|Existing long-term energy and climate scenarios are typically a rather simple extrapolation of past trends. Both qualitative and quantitative outlooks co-exist, but they often focus narrowly on individual perspectives, which is opposed to the interlinked and complex nature of energy and climate. Therefore, this study presents a set of novel and multidisciplinary narratives that give insight into four distinct and extreme yet plausible worlds: base case ‘Business-as-usual’, worst case ‘Survival of the Fittest’, best case ‘Green Cooperation’, and surprise scenario ‘ClimateTech’. Going beyond other outlooks, our narratives focus on changes in the geopolitical landscape and global order, social perspectives on climate issues, and technological progress. These holistic scenarios are designed to overcome previous barriers by an innovative bridging between both qualitative and quantitative methods. We start with the generation of qualitative scenario storylines using techniques of foresight analysis, including a facilitated expert workshop. Then, we calibrate the numerical energy systems model Multimod to reflect the different storylines. Finally, we unite and refine storylines and numerical model results into holistic narratives. In addition to the narratives (which include quantitative results on e.g. emissions, energy consumption, and the electricity mix), the study generates insights on the key uncertainties and drivers of different pathways of (more or less successful) climate change mitigation. Additionally, a set of transparent indicators serves as an early-warning system to identify which of the paths the world might enter. Lessons learnt include the dangers from increased isolationism and the importance of integrating economic and energy-related objectives as well as the large role of public opinion and social transition.
C61|Internal Habit Formation and Optimality|Carroll et al. [7] establish that in a model with internal habits, an increase in economic growth may cause a positive change in savings. The optimality of this result has been recently questioned by several contributions in the literature which have observed that the parametrization used in [7] implies a utility function not jointly concave in consumption and habits. In this short paper, we revisit this issue: firstly we explain that it can be solved only through advanced techniques in Dynamic Programming and then we prove, using them, how the candidate optimal control found in [7] is indeed the unique optimal control
C61|Optimal Sharing Rule for a Household with a Portfolio Management Problem|We study an intra-household decision process in the Merton financial portfolio problem.This writes as an optimal consumption-investment problem in finite horizon for the case of two separate consumption streams and a shared final wealth, in a linear social welfare setting. We show that the aggregate problem for multiple agents can be linearly separated in multiple optimal single agent problems given an optimal sharing rule of the initial endowment. Consequently, an explicit closed form solution is obtained for each subproblem, and for the household as a whole. We show the impact of asymmetric risk aversion and market price of risk on the sharing rule in a specified setting with mean-reverting price of risk, with numerical illustration.
C61|Optimization of age-structured bioeconomic model: recruitment, weight gain and environmental effects|More and more fishery researchers begin to acknowledge that one-dimensional biomass models may omit key information when generating management guidelines. For the more complicated age-structured models, numerous parameters require a proper estimation or a reasonable assumption. In this paper, the effects of recruitment patterns and environmental impacts on the optimal exploitation of a fish population are investigated. Based on a discrete-time age-structured bioeconomic model of Northeast Atlantic mackerel, we introduce the mechanisms that generate 6 scenarios of the problem. Using the simplest scenario, optimizations are conducted under 8 different parameter combinations. Then, the problem is solved for each scenario and simulations are conducted with constant fishing mortalities. It is found that a higher environmental volatility leads to more net profits but with a lower probability of achieving the mean values. Any parameter combination that favours the older fish tends to lend itself to pulse fishing pattern. The simulations indicate that a constant fishing mortality around 0.06 performs the best. A comparison between the optimal and the historical harvest shows that for more than 70% of the time, the optimal exploitation precedes the historical one, leading to 43% higher net profit and 34% lower fishing cost.
C61|Greed is good: from super-harvest to recovery in a stochastic predator-prey system|This paper demonstrates a predator-prey system of cod and capelin that confronts a possible scenario of prey extinction under the first-best policy in a stochastic world. We discover a novel ‘super-harvest’ phenomenon that the optimal harvest of the predator is even higher than the myopic policy, or the ‘greedy solution’, on part of the state space. This intrinsic attempt to harvest more predator to protect the prey is a critical evidence supporting the idea behind ‘greed is good’. We ban prey harvest and increase predator harvest in a designated state space area based on the optimal policy. Three heuristic recovery plans are generated following this principle. We employ stochastic simulations to analyse the probability of prey recovery and evaluate corresponding costs in terms of value loss percentage. We find that the alternative policies enhance prey recovery rates mostly around the area of 50% recovery probability under the optimal policy. When we scale up the predator harvest by 1.5, the prey recovery rate escalates for as much as 28% at a cost of 5% value loss. We establish two strategies: modest deviation from the optimal on a large area or intense measure on a small area. It seems more cost-effective to target the stock space with accuracy than to simply boost predator harvest when the aim is to achieve remarkable improvement of prey recovery probability.
C61|Markets With Memory: Dynamic Channel Optimization Models With Price-Dependent Stochastic Demand|Almost every vendor faces uncertain and time-varying demand. Inventory level and price optimization while catering to stochastic demand are conventionally formulated as variants of newsvendor problem. Despite its ubiquity in potential applications, the time-dependent (multi-period) newsvendor problem in its general form has received limited attention in the literature due to its complexity and the highly nested structure of its ensuing optimization problems. The complexity level rises even more when there are more than one decision maker in a supply channel, trying to reach an equilibrium. The purpose of this paper is to construct an explicit and e cient solution procedure for multi-period price-setting newsvendor problems in a Stackelberg framework. In particular, we show that our recursive solution algorithm can be applied to standard contracts such as buy back contracts, revenue sharing contracts, and their generalizations.
C61|Explicit Solution Algorithms for Order and Price Postponement in Multi-periodic Channel Optimization|Supply channels typically face uncertain and time-varying demand. Nonetheless, time-dependent channel optimization while addressing uncertain demand has received limited attention due to the high level of complexity of the ensuing nested equilibrium problems. The level of complexity rises when demand is dependent on current and previous prices. We consider a decentralized supply channel whose two members, a manufacturer and a retailer, must address the demand for a perishable commodity within a multi-period time horizon. Using a general (additive-multiplicative) stochastic model for the price-dependent demand, the purpose of this paper is to provide the channel members with analytic tools to devise optimal pricing and supply strategies at different times. In the first part of the paper, we propose a constructive theorem providing an explicit solution algorithm to obtain equilibrium states for bilevel optimization in decentralized supply channels. We also prove that the resulting equilibria are subgame perfect. In the second part, we allow the retailer to postpone her supply and pricing decisions until demand uncertainty is resolved at each period. Using subgame perfectness of the equilibria, we propose solution algorithms that use the extra information obtained by postponement. Finally, in a number of comparison theorems, we show that postponement strategies are always beneficial for a centralized channel (whose revenue structure is identical to that of a retailer). Whereas for a decentralized channel, due to vertical competitions, there may be scenarios wherein postponement strategies, i.e. access to extra information, turn out to be detrimental to the manufacturer and even to the whole channel.
C61|Solution Algorithms for Optimal Buy-Back Contracts in Multi-period Channel Equilibria with Stochastic Demand and Delayed Information|We analyze the problem of time-dependent channel coordination in the face of uncertain demand. The channel, composed of a manufacturer and a retailer, is to address a time-varying and uncertain price-dependent demand. The decision variables of the manufacturer are wholesale and (possibly zero) buy-back prices, and those of the retailer are order quantity and retail price. Moreover, at each period, the retailer is allowed to postpone her retail price until demand uncertainty is resolved. In order to place emphasis on the price-decadent nature of demand, we embed a class of memory effects in demand structure, such that current demand at each period demand is affected by pricing history as well as current price. The ensuing equilibria problems, thus, become highly nested in time. We then propose our memory-based solution algorithm which coordinates the channel with optimal buy-back contracts at each period. We show that, contrary to the conventional belief, too generous buy-back prices may not only be suboptimal to the manufacturer, but also decrease the expected profit for the retailer and thus for the whole channel.
C61|Banking, Capital Regulation, Risk and Dynamics|Effects from risk, bankruptcies, and capital regulation of banks is explored in a dynamic stochastic equilibrium model where banks have two controls, dividends and level of risktaking. Unregulated value-maximizing banks, balance current profit against cost of risk. Banks with capitalization below desired level chose a lower level of risk than well-capitalized banks, but their capital adequacy ratios are yet lower. Binding regulation reduces risk-taking and instantaneous risk of bankruptcy but in the process also reduce endogenous growth of bank capital. This leads to an increased risk of bankruptcy that stems from the longer time banks spend poorly capitalized after large negative shocks due to the capital regulation.
C61|FIDELIO 3 manual: Equations and data sources|"FIDELIO (Fully Interregional Dynamic Econometric Long-term Input-Output) is a multi-sectoral model developed by the unit B.5 of the Directorate General Joint Research Centre (JRC) â€” the circular economy and industrial leadership unit. Compared to neoclassical CGE models â€” which assume that the perfect flexibility of prices and quantities ensures the full use of the factors of production at all times â€” FIDELIO integrates some new-Keynesian features: consumption adjusts slowly to its optimal level according to an error correction model and wages do not clear the labour market. The assumptions that prices do not clear the markets and market ""imperfections"" exist generate the dynamics of the model that is solved sequentially (recursive dynamic). In addition, FIDELIO is an econometric model since the calibration of most of the behavioural parameters of the model (dynamic adjustment lags of prices and quantities, and elasticities) is based on econometric estimations.This technical report illustrates the third version of the FIDELIO model, FIDELIO 3. The changes introduced in the subsequent versions of the model have two main objectives. The first one is to increase the coverage of the model. The second one is to improve the efficiency and the capacity of the model to evaluate sustainable production and consumption policies. The aim of this report is twofold. First, it contains all the equations of the current version of the model; second, it illustrates the characteristics of the data used by FIDELIO 3."
C61|Have Econometric Analyses of Happiness Data Been Futile? A Simple Truth About Happiness Scales|Econometric analyses in the happiness literature typically use subjective well-being (SWB) data to compare the mean of observed or latent happiness across samples. Recent critiques show that comparing the mean of ordinal data is only valid under strong assumptions that are usually rejected by SWB data. This leads to an open question whether much of the empirical studies in the economics of happiness literature have been futile. In order to salvage some of the prior results and avoid future issues, we suggest regression analysis of SWB (and other ordinal data) should focus on the median rather than the mean. Median comparisons using parametric models such as the ordered probit and logit can be readily carried out using familiar statistical softwares like STATA. We also show a previously assumed impractical task of estimating a semiparametric median ordered-response model is also possible by using a novel constrained mixed integer optimization technique. We use GSS data to show the famous Easterlin Paradox from the happiness literature holds for the US independent of any parametric assumption.
C61|Strategic Complements in Two Stage, 2 × 2 Games|Strategic complements are well understood for normal form games, but less so for extensive form games. Indeed, there is some evidence that extensive form games with strategic complemen- tarities are a very restrictive class of games (Echenique (2004)). We explore the extent of this restrictiveness in the context of two stage, 2×2 games. We find that the restrictiveness imposed by quasisupermodularity and single crossing property is particularly severe, in the sense that the set of games in which payoffs satisfy these conditions has measure zero. In contrast, the set of games that exhibit strategic complements (in the sense of increasing best responses) has infinite measure. This enlarges the scope of strategic complements in two stage, 2 × 2 games (and provides a basis for possibly greater scope in more general games). Moreover, the set of subgame perfect Nash equilibria in the larger class of games continues to remain a nonempty, complete lattice.
C61|Green Urban Areas|This paper studies the size and location of urban green areas across city spaces. Urban green areas offer amenities that affect residential choices, land consumption and land rent. This paper discusses the socially optimal sizes and locations of urban green areas within a city and their decentralized allocation through land markets. The main result is that the share of land dedicated to urban green areas is a concave function of the distance to the city center. This result is confirmed by the empirical study of urban structures in the 305 largest EU cities. The importance of urban green areas is finally assessed by a counterfactual analysis, where 50% of urban green areas are removed in each city.
C61|The War of Rare Earth Elements: A Dynamic Game Approach|Rare earth elements govern today’s high-tech world and are deemed to be essential for the attainment of sustainable development goals. Since the 1990s, these elements have been predominantly supplied by one single actor, China. However, due to the increasing global relevance of their availability, other countries are now encouraged to enter the market. The objective of this paper is to analyze the strategic interactions among (potential) suppliers. In particular, we are interested in (1) the optimal timing for a newcomer (e. g. the U.S.) to enter the market, (2) the incumbent’s (i. e. China’s) optimal behavior, and (3) the cost-efficiency of cooperative vs. competitive market relations. By setting up a continuous-time dynamic game model, we show that (1) the newcomer should postpone the production launch until its rare earth reserves coincide with those of the incumbent, (2) the incumbent should strive for a late market entry and therefore keep its monopolistic resource extraction at the lowest possible level, (3) compared to the payoffs under competition, cooperation leads to a Pareto improvement when started at an early stage. The findings of our model are particularly relevant for the rational strategic positioning of the two great powers, America and China.
C61|Imposing equilibrium restrictions in the estimation of dynamic discrete games|Imposing equilibrium restrictions provides substantial gains in the estimation of dynamic discrete games. Estimation algorithms imposing these restrictions – MPEC, NFXP, NPL, and variations – have different merits and limitations. MPEC guarantees local convergence, but requires the computation of high-dimensional Jacobians. The NPL algorithm avoids the computation of these matrices, but – in games – may fail to converge to the consistent NPL estimator. We study the asymptotic properties of the NPL algorithm treating the iterative procedure as performed in finite samples. We find that there are always samples for which the algorithm fails to converge, and this introduces a selection bias. We also propose a spectral algorithm to compute the NPL estimator. This algorithm satisfies local convergence and avoids the computation of Jacobian matrices. We present simulation evidence illustrating our theoretical results and the good properties of the spectral algorithm.
C61|Does Class Size Matter? How, and at What Cost?|Using high quality administrative data on Greece we show that class size has a hump shaped effect on achievement. We do so both nonparametrically and parametrically, while controlling for potential endogeneity and allowing for quantile effects. We then embed our estimates for this relationship in a dynamic structural model with costs of hiring and firing. We argue that the linear specification form used in past work may be why it found mixed results. Our work suggests that while discrete reductions in class size may have mixed effects, discrete increases are likely to have very negative effects while marginal changes in class size would have small negative effects. We find optimal class sizes around 27 in the absence of adjustment costs and achievement maximizing ones around 15, and firing costs much larger than hiring costs consistent with the presence of unions. Despite this, reducing firing costs actually reduces achievement. Reducing hiring costs raises achievement and reduces class size. We show that class size caps are costly, and more so for small schools, even when set at levels well above average.
C61|The Dynamics of Exploitation and Inequality in Economies with Heterogeneous Agents|This paper analyses the relation between growth, inequalities, and exploitation as the unequal exchange of labour (UE exploitation). An economy with heterogeneous, intertemporally optimising agents is considered which generalises John Roemer's [52, 53] seminal models. First, a correspondence between prots and the existence (and intensity) of UE exploitation is proved in the dynamic context. This result is important, positively, because the prot rate is one of the key determinants of investment decisions, and, normatively, because it provides a link between UE exploitation and the functional distribution of income. Second, it is shown that asset inequalities are fundamental for the emergence of UE exploitation, but they are not sucient for its persistence, both in equilibria with accumulation and growth, and, perhaps more surprisingly, in stationary intertemporal equilibrium paths. Labour-saving technical progress, however, may yield sustained growth with persistent UE exploitation by keeping labour abundant relative to capital. Persistent inequalities in income and labour exchanged arise from the interaction between labour market conditions and dierential ownership of productive assets.
C61|Convergence of Computed Dynamic Models with Unbounded Shock|The purpose of this paper is to provide the conditions for the convergence of invariant measure obtained from numerical simulations to the exact invariant measure. Santos and Peralta-Alva (2005) have studied the convergence of computed invariant measure of economic models which cannot be solved analytically and must be solved numerically or with some other form of approximation. However, they assume that the state space is compact and therefore, the support of the shock of dynamical system is assumed to be bounded. This paper is to relax the compactness assumption for the convergence of the approximated invariant measure.
C61|Periodic solutions of the one-sector growth model: The role of income effects|The discrete-time version of the neoclassical one-sector growth model with elastic labour supply is considered. It is shown that this model can have periodic solutions only if leisure is not a normal good.
C61|Optimal Credible Warnings| We consider a decision maker who is responsible for issuing fl ood warnings for the population. The population is uncertain about the credibility of the warnings and adjusts its beliefs following false alerts or missed events. We show that low credibility leads the decision maker to issue warnings for lower probabilities of fl ooding. In practice, those probabilities are provided by hydrological forecasts. We therefore use our model to compare welfare under alternative real-world hydrological forecasts. We find that when forecasts include non-realistic extreme scenarios, the economy may remain stuck in a state characterized by many false alerts and poor credibility.
C61|Optimal Credible Warnings|We examine the impact of rainfall variability and cyclones on schooling and work among a cohort of teens and young adults by estimating a bivariate probit model, using a panel survey conducted in 2004 and 2011 in MadagascarÑa poor island nation that is frequently affected by extreme weather events. Our results show that negative rainfall deviations and cyclones reduce the current and lagged probability of attending school and encourage young men and, to a greater extent, women to enter the work force. Less wealthy households are most likely to experience this school-to-work transition in the face of rainfall shocks. The finding is consistent with poorer households having less savings and more limited access to credit and insurance, whichreduces their ability to cope with negative weather shocks.
C61|Economic Hysteresis with Multiple Inputs - A Simplified Treatment|Hysteresis in economics has so far usually been based on a representation of a system with only a single input variable, which has a persistent effect on an economic outcome (i.e. the output variable). However, in general there is more than one factor influencing economic decision problems. As a result, the description of the path-dependency in relation to only one input variable may (possibly) be insufficient. The multidimensional path-dependence phenomenon is addressed (in mathematics and physics) by a vector-hysteresis system, with an input vector of two or more variables. Unfortunately, for practical purposes, these models are quite complicated in economics. However, since standard economic decisions are based on comparing economic values of alternatives (e.g. present values of investments), such models can be used to reduce the dimensions of the hysteresis system. This article outlines how the influence of several original input variables (e.g. price level and interest rate) is captured by the resulting variations of the present value of an investment. This economic value then can be used as a single signal/input variable of a modified hysteresis system. Since this system is dimensionally reduced to the standard hysteresis case with only a single input variable, the standard aggregation procedure for a situation with heterogeneous agents can be applied again.
C61|On-field Performance Assessment in Football: Applying the Connected Network Data Envelopment Analysis Model|In this paper, we apply for the first time the connected network Data Envelopment Analysis (DEA) model to assess the on-field performance of football clubs during a league season. Specifically, we separately measure football clubs’ technical efficiency in offense, defense, and points’ earning by using their: (i) attacking and defending moves as the inputs of offense and defense, respectively; (ii) goals scored and goals conceded as the intermediate measures that simultaneously serve as both: (a) the single outputs of offense and defense, respectively; and (b) the inputs of the points’ earning process; (iii) points earned as the single output of the points’ earning process. To illustrate the usefulness of our theoretical framework, we make use of aggregate match statistics from the 2013-2014 Greek premier football league.
C61|Housing boom-bust cycles and asymmetric macroprudential policy|Macroprudential policy is pre-emptive, aimed at preventing crises. Empirical evidence hints at the existence of asymmetric policy in booms and recessions. This paper uses a New Keynesian model with a financial friction on mortgage borrowing and collateral to show what implications this asymmetry might have on the economy. The main source of fluctuations is a bubble in the housing market, which causes house prices and credit to deviate from their fundamental values, leading to a boom and bust cycle. The main macroprudential tool is the regulatory loan to value (LTV) ratio. The author finds that while the asymmetric policy dampens the boom phase, it introduces more volatility in the economy by exacerbating the correction that follows. The higher the asymmetry in the policy response, the more volatile the economy is relative to one in which policy reacts symmetrically.
C61|Climate Policy under Cooperation and Competition between Regions with Spatial Heat Transport|We build a novel stochastic dynamic regional integrated assessment model (IAM) of the climate and economic system including a number of important climate science elements that are missing in most IAMs. These elements are spatial heat transport from the Equator to the Poles, sea level rise, permafrost thaw and tipping points. We study optimal policies under cooperation and various degrees of competition between regions. Our results suggest that when the elements of climate science which are accounted for in this paper are ignored, important policy variables such as the social cost of carbon and adaptation could be seriously biased.
C61|Linear IV Regression Estimators for Structural Dynamic Discrete Choice Models|In structural dynamic discrete choice models, the presence of serially correlated unobserved states and state variables that are measured with error may lead to biased parameter estimates and misleading inference. In this paper, we show that instrumental variables can address these issues, as long as measurement problems involve state variables that evolve exogenously from the perspective of individual agents (i.e., market-level states). We define a class of linear instrumental variables estimators that rely on Euler equations expressed in terms of conditional choice probabilities (ECCP estimators). These estimators do not require observing or modeling the agent’s entire information set, nor solving or simulating a dynamic program. As such, they are simple to implement and computationally light. We provide constructive identification arguments to identify the model primitives, and establish the consistency and asymptotic normality of the estimator. A Monte Carlo study demonstrates the good finite-sample performance of the ECCP estimator in the context of a dynamic demand model for durable goods.
C61|Technology Adoption in a Hierarchical Network| This paper studies the effect of network structure on technology adoption, in the setting of the Python programming language. A major release of Python, Python 3, provides more advanced but backward-incompatible features to Python 2. We model the dynamics of Python 3 adoption made by package developers. Python packages form a hierarchical network through dependency requirements. The adoption decision involves not only updating one's own source code, but also dealing with dependency packages lacking Python 3 support. We build a dynamic model of technology adoption where each package makes an irreversible decision to provide support for Python 3. The optimal timing of adoption requires a prediction of all future states, for the package itself as well as each of its dependencies. With a complete dataset of package characteristics for all historical releases, we are able to draw the complete hierarchical structure of the network, and simplify the estimation by grouping packages into different layers based on the dependency relationship. We study how individual adoption decisions can propagate along the links in such a hierarchical network. We also test the effectiveness of various counterfactual policies that can promote a faster adoption process.
C61|Optimum Stop Spacing for Accessibility|"This paper describes the connection between stop spacing and person-weighted accessibility for a transit route. Population distribution is assumed to be uniform along the line, but at each station, demand drops with distance from the station. The study reveals that neither short nor excessive stop spacings are efficient in providing accessibility. For the configuration of each transit route, an optimum stop spacing exists that maximizes accessibility. Parameters including transit vehicle acceleration, deceleration, top speed, dwell time, and pedestrian walking speed affect level of accessibility achiev- able, and differ in their effect on accessibility results. The findings provide an anchor of reference both for the planning of future transit systems, and for transit operators to make operational changes to system design parameters that improve accessibility in a cost-effective manner. The study technically justifies the ""rule of thumb"" in setting different stop spacings for metro, streetcars, and other different transit services. Different types of transit vary in their ability to provide accessibility, slower moving streetcar (tram) type urban rails are inherently disadvantaged in that respect. Thus the type of transit service to be built should be of particular concern, if the transit is to effectively serve its intended population."
C61|Financial constraints of innovative firms and sectoral growth|Innovation policies can consist in measures aimed at directly alleviating financial constraints of innovative firms, beyond more traditional fiscal incentives to foster private R&D spendings. To explore the interaction between innovation and financial constraints at the sector level, and evaluate stylized policy scenarios, this paper brings together two analytical frameworks from the endogenous growth and corporate finance literatures. Within this dynamic model, firms innovate and compete for products through destructive creation and accumulate internal funds in relation to financial hindrances occurring when they enter, develop or exit. Including notably asymmetric information between investors and managers of firms with respect to uncertain cash flows, this model is first consistent with the fact that firms tend to spend more on R&D when their internal funds are higher. It then allows for experi­ments addressing growth and overall liquidity holdings for various sectoral contexts. In this specific framework, easing access to initial funding, as fiscal incentives, can have substantial effects. More­over, while a stylized high-tech sector is asso­ciated with higher growth and overall liqui­dity holdings, both variables depend to a large extent on many sectoral characteristics, such as R&D efficiency, entry costs, and cash flow mean and volatility.
C61|Pricing and hedging GDP-linked bonds in incomplete markets|We model the super-replication of payoffs linked to a country’s GDP as a stochastic linear program on a discrete time and state-space scenario tree to price GDP-linked bonds. As a byproduct of the model we obtain a hedging portfolio. Using linear programming duality we compute also the risk premium. The model applies to coupon-indexed and principal-indexed bonds, and allows the analysis of bonds with different design parameters (coupon, target GDP growth rate, and maturity). We calibrate for UK and US instruments, and carry out sensitivity analysis of prices and risk premia to the risk factors and bond design parameters. We also compare coupon-indexed and principal-indexed bonds.
C61|Risk management for sovereign financing within a debt sustainability framework|The mix of instruments used to finance a sovereign is a key determinant of debt sustainability through its effect on funding costs and risks. We extend standard debt sustainability analysis to incorporate debt-financing decisions in the presence of macroeconomic, financial, and fiscal risks. We optimize the maturity of debt instruments to trade off borrowing costs with refinancing risk. Risk is quantified with a coherent measure of tail risk of financing needs, conditional Flow-at-Risk. A constraint on the pace of reduction of debt stocks is also imposed, and we model the effect of debt stocks on the yield curve through endogenous risk and term premia. On a simulated economy, we show that the cost-risk and flow-stock trade-offs embedded in issuance decisions are key determinants of the evolution of debt dynamics and are economically significant. Comparing three alternative optimizing strategies and some simple fixed-issuance rules, we also draw lessons on when and why optimizing matters the most. This depends on the risk tolerance level, the size, cost, and maturity of legacy debt, and the sensitivity of interest rates to debt. Our model quantifies thresholds for the minimum level of refinancing risks and the maximum pace of debt reduction that a sovereign could reach given its economic fundamentals. Going beyond those thresholds is only feasible through adjustments of gross financing needs, and an extension of the baseline model identifies the hot spots for these adjustments, computing their minimum size and optimal timing. Our findings inform policy decisions concerning both official sector borrowing and public finance, with a focus not only on minimizing interest payments but also on managing refinancing risks and increasing debt dynamics.
C61|Measuring Impact of Uncertainty in a Stylized Macro-Economic Climate Model within a Dynamic Game Perspective|In this paper we try to quantify/measure the main factors that influence the equilibrium outcome and pursued strategies in a simplistic model for the use of fossil versus green energy over time. The model is derived using the standard Solow macro-economic growth model in a two-country setting within a dynamic game perspective. After calibrating the model for a setting of OECD versus non-OECD countries we study what kind of uncertainties affect the outcomes of the linearized model most, assuming both countries use Nash strategies to cope with shocks that impact the model. The main outcome of this study is that the parameters that occur in the objective of both players seem to carry the most uncertainty for both the outcome of the model and strategies.
C61|Product Innovation of an Incumbent Firm : A Dynamic Analysis|In case of a product innovation firms start producing a new product. While doing so, such a firm should decide what to do with its existing product after the firm has innovated. Essentially it can choose between replacing the established product by the new one, or keep on producing the established product so that it produces two products at the same time. The aim of this paper is to design a theoretical framework to analyze this problem. Due to technological progress the quality of the newest available technology, and thus the quality of the innovative product that can be produced by this technology, increases over time. The implication is that a later innovation enables the firm to produce a better innovative product. So, typically the firm faces the tradeoff between innovating fast, which boosts its profits soon but only by a small amount, or innovating later, which leads to a larger payoff increase. The drawback here is that the firm is stuck with producing the established product for a longer time. We fund that a highly uncertain economic environment makes the firm delay abolishing the old product market. But if the innovative market is more volatile, the firm enters the market sooner, provided it will be active on the old market, at least for some time. Moreover, the smaller the initial demand for the innovative product market, the better the quality of the innovative product needs to be for the product innovation to be optimal.
C61|Technology Adoption in a Declining Market|Rapid technological developments are inducing the shift in consumer demand from existing products towards new alternatives. When operating in a declining market, the profitability of incumbent firms is largely dependent on the ability to correctly time the introduction of product innovations. This paper contributes to the existing literature on technology adoption by considering the optimal innovation investment in the context of the declining market. We study the problem of a firm that has an option to undertake the innovation investment and thereby either to add a new product to its portfolio (add strategy) or to replace the established product by the new one (replace strategy). We are able to quantify the value of the option to adopt a new technology, as well as the optimal timing to exercise it. We find that it can be optimal for the firm to innovate not only because of the significant technological improvement, but also due to demand saturation. In the latter case profits of the established product may become so low that the firm will adopt a new technology even if the newest available innovation has not improved for some time. This way, our approach allows to explicitly account for the effect of a decline in the established market on technology adoption. Furthermore, we find that under certain conditions an inaction region exists, in which the firm does not innovate, while for lower technology levels it applies the add strategy and for higher technology levels the replace strategy.
C61|Sufficient Statistics for Unobserved Heterogeneity in Structural Dynamic Logit Models|We study the identification and estimation of structural parameters in dynamic panel data logit models where decisions are forward-looking and the joint distribution of unobserved heterogeneity and observable state variables is nonparametric, i.e., fixed-effects model. We consider models with two endogenous state variables: the lagged decision variable, and the time duration in the last choice. This class of models includes as particular cases important economic applications such as models of market entry-exit, occupational choice, machine replacement, inventory and investment decisions, or dynamic demand of differentiated products. The identification of structural parameters requires a sufficient statistic that controls for unobserved heterogeneity not only in current utility but also in the continuation value of the forward-looking decision problem. We obtain the minimal sufficient statistic and prove identification of some structural parameters using a conditional likelihood approach. We apply this estimator to a machine replacement model.
C61|Apports naturels en eau dans les barrages-réservoirs et règle de Hotelling|L’article analyse la gestion annuelle des barrages hydroélectriques quand les apports naturels en eau suivent des cycles prévisibles. Nous montrons que dans ce cadre la règle de Hotelling s’applique par intermittence. En effet, d`es lors qu’un stock est constitué sans que les contraintes de capacité soient liantes, il est possible de réaliser des arbitrages intertemporels semblables à ceux de la gestion d’une ressource non-renouvelable, donc la valeur de l’eau croît au taux de l’intérêt. En revanche, quand la ressource en eau est si rare qu’on ne peut compter que sur les apports instantanés ou, au contraire, trop abondante compte tenu des capacités du réservoir de sorte qu’il faut réaliser des lâchures improductives, la règle d’arbitrage de Hotelling ne s’applique plus. Nous expliquons aussi l’importance des contraintes de turbinage qui, quand elles sont liantes, donnent à l'électricité produite une valeur supérieure `à celle de l’eau qui sert à la produire.
C61|The New Keynesian Model with Stochastically Varying Policies|The Multiplicative Ergodic Theorem provides a novel general method- ology to analyze rational expectations models with stochastically vary- ing coecients. The approach is applied for the first time to economics and analyzes the canonical New Keynesian model with a Taylor rule which switches randomly between an aggressive and a passive reaction to in ation. The paper delineates the trade-o of the central bank of being passive in some periods and aggressive in others. Moreover, it is shown how this trade-o depends on the stochastic process governing the randomness in the central bank's policy. Finally, explicit solution formulas are derived in the case of determinateness as well as inde- terminateness. In doing so he paper considerably extends the current approach.
C61|Economic Measures of Capacity Utilization: A Nonparametric Cost Function Analysis|Cost based measures of capacity utilization and capacity output are important metrics for evaluating firm performance. Understanding where firms are producing on their average cost curve provides information about whether capacity utilization is greater then, less than or equal to one. Most firms are multi-output, multi-input in nature which makes estimation of capacity utilization and capacity output challenging if a cost based measure is desired. For a multi-output firm, the relevant concept is ray average cost (RAC) which can be estimated through non-linear DEA models. This paper offers a simple transformation that linearizes the non-linear DEA program to estimate average, or ray average cost, and to determine capacity utilization and optimal output. The methods are empirically tested on data from a number of U.S. electricity producers for the single output case, and a sample of dental practices for the multi-output case. Results show that for both industries, most firms were operating at less than full capacity, and needed to expand output to minimize their costs. For the dental practices, examination of results from six randomly chosen firms showed the importance of operatories in determining optimal levels of output.
C61|Measures of Labor Use Efficiency from a Cost-Based Dual Representation of the Technology: A Study of Indian Bank Branches|In this paper, we propose a representation of the production technology in the form of a cost set in the output and expenditure space as an alternative to the standard free disposal convex hull of input-output vectors. We show that when all units pay the same input prices, one can construct a free disposal convex hull of outputs and total expenditures to solve the cost minimization problem. We use the proposed model to evaluate the labor use efficiency of a sample of 325 branches of a major Indian public sector bank from four metropolitan cities across two years, 2008 and 2014. This is the first study in the Indian banking context to model the operations of branches using the production approach. Our empirical findings indicate that there is significant inefficiency in labor use in the branches and cost could be curtailed substantially by addressing overstaffing. Across the three types of labor, reducing the expenditure of clerks would have the highest impact for cost saving. We do find, however, that that the extent of overspending on clerks has reduced in 2014, which apparently is a direct consequence of computerization of routine jobs. Efficiency varies across regions. In general, Chennai branches are more efficient than branches from other regions whereas Kolkata branches are the least efficient.
C61|Data Envelopment Analysis with Alternative Returns to Scale|This paper offers an overview of Data Envelopment Analysis as a nonparametric method of measuring efficiency in production. Special attention is devoted to alternative returns to scale assumptions about the technology and identifying the local nature of returns to scale at projections of an inefficient unit on to the frontier. Both radial and non-radial measures of technical efficiency are considered.
C61|Pollution effects on disease transmission and economic stability|In this article, we embed a model of disease spread into a Ramsey model. A stock of pollution, viewed as a productive externality, affects both the disease transmission and the consumption demand. An ecofriendly government levies a proportional Pigouvian tax on production to depollute. We show the coexistence of two steady states in the long run: a disease-free and an endemic steady state. At the endemic steady state, a higher green-tax rate always reduces the pollution level. In the short run, we show the existence of limit cycles (through a Hopf bifurcation) as well as more complex dynamics of codimension two (a Gavrilov-Guckenheimer bifurcation). We complete the study with a numerical illustration of these bifurcations and a new facet of the Green Paradox: a higher tax rate can allow more scope for cycles by lowering the critical aversion to pollution and, thus, contribute to destabilize the economy and promote intergenerational inequalities.
C61|Integrating non-timber objectives into bio-economic models of the forest sector: a review of recent innovations and current shortcomings|This paper gives an overview of non-timber objective modelling in forest sector models (FSM) research through a systematic literature review followed by an in-depth narrative review. Originally conceived to perform projections of timber supply and wood products markets, FSM have been growingly used for forest and climate policy analysis. For this purpose, they have gradually integrated objectives other than timber production, such as habitat conservation, carbon sequestration and bioenergy production. We identify these non-timber objectives and elicit technical innovations that have enabled their integration into FSM. We also discuss their current limits and the new perspectives they bring for a better economic-environmental assessment of forest policies. Results show that the study of non-timber objectives is a growing topic in FSM research, with bioenergy production and climate change mitigation as the most commonly studied. However, there are discrepancies regarding the respective contributions of different families of models, and not all non-timber objectives have been integrated to the same degree. On the one hand, bioenergy production has been thoroughly integrated through marginal modifications of the market component of models. On the other hand, the modelling of carbon sequestration and habitat protection entails deeper changes, such as the addition of new resources to the models, an increase in the complexity of the objective function and associated constraints, or the use of tools and models outside the FSM.
C61|On the minimum correlation between symmetrically distributed random variables|We show that families of symmetrically distributed Bernoulli random variables have a maximal negative correlation that almost always is strictly above the general lower limit. JEL codes: C10, C61
C61|Estimating a nonlinear new Keynesian model with the zero lower bound for Japan|We estimate a small-scale nonlinear DSGE model with the zero lower bound (ZLB) of the nominal interest rate for Japan, where the ZLB has constrained the country’s monetary policy for a considerably long period. We employ the time iteration with linear interpolation method to solve equilibrium and then estimate the model by using the Sequential Monte Carlo Squared method. Results of estimation suggest that (1) the Bank of Japan has been conducting monetary policy that depends on the lagged notional interest rate rather than the lagged actual interest rate and that (2) the estimated series of the natural rate of interest moves very closely to those based on the model without the ZLB.
C61|Asset pricing under optimal contracts|We consider the problem of finding equilibrium asset prices in a financial market in which a portfolio manager (Agent) invests on behalf of an investor (Principal), who compensates the manager with an optimal contract. We extend a model from Buffa, Vayanos and Woolley (2014) by allowing general contracts, and by allowing the portfolio manager to invest privately in individual risky assets or the index. To alleviate the effect of moral hazard, Agent is optimally compensated by benchmarking to the index, which, however, may incentivize him to be too much of a “closet indexer”. To counter those incentives, the optimal contract rewards Agent for taking specific risk of individual assets in excess of the systematic risk of the index, by rewarding the deviation between the portfolio return and the return of an index portfolio, and the deviation's quadratic variation.
C61|Shadow prices, fractional Brownian motion, and portfolio optimisation under transaction costs|The present paper accomplishes a major step towards a reconciliation of two conflicting approaches in mathematical finance: on the one hand, the mainstream approach based on the notion of no arbitrage (Black, Merton & Scholes); and on the other hand, the consideration of non-semimartingale price processes, the archetype of which being fractional Brownian motion (Mandelbrot). Imposing (arbitrarily small) proportional transaction costs and considering logarithmic utility optimisers, we are able to show the existence of a semimartingale, frictionless shadow price process for an exponential fractional Brownian financial market
C61|Mean growth and stochastic stability in endogenous growth models|Under uncertainty, mean growth of, say, wealth is often defined as the growth rate of average wealth, but it can alternatively be defined as the average growth rate of wealth. We argue that stochastic stability points to the latter notion of mean growth as the theoretically relevant one. Our discussion is cast within the class of continuous-time AK-type models subject to geometric Brownian motions. First, stability concepts related to stochastic linear homogeneous differential equations are introduced and applied to the canonical AK model. It is readily shown that exponential balanced-growth paths are not robust to uncertainty. In a second application, we evaluate the quantitative implications of adopting the stochastic-stability-related concept of mean growth for the comparative statics of global diversification in the seminal model due to Obstfeld (1994).
C61|Geographic Environmental Kuznets Curves: The Optimal Growth Linear-Quadratic Case|We solve a linear-quadratic model of a spatio-temporal economy using a polluting one-input technology. Space is continuous and heterogenous: locations differ in productivity, nature self-cleaning technology and environmental awareness. The unique link between locations is transboundary pollution which is modelled as a PDE diffusion equation. The spatio-temporal functional is quadratic in local consumption and linear in pollution. Using a dynamic programming method adapted to our infinite dimensional setting, we solve the associated optimal control problem in closed-form and identify the asymptotic (optimal) spatial distribution of pollution. We show that optimal emissions will decrease at given location if and only if local productivity is larger than a threshold which depends both on the local pollution absorption capacity and environmental awareness. Furthermore, we numerically explore the relationship between the spatial optimal distributions of production and (asymptotic) pollution in order to uncover possible (geographic) Environmental Kuznets Curve cases.
C61|Sufficient Statistics for Unobserved Heterogeneity in Structural Dynamic Logit Models|We study the identification and estimation of structural parameters in dynamic panel data logit models where decisions are forward-looking and the joint distribution of unobserved heterogeneity and observable state variables is nonparametric, i.e., fixed-effects model. We consider models with two endogenous state variables: the lagged decision variable, and the time duration in the last choice. This class of models includes as particular cases important economic applications such as models of market entry-exit, occupational choice, machine replacement, inventory and investment decisions, or dynamic demand of differentiated products. The identification of structural parameters requires a sufficient statistic that controls for unobserved heterogeneity not only in current utility but also in the continuation value of the forward-looking decision problem. We obtain the minimal sufficient statistic and prove identification of some structural parameters using a conditional likelihood approach. We apply this estimator to a machine replacement model.
C61|Portfolio diversification and model uncertainty: a robust dynamic mean-variance approach|This paper is concerned with a multi-asset mean-variance portfolio selection problem under model uncertainty. We develop a continuous time framework for taking into account ambiguity aversion about both expected return rates and correlation matrix of the assets, and for studying the effects on portfolio diversification. We prove a separation principle for the associated robust control problem, which allows to reduce the determination of the optimal dynamic strategy to the parametric computation of the minimal risk premium function. Our results provide a justification for under-diversification, as documented in empirical studies. We explicitly quantify the degree of under-diversification in terms of correlation and Sharpe ratio ambiguity. In particular, we show that an investor with a poor confidence in the expected return estimation does not hold any risky asset, and on the other hand, trades only one risky asset when the level of ambiguity on correlation matrix is large. This extends to the continuous-time setting the results obtained by Garlappi, Uppal and Wang [13], and Liu and Zeng [24] in a one-period model. JEL Classification: G11, C61 MSC Classification: 91G10, 91G80, 60H30
C61|Climate Policy under Cooperation and Competition between Regions with Spatial Heat Transport|We build a novel stochastic dynamic regional integrated assessment model (IAM) of the climate and economic system including a number of important climate science elements that are missing in most IAMs. These elements are spatial heat transport from the Equator to the Poles, sea level rise, permafrost thaw and tipping points. We study optimal policies under cooperation and noncooperation between two regions (the North and the Tropic-South) in the face of risks and recursive utility. We introduce a new general computational algorithm to find feedback Nash equilibrium. Our results suggest that when the elements of climate science are ignored, important policy variables such as the optimal regional carbon tax and adaptation could be seriously biased. We also find the regional carbon tax is significantly smaller in the feedback Nash equilibrium than in the social planner’s problem in each region, and the North has higher carbon taxes than the Tropic-South.
C61|Spatially Structured Deep Uncertainty, Robust Control, and Climate Change Policies|In view of the ambiguities and the deep uncertainty associated with climate change, we study the features of climate change policies that account for spatially structured ambiguity. Ambiguity related to the evolution of the damages from climate change is introduced into a coupled economy-climate model with explicit spatial structure due to heat transport across the globe. We seek to answer questions about how spatial robust regulation regarding climate policies can be formulated; what the potential links of this regulation to the weak and strong version of the precautionary principle (PP) are; and how insights about whether it is costly to follow a PP can be obtained. We also study the emergence of hot spots, which are locations where local deep uncertainty may cause robust regulation to break down for the whole spatial domain.
C61|Performance Cycles|No abstract is available for this item.
C61|Multivariate Rational Inattention|We study optimal control problems in the multivariate linear-quadratic-Gaussian framework under rational inattention and show that the multivariate attention allocation problem can be reduced to a dynamic tracking problem in information theory. We propose a general solution method to solve this problem by using rate distortion functions and semidefinite programming and derive the optimal form and dimension of signals without strong prior restrictions. We provide generalized reverse water-filling solutions for some special cases. Applying our method to solve three multivariate economic models, we obtain some results qualitatively different from the literature.
C61|Measuring income inequalities beyond Gini index|Growing interest in the analysis of interrelationships between income distribution and economic growth has recently stimulated new theoretical as well as empirical research. Since existing theoretical models propose inequality is detrimental to growth, while others point at income inequality as an essential determinant supporting economic growth. Measures such as head-count ratio for poverty index or widely used Gini coefficient are aggregated indicators without deeper insight into income distribution among the poor or the households. To derive an indicator accounting for income distribution among the income groups, we propose output oriented DEA model with inputs equal unit and weights restrictions imposed so as to favour higher income share in lower quantiles. We demonstrate the merit of this approach on the quintile income breakdown data of the European countries. Prioritizing lower income groups ´ welfare, countries –e.g. Slovenia and Slovakia –can be equally favoured by the new proposed indicator while assessed differently by Gini index. Intertemporal analysis reveals a slight deterioration of income distribution over the period of 2007 –2017 in a Rawlsian sense.
C61|Revisiting the Efficiency-Equity Trade-off: A Muli-objective Linear Problem combined with an extended Leontief Input Output Model|In recent years there has been increasing interest in the question of how inequality affects economic growth. This growing interest has recently stimulated new theoretical aswell as empirical research. Some existing theoretical models propose income inequality is detrimental to growth, but alternative theoretical models point at inequality as a determinant furthering economic growth. The main goal of this paper is to obtain deeper insights into the so-called efficiency-equity trade-off. Recently the Stiglitz-Report (Stiglitz et al., 2010) revealed several limits of GDP as an indicator of economic performance and social progress and recommended to shift emphasis towards measuring people's well-being. Following this recommendation,we develop a new multiple criteria decision making model coupled with an extended Leontief input-output model taking into account the social dimension and obtain deeper insights into the so-called efficiency-equity trade-off.
C61|Robust policy schemes for R&D games with asymmetric information|We consider an abstract setting of the differential r&d game, where participating firms are allowed for strategic behavior. We assume the information asymmetry across those firms and the government, which seeks to support newer technologies in a socially optimal manner. We develop a general theory of robust subsidies under such one-sided uncertainty and establish results on relative optimality, duration and size of different policy tools available to the government. It turns out that there might exist multiple sets of second-best robust policies, but there always exist a naturally induced ordering across such sets, implying the optimal choice of a policy exists for the government under different uncertainty levels.
C61|Global warming and technical change: Multiple steady-states and policy options|In this paper we develop an economic growth model that includes anthropogenicclimate change. We include a publicly funded research sector that creates new technologies and simultaneously expands the productivities of existing technologies. The environment is affected by R&D activities both negatively, through the increase of output from productivity growth, as well as positively as new technologies are less harmful for the environment. We find that there may exist two different steadystates of the economy, depending on the amount of research spending: one with less new technologies being developed and the other with more technologies. Thus, a lock-in effect may arise that, however, can be overcome by raising R&D spending sufficiently such that the steady-state becomes unique. We derive the combinations of fiscal policy instruments for which that can be achieved and we study the implications for the economy and for the environment. In particular, the double dividend hypothesis may hold only under some specific conditions.
C61|Heterogeneous R&D spillovers and sustainable growth: Limits to efficient regulation|This paper introduces heterogeneity of cross-technologies interactions into the double-differentiated R&D-based endogenous growth model. In this model new technologies appear continuously and older are outdated generating structural change. All technologies may interact with each other through knowledge spillovers which are technology-specific and this results in innovations' heterogeneity. The conditions on the shape of these interactions for the existence of the (sustained) growth path in the decentralized economy as well as for the social planner's problem are estab- lished. Next the necessity for government interventions depending on the complexity of these interactions is studied. At last the scale and duration of interventions are demonstrated to be functions of spectral properties of the interactions operator.
C61|Games without winners: Catching-up with asymmetric spillovers|Dynamic game with changing leader is studied on the example of R&D co-opetition structure. The leader benefits from higher followers' innovations rate and followers are enjoying a spillover from the leader. Leadership changes because of asymmetric efficiency of investments of players. It is demonstrated that under sufficiently asymmetric players there is no long-run leader in this game and all players act as followers. Moreover this outcome may be the socially optimal one. In decentralised setting additional complex types of dynamics are observed: permanent uctuations around symmetric (pseudo)equilibrium and chaotic dynamics. This last is possible only once strategies of players are interdependent. Cooperative solution is qualitatively similar for any number of players while market solution is progressively complex given all players are asymmetric. Results are extended to an arbitrary linear-quadratic multi-modal differential game with spillovers and the structure necessary for the onset of non-deterministic chaos is discussed.
C61|Dynamic Spatial Autoregressive Models with Time-varying Spatial Weighting Matrices|We propose a new spatio-temporal model with time-varying spatial weighting matrices. We allow for a general parameterization of the spatial matrix, such as: (i) a function of the inverse distances among pairs of units to the power of an unknown time-varying distance decay parameter, and (ii) a negative exponential function of the time-varying parameter as in (i). The filtering procedure of the time-varying parameters is performed using the information in the score of the conditional distribution of the observables. An extensive Monte Carlo simulation study to investigate the finite sample properties of the ML estimator is reported. We analyze the association between eight European countries' perceived risk, suggesting that the economically strong countries have their perceived risk increased due to their spatial connection with the economically weaker countries, and we investigates the evolution of the spatial connection between the house prices in different areas of the UK, identifying periods when the usually adopted sparse weighting matrix is not sufficient to describe the underlying spatial process.
C61|Unintended consequences: The snowball effect of energy communities|Following the development of decentralized generation and smart appliances, energy communities have become a phenomenon of increased interest. While the benefits of such communities have been discussed, there is increasing concern that inadequate grid tariffs may lead to excess adoption of such business models. Furthermore, snowball effects may be observed following the effects these communities have on grid tariffs. We show that restraining the study to a simple cost-benefit analysis is far from satisfactory. Therefore, we use the framework of cooperative game theory to take account of the ability of communities to share gains between members. The interaction between energy communities and the DSO then results in a non-cooperative equilibrium. We provide mathematical formulations and intuitions of such effects, and carry out realistic numerical applications where communities can invest jointly in solar panels and batteries. We show that such a snowball effect may be observed, but its magnitude and its welfare effects will depend on the grid tariff structure that is implemented, leading to possible PV over-investments.<br><small>(This abstract was borrowed from another version of this item.)</small>
C61|Simulation and Evaluation of Zonal Electricity Market Design|Zonal pricing with countertrading (a market-based redispatch) gives arbitrage opportunities to the power producers located in the export-constrained nodes. They can increase their profit by increasing the output in the dayahead market and decrease it in the real-time market (the inc-dec game). We show that this leads to large inefficiencies in a standard zonal market. We also show how the inefficiencies can be significantly mitigated by changing the design of the real-time market. We consider a two-stage game with oligopoly producers, wind-power shocks and real-time shocks. The game is formulated as a two-stage stochastic equilibrium problem with equilibrium constraints (EPEC), which we recast into a two-stage stochastic Mixed-Integer Bilinear Program (MIBLP). We present numerical results for a six-node and the IEEE 24-node system.<br><small>(This abstract was borrowed from another version of this item.)</small>
C61|Increase-Decrease Game under Imperfect Competition in Two-stage Zonal Power Markets – Part I: Concept Analysis|This paper is part I of a two-part paper. It proposes a two-stage game to analyze imperfect competition of producers in zonal power markets with a day-ahead and a real-time market. We consider strategic producers in both markets. They need to take both markets into account when deciding what to bid in each market. The demand shocks between these markets are modeled by several scenarios. The two-stage game is formulated as a Twostage Stochastic Equilibrium Problem with Equilibrium Constraints (TS-EPEC). Then it is further reformulated as a two-stage stochastic Mixed-Integer Linear Program (MILP). The solution of this MILP gives the Subgame Perfect Nash Equilibrium (SPNE). To tackle multiple SPNE, we design a procedure which _nds all SPNE with di_erent total dispatch costs. The proposed MILP model is solved using Benders decomposition embedded in the CPLEX solver. The proposed MILP model is demonstrated on the 6-node and the IEEE 30-node example systems.<br><small>(This abstract was borrowed from another version of this item.)</small>
C61|Increase-Decrease Game under Imperfect Competition in Two-stage Zonal Power Markets – Part II: Solution Algorithm|In part I of this paper, we proposed a Mixed-Integer Linear Program (MILP) to analyse imperfect competition of oligopoly producers in two-stage zonal power markets. In part II of this paper, we propose a solution algorithm which decomposes the proposed MILP model into several subproblems and solve them in parallel and iteratively. Our solution algorithm reduces the solution time of the MILP model and it allows us to analyze largescale examples. To tackle the multiple Subgame Perfect Nash Equilibria (SPNE) situation, we propose a SPNE-band approach. The SPNE band is split into several subintervals and the proposed solution algorithm finds a representative SPNE in each subinterval. Each subinterval is independent from each other, so this structure enables us to use parallel computing. We also design a pre-feasibility test to identify the subintervals without SPNE. Our proposed solution algorithm and our SPNE-band approach are demonstrated on the 6-node and the modified IEEE 30-node example systems. The computational tractability of our solution algorithm is illustrated for the IEEE 118-node and 300-node systems.<br><small>(This abstract was borrowed from another version of this item.)</small>
C61|The Transition to Renewable Energy|The existing economics literature neglects the important role of capacity in the production of renewable energy. To fiill this gap, we construct a model in which renewable energy production is tied to renewable energy capacity, which then becomes a form of capital. This capacity capital can be increased through investment, which we interpret as arising from the allocation of energy, and which therefore comes at the cost of reduced general production. Requiring societal well-being to never decline, we describe how society could optimally elect to split energy in this fashion, the use of non-renewable energy resources, the use of renewable energy resources, and the implied time path of societal well-being. Our model delivers an empirically satisfactory explanation for simultaneous use of non-renewable and renewable energy. We also discuss the optimality of ceasing use of non-renewable energy before the non-renewable resource stock is fully exhausted.
C61|Resurrecting the New-Keynesian Model: (Un)conventional Policy and the Taylor rule|This paper explores the ability of the New-Keynesian (NK) model to explain the recent periods of quiet and stable inflation at near-zero nominal interest rates. We show how (conventional and unconventional) monetary policy shocks enlarge the ability to explain the facts, such that the theory supports both a negative and a positive response of inflation. Central to our finding is that monetary policy shocks may have temporary and/or permanent components. We find that the NK model can explain the recent episodes, even if one considers an active role of monetary policy and restrict ourselves to the regions of (local) determinacy. We also show that a new global solution, capturing highly nonlinear dynamics, is necessary to generate a prolonged period of near-zero interest rates as a policy choice.
C61|Optimal Policy and Network Effects for the Deployment of Zero Emission Vehicles |We analyze the impact of indirect network effects in the deployment of zero emission vehicles in a static partial equilibrium model. In most theoretical analysis direct and indirect effects are conflated, and relatively few authors have explicitly considered indirect network effects. We also introduce the market power of vehicle producers and scale effects in the production function. The model exhibits a multiplicity of local social extrema and of market equilibria, suggesting a possibility of lock-in. The optimal set of subsidies is derived so that the Pareto dominating market equilibrium would coincide with the social optimum. This framework is applied to the case of the fuel cell electric (hydrogen) vehicles.<br><small>(This abstract was borrowed from another version of this item.)</small>
C61|Dissection of Bitcoin's Multiscale Bubble History|We present a detailed bubble analysis of the Bitcoin to US Dollar price dynamics from January 2012 to February 2018. We introduce a robust automatic peak detection method that classifies price time series into periods of uninterrupted market growth (drawups) and regimes of uninterrupted market decrease (drawdowns). In combination with the Lagrange Regularisation Method for detecting the beginning of a new market regime, we identify 3 major peaks and 10 additional smaller peaks, that have punctuated the dynamics of Bitcoin price during the analyzed time period. We explain this classification of long and short bubbles by a number of quantitative metrics and graphs to understand the main socio-economic drivers behind the ascent of Bitcoin over this period. Then, a detailed analysis of the growing risks associated with the three long bubbles using the Log-Periodic Power Law Singularity (LPPLS) model is based on the LPPLS Confidence Indicators, defined as the fraction of qualified fits of the LPPLS model over multiple time windows. Furthermore, for various fictitious present analysis times t2, positioned in advance to bubble crashes, we employ a clustering method to group LPPLS fits over different time scales and the predicted critical times tc (the most probable time for the start of the crash ending the bubble). Each cluster is argued to provide a plausible scenario for the subsequent Bitcoin price evolution. We present these predictions for the three long bubbles and the four short bubbles that our time scale of analysis was able to resolve. Overall, our predictive scheme provides useful information to warn of an imminent crash risk.
C61|Dividends: From Refracting to Ratcheting|In this paper we consider an alternative dividend payment strategy in risk theory, where the dividend rate can never decrease. This addresses a concern that has often been raised in connection with the practical relevance of optimal classical dividend payment strategies of barrier and threshold type. We study the case where once during the lifetime of the risk process the dividend rate can be increased and derive corresponding formulae for the resulting expected discounted dividend payments until ruin. We first consider a general spectrally-negative Lévy risk model, and then re fine the analysis for a diffusion approximation and a compound Poisson risk model. It is shown that for the diffusion approximation the optimal barrier for the ratcheting strategy is characterized by an unexpected relation to the case of refracted dividend payments. Finally, numerical illustrations for the diffusion case indicate that with such a simple ratcheting dividend strategy the expected value of discounted dividends can already get quite close to the respective value of the refracted dividend strategy, the latter being known to be optimal among all admissible dividend strategies.
C61|On Randomized Reinsurance Contracts|In this paper we discuss the potential of randomizing reinsurance treaties for efficient risk management. While it may be considered counter-intuitive to introduce additional external randomness in the determination of the retention function for a given occurred loss, we indicate why and to what extent randomizing a treaty can be interesting for the insurer. We illustrate the approach with a detailed analysis of the effects of randomizing a stop-loss treaty on the expected profit after reinsurance in the framework of a one-year reinsurance model under regulatory solvency constraints and cost of capital considerations.
C61|Conditional Davis Pricing|We introduce the notion of a conditional Davis price and study its properties. Our ultimate goal is to use utility theory to price non-replicable contingent claims in the case when the investor's portfolio already contains a non-replicable component. We show that even in the simplest of settings - such as Samuelson's model - conditional Davis prices are typically not unique and form a non-trivial subinterval of the set of all no-arbitrage prices. Our main result characterizes this set and provides simple conditions under which its two endpoints can be effectively computed. We illustrate the theory with several examples.
C61|A Corporate Financing-Based Asset Pricing Model|I show that an asset pricing model for the equity claims of a value-maximizing firm can be constructed from its optimal financial contracting behavior. I study a dynamic contracting model in which firms trade off the costs and benefits of a given promise to pay external lenders in a specific economic state. Deals between firms and financiers reveal the importance of that state for firm's equity value, namely the stochastic discount factor the firm responds to. I empirically evaluate the model in the cross section of expected equity returns. I find that the financial contracting approach goes a long way in rationalizing observed cross-sectional differences in average returns, also in comparison to leading asset pricing models. In addition, the model discloses that two easily measured variables, the growth rates on net worth and profitability, generate sizeable cross-sectional spreads in returns. Finally, a calibrated version of the model is broadly consistent with observed corporate policies of US listed firms.
C61|Hedge or Rebalance: Optimal Risk Management with Transaction Costs|We solve the problem of optimal risk management for an investor holding an illiquid, alpha generating fund and hedging his position with a liquid futures contract. When the investor is subject to a drawdown constraint, he is forced to reduce the total risk of his portfolio after a drawdown. In this case, he faces a tradeoff of either paying the transaction costs and deleveraging, or keeping his current position in the illiquid instrument and hedging away some of the risk while keeping the residual, unhedgeable risk on his balance sheet. We explicitly characterize this tradeoff and study its dependence on asset characteristics. In particular, we show that higher alpha and lower beta typically widen the no-trading zone, while the impact of volatility is ambiguous.
C61|Decision Rules for Precautionary and Retirement Savings|We report results from an experiment that compares precautionary savings behavior with retirement savings behavior. We ﬁnd that more than 30% of precautionary savings behavior can be categorized as optimal or near optimal, while virtually all of this behavior disappears in favor of simple decision rules that specify constant consumption in each period when retirement savings is added as a motive. We discuss the the costs and beneﬁts of these rules, which make a complex decision-making environment manageable. Our experiment is the ﬁrst to identify how decision-making changes when agents are required to save for retirement.
C61|A Laboratory Study of Nudge with Retirement Savings|We report results from an on-line economics experiment that examines the eﬀect of nudging retirement savings decisions. In the experiments, participants make decisions in a ﬁnitely repeated retirement savings game, in which income during working years is uncertain, and retire-ment age is known. Participants, who are household ﬁnancial decision-makers, are nudged with automatic savings in each period of the game. We ﬁnd that that the nudge simply replaced nat-ural decision-making observed in the absence of a nudge in this experiment, even to the extent that it resulted in nearly identical inferred decision rules. This surprising result highlights the unpredictability of the eﬀect of nudging human behavior.
C61|Retos asociados al uso del suelo: El dilema entre conservación y producción en los Bosques Andinos de Robles del Corredor Ecológico Guantiva-La Rusia-Iguaque|Preservar los Bosques Andinos de Robles es de gran importancia dado su valor intrínseco y los servicios ecosistémicos ofrecidos que favorecen el bienestar humano. No obstante, las prácticas de ganadería extensiva en el territorio nacional generan presión sobre su existencia. Aplicando un modelo de optimización dinámica que considera el valor del bosque por diversos servicios ecosistémicos, se busca entender el trade-off entre conservación y ganadería, así como las complejas relaciones entre los hombres y la naturaleza. De acuerdo a los resultados, debería conservarse entre el 62.7% y 99.4% del bosque dependiendo de la magnitud y valores de los parámetros. Finalmente, este documento busca aportar a la discusión sobre recomendaciones de política que favorezcan la conservación de los ecosistemas boscosos.
C61|Dynamic Macroeconomics: A Didactic Numeric Model|Teaching Dynamic Macroeconomics at undergraduate courses relies exclusively on intuitive prose and graphics depicting behaviours and steady states of the main markets of the economy. But when the case of forward-looking agents and the macroeconomic implications of their actions are discussed, intuitions and graphical representations offered to students may lead to unsupported conclusions. This happens even if the teacher and students use the chapter upon a dynamic macroeconomic model of one of the most didactic and ordered texts ever published: Williamson (2014). In this paper we try to sustain this assertion.
C61|Economic Decision Problems in Multi-Level Flood Prevention: a new graph-based approach used for real world applications|Flood prevention policy is of crucial importance to the Netherlands. We assess economical optimal flood prevention where multiple barrier dams and dikes protect the hinterland against sea level rise and peak river discharges. Current optimal flood prevention methods only consider dike rings with no dependencies between dikes. We propose a graph-based model for a cost-benefit analysis to determine optimal dike heights with multiple dependencies between dikes and barrier dams. Our model provides great flexibility towards the modelling of flood probabilities, damage costs and investments cost. Moreover, our model is easy to implement and can be solved quickly to proven optimality. Our model is developed for and applied to important policy decisions in the Netherlands for the Lake IJssel and Lake Marken region. Two barrier dams together with the dikes surrounding these two lakes protect a large part of the Netherlands. The area contains 17 dike ring areas, including the City of Amsterdam. Our model and application shows the importance of taking into account dependencies between dikes and barrier dams. The results of our model were used for major Dutch flood protection policy decisions, i.e. the decision how to control the water levels in Lake IJssel and Lake Marken and what economic efficient flood protection standards apply to barrier dams and dikes. Dependencies between barrier dams and dike rings have a large impact on economically optimal flood standards. On the basis of our model, the Dutch government has decided not to increase the water level of Lake IJssel with up to 1,5 meter. This saved both the current landscape around Lake IJssel and billions of euros in coming decades.
C61|Managerial Spillovers in Project Selection|Selecting investment or research projects is a general managerial decision, which ranges from managing the portfolio of R&D or marketing campaigns within a company, to determining the very boundaries of the firm -- which units or divisions to encompass, what acquisitions or alliances to pursue. Projects are typically assessed individually. However, the different divisions of a firm share managerial resources, so managers can transfer successful practices from one unit to another unit within the firm, or to different firms within their portfolio. This introduces managerial spillovers, so the value of a portfolio is higher than the aggregate value of the isolated projects. In this paper, we analyze the problem of selecting projects in the presence of managerial spillovers, and provide a simple algorithm that implements its solution. We find that, while a project yielding negative marginal profit can be safely discarded, it may be profitable to pass on multiple projects at once even if some of them yield positive marginal profit. Thus, ignoring the spillovers across projects and focusing on marginal profit can lead to excessively diversified firms or economies, as opposed to firms or economies with fewer (albeit possibly larger-scale) projects.
C61|Matlab, Python, Julia: What to Choose in Economics?|We perform a comparison of Matlab, Python and Julia as programming languages to be used for implementing global nonlinear solution techniques. We consider two popular applications: a neoclassical growth model and a new Keynesian model. The goal of our analysis is twofold: First, it is aimed at helping researchers in economics to choose the programming language that is best suited to their applications and, if needed, help them transit from one programming language to another. Second, our collections of routines can be viewed as a toolbox with a special emphasis on techniques for dealing with high dimensional economic problems. We provide the routines in the three languages for constructing random and quasi-random grids, low-cost monomial integration, various global solution methods, routines for checking the accuracy of the solutions, etc. Our global solution methods are not only accurate but also fast. Solving a new Keynesian model with eight state variables only takes a few seconds, even in the presence of active zero lower bound on nominal interest rates. This speed is important because it then allows the model to be solved repeatedly as one would require in order to do estimation.
C61|Forward Guidance: Is It Useful After the Crisis?|During recent economic crisis, when nominal interest rates were at their effective lower bounds, central banks used forward guidance -- announcements about future policy rates -- to conduct their monetary policy. Many policymakers believe that forward guidance will remain in use after the end of the crisis, however, there is uncertainty about its effectiveness. In this paper, we study the impact of forward guidance in a stylized new Keynesian economy away from the effective lower bound on nominal interest rates. Using closed-form solutions, we show that the impact of forward guidance on the economy depends critically on a specific monetary policy rule, ranging from non-existing to immediate and unrealistically large, the so-called forward guidance puzzle. We show that the puzzle occurs under very special -- empirically implausible and socially suboptimal -- monetary policy rules, whereas empirically relevant Taylor rules lead to sensible implications.
C61|Continuous Time Versus Discrete Time in the New Keynesian Model: Closed-Form Solutions and Implications for Liquidity Trap|Economists often use interchangeably the discrete- and continuous-time versions of the Keynesian model. In the paper, I ask whether or not the two versions effectively lead to the same implications. I analyze several alternative monetary policies, including a Taylor rule, discretionary interest rate choice and forward guidance. I show that the answer depends on a specific scenario and parameterization considered. In particular, in the presence of liquidity trap, the discrete-time analysis helps overcome some negative implications of the continuous-time model, such as excessively strong impact of price stickiness on inflation and output, unrealistically large government multipliers, as well as implausibly large effects of forward guidance.
C61|A new solution to an old problem: a temporary equilibrium version of the Ramsey model|Convergence toward the optimal capital accumulation path in infinite horizon has always been tackled in the literature by means of the assumption that individuals (or a central planner) are able to select the unique convergent (saddle-)path among the infinitely many paths which satisfy the equi-marginality condition of the intertemporal choice problem (the Euler's condition). This is tantamount to assuming that individuals have 'colossal' rational capabilities. Conversely, any minor deviation from the saddle-path would inevitably lead to a crash on a 0 per-capita consumption path. This paper aims to show that this contraposition is false. An asymptotic convergence result to the optimal equilibrium path will be obtained for an individual who plans myopically, that is, that optimizes his present and future consumption levels under a rudimentary hypothesis about future savings. He then partially readjusts his choices in each subsequent period, like people normally do. A similar result was already proved by the author for the central planner problem. In this paper, a 'market' solution is provided, following a temporary equilibrium approach à la Hicks.
C61|Relationships between the stochastic discount factor and the optimal omega ratio|The omega ratio is an interesting performance measure because it fo- cuses on both downside losses and upside gains, and nancial markets are re ecting more and more asymmetry and heavy tails. This paper focuses on the omega ratio optimization in general Banach spaces, which applies for both in nite dimensional approaches related to continuous time stochastic pricing models (Black and Scholes, stochastic volatility, etc.) and more classical problems in portfolio selection. New algorithms will be provided, as well as Fritz John-like and Karush-Kuhn-Tucker-like optimality conditions and duality results, despite the fact that omega is neither di¤er- entiable nor convex. The optimality conditions will be applied to the most important pricing models of Financial Mathematics, and it will be shown that the optimal value of omega only depends on the upper and lower bounds of the pricing model stochastic discount factor. In particular, if the stochastic discount factor is unbounded (Black and Scholes, Heston, etc.) then the optimal omega ratio becomes unbounded too (it may tend to in nity), and the introduction of several nancial constraints does not overcome this caveat. The new algorithms and optimality conditions will also apply to optimize omega in static frameworks, and it will be illustrated that both in nite- and nite-dimensional approaches may be useful to this purpose.
C61|Golden options in financial mathematics|This paper deals with the construction of smooth good deals (SGD), i.e., sequences of self- nancing strategies whose global risk diverges to ∞ and such that every security in every strategy of the sequence is a smooth derivative with a bounded delta. If the selected risk measure is the value at risk then these sequences exist under quite weak conditions, since one can involve risks with both bounded and unbounded expectation, as well as non-friction-free pricing rules. Moreover, every strategy in the sequence is composed of an European option plus a position in a riskless asset. The strike of the option is easily computed in practice, and the ideas may also apply in some actuarial problems such as the selection of an optimal reinsurance contract. If the chosen risk measure is a coherent one then the general setting is more limited. Indeed, though frictions are still accepted, expectations and variances must remain nite. The existence of SGDs will be characterized, and computational issues will be properly addressed as well. It will be shown that SGDs often exist, and for the conditional value at risk they are composed of the riskless asset plus easily replicable European puts. Numerical experiments will be presented in all of the studied cases.
C61|Supporting the Dance Sector. Does Efficiency Clash with Success When Programming?|Dance has received scant attention in economic literature perhaps because it is an activity that appears under a number of legal forms and diverse formats and due to the fact that â€“in many instances- it is affected by a high degree of instability time-wise, thus making it difficult to have the necessary data available required for economic analysis. Nevertheless, this sectorâ€™s dependence on public funding provides grounds for it to be the subject of performance related studies. This work aims to offer an evaluation process for a public project designed to spread dance in Spain. The project involves a number of stakeholders: public authorities, performance venues and dance companies. Each participant plays a role in the project: public authorities provide the funding while the theatres and dance companies offer the artistic idea that is taken to the audience. By applying Data Envelopment Analysis techniques (DEA), the present work seeks to evaluate the efficiency of the stakeholders involved in the project, as well as the efficiency of the programme itself by relating resources and objectives. We find that efficiency in resource performance often runs counter to other cultural aims such as increasing audiences or extending repertoire diversity.
C61|Increase-Decrease Game under Imperfect Competition in Two-stage Zonal Power Markets Part I: Concept Analysis|This paper is part I of a two-part paper. It proposes a two-stage game to analyze imperfect competition of producers in zonal power markets with a day-ahead and a real-time market. We consider strategic producers in both markets. They need to take both markets into account when deciding what to bid in each market. The demand shocks between these markets are modeled by several scenarios. The two-stage game is formulated as a Twostage Stochastic Equilibrium Problem with Equilibrium Constraints (TS-EPEC). Then it is further reformulated as a two-stage stochastic Mixed-Integer Linear Program (MILP). The solution of this MILP gives the Subgame Perfect Nash Equilibrium (SPNE). To tackle multiple SPNE, we design a procedure which _nds all SPNE with di_erent total dispatch costs. The proposed MILP model is solved using Benders decomposition embedded in the CPLEX solver. The proposed MILP model is demonstrated on the 6-node and the IEEE 30-node example systems.
C61|Increase-Decrease Game under Imperfect Competition in Two-stage Zonal Power Markets Part II: Solution Algorithm|In part I of this paper, we proposed a Mixed-Integer Linear Program (MILP) to analyse imperfect competition of oligopoly producers in two-stage zonal power markets. In part II of this paper, we propose a solution algorithm which decomposes the proposed MILP model into several subproblems and solve them in parallel and iteratively. Our solution algorithm reduces the solution time of the MILP model and it allows us to analyze largescale examples. To tackle the multiple Subgame Perfect Nash Equilibria (SPNE) situation, we propose a SPNE-band approach. The SPNE band is split into several subintervals and the proposed solution algorithm finds a representative SPNE in each subinterval. Each subinterval is independent from each other, so this structure enables us to use parallel computing. We also design a pre-feasibility test to identify the subintervals without SPNE. Our proposed solution algorithm and our SPNE-band approach are demonstrated on the 6-node and the modified IEEE 30-node example systems. The computational tractability of our solution algorithm is illustrated for the IEEE 118-node and 300-node systems.
C61|Unintended consequences: The snowball effect of energy communities|Following the development of decentralized generation and smart appliances, energy communities have become a phenomenon of increased interest. While the benefits of such communities have been discussed, there is increasing concern that inadequate grid tariffs may lead to excess adoption of such business models. Furthermore, snowball effects may be observed following the effects these communities have on grid tariffs. We show that restraining the study to a simple cost-benefit analysis is far from satisfactory. Therefore, we use the framework of cooperative game theory to take account of the ability of communities to share gains between members. The interaction between energy communities and the DSO then results in a non-cooperative equilibrium. We provide mathematical formulations and intuitions of such effects, and carry out realistic numerical applications where communities can invest jointly in solar panels and batteries. We show that such a snowball effect may be observed, but its magnitude and its welfare effects will depend on the grid tariff structure that is implemented, leading to possible PV over-investments.
C61|Simulation and Evaluation of Zonal Electricity Market Designs|Zonal pricing with countertrading (a market-based redispatch) gives arbitrage opportunities to the power producers located in the export-constrained nodes. They can increase their profit by increasing the output in the dayahead market and decrease it in the real-time market (the inc-dec game). We show that this leads to large inefficiencies in a standard zonal market. We also show how the inefficiencies can be significantly mitigated by changing the design of the real-time market. We consider a two-stage game with oligopoly producers, wind-power shocks and real-time shocks. The game is formulated as a two-stage stochastic equilibrium problem with equilibrium constraints (EPEC), which we recast into a two-stage stochastic Mixed-Integer Bilinear Program (MIBLP). We present numerical results for a six-node and the IEEE 24-node system.
C61|Variational Bayes inference in high-dimensional time-varying parameter models|This paper proposes a mean field variational Bayes algorithm for efficient posterior and predictive inference in time-varying parameter models. Our approach involves: i) computationally trivial Kalman filter updates of regression coefficients, ii) a dynamic variable selection prior that removes irrelevant variables in each time period, and iii) a fast approximate state-space estimator of the regression volatility parameter. In an exercise involving simulated data we evaluate the new algorithm numerically and establish its computational advantages. Using macroeconomic data for the US we find that regression models that combine time-varying parameters with the information in many predictors have the potential to improve forecasts over a number of alternatives.
C61|The Economics of Renewable Energy Support|This paper uses theoretical and numerical economic equilibrium models to examine optimal renewable energy (RE) support policies for wind and solar resources in the presence of a carbon externality associated with the use of fossil fuels. We emphasize three main issues for policy design: the heterogeneity of intermittent natural resources, budget-neutral financing rules, and incentives for carbon mitigation. We find that differentiated subsidies for wind and solar, while being optimal, only yield negligible efficiency gains. Policies with smart financing of RE subsidies which either relax budget neutrality or use “polluter-pays-the-price” financing in the context of budget-neutral schemes can, however, approximate socially optimal outcomes. Our analysis suggests that optimally designed RE support policies do not necessarily have to be viewed as a costly second-best option when carbon pricing is unavailable.
C61|A dynamic model of recycling with endogenous technological breakthrough|We present a general equilibrium growth model in which the use of a non renewable resource yields waste. Recycling waste produces materials of poor quality. These materials can be reused for production only once a dedicated R&D activity has made their quality reach a certain minimum threshold. The economy then switches to a fully recycling regime. We refer to this switch as the technological breakthrough. We analyze the optimal trajectories of the economy and interpret the Ramsey-Keynes and Hotelling conditions in this specific context. We characterize the determinants of the date of the breakthrough, which is endogenous, as well as the discontinuity in the variables' paths that is induced by this breakthrough. We show, in particular, that the availability of a recycling technology leads to an over-exploitation of the resource and possibly to lower levels of consumption before the breakthrough. We also find that the breakthrough can have a negative impact on utility over a finite period.
C61|Impact of German Energiewende on transmission lines in the Central European region|The impacts of renewable energy production and German nuclear phase-out on the electricity transmission systems in Central Europe is investigated with focus on the disparity between the growth of renewable production and the pace at which new electricity transmission lines have been built, especially in Germany. This imbalance endangers the system stability and reliability in the whole region. The assessment of these impacts on the transmission grid is analysed by the direct current load flow model ELMOD. Two scenarios for the year 2025 are evaluated from different perspectives. The distribution of loads in the grids is shown. Hourly patterns are analysed. Geographical decomposition is made, and problematic regions are identified. The high solar or wind power generation decrease the periods of very low transmission load and increase the mid- and high load on the transmission lines. High solar feed-in has less detrimental impacts on the transmission grid than high wind feed-in. High wind feed-in burdens the transmission lines in the north-south direction in Germany and water-pump-storage areas in Austria.
C61|A Variational Approach to Network Games|This paper studies strategic interaction in networks. We focus on games of strategic substitutes and strategic complements, and departing from previous literature, we do not assume particular functional forms on players' payoffs. By exploiting variational methods, we show that the uniqueness, the comparative statics, and the approximation of a Nash equilibrium are determined by a precise relationship between the lowest eigenvalue of the network, a measure of players' payoff concavity, and a parameter capturing the strength of the strategic interaction among players. We apply our framework to the study of aggregative network games, games of mixed interactions, and Bayesian network games.
C61|Does Smooth Ambiguity Matter for Asset Pricing?|We use the Bayesian method introduced by Gallant and McCulloch (2009) to estimate consumption-based asset pricing models featuring smooth ambiguity preferences. We rely on semi-nonparametric estimation of a flexible auxiliary model in our structural estimation. Based on the market and aggregate consumption data, our estimation provides statistical support for asset pricing models with smooth ambiguity. Statistical model comparison shows that models with ambiguity, learning and time-varying volatility are preferred to the long-run risk model. We analyze asset pricing implications of the estimated models.
C61|Growth and Welfare Gains from Financial Integration Under Model Uncertainty|We build a robustness (RB) version of the Obstfeld (1994) model to study the effects of financial integration on growth and welfare. Our model can account for the empirically observed heterogeneity in the relationship between growth and volatility for different countries. The calibrated model shows that financial integration leads to significantly larger gains in growth and welfare for advanced countries than developing countries, with some developing countries experiencing growth and welfare loss in financial integration. Our analytical solutions help uncover the key mechanisms by which this happens.
C61|Contingent Debt and Performance Pricing in an Optimal Capital Structure Model with Financial Distress and Reorganization|Building on the trade-off between agency costs and monitoring costs, we develop a dynamic theory of optimal capital structure with financial distress and reorganization. Costly monitoring eliminates the agency friction and thus the risk of inefficient liquidation. Our key assumption is that monitoring cannot be applied instantaneously. Rather, transitions between agency and monitoring are subject to search frictions. In the optimal contract, the firm seeks a monitoring opportunity whenever it is financially distressed, i.e., when the risk of liquidation is high. If a monitoring opportunity arrives in time, the manager is dismissed, the capital structure is reorganized as in Chapter 11 bankruptcy, and the search for a new manager begins. In agency, an optimal capital structure consists of equity, long-term debt, contingent long-term debt, and a credit line with performance pricing. In financial distress, coupon payments to contingent debt are suspended but the interest rate on the credit line is stepped-up, which gives the firm simultaneously debt relief and a steep incentive to improve its financial position. An episode of distress can end with financial recovery, transition to bankruptcy reorganization, or liquidation.
C61|Testing productivity change, frontier shift, and efficiency change|Inference about productivity change over time based on data envelopment (DEA) has focused primarily on the Malmquist index and is based on asymptotic properties of the index. In this paper we propose a novel set of significance tests for DEA based productivity change measures based on permutations and accounting for the inherent correlations when panel data are observed. The tests are easily implementable and give exact significance probabilities as they are not based on asymptotic properties. Tests are formulated both for the geometric means of the Malmquist index, and also of its components, i.e. the frontier shift index and the eciency change index, which together enable analysis of not only the presence of differences, but also gives an indication of whether the productivity change is due to shifts in the frontiers and/or changes in the efficiency distributions. Simulation results show the power of, and suggest how to interpret the results of, the proposed tests. Finally, the tests are illustrated using a data set from the literature.
C61|Growth and Agglomeration in the Heterogeneous Space: A Generalized AK Approach|Abstract. We provide with an optimal growth spatio-temporal setting with capital accumulation and diffusion across space in order to study the link between economic growth triggered by capital spatio-temporal dynamics and agglomeration across space. We choose the simplest production function generating growth endogenously, the AK technology but in sharp contrast to the related literature which considers homogeneous space, we derive optimal location outcomes for any given space distributions for technology (through the productivity parameter A) and population. Beside the mathematical tour de force, we ultimately show that agglomeration may show up in our optimal growth with linear technology, its exact shape depending on the interaction of two main effects, a population dilution effect versus a technology space discrepancy effect.
C61|Spatial resource wars: A two region example|We develop a spatial resource model in continuous time in which two agents strategically exploit a mobile resource in a two-locations setup. In order to contrast the overexploitation of the resource (the tragedy of commons) that occurs when the player are free to choose where to fish/hunt/extract/harvest, the regulator can establish a series of spatially structured policies. We compare the three situations in which the regulator: (a) leaves the player free to choose where to harvest; (b) establishes a natural reserve where nobody is allowed to harvest; (c) assigns to each player a specific exclusive location to hunt. We show that when preference parameters dictate a low harvesting intensity, the policies cannot mitigate the overexploitation and in addition they worsen the utilities of the players. Conversely, in a context of harsher harvesting intensity, the intervention can help to safeguard the resource, preventing the extinction and also improving the welfare of both players.
C61|Geographic Environmental Kuznets Curves: The Optimal Growth Linear-Quadratic Case|We solve a linear-quadratic model of a spatio-temporal economy using a polluting one-input technology. Space is continuous and heterogenous: locations differ in productivity, nature self-cleaning technology and environmental awareness. The unique link between locations is transboundary pollution which is modelled as a PDE diffusion equation. The spatio-temporal functional is quadratic in local consumption and linear in pollution. Using a dynamic programming method adapted to our infinite dimensional setting, we solve the associated optimal control problem in closed-form and identify the asymptotic (optimal) spatial distribution of pollution. We show that optimal emissions will decrease at given location if and only if local productivity is larger than a threshold which depends both on the local pollution absorption capacity and environmental awareness. Furthermore, we numerically explore the relationship between the spatial optimal distributions of production and (asymptotic) pollution in order to uncover possible (geographic) Environmental Kuznets Curve cases.
C61|Contracting in a Continuous-Time Model with Three-Sided Moral Hazard and Cost Synergies| We study optimal effort and compensation in a continuous-time model with threesided moral hazard and cost synergies. One agent exerts initial effort to start the project; the other two agents exert ongoing effort to manage it. The project generates cash flow at a fixed rate over its lifespan; cash flow stops if a failure occurs. The three agents’ efforts jointly determine the probability of the project’s survival and thus its expected cash flows. We model cost synergies between the two agents exerting ongoing effort as one’s effort reduces the other’s cost of effort. In the optimal contract, the timing of payments reflects the timing of efforts as well as cost synergies across agents. The agent exerting upfront effort claims all cash flows prior to a predetermined cutoff date, and the two agents exerting ongoing effort divide all subsequent cash flows. Delaying payments motivate these two agents to work hard throughout. Between them, the agent with greater degree of moral hazard and bigger impact on reducing the other agent’s cost claims a larger fraction of the cash flow. Our study sheds light on a broad set of contracting problems, such as compensation plans in startups and profit sharing among business partners.
C61|Delegated Project Search| This paper explores a new continuous-time principal-agent problem for a firm with both moral hazard and adverse selection. Adverse selection appears at random times. The agent finds projects sequentially by exerting costly effort. Each project brings output to the firm, subject to the agent’s private shocks. These serial shocks are i.i.d and independent of the arrival time of new projects and the agent’s efforts. The shocks and efforts constitute the agent’s asymmetric information. We provide a full characterization of optimal contracts in which moral hazard effect and adverse selection effects interact. The second-best contract with moral hazard can achieve first-best efficiency, and third-best contract with the moral hazard and adverse selection can achieve second-best efficiency under pure adverse selection, if the agent is expectably rich enough. The payment is front-loaded under pure moral hazard. When moral hazard is combined with adverse selection, the payment can be backloaded or front-loaded, depending on the level of expectable wealth.
