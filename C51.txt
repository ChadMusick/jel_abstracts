C51|Consistency and asymptotic normality of maximum likelihood estimators of a multiplicative time-varying smooth transition correlation GARCH model|A new multivariate volatility model that belongs to the family of conditional correlation GARCH models is introduced. The GARCH equations of this model contain a multiplicative deterministic component to describe long-run movements in volatility and, in addition, the correlations are deterministically time-varying. Parameters of the model are estimated jointly using maximum likelihood. Consistency and asymptotic normality of maximum likelihood estimators is proved. Numerical aspects of the estimation algorithm are discussed. A bivariate empirical example is provided.
C51|Transition from the Taylor rule to the zero lower bound|This paper examines the Taylor rule in the context of United States monetary policy since 1965, particularly with respect to the zero-lower-bound era of the federal funds rate from 2009 to 2016. A nonlinear Taylor rule is developed which features smooth transitions in the first two moments of the federal funds rate. This exible specification is found to usefully capture observed nonlinearity, while accounting for the well-documented structural changes in monetary policy formation at the Federal Reserve in the last fifty years, and especially in the recent zero-lower-bound era.
C51|Nonlinear models in macroeconometrics|This article contains a short review of nonlinear models that are applied to modelling macroeconomic time series. Brief descriptions of relevant models, both univariate, dynamic single-equation, and vector autoregressive ones are presented. Their application is illuminated by a number of selected examples.
C51|Forecasting dynamically asymmetric fluctuations of the U.S. business cycle|The generalized smooth transition autoregression (GSTAR) parametrizes the joint asymmetry in the duration and length of cycles in macroeconomic time series by using particular generalizations of the logistic function. The symmetric smooth transition and linear autoregressions are nested in the GSTAR. A test for the null hypothesis of dynamic symmetry is presented. Two case studies indicate that dynamic asymmetry is a key feature of the U.S. economy. The GSTAR model beats its competitors for point forecasting, but this superiority becomes less evident for density forecasting and in uncertain forecasting environments.
C51|The Qualitative Expectations Hypothesis: Model Ambiguity, Consistent Representations Of Market Forecasts, And Sentiment|We introduce the Qualitative Expectations Hypothesis (QEH) as a new approach to modeling macroeconomic and financial outcomes. Building on John Muth's seminal insight underpinning the Rational Expectations Hypothesis (REH), QEH represents the market's forecasts to be consistent with the predictions of an economist's model. However, by assuming that outcomes lie within stochastic intervals, QEH, unlike REH, recognizes the ambiguity faced by an economist and market participants alike. Moreover, QEH leaves the model open to ambiguity by not specifying a mechanism determining specific values that outcomes take within these intervals. In order to examine a QEH model's empirical relevance, we formulate and estimate its statistical analog based on simulated data. We show that the proposed statistical model adequately represents an illustrative sample from the QEH model. We also illustrate how estimates of the statistical model's parameters can be used to assess the QEH model's qualitative implications.
C51|Models with Multiplicative Decomposition of Conditional Variances and Correlations|Univariate and multivariate GARCH type models with multiplicative decomposition of the variance to short and long run components are surveyed. The latter component can be either deterministic or stochastic. Examples of both types are studied.
C51|The Shifting Seasonal Mean Autoregressive Model and Seasonality in the Central England Monthly Temperature Series, 1772-2016|In this paper we introduce an autoregressive model with seasonal dummy variables in which coefficients of seasonal dummies vary smoothly and deterministically over time. The error variance of the model is seasonally heteroskedastic and multiplicatively decomposed, the decomposition being similar to that in well known ARCH and GARCH models. This variance is also allowed to be smoothly and deterministically time-varying. Under regularity conditions, consistency and asymptotic normality of the maximum likelihood estimators of parameters of this model is proved. A test of constancy of the seasonal coefficients is derived. The test is generalised to specifying the parametric structure of the model. A test of constancy over time of the heteroskedastic error variance is presented. The purpose of building this model is to use it for describing changing seasonality in the well-known monthly central England temperature series. More specifically, the idea is to find out in which way and by how much the monthly temperatures are varying over time during the period of more than 240 years, if they do. Misspecification tests are applied to the estimated model and the findings discussed.
C51|State-Space Models on the Stiefel Manifold with a New Approach to Nonlinear Filtering|We develop novel multivariate state-space models wherein the latent states evolve on the Stiefel manifold and follow a conditional matrix Langevin distribution. The latent states correspond to time-varying reduced rank parameter matrices, like the loadings in dynamic factor models and the parameters of cointegrating relations in vector error-correction models. The corresponding nonlinear filtering algorithms are developed and evaluated by means of simulation experiments.
C51|State-dependent Hawkes processes and their application to limit order book modelling|We study statistical aspects of state-dependent Hawkes processes, which are an extension of Hawkes processes where a self- and cross-exciting counting process and a state process are fully coupled, interacting with each other. The excitation kernel of the counting process depends on the state process that, reciprocally, switches state when there is an event in the counting process. We first establish the existence and uniqueness of state-dependent Hawkes processes and explain how they can be simulated. Then we develop maximum likelihood estimation methodology for parametric specifications of the process. We apply state-dependent Hawkes processes to high-frequency limit order book data, allowing us to build a novel model that captures the feedback loop between the order flow and the shape of the limit order book. We estimate two specifications of the model, using the bid-ask spread and the queue imbalance as state variables, and find that excitation effects in the order flow are strongly state-dependent. Additionally, we find that the endogeneity of the order flow, measured by the magnitude of excitation, is also state-dependent, being more pronounced in disequilibrium states of the limit order book. MSC 2010 Classification: 60G55, 62M09, 62P05
C51|The Importance Of Punishment Substitutability In Criminometric Studies|This study investigates the role of punishment substitutability in the empirical estimation of the economic model of crime. Using a dynamic panel data model fitted to a panel of Local Government Areas in New South Wales, Australia, we evaluate the effects of financial penalties and imprisonment on the crime rate. Our results show that crime is clearly a dynamic phenomenon, and that failure to incorporate both financial penalties and imprisonment can lead to a misspecified model. Furthermore, our results vary significantly for different crime categories, highlighting the importance of analysing specific crime categories separately.
C51|Multivariate Filter Estimation of Potential Output for the United States: An Extension with Labor Market Hysteresis|This paper extends the multivariate filter approach of estimating potential output developed by Alichi and others (2018) to incorporate labor market hysteresis. This extension captures the idea that long and deep recessions (expansions) cause persistent damage (improvement) to the labor market, thereby reducing (increasing) potential output. Applying the model to U.S. data results in significantly smaller estimates of output gaps, and higher estimates of the NAIRU, after the global financial crisis, compared to estimates without hysteresis. The smaller output gaps partly explain the absence of persistent deflation despite the slow recovery during 2010-2017. Going forward, if strong growth performance continues well beyond 2018, hysteresis is expected to result in a structural improvement in growth and employment.
C51|The Institutional Economics of Collective Waste Recovery Systems: an Empirical Investigation|The main purpose of the study is to develop the model for transaction costs measurement in the Collective Waste Recovery Systems. The methodology of New Institutional Economics is used in the research. The impact of the study is related both to the enlargement of the limits of the theory about the interaction between transaction costs and social costs and to the identification of institutional failures of the EU concept for the circular economy. A new model for social costs measurement is developed.
C51|Multivariate Fractional Components Analysis|We propose a setup for fractionally cointegrated time series which is formulated in terms of latent integrated and short-memory components. It accommodates nonstationary processes with different fractional orders and cointegration of different strengths and is applicable in high-dimensional settings. In an application to realized covariance matrices, we find that orthogonal short- and long-memory components provide a reasonable fit and competitive out-of-sample performance compared to several competing methods.
C51|Explaining the Interplay Between Merchant Acceptance and Consumer Adoption in Two-Sided Markets for Payment Methods|Recent consumer and merchant surveys show a decrease in the use of cash at point-of-sale (POS). Increasingly, consumers and merchants have access to a growing array of payment innovations as substitutes for cash. The market for payments is two-sided, meaning that a method of payment can be used only if both consumers and merchants adopt/accept it. This paper develops a model to assess how innovations affect consumers’ adoption and merchants’ acceptance of various payment instruments, and their usage patterns at the POS. We model this interdependence in two stages. First, consumers and merchants decide on which methods of payment to adopt and accept, respectively. Second, consumers and merchants meet at the POS, and the consumer chooses their preferred method of payment given what the merchant accepts. Estimates from our model suggest that both sides of the market can benefit from accepting all means of payment. Further, our model predicts that merchants are much more sensitive to an increase in their payment costs than consumers. We use our model to predict what would happen under three scenarios. First, increasing merchants’ cost of using credit cards would significantly reduce merchant acceptance of this payment instrument in favour of debit. Second, the cost of using cash would have to increase substantially on both sides of the market for cash usage to fall below 1 percent of transaction volume. Finally, even if all consumers/merchants adopted/accepted all methods of payment, cash would fall but would remain at 20 percent of transaction volume. These findings suggest a completely cashless society is unlikely in the foreseeable future.
C51|The Simple Economics of Global Fuel Consumption|This paper presents a structural framework of the global oil market that relies on information on global fuel consumption to identify flow demand for oil. We show that under mild identifying assumptions, data on global fuel consumption help to provide comparatively sharp insights on elasticities and other key structural parameters of the global oil market. The estimated elasticity of global fuel demand in the short run with respect to crude oil prices is around -2 percent, which is considerably more inelastic than estimates of local fuel demand elasticities based on disaggregated data. Our framework is particularly suitable for understanding the evolution of quantities in the global oil market and provides new evidence on the magnitude of different types of oil price shocks and their macroeconomic and environmental impacts.
C51|The Exchange Rate and Oil Prices in Colombia: A High Frequency Analysis|We study the relationship between daily oil prices and nominal exchange rates between 1995 and 2019 in Colombia through a Time-Varying Vector Auto-Regressions with residual Stochastic Volatility, TV-VAR-SV, model. For this task we also employ cointegration, Univariate Auto-Regressions with residual Stochastic Volatility, UAR-SVTV, and De-trended Cross Correlation, DCC analyses. We found that a stable longrun relationship between the two processes is lacking. We also found significant time variation in residual volatility and co-volatility. More specifically, we found that both periods of time, the international financial crisis and the oil price drop of 2015, behave conspicuously different from other “more normal” times. These results are consistent with a shift in the features of the DCC at the start of the crisis. Before the crises the DCCs are positive but weak for different windows sizes, turning negative and significant after it. The latter DCCs and their significance increase with the window size. These results are concurrent, also, with two clearly differentiated periods of time; one when oil production was not financially feasible, and thus production, exports and oil related currency inflows were small, and the other when oil production became feasible because of the price increase, which led to a boom in exploration, production, exports and oil related currency inflows. **** RESUMEN: Estudiamos la evolución de la relación entre los precios diarios del petróleo y la tasa de cambio nominal Colombiana entre 1995 y 2019 a través de un modelo de Vectores Auto-Regresivos Tiempo-Variantes con Volatilidad Estocástica Residual. Para esto empleamos también técnicas de co-integración, Auto-Regresiones Univariadas con Volatilidad Estocástica Residual, y Correlaciones Cruzadas Des-tendeciadas. Se encontró que no existe una relación estable de largo plazo entre estos dos procesos. También hallamos evidencia de variación temporal de la volatilidad y co-volatilidad residual. Más específicamente, encontramos que tanto la crisis financiera global como la reducción de los precios de petróleo de 2015 son periodos particularmente distintos de otros periodos “más normales”. Estos resultados son consistentes con un cambio en el comportamiento de las DCC al inicio de la crisis financiera de 2008. En efecto, antes de la crisis estas correlaciones eran positivas pero poco significativas para diferentes tama˜nos de la ventana de estimación, pero después de la crisis se tornaron negativas y significancias. Las últimas DCCs y su significancia se incrementaron a mayores tama˜nos de la ventana. Estos resultados coinciden con dos periodos de tiempo claramente diferenciados en Colombia, uno en el cual la producción petrolera no era financieramente factible, y en consecuencia la producción, exportación y flujos entrantes de divisas por petróleo eran pequeños, y otro donde la producción fue factible, conduciendo a un boom en la exploración, producción, exportación y en los flujos entrantes de divisas relacionados con petróleo.
C51|French Households’ Portfolio: The Financial Almost Ideal Demand System Appraisal|Over the last decades, the composition of financial wealth of French households has dramatically changed. We seek explanatory factors for these changes by estimating an extended version of Deaton and Muellbauer (1980) model applied to French households’ portfolio choices. We find that most of the estimated parameters of the benchmark model are in line with economic priors. In particular, wealth and real returns are the key determinants of the long run dynamics of the different asset shares in the portfolio. We use the model to simulate the effect on French households’ portfolio allocation for the replacement in 2018 of the various tax regimes of most financial products with a flat tax on savings income. We find that the flat tax should support investment in equities at the expense of life insurance contracts.
C51|Time varying cointegration and the UK Great Ratios|We re-examine the great ratios associated with balanced growth models and ask whether they have remained constant over time. We first use a benchmark DSGE model to explore how plausible smooth variations in structural parameters lead to movements in great ratios that are comparable to those seen in the UK data. We then employ a nonparametric methodology that allows for slowly varying coefficients to estimate trends over time. To formally test for stable relationships in the great ratios, we propose a statistical test based on these nonparametric estimators devised to detect time varying cointegrating relationships. Small sample properties of the test are explored in a small Monte Carlo exercise. Generally, we find no evidence for cointegration when parameters are constant, but strong evidence when allowing for time variation. The implications are that in macroeconometric models allowance should be made for shifting long-run relationships, including DSGE models where smooth variation should be allowed in the deep structural relationships.
C51|Should we care? : The economic effects of financial sanctions on the Russian economy|We employ a Bayesian VAR model to estimate the economic effects on the Russian economy from Western financial sanctions imposed in 2014. Sanctions caused a decrease in the amount of out-standing Russian corporate external debt, but it occurred during an episode of falling oil prices. We disentangle the effects of sanctions and oil prices by computing out-of-sample projections of key Russian macroeconomic variables conditioned solely on the oil price drop and on both the oil price drop and external debt deleveraging. Declining oil prices alone do not explain the depth of economic crisis in Russia, but we get rather accurate conditional forecasts when the actual path of external debt deleveraging is added. We treat the difference between these two projections as the effect of sanctions against Russia. The effect is modest, yet significant, for most of the variables discussed. While our estimate of the impact of sanctions on GDP growth has large uncertainty, over two-thirds of the density lies in the negative area.
C51|Assessing U.S. Aggregate Fluctuations Across Time and Frequencies|We study the behavior of key macroeconomic variables in the time and frequency domain. For this purpose, we decompose U.S. time series into various frequency components. This allows us to identify a set of stylized facts: GDP growth is largely a high-frequency phenomenon whereby inflation and nominal interest rates are characterized largely by low-frequency components. In contrast, unemployment is a medium-term phenomenon. We use these decompositions jointly in a structural VAR where we identify monetary policy shocks using a sign restriction approach. We find that monetary policy shocks affect these key variables in a broadly similar manner across all frequency bands. Finally, we assess the ability of standard DSGE models to replicate these findings. While the models generally capture low-frequency movements via stochastic trends and business-cycle fluctuations through various frictions, they fail at capturing the medium-term cycle.
C51|Non-performing loans, governance indicators and systemic liquidity risk: evidence from Greece|In this study we propose a new determinant of non-performing loans for the case of the Greek banking sector. We employ aggregate yearly data for the period 1996-2016 and we conduct a Principal Component Analysis for all the Worldwide Governance Indicators (WGI) for Greece, aiming to isolate the common component and thus to create the GOVERNANCE indicator. We find that the GOVERNANCE indicator is a significant determinant of Greek banks’ non-performing loans indicating that both political and governance factors impact on the level of the Greek non-performing loans. An additional variable that also has a statistically significant impact on the level of Greek non-performing loans, when combined with WGI in the dynamic specification of our model, is systemic liquidity risk. Our results could be of interest to policy makers and regulators as a macro prudential policy tool.
C51|A Structural Analysis of Vacancy Referrals with Imperfect Monitoring and Sickness Absence|In many OECD countries unemployment insurance agencies send out job vacancy referrals (VRs) to unemployment benefit recipients. Refusals to apply for VRs are sanctioned with temporary benefit reductions. In this paper we study the impact of VRs and sanctions on unemployed workers’ job search behavior, accounting for the possibility that workers may report sick to avoid sanctions. We develop a structural job search model that incorporates VRs, sanctions and sick reporting. We estimate our model using German administrative data from social security records that are linked to caseworker records on VRs, sick reporting and sanctions. Based on the estimated model we study a range of counterfactual policy scenarios. We find that increasing sanction enforcement reduces reservation wages, thereby leading to a higher job finding rate. Increasing the VR rate, in contrast, leads to higher reservation wages by raising the option value of search, but nevertheless elevates the job finding rate by increasing the job offer frequency. According to our estimates 9.2% of sick reports among unemployed workers are induced by VRs. We find substantial heterogeneity in the effects of eliminating VR induced sick reporting on job search outcomes. Effects are modest for around 75% of the population. For the remaining 25% of unemployed workers shutting down VR induced sick reporting reduces the mean unemployment duration by one week and a day.
C51|On Event Study Designs and Distributed-Lag Models: Equivalence, Generalization and Practical Implications|We discuss important features and pitfalls of panel-data event study designs. We derive the following main results: First, event study designs and distributed-lag models are numerically identical leading to the same parameter estimates after correct reparametrization. Second, binning of effect window endpoints allows identification of dynamic treatment effects even when no never-treated units are present. Third, classic dummy variable event study designs can be naturally generalized to models that account for multiple events of different sign and intensity of the treatment, which are particularly interesting for research in labor economics and public finance.
C51|Marginal Compensated Effects in Discrete Labor Supply Models|This paper develops analytic results for marginal compensated effects of discrete labor supply models, including Slutsky equations. It matters, when evaluating marginal compensated effects in discrete choice labor supply models, whether one considers wage increase (right marginal effects) or wage decrease (left marginal effects). We show how the results obtained can be used to calculate the marginal cost of public funds in the context of discrete labor supply models. Subsequently, we use the empirical labor supply model of Dagsvik and Strøm (2006) to compute numerical compensated (Hicksian) and uncompensated marginal (Marshallian) effects resulting from wage changes. The mean Hicksian labor supply elasticities are larger than the Marshallian, but the difference is small.
C51|Intratemporal Nonseparability between Housing and Nondurable Consumption: Evidence from Reinvestment in Housing Stock|Using the data on maintenance expenditures and self-assessed house value, I separate the measure of individual housing stock and house prices, and use these data for testing whether nondurable consumption and housing are characterized by intratemporal nonseparability in households' preferences. I find evidence in favor of intratemporal dependence between total nondurable consumption and housing. I reach a similar conclusion for some separate consumption categories, such as food and utility services. My findings also indicate households are more willing to substitute housing and nondurable consumption within a period than to substitute composite consumption bundles over different time periods.
C51|A model for international spillovers to emerging markets|This paper develops a small open economy (SOE) dynamic stochastic general equilibrium (DSGE) model that helps to explain business cycle synchronization between an emerging market and advanced economies. The model captures the specificities of both economies (e.g. primary commodity, manufacturing, intermediate inputs, and credit) that are most relevant for understanding the importance as well as the transmission mechanisms of a wide range of domestic and foreign (supply, demand, monetary policy, credit, primary commodity) shocks facing an emerging economy. We estimate the model with Bayesian methods using quarterly data from South Africa, the US and G7 countries. In contrast to the predictions of standard SOE models, we are able to replicate two stylized facts. First, our model predicts a high degree of business cycle synchronization between South Africa and advanced economies. Second, the model is able to account for the influence of foreign shocks in South Africa. We are also able to demonstrate the specific roles these shocks played during key historical episodes such as the global financial crisis in 2008 and the commodity price slump in 2015. The ability of our framework to capture endogenous responses of commodity and financial sectors to structural shocks is crucial to identify the importance of these shocks in South Africa.
C51|A Flexible Regime Switching Model for Asset Returns|A non-Gaussian multivariate regime switching dynamic correlation model for fi nancial asset returns is proposed. It incorporates the multivariate generalized hyperbolic law for the conditional distribution of returns. All model parameters are estimated consistently using a new two-stage expectation-maximization algorithm that also allows for incorporation of shrinkage estimation via quasi-Bayesian priors. It is shown that use of Markov switching correlation dynamics not only leads to highly accurate risk forecasts, but also potentially reduces the regulatory capital requirements during periods of distress. In terms of portfolio performance, the new regime switching model delivers consistently higher Sharpe ratios and smaller losses than the equally weighted portfolio and all competing models. Finally, the regime forecasts are employed in a dynamic risk control strategy that avoids most losses during the fi nancial crisis and vastly improves risk-adjusted returns.
C51|Estimation of Large Dimensional Conditional Factor Models in Finance|This chapter provides an econometric methodology for inference in large-dimensional conditional factor models in finance. Changes in the business cycle and asset characteristics induce time variation in factor loadings and risk premia to be accounted for. The growing trend in the use of disaggregated data for individual securities motivates our focus on methodologies for a large number of assets. The beginning of the chapter outlines the concept of approximate factor structure in the presence of conditional information, and develops an arbitrage pricing theory for large-dimensional factor models in this framework. Then we distinguish between two different cases for inference depending on whether factors are observable or not. We focus on diagnosing model specification, estimating conditional risk premia, and testing asset pricing restrictions under increasing cross-sectional and time series dimensions. At the end of the chapter, we review some of the empirical findings and contrast analysis based on individual stocks and standard sets of portfolios. We also discuss the impact on computing time-varying cost of equity for a firm, and summarize differences between results for developed and emerging markets in an international setting.
C51|Efectos de las variaciones del IPC en las decisiones financieras|En este documento se desarrolló un análisis de mercado que derivó en un modelo econométrico con miras a determinar el comportamiento del Índice de Precios al Consumidor en un horizonte de tiempo de 2 años, para dar apoyo a la toma de decisiones financieras de inversión y financiamiento. En el análisis se tuvieron en cuenta las Encuestas a expertos y los Pronósticos a entidades financieras. Sin embargo, al enfrentar dicha información con el IPC observado, se concluyó que los pronósticos y encuestas mencionadas no tenían una capacidad de predicción a dos años confiable. Debido a que el mercado no permitió cumplir con el objetivo propuesto, fue necesario desarrollar un modelo econométrico de tipo ARIMA con datos mensuales desde entre enero de 2010 y diciembre de 2018. En la construcción del modelo se determinó que la volatilidad del IPC estaba fuertemente influida por el precio de los alimentos, y por ende serían el fenómeno del niño y los paros de transporte las variables idóneas en la conformación del modelo. Como resultado, se obtuvo una proyección del IPC a dos años. No obstante, el pronóstico presentó una desviación estándar considerable y creciente en el tiempo que redujo la efectividad del modelo a un año. En el desarrollo del modelo como paso a seguir, se plantea necesario realizar una función impulso respuesta de las variables Dummy y adaptar el modelo a la nueva metodología del IPC propuesta por el DANE para 2019. *** In this document, a market analysis was developed which derived in an econometric model with a view to determining the behaviour of the Consumer Price Index in a time horizon of 2 years, to give support to the financial decision making of investment and financing. The analysis took into account the Surveys to experts and the Forecasts to financial entities. However, when facing said information with the observed CPI, it was concluded that the forecasts and surveys mentioned did not have a reliable two-year prediction capacity. Since the market did not comply with the proposed objective, it was necessary to develop an ARIMA-type econometric model with monthly data from January 2010 to December 2018. In the construction of the model, it was determined that the volatility of the CPI was strongly influenced by the food price, and therefore the El Niño phenomenon and transport stoppages would be the ideal variables in shaping the model. As a result, a two-year CPI projection was obtained. However, the forecast presented a considerable and increasing standard deviation over time that reduced the effectiveness of the model to one year. In the development of the model as a step to follow, it is necessary to carry out a response impulse function of the Dummy variables and to adapt the model to the new CPI methodology proposed by DANE for 2019.
C51|On Event Study Designs and Distributed-Lag Models: Equivalence, Generalization and Practical Implications|We discuss important features and pitfalls of panel-data event study designs. We derive the following main results: First, event study designs and distributed-lag models are numerically identical leading to the same parameter estimates after correct reparametrization. Second, binning of effect window endpoints allows identification of dynamic treatment effects even when no never-treated units are present. Third, classic dummy variable event study designs can be naturally generalized to models that account for multiple events of different sign and intensity of the treatment, which are particularly interesting for research in labor economics and public finance.
C51|Identification versus misspecification in New Keynesian monetary policy models|In this paper, we study identification and misspecification problems in standard closed and open-economy empirical New-Keynesian DSGE models used in monetary policy analysis. We find that problems with model misspecification still appear to be a first-order issue in monetary DSGE models, and argue that it is problems with model misspecification that may benefit the most from moving from a classical to a Bayesian framework. We also argue that lack of identification should neither be ignored nor be assumed to affect all DSGE models. Fortunately, identification problems can be readily assessed on a case-by-case basis, by applying recently developed pre-tests of identification.
C51|Dealing with misspecification in structural macroeconometric models|We consider a set of potentially misspecified structural models, geometrically combine their likelihood functions, and estimate the parameters using composite methods. Composite estimators may be preferable to likelihood-based estimators in the mean squared error. Composite models may be superior to individual models in the Kullback-Leibler sense. We describe Bayesian quasi-posterior computations and compare the approach to Bayesian model averaging, finite mixture methods, and robustness procedures. We robustify inference using the composite posterior distribution of the parameters and the pool of models. We provide estimates of the marginal propensity to consume and evaluate the role of technology shocks for output fluctuations.
C51|The effect of observables, functional specifications, model features and shocks on identification in linearized DSGE models|Both the investment adjustment costs parameters in Kim (2003) and the monetary policy rule parameters in An & Schorfheide (2007) are locally not identifiable. We show means to dissolve this theoretical lack of identification by looking at (1) the set of observed variables, (2) functional specifications (level vs. growth costs, output-gap definition), (3) model features (capital utilization, partial inflation indexation), and (4) additional shocks (investment-specific technology, preference). Moreover, we discuss the effect of these changes on the strength of parameter identification from a Bayesian point of view. Our results indicate that researchers should treat parameter identification as a model property, i.e. from a model building perspective.
C51|Co-integration and common trends analysis with score-driven models : an application to the federal funds effective rate and US inflation rate|Co-integration and common trends are studied for time series variables, by introducing the new t-QVARMA (quasi-vector autoregressive moving average) model. t-QVARMA is an outlier-robust nonlinear score-driven model for the multivariate t-distribution. In t-QVARMA, the I(0) and I(1) components of the variables are separated in a way that is similar to the Granger-representation of VAR models. The relationship between the co-integrated federal funds effective rate and United States (US) inflation rate variables is studied for the period of July 1954 to January 2019. The in-sample statistical and out-of-sample forecasting performances of t-QVARMA are superior to those of the classical Gaussian-VAR model
C51|Investment Climate Effects on Alternative Firm-Level Productivity Measures|Developing countries are increasingly concerned about improving country competitiveness and productivity. Investment Climate surveys (ICs) at the firm level, are becoming the standard way for the World Bank to identify key obstacles to country competitiveness. This paper develops a general to specific econometric methodology, based on firm level observable fixed effects that generate robust investment climate effects (elasticities) on total factor productivity (TFP). By robust IC elasticities on TFP we mean elasticity estimates with equal signs and of similar magnitudes for several competing TFP measures. We apply this econometric methodology to the IC survey of Costa Rica showing how robust the investment climate effects are for several measures of TFP when conditioning on relevant plant-level information that is usually unobserved. For the economic evaluation we estimate the marginal effects of each IC variable on TFP as well as their IC impacts on average TFP obtaining important economic differences. These IC estimates are obtained from five blocks of IC variables, (i) infrastructure, (ii) red tape, corruption and crime, (iii) finance and corporate governance, (iv) quality, innovation and labor skills and (v) other control variables, could be used as benchmarks to assess cross-country IC assessments of TFP.
C51|Exploring option pricing and hedging via volatility asymmetry|This paper evaluates the application of two well-known asymmetric stochastic volatility (ASV) models to option price forecasting and dynamic delta hedging. They are specied in discrete time in contrast to the classical stochastic volatility (SV) models used in option pricing. There is some related literature, but little is known about the empirical implications of volatility asymmetry on option pricing. The objectives of this paper are to estimate ASV option pricing models using a Bayesian approach unknown in this type of literature, and to investigate the e ect of volatility asymmetry on option pricing for di erent size equity sectors and periods of volatility. Using the S&P MidCap 400 and S&P 500 European call option quotes, results show that volatility asymmetry benets the accuracy of option price forecasting and hedging cost e ectiveness in the large-cap equity sector. However, asymmetric SV models do not improve the option price forecasting and dynamic hedging in the mid-cap equity sector.
C51|Residual shape risk on natural gas market with mixed jump diffusion|This paper introduces residual shape risk as a new subclass of energy commodity risk. Residual shape risk is caused by insufficient liquidity of energy forward market when retail energy supplier has to hedge his short sales by a non-flexible standard baseload product available on wholesale market. Because of this inflexibility energy supplier is left with residual unhedged position which has to be closed at spot market. The residual shape risk is defined as a difference between spot and forward prices weighted by residual unhedged position which size depends on the shape of customers’ portfolio of a given retail energy supplier. We evaluated residual shape risk over the years 2014 - 2018 with a real portfolio of a leading natural gas retail supplier in the Czech Republic. The size of residual shape risk in our example corresponds approximately to 1 percent of profit margin of natural gas retail supplier.
C51|Implications of partial information for econometric modeling of macroeconomic systems|Representative models of the macroeconomy (RMs), such as DSGE models, frequently contain unobserved variables. A finite-order VAR representation in the observed variables may not exist, and therefore the impulse responses of the RMs and SVAR models may differ. We demonstrate this divergence often is: (i) not substantial; (ii) reflects the omission of stock variables from the VAR; and (iii) when the RM features I (1) variables can be ameliorated by estimating a latent-variable VECM. We show that DSGE models utilize identifying restrictions stemming from common factor dynamics reflecting statistical, not economic, assumptions. We analyze the use of measurement error, and demonstrate that it may result in unintended consequences, particularly in models featuring I (1) variables.
C51|Transmission of a resource boom: The case of Australia|This paper presents evidence on the macroeconomic adjustment of a resource-rich country to a resource boom using the effects of Chinese industrialisation on Australia from 1988 to 2016. An SVAR model is specified, incorporating a proxy for Chinese resource demand and commodity prices to identify the effects of commodity supply and demand shocks on the Australian macroeconomy. We develop a multivariate historical decomposition to show how resource sector shocks lead the economy to deviate from a long-run projection. The paper identifies four phases of the transmission of the resource boom before its conclusion in 2015.
C51|What They Did Not Tell You about Algebraic (Non-) Existence, Mathematical (IR-)Regularity, and (Non-) Asymptotic Properties of the Dynamic Conditional Correlation (DCC) Model|In order to hedge efficiently, persistently high negative covariances or, equivalently, correlations, between risky assets and the hedging instruments are intended to mitigate against financial risk and subsequent losses. If there is more than one hedging instrument, multivariate covariances and correlations have to be calculated. As optimal hedge ratios are unlikely to remain constant using high frequency data, it is essential to specify dynamic time-varying models of covariances and correlations. These values can either be determined analytically or numerically on the basis of highly advanced computer simulations. Analytical developments are occasionally promulgated for multivariate conditional volatility models. The primary purpose of this paper is to analyze purported analytical developments for the only multivariate dynamic conditional correlation model to have been developed to date, namely the widely used Dynamic Conditional Correlation (DCC) model. Dynamic models are not straightforward (or even possible) to translate in terms of the algebraic existence, underlying stochastic processes, specification, mathematical regularity conditions, and asymptotic properties of consistency and asymptotic normality, or the lack thereof. This paper presents a critical analysis, discussion, evaluation, and presentation of caveats relating to the DCC model, with an emphasis on the numerous dos and don’ts in implementing the DCC model, as well as a related model, in practice.
C51|What They Did Not Tell You about Algebraic (Non-) Existence, Mathematical (IR-)Regularity and (Non-) Asymptotic Properties of the Full BEKK Dynamic Conditional Covariance Model|Persistently high negative covariances between risky assets and hedging instruments are intended to mitigate against risk and subsequent financial losses. In the event of having more than one hedging instrument, multivariate covariances need to be calculated. Optimal hedge ratios are unlikely to remain constant using high frequency data, so it is essential to specify dynamic covariance models. These values can either be determined analytically or numerically on the basis of highly advanced computer simulations. Analytical developments are occasionally promulgated for multivariate conditional volatility models. The primary purpose of the paper is to analyze purported analytical developments for the most widely-used multivariate dynamic conditional covariance model to have been developed to date, namely the Full BEKK model, named for Baba, Engle, Kraft and Kroner. Dynamic models are not straightforward (or even possible) to translate in terms of the algebraic existence, underlying stochastic processes, specification, mathematical regularity conditions, and asymptotic properties of consistency and asymptotic normality, or the lack thereof. The paper presents a critical analysis, discussion, evaluation and presentation of caveats relating to the Full BEKK model, and an emphasis on the numerous dos and don’ts in implementing the Full BEKK and related non-Diagonal BEKK models, such as Triangular BEKK and Hadamard BEKK, in practice.
C51|Fed’s Unconventional Monetary Policy and Risk Spillover in the US Financial Markets|This study examines volatility spillover dynamics among the S&P 500 index, the US 10-year Treasury yield, the US dollar index futures and the commodity price index. The focus of the study is to analyze effects of Fed’s unconventional monetary policy on the US financial markets. We use realized volatility measures based on daily data covering the period from December 29, 1996 to November 12, 2018. To address nonlinear and asymmetric spillover dynamics in low and high volatility states, we propose a new regime-dependent spillover index based on a smooth transition vector autoregressive (STVAR) model, extending the study of Diebold and Yilmaz (2009,2012) to regime switching models. When applied to US financial data, we find strong evidence that the US financial market risk structure changes after the announcement of quantitively easing (QE) through the portfolio balance channel. The risk spillover moves from purchased assets to non-purchased assets after the QE announcements.
C51|Econometric Ways to Estimate the Age and Price of Abalone|Abalone is a rich nutritious food resource in the many parts of the world. The economic value of abalone is positively correlated with its age. However, determining the age of abalone is a cumbersome as well as expensive process which increases the cost and limits its popularity. This article proposes very simple ways to determine the age of abalones using econometric methods to reduce the costs of producers as well as consumers.
C51|A sectoral approach to the electricity-growth nexus in the Eastern Cape province of South Africa|This paper takes a sectoral, panel approach to investigating the electricity-growth nexus for the Eastern Cape province of South Africa between the period of 2003 and 2017. The empirical investigation was carried out using the Pooled Mean Group (PMG) panel estimators applied to an augmented dynamic growth model whilst the caulisty tests between electricity consumption and growth where performed using the Dumitrescu-Hurlin (2012) panel non-causality tests. The findings confirm the absence of significant long-run relationship between electricity and growth whilst finding a significant and positive effect over the short-run. Moreover, our causality tests provide strong evidence of causality running from electricity consumption to economic growth hence supporting the “growth hypothesis”. In a nutshell, our results not only demonstrate the importance of performing the electricity-growth analysis at provincial level as opposed to relying on national aggregated estimates but also provides important provincial-specific policy implications and recommendations.
C51|Модель Реального Обменного Курса Рубля С Марковскими Переключениями Режимов<BR>[Modeling real exchange rate of the Russian ruble using Markov regime-switching approach]|In this paper we analyze the relationship between the real Russian ruble exchange rate and real oil prices using the error correction model with Markov regime switching, which allows for changes in exchange rate policy. We find that during the period 1999-2018 real exchange rate dynamics was characterized by two clearly distinguishable regimes, one with fast and the other with slow adjustment to long-term equilibrium in response to oil price shocks. Further model testing shows that long-term relationship between real exchange rate and oil price is invariant to regime change. We also find that, despite adoption of a floating exchange rate policy in 2014, inflexible real exchange rate regime has been periodically identified in recent years. This could be due to the new budget rule, according to which Russian Ministry of Finance in February 2017 started purchasing foreign currencies in amount of excess oil and gas earnings of the federal budget.
C51|TF-MIDAS: a new mixed-frequency model to forecast macroeconomic variables|This paper tackles the mixed-frequency modeling problem from a new perspective. Instead of drawing upon the common distributed lag polynomial model, we use a transfer function representation to develop a new type of models, named TF-MIDAS. We derive the theoretical TF-MIDAS implied by the high-frequency VARMA family models and as a function of the aggregation scheme (flow and stock). This exact correspondence leads to potential gains in terms of nowcasting and forecasting performance against the current alternatives. A Monte Carlo simulation exercise confirms that TF-MIDAS beats UMIDAS models in terms of out-of-sample nowcasting performance for several data generating high-frequency processes.
C51|Monotonic Estimation for the Survival Probability over a Risk-Rated Portfolio by Discrete-Time Hazard Rate Models|Monotonic estimation for the survival probability of a loan in a risk-rated portfolio is based on the observation arising, for example, from loan pricing that a loan with a lower credit risk rating is more likely to survive than a loan with a higher credit risk rating, given the same additional risk covariates. Two probit-type discrete-time hazard rate models that generate monotonic survival probabilities are proposed in this paper. The first model calculates the discrete-time hazard rate conditional on systematic risk factors. As for the Cox proportion hazard rate model, the model formulates the discrete-time hazard rate by including a baseline component. This baseline component can be estimated outside the model in the absence of model covariates using the long-run average discrete-time hazard rate. This results in a significant reduction in the number of parameters to be otherwise estimated inside the model. The second model is a general form model where loan level factors can be included. Parameter estimation algorithms are also proposed. The models and algorithms proposed in this paper can be used for loan pricing, stress testing, expected credit loss estimation, and modeling of the probability of default term structure.
C51|Monotonic Estimation for Probability Distribution and Multivariate Risk Scales by Constrained Minimum Generalized Cross-Entropy|Minimum cross-entropy estimation is an extension to the maximum likelihood estimation for multinomial probabilities. Given a probability distribution {r_i }_(i=1)^k, we show in this paper that the monotonic estimates {p_i }_(i=1)^k for the probability distribution by minimum cross-entropy are each given by the simple average of the given distribution values over some consecutive indexes. Results extend to the monotonic estimation for multivariate outcomes by generalized cross-entropy. These estimates are the exact solution for the corresponding constrained optimization and coincide with the monotonic estimates by least squares. A non-parametric algorithm for the exact solution is proposed. The algorithm is compared to the “pool adjacent violators” algorithm in least squares case for the isotonic regression problem. Applications to monotonic estimation of migration matrices and risk scales for multivariate outcomes are discussed.
C51|Forecasting Realized Volatility of Russian stocks using Google Trends and Implied Volatility|This work proposes to forecast the Realized Volatility (RV) and the Value-at-Risk (VaR) of the most liquid Russian stocks using GARCH, ARFIMA and HAR models, including both the implied volatility computed from options prices and Google Trends data. The in-sample analysis showed that only the implied volatility had a significant effect on the realized volatility across most stocks and estimated models, whereas Google Trends did not have any significant effect. The outof-sample analysis highlighted that models including the implied volatility improved their forecasting performances, whereas models including internet search activity worsened their performances in several cases. Moreover, simple HAR and ARFIMA models without additional regressors often reported the best forecasts for the daily realized volatility and for the daily Value-at-Risk at the 1 % probability level, thus showing that efficiency gains more than compensate any possible model misspecifications and parameters biases. Our empirical evidence shows that, in the case of Russian stocks, Google Trends does not capture any additional information already included in the implied volatility.
C51|IFRS9 Expected Credit Loss Estimation: Advanced Models for Estimating Portfolio Loss and Weighting Scenario Losses|Estimation of portfolio expected credit loss is required for IFRS9 regulatory purposes. It starts with the estimation of scenario loss at loan level, and then aggregated and summed up by scenario probability weights to obtain portfolio expected loss. This estimated loss can vary significantly, depending on the levels of loss severity generated by the IFSR9 models, and the probability weights chosen. There is a need for a quantitative approach for determining the weights for scenario losses. In this paper, we propose a model to estimate the expected portfolio losses brought by recession risk, and a quantitative approach for determining the scenario weights. The model and approach are validated by an empirical example, where we stress portfolio expected loss by recession risk, and calculate the scenario weights accordingly.
C51|Synthetic Estimation of Dynamic Panel Models When Either N or T or Both Are Not Large: Bias Decomposition in Systematic and Random Components|By increasing the dimensions N or T, or both, in data panel analysis, bias can be reduced asymptotically to zero. This research deals with an econometric methodology to separate and measure bias through synthetic estimators without altering the data panel dimensions. This is done by recursively decomposing bias in systematic and random components. The methodology provides consistent synthetic estimators.
C51|Productive Performance and Technology Gaps using a Bayesian Metafrontier Production Function: A cross-country comparison|Growth theory argues on the role of heterogeneity that can lead to multiple regimes examining countries’ performance. A meta-production stochastic function under a Bayesian perspective has been developed to estimate technical efficiencies across countries over a time period. The metafrontier model is used to highlight heterogeneity among cluster of countries revealing catch up phenomena. The estimation procedure relies on the solution of an optimization problem and on the concept of the upper orthant order of two multinormal random variables. The proposed models are applied in a real dataset consisting of 109 countries for a 20-year period from 1995-2014. The productive performance differential and the associated technology gaps were investigated using two distinct frontiers (OECD vs non-OECD countries). Empirical results reveal that heterogeneity indeed plays a significant and distinctive role in determining technological gaps.
C51|Incremental Risk Charge Methodology|The incremental risk charge (IRC) is a new regulatory requirement from the Basel Committee in response to the recent financial crisis. Notably few models for IRC have been developed in the literature. This paper proposes a methodology consisting of two Monte Carlo simulations. The first Monte Carlo simulation simulates default, migration, and concentration in an integrated way. Combining with full re-valuation, the loss distribution at the first liquidity horizon for a subportfolio can be generated. The second Monte Carlo simulation is the random draws based on the constant level of risk assumption. It convolutes the copies of the single loss distribution to produce one year loss distribution. The aggregation of different subportfolios with different liquidity horizons is addressed. Moreover, the methodology for equity is also included, even though it is optional in IRC.
C51|On the (in)efficiency of cryptocurrencies: Have they taken daily or weekly random walks?|The legitimacy of virtual currencies as an alternative form of monetary exchange has been the centre of an ongoing heated debated since the catastrophic global financial meltdown of 2007-2008. We contribute to the relative fresh body of empirical research on the informational market efficiency of cryptomarkets by investigating the weak-form efficiency of the top-five cryptocurrencies. In differing from previous studies, we implement random walk testing procedures which are robust to asymmetries and unobserved smooth structural breaks. Moreover, our study employs two frequencies of cryptocurrency returns, one corresponding to daily returns and the other to weekly returns. Our findings validate the random walk hypothesis for daily series hence validating the weak-form efficiency for daily returns. On the other hand, weekly returns are observed to be stationary processes which is evidence against weak-form efficiency for weekly returns. Overall, our study has important implications for market participants within cryptocurrency markets.
C51|Economic growth, environmental degradation and business cycles in Eswatini|This study investigates the impact of the business cycle on the Environmental Kuznets Curve (EKC) for the Eswatini Kingdom over the period 1970 – 2014. To this end, we employ the nonlinear autoregressive distributive lag (NARDL) model to capture the long-run and short-run cointegration effects between economic activity and greenhouse gas (GHG) emissions over different phases of the business cycle. Our findings reveal that economic activity only degrades the environment during upswing of the economic cycle whilst this relationship is insignificant during downswing of the cycle. We specifically compute a value of $3.57 worth of output been gained at the cost of a metric unit of emissions during economic expansionary phases. Altogether, these results insinuate much needed government intervention in the market for emissions via environmental tax reforms (ETR) which should be designed with countercyclical bias towards upswing the business cycle.
C51|The human capital-economic growth nexus in SSA countries: What can strengthen the relationship?|The World Bank has recently placed increasing emphasis on the role of human capital development in facilitating economic development in the Sub-Saharan African (SSA) region. Our study examines the impact of human capital on economic growth for a selected sample of 9 SSA countries between 1980 and 2016 using a panel econometric approach. Interestingly enough, our empirical analysis shows an insignificant effect of human capital on economic growth for our selected sample. These findings remain unchanged even after adding interactive terms to human capital which are representative of government spending as well as foreign direct investment. Nevertheless, we establish a positive and significant effect of the interactive term between urbanization and human capital on economic growth, a result which emphasizes the importance of developing urbanized, ‘smart’, technologically-driven cities within the SSA region as a platform towards strengthening the impact of human capital- economic growth relationship.
C51|The Relevance of Crude Oil Prices on Natural Gas Pricing Expectations: A Dynamic Model Based Empirical Study|The natural gas price is an important and often decisive variable for economic policy makers. Many studies have been developed in order to establish a stochastic process that can represent the movements or the returns of natural gas prices or variations of such prices time series to forecast price expectations. This work aims to study the relationship between natural gas and crude oil prices in the international market, proposing to investigate its nature and long term equilibrium, through the development of adequate econometric models for determining future expectations of major natural gas price benchmarks, or of their returns. In order to accomplish this, time series for both benchmark crude oil and natural gas prices are subjected to statistical tests with the purpose of verifying the underlying hypotheses behind the appropriate autoregressive dynamic models. The conditional heteroskedasticity and non-normality of the return series, which are prevalent characteristics in energy markets, are considered when elaborating these models. To reach the purpose of this work weekly natural gas and crude oil prices benchmarks traded in the international market were collected.
C51|A multivariate approach for the simultaneous modelling of market risk and credit risk for cryptocurrencies|This paper proposes a set of models which can be used to estimate the market risk for a portfolio of crypto-currencies, and simultaneously to estimate also their credit risk using the Zero Price Probability (ZPP) model by Fantazzini et al (2008), which is a methodology to compute the probabilities of default using only market prices. For this purpose, both univariate and multivariate models with different specifications are employed. Two special cases of the ZPP with closed-form formulas in case of normally distributed errors are also developed using recent results from barrier option theory. A backtesting exercise using two datasets of 5 and 15 coins for market risk forecasting and a dataset of 42 coins for credit risk forecasting was performed. The Value-at-Risk and the Expected Shortfall for single coins and for an equally weighted portfolio were calculated and evaluated with several tests. The ZPP approach was used for the estimation of the probability of default/death of the single coins and compared to classical credit scoring models (logit and probit) and to a machine learning algorithm (Random Forest). Our results reveal the superiority of the t-copula/skewed-t GARCH model for market risk, and the ZPP-based models for credit risk.
C51|The importance of being informed: forecasting market risk measures for the Russian RTS index future using online data and implied volatility over two decades|This paper focuses on the forecasting of market risk measures for the Russian RTS index future, and examines whether augmenting a large class of volatility models with implied volatility and Google Trends data improves the quality of the estimated risk measures. We considered a time sample of daily data from 2006 till 2019, which includes several episodes of large-scale turbulence in the Russian future market. We found that the predictive power of several models did not increase if these two variables were added, but actually decreased. The worst results were obtained when these two variables were added jointly and during periods of high volatility, when parameters estimates became very unstable. Moreover, several models augmented with these variables did not reach numerical convergence. Our empirical evidence shows that, in the case of Russian future markets, T-GARCH models with implied volatility and student’s t errors are better choices if robust market risk measures are of concern.
C51|Nowcasting GDP Growth Using a Coincident Economic Indicator for India|In India, the first official estimate of quarterly GDP is released approximately 7-8 weeks after the end of the reference quarter. To provide an early estimate of the current quarter GDP growth, we construct a Coincident Economic Indicator for India (CEII) using 6, 9 and 12 high-frequency indicators. These indicators represent various sectors, display high contemporaneous correlation with GDP, and track GDP turning points well. While CEII-6 includes domestic economic activity indicators, CEII-9 combines indicators on trade and services along with the indicators used in CEII-6. Finally, CEII-12 adds financial indicators to the indicators used in CEII-9. In addition to the conventional economic activity indicators, we include a financial block in CEII-12 to reflect the growing influence of the financial sector on economic activity. CEII is estimated using a dynamic factor model to extract a common trend underlying the highfrequency indicators. We use the underlying trend to gauge the state of the economy and to identify sectors contributing to economic fluctuations. Further, CEIIs are used to nowcast GDP growth, which closely tracks the actual GDP growth, both in-sample and out-of-sample.
C51|User-Specified General-to-Specific and Indicator Saturation Methods|General-to-Specific (GETS) modelling provides a comprehensive, systematic and cumulative approach to modelling that is ideally suited for conditional forecasting and counterfactual analysis, whereas Indicator Saturation (ISAT) is a powerful and flexible approach to the detection and estimation of structural breaks (e.g. changes in parameters), and to the detection of outliers. To these ends, multi-path backwards elimination, single and multiple hypothesis tests on the coefficients, diagnostics tests and goodness-of-fit measures are combined to produce a parsimonious final model. In many situations a specific model or estimator is needed, a specific set of diagnostics tests may be required, or a specific fit criterion is preferred. In these situations, if the combination of estimator/model, diagnostics tests and fit criterion is not offered by publicly available software, then the implementation of user-specified GETS and ISAT methods puts a large programming-burden on the user. Generic functions and procedures that facilitate the implementation of user-specified GETS and ISAT methods for specific problems can therefore be of great benefit. The R package gets, version 0.20 (September 2019), is the first software - both inside and outside the R universe - to provide a complete set of facilities for user-specified GETS and ISAT methods: User-specified model/estimator, user-specified diagnostics and user-specified goodness-of-fit criteria. The aim of this article is to illustrate how user-specified GETS and ISAT methods can be implemented.
C51|Statistical Inference for Aggregation of Malmquist Productivity Indices|The Malmquist Productivity Index (MPI) has gained popularity amongst studies on dynamic change of productivity of decision making units (DMUs). In practice, this index is frequently reported at aggregate levels (e.g., public and private rms) in the form of simple equally-weighted arithmetic or geometric means of individual MPIs. A number of studies have emphasized that it is necessary to account for the relative importance of individual DMUs in the aggregations of indices in general and of MPI in particular. While more suitable aggregations of MPIs have been introduced in the literature, their statistical properties have not been revealed yet, preventing applied researchers from making essential statistical inferences such as con dence intervals and hypothesis testing. In this paper, we will ll this gap by developing a full asymptotic theory for an appealing aggregation of MPIs. On the basis of this, some meaningful statistical inferences are proposed and their nite-sample performances are veri ed via extensive Monte Carlo experiments.
C51|A Practical Guide to Harnessing the HAR Volatility Model|The standard heterogeneous autoregressive (HAR) model is perhaps the most popular benchmark model for forecasting return volatility. It is often estimated using raw realized variance (RV) and ordinary least squares (OLS). However, given the stylized facts of RV and wellknown properties of OLS, this combination should be far from ideal. One goal of this paper is to investigate how the predictive accuracy of the HAR model depends on the choice of estimator, transformation, and forecasting scheme made by the market practitioner. Another goal is to examine the effect of replacing its high-frequency data based volatility proxy (RV) with a proxy based on free and publicly available low-frequency data (logarithmic range). In an out-of-sample study, covering three major stock market indices over 16 years, it is found that simple remedies systematically outperform not only standard HAR but also state of the art HARQ forecasts, and that HAR models using logarithmic range can often produce forecasts of similar quality to those based on RV.
C51|Robust Tests for Convergence Clubs|In many applications common in testing for convergence the number of cross-sectional units is large and the number of time periods are few. In these situations asymptotic tests based on an omnibus null hypothesis are characterised by a number of problems. In this paper we propose a multiple pairwise comparisons method based on an a recursive bootstrap to test for convergence with no prior information on the composition of convergence clubs. Monte Carlo simulations suggest that our bootstrap-based test performs well to correctly identify convergence clubs when compared with other similar tests that rely on asymptotic arguments. Across a potentially large number of regions, using both cross-country and regional data for the European Union we find that the size distortion which afflicts standard tests and results in a bias towards finnding less convergence, is ameliorated when we utilise our bootstrap test.
C51|Predictability, Real Time Estimation, and the Formulation of Unobserved Components Models|The formulation of unobserved components models raises some relevant interpretative issues, owing to the existence of alternative observationally equivalent specifications, differing for the timing of the disturbances and their covariance matrix. We illustrate them with reference to unobserved components models with ARMA(m;m) reduced form, performing the decomposition of the series into an ARMA(m; q) signal, q m, and a noise component. We provide a characterization of the set of covariance structures that are observationally equivalent, when the models are formulated both in the future and the contemporaneous forms. Hence, we show that, while the point predictions and the contemporaneous real time estimates are invariant to the specification of the disturbances covariance matrix, the reliability cannot be identified, except for special cases requiring q
C51|Flexible Estimation of Heteroskedastic Stochastic Frontier Models via Two-step Iterative Nonlinear Least Squares|This article illustrates a straightforward and useful method for incorporating exogenous inefficiency effects in the estimation of semiparametric stochastic frontier models. An iterative estimation algorithm based on two-step nonlinear least squares is developed allowing for any flexible and monotonic specification of the production technology. We investigate the behavior of the proposed procedure through a set of Monte Carlo experiments comparing its finite sample properties with those of available alternatives. The new algorithm provides very good performance, outperforming the competitors in small samples and in presence of small signal-to-noise ratios. Two applications to agricultural data illustrate the usefulness of the proposed algorithm, even when it is used as a tool for sensitivity analysis.
C51|Nearest Comoment Estimation With Unobserved Factors|We propose a minimum distance estimator for the higher-order comoments of a multivariate distribution exhibiting a lower dimensional latent factor structure. We derive the in uence function of the proposed estimator and prove its consistency and asymptotic normality. The simulation study confirms the large gains in accuracy compared to the traditional sample comoments. The empirical usefulness of the novel framework is shown in applications to portfolio allocation under non-Gaussian objective functions and to the extraction of factor loadings in a dataset with mental ability scores.
C51|Using text mining in social science - tweet analysis of generation Y and Z|"Millennials are a generation of people born in the eighth and ninth decades of the twentieth century. They are also called the digital generation. Around this generation, as well as the previous age formation (eg baby boomers), many stereotypes have been created to give certain characteristics to a given population. Millennials are often referred to as the ""global village"", with a high level of self-confidence, caring for the quality of life and further development. Generaration Z is demographic cohort born after millennials, they have Internet even before they were born. Poster aims to present research in the area of ??text exploration analysis. There are 400 tweets in English, marked with hashtags: #millenials, #genY, #millenial, #genZ, #iGen, #postMillenial. An analysis of the mutual understanding of the phrases and phrases. Selected tweets have been published in January 2018. The obtained data are to serve as a confirmation or denial of the popularly generated stereotypes about both generations, and indicate the main areas of difficulty faced by representatives of this age groups."
C51|The IT-value stream model for Hospital Networks|"Hospitals, especially in regional settings, often struggle to fulfil all medical demands placed on them due their size and capacity. Thus, it is helpful to cooperate with other hospitals, perhaps in the neighbourhood as a network to exchange resources such as technical equipment or medi-cal staff such as qualified nurses who are in high demand. To solve this problem we can trans-fer some theoretical aspects from other similar situations such as network cooperation in con-nected industry-companies. This only works by using modern Information Technology based on internet technology and connected IT-Processes. In Germany, the government has implemented an extensive scientific and industrial project called ?Industry 4.0?.The idea behind ""Industry 4.0"" is that all industry processes use digital methods, which are connected but decentralized.Through the use of Smart technology (computer, smartphone or personal digital assistant - PDA), a Digital Twin representing a complete digital footprint of all units, products and re-sources which are not digital (i.e. human employees) can be created and all digital processes work together (cloud based) to solve problems in real time.This idea of ""Industry 4.0"" is useful for Hospital Networks, which often have similar structures. To develop Industry 4.0 for Hospital Networks, we need a special modelling method. An exten-sion of the value stream model (the IT-value stream model) can fulfil this task. In this publica-tion, we demonstrate how an extension of the value stream model can solve the resource prob-lems in Hospital Networks."
C51|Quantification of feedback effects in FX options markets|We model the feedback effect of delta hedging for the spot market volatility of the forex market (dollar-yen and dollar-euro) using an economy of two types of traders, an option market maker (OMM) and an option market taker (OMT), whose exposures reflect the total outstanding positions of all option traders in the market. A different hedge ratio of the OMM and OMT leads to a net delta hedge activity that introduces market friction and feedback effects. This friction is represented by a simple linear permanent impact model for the net delta hedge volumes that are executed in the spot market. This approach allows us to derive the dependence of the spot market volatility on the gamma exposure of the trader that hedges a larger share of her delta exposure and on the market impact of the delta hedge transactions. We reconstruct the aggregated OMM's gamma exposure by using publicly available DTCC trade repository data and find that it is negative, as expected: the OMT usually buys options with either a view on the spot price or with the desire to hedge other positions and, thus, is net long on options. As the OMM provides liquidity as a service to the market, their position is reversed compared with the OMT. Our regressions show a high goodness of fit, a highly significant parameter for the gamma exposure of the OMM and, as expected, that the volatility is increased by the OMM's short gamma exposure. Quantitatively, a negative gamma exposure of the OMM of approximately -1000 billion USD (which is around what we observe from our reconstructed OMM data) leads to an absolute increase in volatility of 0.7% in EURUSD and 0.9% in USDJPY. If we assume that the hedge ratios in the two markets are the same, the difference can be directly explained by the higher market impact of a transaction in the USDJPY spot market compared to the EURUSD spot market, as the liquidity of the EURUSD spot market is higher than that of the USDJPY spot market. Our results are in line with and empirically confirm previous theoretical work on the feedback effect of delta hedging strategies on spot market volatility.
C51|The consumption Euler equation or the Keynesian consumption function?|We formulate a general cointegrated vector autoregressive (CVAR) model that nests both a class of consumption Euler equations and various Keynesian type consumption functions. Using likelihoodbased methods and Norwegian data, we find support for cointegration between consumption, income and wealth once a structural break around the financial crisis is allowed for. That consumption cointegrates with both income and wealth and not only with income points to the empirical irrelevance of an Euler equation. Moreover, we find that consumption equilibrium corrects to changes in income and wealth and not that income equilibrium corrects to changes in consumption, which would be the case if an Euler equation is true. We also find that most of the parameters stemming from the class of Euler equations are not corroborated by the data when considering conditional expectations of future consumption and income in CVAR models. Only habit formation seems important in explaining the Norwegian consumer behaviour. Our preferred model is a dynamic Keynesian type consumption function with a first year marginal propensity to consume out of income close to 25 per cent.
C51|The Global Multi-Country Model (GM): an Estimated DSGE Model for the Euro Area Countries|This paper presents the European Commission's Global Multi-country model (the GM model). The GM model is an estimated multi-country DSGE model, developed by the European Commission, that can be used for spillover analysis, forecasting and medium term projections. Its development is jointly performed by the Joint Research Centre and DG ECFIN. Since the GM model is developed to be flexible under different country configurations,we present the GM model in its configuration designed for EMU-countries (GM3-EMU), which has been estimated for the four largest European economies (Germany, France, Italy and Spain). We analyse business cycle properties, present the model fit and provide a quantitative assessment of the relative importance that supply, demand and international shocks as well as discretionary policy interventions had in explaining the cyclical patterns observed in each country since the establishment of the EMU.
C51|The Contribution of Jump Signs and Activity to Forecasting Stock Price Volatility|We document the forecasting gains achieved by incorporating measures of signed, finite and infinite jumps in forecasting the volatility of equity prices, using high-frequency data from 2000 to 2016. We consider the SPY and 20 stocks that vary by sector, volume and degree of jump activity. We use extended HAR-RV models, and consider different frequencies (5, 60 and 300 seconds), forecast horizons (1, 5, 22 and 66 days) and the use of standard and robust-to-noise volatility and threshold bipower variation measures. Incorporating signed finite and infinite jumps generates significantly better real-time forecasts than the HAR-RV model, although no single extended model dominates. In general, standard volatility measures at the 300-second frequency generate the smallest real-time mean squared forecast errors. Finally, the forecasts from simple model averages generally outperform forecasts from the single best model.
C51|A Generalized Approach to Indeterminacy in Linear Rational Expectations Models|We propose a novel approach to deal with the problem of indeterminacy in Linear Rational Expectations models. The method consists of augmenting the original state space with a set of auxiliary exogenous equations to provide the adequate number of explosive roots in presence of indeterminacy. The solution in this expanded state space, if it exists, is always determinate, and is identical to the indeterminate solution of the original model. The proposed approach accommodates determinacy and any degree of indeterminacy, and it can be implemented even when the boundaries of the determinacy region are unknown. Thus, the researcher can estimate the model using standard packages without restricting the estimates to the determinacy region. We apply our method to estimate the New-Keynesian model with rational bubbles by Galí (2017) over the period 1982:Q4 until 2007:Q3. We find that the data support the presence of two degrees of indeterminacy, implying that the central bank was not reacting strongly enough to the bubble component.
C51|Money, credit, monetary policy and the business cycle in the euro area: what has changed since the crisis?|This paper studies the relationship between the business cycle and financial intermediation in the euro area. We establish stylized facts and study their stability during the global financial crisis and the European sovereign debt crisis. Long-term interest rates have been exceptionally high and long-term loans and deposits exceptionally low since the Lehman collapse. Instead, short-term interest rates and short-term loans and deposits did not show abnormal dynamics in the course of the financial and sovereign debt crisis. JEL Classification: E32, E51, E52, C32, C51
C51|Assessing U.S. aggregate fluctuations across time and frequencies|We study the behavior of key macroeconomic variables in the time and frequency domain. For this purpose, we decompose U.S. time series into various frequency components. This allows us to identify a set of stylized facts: GDP growth is largely a high-frequency phenomenon whereby inflation and nominal interest rates are characterized largely by low-frequency components. In contrast, unemployment is a medium-term phenomenon. We use these decompositions jointly in a structural VAR where we identify monetary policy shocks using a sign restriction approach. We find that monetary policy shocks affect these key variables in a broadly similar manner across all frequency bands. Finally, we assess the ability of standard DSGE models to replicate these findings. While the models generally capture low-frequency movements via stochastic trends and business cycle fluctuations through various frictions they fail at capturing the medium-term cycle.
C51|Realized Volatility Forecasting: Robustness to Measurement Errors|In this paper, we reconsider the issue of measurement errors affecting the estimates of a dynamic model for the conditional expectation of realized variance arguing that heteroskedasticity of such errors may be adequately represented with a multiplicative error model. Empirically we show that the significance of quarticity/quadratic terms capturing attenuation bias is very important within an HAR model, but is greatly diminished within an AMEM, and more so when regime specific dynamics account for a faster mean reversion when volatility is high. Model Confidence Sets confirm such robustness both inâ€“ and outâ€“ofâ€“sample.
C51|Estimating monetary policy rules in small open economies|This paper presents an approach for empirically estimating long-run monetary policy rules in small open economies. The approach utilizes the cointegrated VAR methodology and statistical tests on long- and short-run relations, and investigates policy responses. An application is presented for the case of Trinidad and Tobago. The analysis reveals an empirically supported long-run monetary policy rule for the nominal exchange rate, and provides empirical evidence that oil price shocks are transmitted through the TT economy in part via the effects on US prices. Dynamic specification of the nominal exchange rate reveals significant adjustment towards the target equilibrium level, and significant effects from foreign and domestic variables save for the exchange rate. Forecast analysis reveals the significance of oil-price forecasts, and forecast-errors, on monetary policy. The parsimonious model and its parameter estimates are empirically constant and generate reliable forecasts that provide important implications for using estimated policy rules.
C51|TINFORGE – Trade in INFORGE. Methoden-Update 2019|Das Welthandelsmodell TINFORGE wurde 2014 (Wolter et al. 2014) mit dem Ziel entwickelt, eine möglichst detaillierte Abbildung des Außenhandels in das nationale makroökonometrische Input-Output-Modell INFORGE (Ahlert et al. 2009) zu integrieren. Aufgabe des TINFORGE-Modells ist es, sowohl eine breite Abdeckung an Ländern weltweit zu erreichen als auch eine jährliche Aktualisierbarkeit zu gewährleisten. Die breite Abdeckung von Ländern ist notwendig, weil neue Partnerländer in Afrika oder Asien mit großer Dynamik im Welthandel sichtbar werden (Nigeria, Vietnam) und deren Wirkungen auf den deutschen Export abschätzbar sein sollen. Die jährliche Aktualisierung wird angestrebt, da konjunkturelle Schwankungen und Handelsbarrieren vor allem in den letzten Jahren deutlich zugenommen haben. Deren Wirkungen auch beobachten und nicht nur beschreiben zu können, ist aber Voraussetzung für eine empirisch basierte Projektion des Welthandels. Um also eine schnelle und zeitnahe Aktualisierung zu gewährleisten, wurde TINFORGE entwickelt und seit der Version INFORGE14_1 als integraler Bestandteil für die Abschätzung der Folgen des Welthandels für deutsche Exporte in INFORGE verwendet. Seit der Erstauflage von TINFORGE, welches in Wolter et al. (2014) beschrieben ist, wurde das Modell methodisch fortentwickelt. Dieses Discussion Paper ist ein Update des Papers aus dem Jahr 2014 und beschreibt den aktuellen Modellstand von INFORGE.
C51|Health condition and job status interactions: econometric evidence of causality from a French longitudinal survey|Abstract This article investigates the causal links between health and employment status. To disentangle correlation from causality effects, the authors leverage a French panel survey to estimate a bivariate dynamic probit model that can account for the persistence effect, initial conditions, and unobserved heterogeneity. The results highlight the crucial role of all three components and reveal strong dual causality between health and employment status. The findings clearly support demands for better coordination between employment and health public policies.
C51|Least impulse response estimator for stress test exercises|We introduce new semi-parametric models for the analysis of rates and proportions, such as proportions of default, (expected) loss-given-default and credit conversion factor encountered in credit risk analysis. These models are especially convenient for the stress test exercises demanded in the current prudential regulation. We show that the Least Impulse Response Estimator, which minimizes the estimated effect of a stress, leads to consistent parameter estimates. The new models with their associated estimation method are compared with the other approaches currently proposed in the literature such as the beta and logistic regressions. The approach is illustrated by both simulation experiments and the case study of a retail P2P lending portfolio.
C51|The relationship between property transaction prices, turnover rates and buyers' and sellers' reservation price distributions|This paper analyzes the relationship between movements in property transaction prices and movements in the underlying reservation price distributions of buyers and sellers and how these movements are linked to time varying turnover rate. A main conclusion in previous research is that transaction prices lag changes in buyers’ reservation price distribution and that an index tracking transaction prices is less volatile than an index tracking buyer reserves. We show that our less restrictive model of search and price formation reverses the volatility result in previous papers in realistic scenarios, i.e., transaction prices may be more volatile than underlying buyer reserves. We model transaction prices and turnover rates as functions of the moments of buyers’ and sellers’ reservation price distributions, the search intensity and the average bargaining power among buyers and sellers respectively. We derive the probability density function of transaction prices as a function of these parameters and hence a Maximum-likelihood estimator of the parameters, which serves as a new method of estimating indexes tracking movements in reservation price distributions from transaction data. We perform simulations where we show that the Maximum-likelihood estimator works as intended.
C51|A Bayesian VAR Approach to Short-Term Inflation Forecasting|In this paper, we discuss the forecasting performance of Bayesian vector autoregression (BVAR) models for inflation under alternative specifications. In particular, we consider modelling in levels or in differences; choice of tightness; estimating BVARs of different model sizes and the accuracy of conditional and unconditional forecasts. Our empirical results point out that BVAR forecasts using variables in log-difference form outperform the ones using log-levels of the data. When we evaluate forecast performance in terms of model size, the lowest forecast errors belong to the models having relatively small number of variables, though we find only small difference in forecast accuracy among models of various sizes up to two quarter ahead. Finally, the conditioning seems to help to forecast inflation. Overall, pseudo evaluation findings suggest that small to medium size BVAR models having wisely selected variables in difference form and conditioning on the future paths of some variables appear to be a good choice to forecast inflation in Turkey.
C51|An improved approach for estimating large losses in insurance analytics and operational risk using the g-and-h distribution|In this paper, we study the estimation of parameters for g-and-h distributions. These distributions find applications in modeling highly skewed and fat-tailed data, like extreme losses in the banking and insurance sector. We first introduce two estimation methods: a numerical maximum likelihood technique, and an indirect inference approach with a bootstrap weighting scheme. In a realistic simulation study, we show that indirect inference is computationally more efficient and provides better estimates in case of extreme features of the data. Empirical illustrations on insurance and operational losses illustrate these findings.
C51|Price discovery in a continuous-time setting|We formulate a continuous-time price discovery model and investigate how the standard price discovery measures vary with respect to the sampling interval. We ï¬ nd that the component share measure is invariant to the sampling interval, and hence, discrete-sampled prices suï¬ƒce to identify the continuous-time component share. In contrast, information share estimates are not comparable across diï¬€erent sampling intervals because the contemporaneous correlation between markets increases in magnitude as the sampling interval grows. We show how to back out the continuous-time information share from discrete-sampled prices under cer-tain assumptions on the contemporaneous correlation. We assess our continuous-time model by comparing the estimates of the (continuous-time) component and information shares at diï¬€erent sampling intervals for 30 stocks in the US. We ï¬ nd that both price discovery measures are typ-ically stable across the diï¬€erent sampling intervals, suggesting that our continuous-time price discovery model ï¬ ts the data very well.
C51|Time-Varying Cointegration and the Kalman Filter|We show that time-varying parameter state-space models estimated using the Kalman filter are particularly vulnerable to the problem of spurious regression, because the integrated error is transferred to the estimated state equation. We offer a simple yet effective methodology to reliably recover the instability in cointegrating vectors. In the process, the proposed methodology successfully distinguishes between the cases of no cointegration, fixed cointegration, and time-varying cointegration. We apply these proposed tests to elucidate the relationship between concentrations of greenhouse gases and global temperatures, an important relationship to both climate scientists and economists.
C51|Marginal Compensated Effects in Discrete Labor Supply Models|This paper develops analytic results for marginal compensated effects of discrete labor supply models, including Slutsky equations. It matters, when evaluating marginal compensated effects in discrete choice labor supply models, whether one considers wage increase (right marginal effects) or wage decrease (left marginal effects). We show how the results obtained can be used to calculate the marginal cost of public funds in the context of discrete labor supply models. Subsequently, we use the empirical labor supply model of Dagsvik and Strøm (2006) to compute numerical compensated (Hicksian) and uncompensated marginal (Marshallian) effects resulting from wage changes. The mean Hicksian labor supply elasticities are larger than the Marshallian, but the difference is small.
C51|Three dimensions of central bank credibility and inferential expectations: The Euro zone|We use the behavior of inflation among Eurozone countries to provide information about the degree of credibility of the European Central Bank (ECB) since 2008. We define credibility along three dimensions–official target credibility, cohesion credibility and anchoring credibility–and show in a new econometric framework that the latter has deteriorated in recent history; that is, price setters are less likely to rely on the ECB target when forming inflation expectations.
C51|Efficient matrix approach for classical inference in state space models|Reformulating a Gaussian state space model in matrix form, we obtain expressions for the likelihood function and the smoothing vector that are generally more efficient than the standard recursive algorithm. We also retrieve filtering weights and deal with data irregularities.
C51|Regional Output Growth in the United Kingdom: More Timely and Higher Frequency Estimates, 1970-2017|Output growth estimates for the regions of the UK are currently published at the annual frequency only and are released with a long delay. Regional economists and policymakers would benefit from having higher frequency and more timely estimates. In this paper we develop a mixed frequency Vector Autoregressive (MF-VAR) model and use it to produce estimates of quarterly regional output growth. Temporal and cross-sectional restrictions are imposed in the model to ensure that our quarterly regional estimates are consistent with the annual regional observations and the observed quarterly UK totals. We use a machine learning method based on the hierarchical Dirichlet-Laplace prior to ensure optimal shrinkage and parsimony in our over-parameterised MF-VAR. Thus,this paper presents a new, regional quarterly database of nominal and real Gross Value Added dating back to 1970. We describe how we update and evaluate these estimates on an ongoing, quarterly basis to publish online (at www.escoe.ac.uk/regionalnowcasting) more timely estimates of regional economic growth. We illustrate how the new quarterly data can be used to contribute to our historical understanding of business cycle dynamics and connectedness between regions.
C51|Electricity price forecasting|Electricity price forecasting (EPF) is a branch of energy forecasting on the interface between econometrics/statistics and engineering, which focuses on predicting the spot and forward prices in wholesale electricity markets. Its beginnings can be traced back to the early 1990s, when power sector deregulation led to the introduction of competitive markets in the UK and Scandinavia. The changes quickly spread throughout Europe and North America, and nowadays - in many countries worldwide - electricity is traded under market rules using spot and derivative contracts. Over the last 25 years, a variety of methods and ideas have been tried for EPF, with varying degrees of success. In this chapter we first briefly discuss the forecasting horizons and the types of forecasts, then review the forecasting tools and the evaluation techniques used in the EPF literature.
C51|Assessing the impact of renewable energy sources on the electricity price level and variability - a Quantile Regression approach|The literature on renewable energy sources indicates that an increase of the intermittent wind and solar generation affects significantly the distribution of electricity prices. In this article, the influence of two types of renewable energy sources (wind and solar photo voltaic) on the level and variability of German electricity spot prices is analyzed. The quantile regression models are built to estimate the merit order effect for different quantiles of electricity prices. The results indicate that both types of renewable generations have a similar, negative impact on the price level, approximated by the price median. When the price volatility, measured by the inter-quantile range (IQR), is considered, the outcomes show that wind and solar influence prices differently. Conditional on the level of the total demand, the wind generation would either increase (when the demand is low) or decrease (when the demand is high) the IQR. Meanwhile, the increase of solar power stabilizes the price variance for moderate demand level. Thus, policy supporting the development and integration of RES should search for a balance between the wind and solar power.
C51|Connectedness between G10 currencies: Searching for the causal structure|This paper presents a new approach for modelling the connectedness between asset returns. We adapt the measure of Diebold and Y¸lmaz (2014), which is based on the forecast error variance decomposition of a VAR model. However, their connectedness measure hinges on critical assumptions with regard to the variance-covariance matrix of the error terms. We propose to use a more agnostic empirical approach, based on a machine learning algorithm, to identify the contemporaneous structure. In a Monte Carlo study we compare the different connectedness measures and discuss their advantages and disadvantages. In an empirical application we analyse the connectedness between the G10 currencies. Our results suggest that the US dollar as well as the Norwegian krone are the most independent currencies in our sample. By contrast, the Swiss franc and New Zealand dollar have a negligible impact on other currencies. Moreover, a cluster analysis suggests that the currencies can be divided into three groups, which we classify as: commodity currencies, European currencies and safe haven/carry trade financing currencies.
C51|A novel housing price misalignment indicator for Germany|From 2014 until present, housing prices in Germany have been rising faster than consumer prices in all quarters except one, raising concerns about an excessive over-heating of the housing market. To assess the vulnerability of the German housing market to a future realignment of prices or even a housing bust, this paper develops a housing price misalignment indicator that is composed of seven indicators, which are commonly associated with the fundamental value of residential property. An empirical application to the most recent data suggests that the German housing market exhibits an overvaluation of approximately 11%, where interest rate risk and a relatively advanced stage of the housing cycle are identified as the main factors fueling these imbalances, while a rather solid debt-servicing capacity mitigates these imbalances since end-2009.
C51|Metcalfe's law and herding behaviour in the cryptocurrencies market|In this paper, the authors investigate the statistical properties of some cryptocurrencies by using three layers of analysis: alpha-stable distributions, Metcalfe's law and the bubble behaviour through the LPPL modelling. The results show, in the medium to long-run, the validity of Metcalfe's law (the value of a network is proportional to the square of the number of connected users of the system) for the evaluation of cryptocurrencies; however, in the short-run, the validity of Metcalfe's law for Bitcoin is questionable. As the results showed a potential for herding behaviour, the authors then used LPPL models to capture the behaviour of cryptocurrencies exchange rates during an endogenous bubble and to predict the most probable time of the regime switching. The main conclusion is that Metcalfe's law may be valid in the long-run, however in the short-run, on various data regimes, its validity is highly debatable.
C51|VC - A method for estimating time-varying coefficients in linear models|This paper describes a moments estimator for a standard state-space model with coefficients generated by a random walk. This estimator does not require that disturbances are normally distributed, but if they are, the proposed estimator is asymptotically equivalent to the maximum likelihood estimator.
C51|Dividend payout ratio follows a Tweedie distribution: International evidence|Dividend policy is still a largely discussed issue in corporate finance literature. One of the main indicators used in analysing the dividend policy is the dividend payout ratio. Using a database consisting of 12,085 companies operating in 73 countries, for the period 2008-2014, the authors found that the dividend payout ratio follows a Tweedie distribution, and not a normal one. This distribution is stable over time for the entire analysed period. In addition, it describes the case of almost all the countries included in the sample. Thus, a better estimation of the probability that dividend payout ratio is lower or higher than a benchmark can be provided. Also, an analysis of dividend policy, distinctly considering payer versus non-payer companies, can offer additional important information for both practitioners and academics.
C51|Ecuaciones Salariales de Parejas bajo Selección Muestral Bivariada. Una Aplicación al Caso Argentino|This working paper explores the effect of joint labor decisions on the study of wage regression models. The estimation of Mincer equations suffers from numerous sources of bias, including the sample selection problem generated by the fact that the agent decision to work is not independent of the wage. Most of the papers corrects this bias using a model of individual labor participation. However, recent trends in the labor market show greater participation of women in the labor force and seem to indicate that the joint decision of the spouses is increasingly relevant in determining the selection mechanism. A bivariate version of Heckman's method appears as an interesting alternative to solve this problem. Although the estimates are in line with the previous literature, the results indicate that the joint decision of the couple is a relevant factor in the selection bias. Este documento de trabajo explora el efecto de las decisiones laborales conjuntas sobre el estudio de los determinantes del salario laboral. La estimación de ecuaciones de Mincer sufre de numerosas fuentes de sesgos, entre ellas el problema de selección muestral generado por el hecho de que la decisión de trabajar no es independiente del salario individual. La mayoría de los trabajos corrigen dicho sesgo consideran un modelo de participación laboral individual. Sin embargo, las tendencias recientes del mercado laboral muestran una mayor participación de las mujeres en la fuerza de trabajo y parecieran indicar que la decisión conjunta de los cónyuges es cada vez más relevante en la determinación del mecanismo de selección. Una versión bivariada del método de Heckman permite incorporar una solución cuantitativa a este problema. Si bien las estimaciones realizadas están en línea con la literatura previa, los resultados indican que la decisión conjunta de la pareja es un factor relevante en el sesgo por selección muestral.
C51|Time-Varying Risk Premia in Large International Equity Markets|We use an estimation methodology tailored for large unbalanced panels of individual stock returns to address key economic questions about the factor structure, pricing performance of factor models, and time-variations in factor risk premia in international equity markets. We estimate factor models with time-varying factor exposures and risk premia at the individual stock level using 62,320 stocks in 46 countries over the 1985-2018 period. We consider market, size, value, momentum, profitability, and investment factors aggregated at the country, regional, and world level. We find that adding an excess country market factor to world or regional factors is sufficient to capture the factor structure for both developed and emerging markets. We do not reject asset pricing restriction tests for multifactor models in 74% to 91% of countries. Value and momentum premia show more variability over time and across countries than profitability and investment premia. The excess country market premium is statistically significant in many developed and emerging markets but economically larger in emerging markets.
C51|Money, credit, monetary policy, and the business cycle in the euro area: what has changed since the crisis?|This paper studies the relationship between the business cycle and financial intermediation in the euro area. We establish stylized facts and study their stability during the global financial crisis and the European sovereign debt crisis. Long-term interest rates have been exceptionally high and long-term loans and deposits exceptionally low since the Lehman collapse. Instead, short-term interest rates and short-term loans and deposits did not show abnormal dynamics in the course of the financial and sovereign debt crisis.
C51|Estimation of Industry-level Productivity with Cross-sectional Dependence by Using Spatial Analysis|We examine aggregate productivity in the presence of inter-sectoral linkages. Cross-sectional dependence is inevitable among industries, in which each sector serves as a supplier to the other sectors. However, the chains of such interconnections cause indirect relationship among industries. Spatial analysis is one of the approaches to address cross-sectional dependence by using a priori a specified spatial weights matrix. We exploit the linkage patterns from the input-output tables and use them to assign spatial weights to describe the economic interdependencies. By using the spatial weights matrix, we can estimate the industry-level production functions and productivity of the U.S. from 1947 to 2010. Cross-sectional dependencies are the consequences of indirect effects, and they reflect the interactions among industries linked via their supply chain networks result in larger output elasticities as well as scale effects for the networked production processes. However, productivity growth estimates are reportedly comparable across various spatial and non-spatial model specications.
C51|Generalised Anderson-Rubin statistic based inference in the presence of a singular moment variance matrix| The particular concern of this paper is the construction of a confidence region with pointwise asymptotically correct size for the true value of a parameter of interest based on the generalized Anderson-Rubin (GAR) statistic when the moment variance matrix is singular. The large sample behaviour of the GAR statistic is analysed using a Laurent series expansion around the points of moment variance singularity. Under a condition termed first order moment singularity the GAR statistic is shown to possess a limiting chi-square distribution on parameter sequences converging to the true parameter value. Violation, however, of this condition renders the GAR statistic unbounded asymptotically. The paper details an appropriate discretisation of the parameter space to implement a feasible GAR-based confidence region that contains the true parameter value with pointwise asymptotically correct size. Simulation evidence is provided that demonstrates the efficacy of the GAR-based approach to moment-based inference described in this paper.
C51|Multivariate Filter Estimation of Potential Output for the United States: An Extension with Labor Market Hysteresis|This paper extends the multivariate filter approach of estimating potential output developed by Alichi and others (2018) to incorporate labor market hysteresis. This extension captures the idea that long and deep recessions (expansions) cause persistent damage (improvement) to the labor market, thereby reducing (increasing) potential output. Applying the model to U.S. data results in significantly smaller estimates of output gaps, and higher estimates of the NAIRU, after the global financial crisis, compared to estimates without hysteresis. The smaller output gaps partly explain the absence of persistent deflation despite the slow recovery during 2010-2017. Going forward, if strong growth performance continues well beyond 2018, hysteresis is expected to result in a structural improvement in growth and employment.
C51|On the use of Hedonic Regression Models to Measure the Effect of Energy Efficiency on Residential Property Transaction Prices: Evidence for Portugal and Selected Data Issues|Using a unique dataset containing information of around 256 thousand residential property sales, this paper discloses a clear sales premium for most energy efficient dwellings, which is more pronounced for apartments (13%) than for houses (5 to 6%). Cross-country comparisons support the finding that energy efficiency price premiums are higher in the Portuguese residential market than in central and northern European markets. Results emphasize the relevance of data issues in hedonic regression models. They illustrate how the use of appraisal prices, explanatory variables with measurement errors, and the omission of variables associated with the quality of the properties, may seriously bias energy efficiency partial effect estimates. These findings provide valuable information not only to policy-makers, but also to researchers interested in this area.
C51|Assessing Pension Expenditure Determinants – the Case of Portugal|Pension expenditure is a concern for the sustainability of public finances in the European Union. Therefore, assessing pension expenditure determinants is crucial. This study aims to disentangle the impact of demographic and economic variables, such as ageing, productivity, and unemployment, on pension expenditure. Using Portuguese time-series data, from 1975 to 2014, statistical evidence was found of co-integration between unemployed people aged between 15 and 64 years old, apparent productivity of labour, the old-age dependence index and pension expenditure as a share of gross domestic product. The use of a vector error correction model, with impulse-response functions and variance decomposition, showed that ageing has an almost insignificant impact in the long-run, when compared with unemployment and productivity.
C51|Market Timing with Option-Implied Distributions in an Exponentially Tempered Stable Lévy Market|This paper explores the empirical implementation of a dynamic asset allocation strategy using option-implied distributions when the underlying risky asset price is modeled by an exponential Lévy process. One month risk-neutral densities are extracted from option prices and are subsequently transformed to the risk-adjusted, or real-world densities. Optimal portfolios consisting of a risky and risk-free asset rebalanced on a monthly basis are then constructed and their performance analyzed. It is found that the portfolios formed using option-implied expectations under the Lévy market assumption, which are flexible enough to capture the higher moments of the implied distribution, are far more robust to left-tail market risks and offer statistically significant improvements to risk-adjusted performance when investor risk aversion is low, however this diminishes as risk aversion increases.
C51|The dynamic relationship between stock market indexes and foreign exchange|his empirical study analyses the dynamic relationship between the FTSE100 Index and the EuroSTOXX50 Index and the USD/EUR and USD/GBP exchange rates, from January 2007 to April 2017. The Johansen co-integration tests suggest that these variables have a long-term relationship.The Granger causality test was conducted through the use of VECM equations, showing that the FTSE100 and the EuroSTOXX50 Index both have a causal feedback relationship. A unidirectional relationship was found between the FTSE100 Index stock prices and the USD/EURexchange rate.The presence of a unidirectional relationship between the USD/GBP exchange rate and FTSE100 and EuroSTOXX50 Index stock priceswas also detected.
C51|Exploiting Information from Singletons in Panel Data Analysis: A GMM Approach|We propose a novel procedure, built within a Generalized Method of Moments framework, which exploits unpaired observations (singletons) to increase the efficiency of longitudinal fixed effect estimates. The approach allows increasing estimation efficiency, while properly tackling the bias due to unobserved time-invariant characteristics. We assess its properties by means of Monte Carlo simulations, and apply it to a traditional Total Factor Productivity regression, showing efficiency gains of approximately 8-9 percent.
C51|Accounting for Skewed or One-Sided Measurement Error in the Dependent Variable|"While classical measurement error in the dependent variable in a linear regression framework results only in a loss of precision, non-classical measurement error can lead to estimates which are biased and inference which lacks power. Here, we consider a particular type of non-classical measurement error: skewed errors. Unfortunately, skewed measurement error is likely to be a relatively common feature of many outcomes of interest in political science research. This study highlights the bias that can result even from relatively ""small"" amounts of skewed measurement error, particularly if the measurement error is heteroskedastic. We also assess potential solutions to this problem, focusing on the stochastic frontier model and nonlinear least squares. Simulations and three replications highlight the importance of thinking carefully about skewed measurement error, as well as appropriate solutions."
C51|The Knightian Uncertainty Hypothesis: Unforeseeable Change and Muthï¿½s Consistency Constraint in Modeling Aggregate Outcomes|This paper proposes the Knightian Uncertainty Hypothesis (KUH), a new approach to macroeconomics and finance theory. KUH rests on a novel mathematical framework that characterizes both measurable and Knightian uncertainty about economic outcomes. Relying on this framework and Muthï¿½s pathbreaking hypothesis, KUH represents participantsï¿½ forecasts to be consistent with both uncertainties. KUH thus enables models of aggregate outcomes that 1) are premised on market participantsï¿½ rationality, and 2) accord a role to both fundamental and psychological (and other non-fundamental) factors in driving outcomes. The paper also suggests how a KUH modelï¿½s quantitative predictions can be confronted with time-series data.
C51|Economic growth, Environmental degradation and business cycles in Eswatini|This study investigates the impact of the business cycle on the Environmental Kuznets Curve (EKC) for the Eswatini Kingdom over the period 1970 – 2014. To this end, we employ the nonlinear autoregressive distributive lag (NARDL) model to capture the long-run and short-run cointegration effects between economic activity and greenhouse gas (GHG) emissions over different phases of the business cycle. Our findings reveal that economic activity only degrades the environment during upswing of the economic cycle whilst this relationship is insignificant during downswing of the cycle. We specifically compute a value of $3.57 worth of output been gained at the cost of a metric unit of emissions during economic expansionary phases. Altogether, these results insinuate much needed government intervention in the market for emissions via environmental tax reforms (ETR) which should be designed with countercyclical bias towards upswing the business cycle.
C51|A sectoral approach to the electricity-growth nexus in the Eastern Cape province of South Africa|This paper takes a sectoral, panel approach to investigating the electricity-growth nexus for the Eastern Cape province of South Africa between the period of 2003 and 2017. The empirical investigation was carried out using the Pooled Mean Group (PMG) panel estimators applied to an augmented dynamic growth model whilst the caulisty tests between electricity consumption and growth where performed using the Dumitrescu-Hurlin (2012) panel non-causality tests. The findings confirm the absence of significant long-run relationship between electricity and growth whilst finding a significant and positive effect over the short-run. Moreover, our causality tests provide strong evidence of causality running from electricity consumption to economic growth hence supporting the “growth hypothesis”. In a nutshell, our results not only demonstrate the importance of performing the electricity-growth analysis at provincial level as opposed to relying on national aggregated estimates but also provides important provincial-specific policy implications and recommendations.
C51|On the (in)efficiency of cryptocurrencies: Have they taken daily or weekly random walks?|The legitimacy of virtual currencies as an alternative form of monetary exchange has been the centre of an ongoing heated debated since the catastrophic global financial meltdown of 2007-2008. We contribute to the relative fresh body of empirical research on the informational market efficiency of cryptomarkets by investigating the weak-form efficiency of the top-five cryptocurrencies. In differing from previous studies, we implement random walk testing procedures which are robust to asymmetries and unobserved smooth structural breaks. Moreover, our study employs two frequencies of cryptocurrency returns, one corresponding to daily returns and the other to weekly returns. Our findings validate the random walk hypothesis for daily series hence validating the weak-form efficiency for daily returns. On the other hand, weekly returns are observed to be stationary processes which is evidence against weak-form efficiency for weekly returns. Overall, our study has important implications for market participants within cryptocurrency markets.
C51|The human capital-economic growth nexus in SSA countries: What can strengthen the relationship?|The World Bank has recently placed increasing emphasis on the role of human capital development in facilitating economic development in the Sub-Saharan African (SSA) region. Our study examines the impact of human capital on economic growth for a selected sample of 9 SSA countries between 1980 and 2016 using a panel econometric approach. Interestingly enough, our empirical analysis shows an insignificant effect of human capital on economic growth for our selected sample. These findings remain unchanged even after adding interactive terms to human capital which are representative of government spending as well as foreign direct investment. Nevertheless, we establish a positive and significant effect of the interactive term between urbanization and human capital on economic growth, a result which emphasizes the importance of developing urbanized, ‘smart’, technologically-driven cities within the SSA region as a platform towards strengthening the impact of human capital- economic growth relationship.
C51|Fiscal cyclicality in South African public expenditures: Do asymmetries explain inconsistencies?|The most recent sub-prime crisis, the Euro-debt crisis and the global recessionary period have resurrected a debate on the nature of fiscal cyclicality in the South African economy. Our study questions whether the cyclicality of public policy has evolved asymmetrically and holds differently over the recessionary and expansionary phases of the South African business cycle for a quarterly period of 2001:01 – 2018:04. To ensure that the business cycle is inherent to our estimation process we rely on the nonlinear autoregressive distributive lag (N-ARDL) model as empirical framework. Our empirical results point to nonlinear cyclicality in fiscal expenditures where government behaves procyclical in the upswing of the business cycle whilst behaving countercyclical during economic downswings. These findings are robust to alternative specifications, inclusion of control variables and estimations across different subsamples. Policy implications are also offered.
C51|A Model for International Spillovers to Emerging Markets|This paper develops a small open economy (SOE) dynamic stochastic general equilibrium (DSGE) model that helps to explain business cycle synchronization between an emerging market and advanced economies. The model captures the specificities of both economies (e.g. primary commodity, manufacturing, intermediate inputs, and credit) that are most relevant for understanding the importance as well as the transmission mechanisms of a wide range of domestic and foreign (supply, demand, monetary policy, credit, primary commodity) shocks facing an emerging economy. We estimate the model with Bayesian methods using quarterly data from South Africa, the US and G7 countries. In contrast to the predictions of standard SOE models, we are able to replicate two stylized facts. First, our model predicts a high degree of business cycle synchronization between South Africa and advanced economies. Second, the model is able to account for the influence of foreign shocks in South Africa. We are also able to demonstrate the specific roles these shocks played during key historical episodes such as the global financial crisis in 2008 and the commodity price slump in 2015. The ability of our framework to capture endogenous responses of commodity and financial sectors to structural shocks is crucial to identify the importance of these shocks in South Africa.
C51|An Empirical Framework for Sequential Assignment: The Allocation of Deceased Donor Kidneys|An organ transplant can improve a patient’s life while also realizing substantial savings in healthcare expenditures. Like many other scarce public resources, organs from deceased donors are rationed to patients on a waitlist via a sequential offer mechanism. The theoretical trade-offs in designing these rationing systems are not well understood and depend on agent preferences. This paper establishes an empirical framework for analyzing waitlist systems and applies it to study the allocation of deceased donor kidneys. We model the decision to accept an organ or wait for a more preferable organ as an optimal stopping problem, and develop techniques to compute equilibria of counterfactual mechanisms. Our estimates show that while some types of kidneys are desirable for all patients, there is substantial match-specific heterogeneity in values. We then evaluate alternative mechanisms by comparing their effect on patient welfare to an equivalent change in donor supply. Past reforms to the kidney waitlist primarily resulted in redistribution, with similar welfare and organ discard rates as the benchmark first come first served mechanism. These mechanisms and other commonly studied theoretical benchmarks remain far from optimal: we design a mechanism that increases patient welfare by the equivalent of a 14.2 percent increase in donor supply.
C51|Cross-Sectional Dispersion of Risk in Trading Time|We study the temporal behavior of the cross-sectional distribution of assets' market exposure, or betas, using a large panel of high-frequency returns. The asymptotic setup has the sampling frequency of the returns increasing to infinity, while the time span of the data remains fixed, and the cross-sectional dimension is fixed or increasing. We derive a Central Limit Theorem (CLT) for the cross-sectional beta dispersion at a point in time, enabling us to test whether this quantity varies across the trading day. We further derive a functional CLT for the dispersion statistics, allowing us to test if the beta dispersion, as a function of time-of-day, changes across days. We extend this further by developing inference techniques for the entire cross-sectional beta distribution at fixed points in time. We demonstrate, for constituents of the S&P 500 index, that the beta dispersion is elevated at the market open, gradually declines over the trading day, and is less than half the original value by the market close. The intraday beta dispersion pattern also changes over time and evolves differently on macroeconomic announcement days. Importantly, we find that the intraday variation in market betas is a source of priced risk.
C51|Financial structure, institutional quality and monetary policy transmission: A Meta-Analysis|The long-standing empirical literature of monetary policy transmission acknowledges weak transmission of monetary policy shock to real activities and inflation in emerging economies. Fragile financial system, low level of financial integration and weak institutions are often cited as the reasons for lack of monetary policy transmission in these economy. This paper investigates to what extent these factors explain the variation in the extent of monetary policy transmission in a comprehensive set of developed and developing economies using meta-analysis framework. We find that the degree of financial development captured by various financial indicators explain cross-country variations in the magnitude and time lag of monetary policy transmission. We also find the role of financial accelerator in transmission magnitude to output growth.
C51|Factor-Driven Two-Regime Regression|We propose a novel two-regime regression model where the switching between the regimes is driven by a vector of possibly unobservable factors. When the factors are latent, we estimate them by the principal component analysis of a panel data set. We show that the optimization problem can be reformulated as mixed integer optimization and present two alternative computational algorithms. We derive the asymptotic distributions of the resulting estimators under the scheme that the threshold effect shrinks to zero. In particular, we establish a phase transition that describes the effect of first stage factor estimation as the cross-sectional dimension of panel data increases relative to the time-series dimension. Moreover, we develop a consistent factor selection procedure with a penalty term on the number of factors and present bootstrap methods for carrying out inference and testing linearity with the aid of efficient computational algorithms. Finally, we illustrate our methods via numerical studies.
C51|Puzzling out the Feldstein-Horioka Paradox for Turkey by a Time-Varying Parameter Approach|This study would like to contribute to the existing literature on the Feldstein-Horioka paradox by focusing on Turkey for the period 1960-2014 and by scrutinizing the correlation between domestic savings and investments within a time-varying parameter approach (which is warranted especially for emerging countries due to their political and economic instability and due to the frequency of policy changes). Our time-varying parameter approach is able to capture the impact of various economic and political interruptions on the correlation between domestic savings and investments, especially the military coups in the early 1960s, 1970s and 1980s, and the economic and financial crises in the mid-1990s, in the late 1990s, and in the early 2000s, as well as the financial crises affecting various countries in the globe in the late 1990s and 2000s. Our empirical analysis suggests a high correlation between domestic savings and investments in the 1960s, which was decreasing (increasing) during the 1970s (1980s), and which was decreasing since the 1990s. Furthermore, in the post-2002 era, with a further decline in the correlation coefficient, the saving-investment nexus has turned out to be statistically insignificant.
C51|"Confidence Intervals for Ratios: Econometric Examples with Stata ""Abstract: Ratios of parameter estimates are often used in econometric applications. However, the test of these ratios when estimated can cause difficulties since the ratio of asymptotically normally distributed random variables have a Cauchy distribution for which there are no finite moments. This paper presents a method for the estimation of confidence intervals based on the Fieller approach that has been shown to be preferable to the usual Delta method. Using example applications, we demonstrate that a few extra steps in the examination of the estimate of the ratio may provide a confidence interval with superior coverage."""|No abstract is available for this item.
C51|The inflation-growth relationship in SSA inflation targeting countries|This paper investigates the relationship between inflation and economic growth for South Africa and Ghana using quarterly empirical data collected from 2001 to 2016 applied to the quantile regression method. For our full sample estimates we find that inflation is positively related with growth in Ghana at high inflation levels whilst inflation in South Africa exerts its least adverse effects at high inflation levels. However, when particularly focusing on the post-crisis period, we find inflation exerts negative effects at all levels of inflation for both countries with inflation having its least adverse effects at high levels for Ghana and at moderate levels for South Arica. Based on these findings bear important implications for inflation targeting frameworks adopted by Central Banks in both countries.
C51|Determinants of FDI in South Africa: Do macroeconomic variables matter?|In this study we examine the macroeconomic determinants of FDI for the South African economy using data collected between 1994 and 2016 using the ARDL model for cointegration. The specific macroeconomic determinants which are used in the study are per capita GDP, the inflation rate, government size, real interest rate variable, and terms of trade. With the exception of inflation the remaining macroeconomic determinants employed in the study are positively and significantly related with FDI. However, in the short-run all variables are positively and significantly correlated with FDI. Collectively, these results have important implications for policymakers.
C51|How sustainable are fiscal budgets in the Kingdom of Swaziland?|The recently experienced Swazi fiscal crisis of 2011 has facilitated the need for an academic probe into the sustainability of fiscal budgets in the Kingdom. Against the absence of empirical evidence evaluating the sustainability of Swazi fiscal budget, our study fills the hiatus by econometrically evaluating the sustainability of the fiscal budget of the Swazi economy between 1999 and 2016. Our empirical study depends on a combination of linear and asymmetric unit root and cointegration empirical procedures to attain this objective. In reviewing the obtained results, the evidence obtained from the linear econometric frameworks is inconclusive whereas the results from the more vigorous asymmetric models point to the unsustainability of Swazi fiscal budget over both the short and long-run. Important policy implications for Swazi fiscal policymakers are drawn from the analysis.
C51|Structural changes in exchange rate-stock returns dynamics in South Africa: Examining the role of crisis and new trading platform|The 2007 sub-prime crisis and the adoption of Millennium trading platform represent two of the most important recent structural developments for the Johannesburg Stock Exchange (JSE). Under an environment of flexible and volatile exchange rates, this study seeks to examine the effects of these two structural events on the exchange rate-equity returns nexus for 4 JSE indices using the nonlinear autoregressive distributive lag (N-ARDL) cointegration. We use monthly data collected from 2000:M01 to 2017:M12, and conduct our empirical analysis over sub-periods corresponding to breaks caused by the crisis and the use of a new trading platform. We find prior the crisis exchange rates appreciations generally cause stock returns whereas depreciations are unlikely to cause stock returns to decrease. However, during crisis period this relationship entire disappears whilst resurfacing subsequent to the adoption of a new trading platform although the dynamics of the time series differs between sectors. Our overall empirical results caution regulatory authorities to closely monitor stock market developments as the new trading platform offers market participants opportunities of using the exchange rate to beat the market.
C51|Is it the natural rate or hysteresis hypothesis for unemployment in Newly Industrialized Economies?|The focus of our study is on determining whether unemployment rates in 8 New Industrialized Economies conform to the natural rate hypothesis or the hysteresis hypothesis. To this end, we employ a variety of unit of unit root testing procedures to quarterly data collected between 2002:q1 and 2017:q1. In summary of our findings, conventional unit root tests which neither account for asymmetries or structural breaks produce the most inconclusive results. On the other hand, tests which incorporate structural breaks whilst ignoring asymmetries tends to favour the natural rate hypothesis for our panel of countries. However, simultaneously accounting for asymmetries and unobserved structural breaks seemingly produces the most robust findings and confirms hysteresis in all unemployment rates except for the Asian economies/countries of Thailand and the Philippines.
C51|A provincial perspective of nonlinear Okun's law for emerging markets: The case of South Africa|A provincial analysis of Okun’s law in South Africa is provided in this article over a period of 1996 to 2016. Empirically, we rely on the nonlinear autoregressive distributive lag (N-ARDL) model whilst the Corbae-Ouliaris filter is used to extract the ‘gap’ variables required for our regression estimates. Okun’s law is found to be significant hold in the long-run exclusively for the Western Cape and Kwa-Zulu Natal provinces whereas the remaining provinces partially display significant short-run effects. Our sensitivity analysis in which panel N-ARDL estimations for all provinces finds insignificant long-run Okun effects for the country as a whole, whilst validating the relationship only in the short-run. Our study hence implies that/advices that the epicentre of policy efforts in addressing the country’s high unemployment and low economic growth dilemma should be concentrated at a provincial level.
C51|Robust analysis of convergence in per capita GDP in BRICS economies|Whilst the issue of whether or not per capita GDP adheres to the convergence theory continues to draw increasing attention within the academic paradigm, with very little consensus having been reached in the literature thus far. Our study contributes to the literature by examining the stationarity of per capita GDP for BRICS countries using annual data collected between 1971 and 2015. Considering that our sample covers a period underlying a number of crisis and structural breaks within and amongst the BRICS countries, we rely on a robust nonlinear unit root testing procedure which captures a series of unobserved structural breaks. Our results confirm on Brazil and China being the only two BRICS economies who present the most convincing evidence of per capita GDP converging back to it’s natural equilibrium after an economic shock, whilst Russia and South Africa provide less convincing evidence of convergence dynamics in the time series and India having the weakest convergence properties.
C51|FDI as a contributing factor to economic growth in Burkina Faso: How true is this?|Much emphasis has been placed on attracting FDI into Burkina Faso as a catalyst for improved economic growth within the economy. Against the lack of empirical evidence evaluating this claim, we use data collected from 1970 to 2017 to investigate the FDI-growth nexus for the country using the ARDL bounds cointegration analysis. Our empirical model is derived from endogenous growth theoretical framework in which FDI may have direct or spillover effects on economic growth via improved human capital development as well technological developments reflected in urbanization and improved export growth. Our findings fail to establish any direct or indirect effects of FDI on economic growth except for FDI’s positive interaction with export-oriented growth, albeit being constrained to the short-run. Therefore, in summing up our recommendations, political reforms and the building of stronger economic ties with the international community in order to raise investor confidence, which has been historically problematic, should be at the top of the agenda for policymakers in Burkina Faso.
C51|Persistence of suicides in G20 countries: SPSM approach to three generations of unit root tests|Suicides represent an encompassing measure of psychological well-being, emotional stability as well as life satisfaction and have been recently identified by the World Health Organization (WHO) as a major global health concern. The G20 countries represent the powerhouse of global economic governance and hence possess the ability to influence the direction of global suicide rates. In applying the sequential panel selection method (SPSM) to three generations of unit root testing procedures, the study investigates whether G20 countries should be concerned with possible persistence within suicide rates. The results obtained from all three generation of tests provide rigid evidence of persistence within the suicides for most member states of the G20 countries hence supporting the current strategic agenda pushed by the WHO in reducing suicides to a target rate of 10 percent. In addition, we further propose that such strategies should emulate from within G20 countries and spread globally thereafter.
C51|Unobserved structural shifts and asymmetries in the random walk model for stock returns in African frontier markets|The purpose of this study is to examine the weak-form market efficiency hypothesis (EMH) for 8 African Frontier markets (Nairobi Securities Exchange of Kenya, the Nigerian Stock Exchange of Nigeria, Botswana Stock Exchange of Botswana, Zimbabwe Stock Exchange of Zimbabwe, Johannesburg Stock Exchange of South Africa, Egyptian Exchange of Egypt, Casablanca Stock Exchange of Morocco, the Tunis Stock Exchange of Tunisia). To achieve this purpose we employ unit root testing procedures which are robust to both nonlinearities and smooth structural breaks. To further allow for vigorousness in our empirical analysis we employ two time series datasets for each of the capital markets, namely daily and weekly time series. To the best of our knowledge, our study becomes the first, to investigate the weak-form EMH for all 8 African frontier markets whilst simultaneously accounting for asymmetries and smooth structural breaks. Our empirical findings suggest that most African frontier markets are not market efficient, in the weak sense form, with the exception of the Kenyan stock market and to a very much lesser extent the Botswana and South African stock series. Important policy and investor implications are drawn in our study.
C51|Endogenous monetary approach to optimal inflation-growth nexus in Swaziland|With the inflation-growth nexus being a hotly debated issue within the academic paradigm, the purpose of our study is to examine the relationship for Swaziland between 1975 and 2016 of which there currently exists very limited country-specific evidence. In the design of our study we theoretically depend on an endogenous monetary model of economic growth augmented with a credit technology which causes a nonlinear relationship between inflation and growth. Econometrically, we rely on the smooth transition regression (STR) which allows us to estimate an optimal inflation rate characterized by smooth transition between different inflation regimes. Our empirical results point to an inflation threshold estimate of 7.64% at which economic growth gains are maximized or similarly growth losses are minimized. In particular, we find that above the inflation threshold economic agents may be able to protect themselves from inflation through credit technology and a more urbanized population yet such high inflation adversely affects the influence of exports on economic growth. This noteworthy since a majority of government revenues is from trade activity via the country’s affiliation with the Southern African Customs Union (SACU). Nevertheless, the major contribution of this paper is that it becomes the first to use endogenous growth theory to estimate the inflation threshold for any African country which will hopefully pave a way for similar studies on other African countries.
C51|On parameters estimation of the Seasonal FISSAR Model|In this paper, we discuss the methods of estimating the parameters of the Seasonal FISSAR (Fractionally Integrated Separable Spatial Autoregressive with seasonality) model. First we implement the regression method based on the log-periodogram and the classical Whittle method for estimating memory parameters. To estimate the model's parameters simultaneously - innovation parameters and memory parameters- the maximum likelihood method, and the Whittle method based on the MCMC simulation are considered. We are investigated the consistency and the asymptotic normality of the estimators by simulation
C51|Series estimation for single-index models under constraints|This paper discusses a semiparametric single-index model. The link function is allowed to be unbounded and has unbounded support that fill the gap in the literature. The link function is treated as a point in an infinitely many dimensional function space which enables us to derive the estimates for the index parameter and the link function simultaneously. This approach is different from the profile method commonly used in the literature. The estimator is derived from an optimization with the constraint of an identification condition for the index parameter, which solves an important problem in the literature of single-index models. In addition, making use of a property of Hermite orthogonal polynomials, an explicit estimator for the index parameter is obtained. Asymptotic properties of the two estimators of the index parameter are established. Their efficiency is discussed in some special cases as well. The finite sample properties of the two estimators are demonstrated through an extensive Monte Carlo study and an empirical example.
C51|On endogeneity and shape invariance in extended partially linear single index models|In this paper, the important (but so far unrevealed) usefulness of the extended generalized partially linear single-index (EGPLSI) model introduced by Xia et al. (1999) in its ability to model a flexible shape-invariant specification is elaborated. More importantly, a control function approach is proposed to address the potential endogeneity problems in the EGPLSI model in order to enhance its applicability to empirical studies. In the process, it is shown that the attractive asymptotic features of the single-index type of a semiparametric model are still valid in our proposed estimation procedure given intrinsic generated covariates. Our newly developed method is then applied to address the endogeneity of expenditure in the semiparametric analysis of a system of empirical Engel curves by using the British data, highlights the convenient applicability of our proposed method.
C51|Linear IV Regression Estimators for Structural Dynamic Discrete Choice Models|In structural dynamic discrete choice models, the presence of serially correlated unobserved states and state variables that are measured with error may lead to biased parameter estimates and misleading inference. In this paper, we show that instrumental variables can address these issues, as long as measurement problems involve state variables that evolve exogenously from the perspective of individual agents (i.e., market-level states). We define a class of linear instrumental variables estimators that rely on Euler equations expressed in terms of conditional choice probabilities (ECCP estimators). These estimators do not require observing or modeling the agent’s entire information set, nor solving or simulating a dynamic program. As such, they are simple to implement and computationally light. We provide constructive identification arguments to identify the model primitives, and establish the consistency and asymptotic normality of the estimator. A Monte Carlo study demonstrates the good finite-sample performance of the ECCP estimator in the context of a dynamic demand model for durable goods.
C51|Conditional dynamics and the multi-horizon risk-return trade-off|We propose testing asset-pricing models using multi-horizon returns (MHR). MHR serve as powerful source of conditional information that is economically important and not data-mined. We apply MHR-based testing to linear factor models. These models seek to construct the unconditionally mean-variance efficient portfolio. We reject all state-of-the-art models that imply high maximum Sharpe ratios in a single-horizon setting. Thus, the models do a poor job in accounting for the risk-return trade-off at longer horizons. Across the different models, the mean absolute pricing errors associated with MHR are positively related to the magnitude of maximal Sharpe ratio in the single-horizon setting. Model misspecification manifests itself in strong intertemporal dynamics of the factor loadings in the SDF representation. We suggest that misspecification of the dynamics of loadings arises from the common approach towards factor construction via portfolio sorts.
C51|A New Approach to Nowcasting Indian Gross Value Added|In India, quarterly growth of Gross Value Added (GVA) is published with a large lag and nowcasts are exacerbated by data challenges typically faced by emerging market economies, such as big data revisions, mixed frequencies data publication, small sample size, non-synchronous nature of data releases, and data releases with varying lags. This paper presents a new framework to nowcast India’s GVA that incorporates information of mixed data frequencies and other data characteristics. In addition, evening-hour luminosity has been added as a crucial high-frequency indicator. Changes in nightlight intensity contain information about economic activity, especially in countries with a large informal sector and significant data challenges, including in India. The framework for the ‘trade, hotels, transport, communication and services related to broadcasting’ bloc of the Indian GVA has been illustrated in this paper.
C51|The Persistent Statistical Structure of the US Input-Output Coefficient Matrices: 1963-2007|The paper finds evidence for the existence of a statistical structure in the US input-output (I-O) coefficient matrices A = f{aij} for 1963-2007. For various aspects of matrices A we find smooth and unimodal empirical frequency distributions (EFD) with a remarkable stability in their functional form for most of the samples. The EFD of all entries, diagonal entries, row sums, and the (left and right) Perron-Frobenius eigenvectors are well described by fat-tailed distributions while the EFD of column sums and eigenvalue moduli are well explained by the normal distribution and the Beta distribution, respectively. The paper provides several economic interpretations of these statistical results based on the recent developments in the I-O analysis and the price of production literature. Our findings question some probabilistic assumptions conventionally adopted in the stochastic I-O analysis literature and call for a statistical approach to the discussion of the structure of I-O matrices.
C51|Models with Multiplicative Decomposition of Conditional Variances and Correlations|Univariate and multivariate GARCH type models with multiplicative decomposition of the variance to short and long run components are surveyed. The latter component can be either deterministic or stochastic. Examples of both types are studied.
C51|Effects of Happiness on Income Generation and Inequality|This paper examines how happiness affects the income generating capacity of individuals and thereby the distribution of income. It is hypothesized that happiness impacts upon the income generating capacity of individuals directly by stimulating work efficiency, and indirectly by affecting their allocation of time for paid work. Both these effects of happiness on income are tested in a model consisting of an income generating function and a work-hour equation. The Australian panel survey data from the first 14 Waves of HILDA (2001-2014) are used to estimate the model. The income flows of happiness and other variables obtained from the model are inserted into the inequality decomposition equations (rules) to obtain their relative contributions. The study concludes that happiness has a positive and significant effect on income generation and contributes to the reduction of inequality in Australia.
C51|Poverty and inequality impact of natural disasters: Myanmar, 2005 to 2010|According to national household survey data for Myanmar, spanning the five-year interval 2005 to 2010, average real household consumption expenditures remained stagnant, but measured poverty incidence and inequality both declined significantly. The distribution of the economic pie shifted in favor of the poor while the overall size of the pie barely changed. This paper examines the possibility that the hitherto unexplained reduction in measured inequality was caused, at least partly, by a natural disaster, Tropical Cyclone Nargis, which devastated parts of Myanmar in May 2008. This hypothesis is supported by a recent historical study which argues that, globally, large reductions in inequality normally occur only through either man-made or natural disasters. The paper develops a method, based on regression analysis of household level data, for isolating the impact of an exogenous natural event like a cyclone. The estimated regression model is used to simulate a counterfactual distribution of expenditures in which, hypothetically, the cyclone did not occur. The estimated impact of the cyclone is the difference between the observed outcome, in which the cyclone happened, and this simulated, counterfactual outcome in which it did not. The findings indicate that the cyclone reduced inequality between regions of Myanmar, because the negatively affected regions were on average better-off than the unaffected regions, both before and after the cyclone. Within the affected regions the negative impact of the cyclone was largest in absolute terms among richer households, but as a proportion of household expenditures, these negative effects were larger among the poorer households. The cyclone therefore increased economic inequality within the affected regions. Overall measured inequality declined because the between-region reduction exceeded the within-region increase. The hypothesis that the cyclone caused the reduction in inequality is rejected.
C51|A comparison study of realized kernels using different sampling frequencies| A crucial problem by applying realized kernels (RK) is the selection of the bandwidth. We improve the iterative plug-in (IPI) algorithms for selecting bandwidth of RK under independent (Feng and Zhou, 2015b) and dependent microstructure (MS) noise assumptions (Wang, 2014). The realized estimators calculated by both algorithms are consistent to the daily integrated volatility. The nice practical performance of these algorithms are illustrated by the application to 9 years of data on 10 European firms. Moreover, using these two algorithms we calculate RK based on different sampling frequencies and compare them with several other realized estimators. The comparison of these realized estimators is carried out by assessing their performances in the computation of Value-at-Risk (VaR) based on the Semi-FI-Log-ACD model. The RK estimators based on the tick-by-tick returns calculated by both IPI algorithms mentioned above have good performances and are hence recommended using as the estimators of volatility in practice.
C51|Import demand function for Turkey|This study revisits the import demand function for Turkey using the newly defined national income data and examines the evolution of the income and price elasticities over time. In this respect, demand functions are estimated for the total imports and its subcomponents separately, and the corresponding time varying elasticities are obtained by applying the method of Kalman filter between 2003 and 2018. The findings suggest that the growth of total imports is mainly explained by income and relative price changes. The income and expenditure elasticities decrease over time in total imports and in sub-components except for intermediate goods. The relative price elasticity remains almost unchanged for investment and consumption goods imports but increases considerably for the intermediate goods imports and total imports.
C51|Multivariate Filter for Estimating Potential Output and Output Gap in Turkey|This paper estimates the potential output and output gap in Turkey using a multivariate filter. The filter employed links the output gap to slack in the labor market and changes in inflation. Additionally, it produces the output gap taking into account some macroeconomic variables. Though end-of-sample problem remains an issue, results show that the output gap estimates provided by the multivariate filter have a stronger relationship with inflation and are subject to smaller revisions compared to the Hodrick-Prescott filter.
C51|Strong consistency of the least squares estimator in regression models with adaptive learning|This paper looks at the strong consistency of the ordinary least squares (OLS) estimator in a stereotypical macroeconomic model with adaptive learning. It is a companion to Christopeit & Massmann (2017, Econometric Theory) which considers the estimator’s convergence in distribution and its weak consistency in the same setting. Under constant gain learning, the model is closely related to stationary, (alternating) unit root or explosive autoregressive processes. Under decreasing gain learning, the regressors in the model are asymptotically collinear. The paper examines, first, the issue of strong convergence of the learning recursion: It is argued that, under constant gain learning, the recursion does not converge in any probabilistic sense, while for decreasing gain learning rates are derived at which the recursion converges almost surely to the rational expectations equilibrium. Secondly, the paper establishes the strong consistency of the OLS estimators, under both constant and decreasing gain learning, as well as rates at which the estimators converge almost surely. In the constant gain model, separate estimators for the intercept and slope parameters are juxtaposed to the joint estimator, drawing on the recent literature on explosive autoregressive models. Thirdly, it is emphasised that strong consistency is obtained in all models although the near-optimal condition for the strong consistency of OLS in linear regression models with stochastic regressors, established by Lai & Wei (1982), is not always met.
C51|Sufficient Statistics for Unobserved Heterogeneity in Structural Dynamic Logit Models|We study the identification and estimation of structural parameters in dynamic panel data logit models where decisions are forward-looking and the joint distribution of unobserved heterogeneity and observable state variables is nonparametric, i.e., fixed-effects model. We consider models with two endogenous state variables: the lagged decision variable, and the time duration in the last choice. This class of models includes as particular cases important economic applications such as models of market entry-exit, occupational choice, machine replacement, inventory and investment decisions, or dynamic demand of differentiated products. The identification of structural parameters requires a sufficient statistic that controls for unobserved heterogeneity not only in current utility but also in the continuation value of the forward-looking decision problem. We obtain the minimal sufficient statistic and prove identification of some structural parameters using a conditional likelihood approach. We apply this estimator to a machine replacement model.
C51|Estimating Value-at-Risk for the g-and-h distribution: an indirect inference approach|TThe g-and-h distribution is a flexible model with desirable theoretical properties. Especially, it is able to handle well the complex behavior of loss data and it is suitable for VaR estimation when large skewness and kurtosis are at stake. However, parameter estimation is di cult, because the density cannot be written in closed form. In this paper we develop an indirect inference method using the skewed- t distribution as instrumental model. We show that the skewed-t is a well suited auxiliary model and study the numerical issues related to its implementation. A Monte Carlo analysis and an application to operational losses suggest that the indirect inference estimators of the parameters and of the VaR outperform the quantile-based estimators.
C51|Size matters: Estimation sample length and electricity price forecasting accuracy|Electricity price forecasting models are typically estimated via rolling windows, i.e. by using only the most recent observations. Nonetheless, the current literature does not provide much guidance on how to select the size of such windows. This paper shows that determining the appropriate window prior to estimation dramatically improves forecasting performances. In addition, it proposes a simple two-step approach to choose the best performing models and window sizes. The value of this methodology is illustrated by analyzing hourly datasets from two large power markets with a selection of ten different forecasting models. Incidentally, our empirical application reveals that simple models, such as the linear regression, can perform surprisingly well if estimated on extremely short samples.
C51|Estimating a Model of Qualitative and Quantitative Education Choices in France|We estimate a structural model of education choices in which individuals choose between a professional (or technical) and a general track at both high school and university levels using French panel data (Génération 98). The average per-period utility of attending general high school (about 10,000 euros per year) is 20% higher than that of professional high school (about 8000 euros per year). About 64% of total higher education enrollments are explained by this differential. At the same time, professional high school graduates would earn 5% to 6% more than general high school graduates if they both entered the labor market around age 18. The return to post-high school general education is highly convex (as in the US) and is reaped mostly toward the end of the higher education curriculum. Public policies targeting an increase in professional high school enrollments of 10 percentage points would require a subsidy of 300 euros per year of professional high school.
C51|Decomposition of changes in the consumption of macronutrients in Vietnam between 2004 and 2014|Vietnam is undergoing a nutritional transition like many middle-income countries. This transition is characterized by an increase in per capita total calorie intake resulting from an increase in the consumption of fat and protein while the carbohydrate consumption decreases. This paper proposes to highlight the sociodemographic drivers of this transition over the period 2004–2014, using Vietnam Household Living Standard Survey data. We implement a method of decomposition of between-year differences in economic outcomes recently proposed in the literature. This method decomposes the between-year change in various indicators related to the outcome distribution (mean, median, quantiles, etc.) into (1) the effect due to between-year change in the conditional distribution of the outcome given sociodemographic characteristics, or “structure effect”, and (2) the effect due to the differences in sociodemographic characteristics across years, or “composition effect”. In turn, this last effect is decomposed into direct contributions of each sociodemographic characteristics and effects of their interactions. The composition effect, always positive, generally outweighs the structure effect when considering the between-year changes in distributions of per capita calorie intake or calorie intake coming from protein or fat. The effects of changes in the composition of the Vietnamese population thus overcome the effects of changes in preferences of the same population. This finding is reversed in the case of carbohydrates. Food expenditure and household size appear to be the main contributors to the composition effect. The positive effects of these two variables explain well most of the between-year shifts observed in the calorie intake distributions. Urbanization and level of education contribute negatively to the composition effect, with the noticeable exception of fat where the effect of urbanization is positive. But these two variables effects are negligible compared to those of food expenditure and household size.
C51|Measuring the progress of the timeliness childhood immunization compliance in Vietnam between 2006-2014: A decomposition analysis|Vietnam launched the national Expanded Program on Immunization in 1981. Since then, this program has contributed signi cantly to the improvement of child health and to the reduction of child mortality rate. Despite of the fact that the coverage of the national EPI keeps expanding, the number of children who complied with the recommended immunization schedule remains low. This article studies the progress of the timeliness childhood immunization compliance among children between 0-5 years of age in Vietnam from 2006 to 2014 and analyzes the socio-economic factors that account for the changes of the compliance rate during this period. The dataset is extracted from the Multiple Indicator Cluster Survey in 2006 and 2014. We rst identify the socio-economic factors that impact on the vaccination compliance rate using a logistic regression model. Next, we apply the decomposition method to determine the contribution of each factor on the evolution of the timeliness childhood immunization compliance. The progress of the timeliness childhood immunization has been positive and the major contribution comes from the structure e ect (unmeasured e ect). Rural areas show a stronger improvement as of 2014. Among the socio-economic factors, mother education and birth order are the ones that have the larger in uence on the childhood immunization compliance rate. However, these factors have di erent implications in urban and rural areas. These ndings are critical to the current context of Vietnam where the government is designing a strategy focusing on the e ectiveness rather than the traditional coverage indicator.
C51|Macronutrient balances and body mass index: a new insight using compositional data analysis with a total at various quantile orders|Vietnam launched the national Expanded Program on Immunization in 1981. Since then, this program has contributed signi cantly to the improvement of child health and to the reduction of child mortality rate. Despite of the fact that the coverage of the national EPI keeps expanding, the number of children who complied with the recommended immunization schedule remains low. This article studies the progress of the timeliness childhood immunization compliance among children between 0-5 years of age in Vietnam from 2006 to 2014 and analyzes the socio-economic factors that account for the changes of the compliance rate during this period. The dataset is extracted from the Multiple Indicator Cluster Survey in 2006 and 2014. We rst identify the socio-economic factors that impact on the vaccination compliance rate using a logistic regression model. Next, we apply the decomposition method to determine the contribution of each factor on the evolution of the timeliness childhood immunization compliance. The progress of the timeliness childhood immunization has been positive and the major contribution comes from the structure e ect (unmeasured e ect). Rural areas show a stronger improvement as of 2014. Among the socio-economic factors, mother education and birth order are the ones that have the larger in uence on the childhood immunization compliance rate. However, these factors have di erent implications in urban and rural areas. These ndings are critical to the current context of Vietnam where the government is designing a strategy focusing on the e ectiveness rather than the traditional coverage indicator.
C51|Does Social Pressure Hinder Entrepreneurship in Africa? The Forced Mutual Help Hypothesis|In the absence of a public safety net, wealthy Africans have the social obligation to share their re- sources with their needy relatives in the form of cash transfers and inefficient family hiring. We develop a model of entrepreneurial choice that accounts for this social redistributive constraint. We derive pre- dictions regarding employment choices, productivity, and profitability of firms ran by entrepreneurs of African versus non-African origin. Everything else equal, local firms are over-staffed and less productive than firms owned by nonlocals, which discourages local entrepreneurship. Using data from the manu- facturing sector, we illustrate the theory by structurally estimating the proportion of missing African entrepreneurs. Our estimates, which are suggestive due to the data limitation, vary between 8% and 12.6% of the formal sector workforce. Implications for the role of social protection are discussed.
C51|Cointegrated Dynamics for A Generalized Long Memory Process: An Application to Interest Rates|Recent developments in econometric methods enable estimation and testing of general long memory process, which include the general Gegenbauer process. This paper considers the error correction model for a vector general long memory process, which encompasses the vector autoregressive fractionally-integrated moving average and general Gegenbauer process. We modify the tests for unit roots and cointegration, based on the concept of heterogeneous autoregression. The Monte Carlo simulations show that the finite sample properties of the modified tests are satisfactory, while the conventional tests suffer from size distortion. Empirical results for interest rates series for the U.S.A. and Australia indicate that: (1) the modified unit root test detected unit roots for all series, (2) after differencing, all series favour the general Gegenbauer process, (3) the modified test for cointegration found only two cointegrating vectors, and (4) the zero interest rate policy in the U.S.A. has no effect on the cointegrating vector for the two countries.
C51|Unambiguous inference in sign-restricted VAR models|This paper demonstrates how sign restrictions can be used to infer the signs of certain historical shocks from reduced form VAR residuals. This is achieved without recourse to non-sign information. The method is illustrated by an application to the AD-AS model using UK data.
C51|Bayesian Dynamic Tensor Regression|Multidimensional arrays (i.e. tensors) of data are becoming increasingly available and call for suitable econometric tools. We propose a new dynamic linear regression model for tensor-valued response variables and covariates that encompasses some well-known multivariate models such as SUR, VAR, VECM, panel VAR and matrix regression models as special cases. For dealing with the over-parametrization and over-fitting issues due to the curse of dimensionality, we exploit a suitable parametrization based on the parallel factor (PARAFAC) decomposition which enables to achieve both parameter parsimony and to incorporate sparsity effects. Our contribution is twofold: first, we provide an extension of multivariate econometric models to account for both tensor-variate response and covariates; second, we show the effectiveness of proposed methodology in defining an autoregressive process for time-varying real economic networks. Inference is carried out in the Bayesian framework combined with Monte Carlo Markov Chain (MCMC). We show the efficiency of the MCMC procedure on simulated datasets, with different size of the response and independent variables, proving computational efficiency even with high-dimensions of the parameter space. Finally, we apply the model for studying the temporal evolution of real economic networks.
C51|Bayesian Markov Switching Tensor Regression for Time-varying Networks|We propose a new Bayesian Markov switching regression model for multi-dimensional arrays (tensors) of binary time series. We assume a zero-inflated logit dynamics with time-varying parameters and apply it to multi-layer temporal networks. The original contribution is threefold. First, in order to avoid over-fitting we propose a parsimonious parametrization of the model, based on a low-rank decomposition of the tensor of regression coefficients. Second, the parameters of the tensor model are driven by a hidden Markov chain, thus allowing for structural changes. The regimes are identified through prior constraints on the mixing probability of the zero-inflated model. Finally, we model the jointly dynamics of the network and of a set of variables of interest. We follow a Bayesian approach to inference, exploiting the Pólya-Gamma data augmentation scheme for logit models in order to provide an efficient Gibbs sampler for posterior approximation. We show the effectiveness of the sampler on simulated datasets of medium-big sizes, finally we apply the methodology to a real dataset of financial networks.
C51|Nonparametric Forecasting of Multivariate Probability Density Functions|The study of dependence between random variables is the core of theoretical and applied statistics. Static and dynamic copula models are useful for describing the dependence structure, which is fully encrypted in the copula probability density function. However, these models are not always able to describe the temporal change of the dependence patterns, which is a key characteristic of financial data. We propose a novel nonparametric framework for modelling a time series of copula probability density functions, which allows to forecast the entire function without the need of post-processing procedures to grant positiveness and unit integral. We exploit a suitable isometry that allows to transfer the analysis in a subset of the space of square integrable functions, where we build on nonparametric functional data analysis techniques to perform the analysis. The framework does not assume the densities to belong to any parametric family and it can be successfully applied also to general multivariate probability density functions with bounded or unbounded support. Finally, a noteworthy field of application pertains the study of time varying networks represented through vine copula models. We apply the proposed methodology for estimating and forecasting the time varying dependence structure between the S&P500 and NASDAQ indices.
C51|The Joint Estimate of Singleton and Longitudinal Observations: a GMM Approach for Improved Efficiency|"We devise an innovative methodology that allows exploiting information from singleton and longitudinal observations for the estimation of fixed effects panel data models. The approach can be applied to join cross-sectional data and longitudinal data, in order to increase estimation efficiency, while properly tackling the potential bias due to unobserved individual characteristics. Estimation is framed within the GMM context and we assess its properties by means of Monte Carlo simulations. The method is applied to an unbalanced panel of firm data to estimate a Total Factor Productivity regression based on the renown Business Environment and Enterprise Performance Survey (BEEPs) database. Under the assumption that the relationship between observed and unobserved characteristics is homogeneous across singleton and longitudinal observations (or across different samples), information from longitudinal data is used to ""clean"" the bias in the unpaired sample of singletons. This reduces the standard errors of the estimation (in our application, by approximately 8-9 percent) and has the potential to increase the significance of the coefficients."
C51|Are factor biases and substitution identifiable? The Canadian evidence|Revised productivity accounts recently released by Statistics Canada are used to estimate a KlumpMcAdamWillman (KMW) normalized CES supply-side system for the half-century 19612012. The model permits distinct rates of factor-augmenting technical change for capital and labour that distinguish between short-term versus long-term effects, as well as a non-unitary elasticity of substitution and time-varying factor shares. The advantage of the Canadian data for this purpose is that they provide a unified treatment of measurement issues that have had to be improvised in the US and European data used by previous researchers. In contrast to previous results, we find that an elasticity of substitution and distinct factor biases of technological progress are not well determined by the model. For the Canadian data, the KMW model does not appear to provide a framework that overcomes the classic DiamondMcFaddenRodriguez non-identification result. That impossibility theorem is manifested in our findings, not overcome by them.
C51|Regulation of Geo-blocking: does it address the problem of low intraEU iTrade?|The goal of the article is to critically verify and to discuss main areas necessary for increasing intraEU iTrade. To achieve this aim, the legal analysis of the regulation is confronted with the econometric approach identifying the main determinants of cross-border e-commerce. The model is based on data from a survey conducted among 6901 Polish business owners in 2017. Our hypotheses include the following issues: (1) the main barrier of intraEU iTrade is the low level of integration of digital technologies by SMEs; therefore (2) the character of the impact of legal means adopted in the regulation 2018/302 on the development of transnational service provision and entrepreneurship based on e-commerce is doubtful. We argue that the introduction of more and more advanced requirements by EC will only reduce their interest in developing business abroad through cross-border e-commerce and result in losing the opportunities linked to the development of digital economy.
C51|Hybrid choice models vs. endogeneity of indicator variables: a Monte Carlo investigation|We investigate the problem of endogeneity in the context of hybrid choice (integrated choice and latent variable) models. We first provide a thorough analysis of potential causes of endogeneity and propose a working taxonomy. We demonstrate that although it is widely believed that the hybrid choice framework is devoid of the endogeneity problem, there is no theoretical reason to expect that this is the case. We then demonstrate empirically that the problem exists in the hybrid choice framework too. By conducting a Monte Carlo experiment, we display the extent of the bias resulting from measurement and endogeneity biases. Finally, we propose two novel solutions to address the problem: by explicitly accounting for correlation between structural and discrete choice component error terms (or with random parameters in a utility function), or by introducing additional latent variables. Using simulated data, we demonstrate that these approaches work as expected, that is, they result in unbiased estimates of all model parameters.
C51|Estimation and Inference in Adaptive Learning Models with Slowly Decreasing Gains|This paper develops techniques of estimation and inference in a prototypical macroeconomic adaptive learning model with slowly decreasing gains. A sequential three-step procedure based on a ‘super-consistent’ estimator of the rational expectations equilibrium parameter is proposed. It is shown that this procedure is asymptotically equivalent to first estimating the structural parameters jointly via ordinary least-squares (OLS) and then using the so-obtained estimates to form a plug-in estimator of the rational expectations equilibrium parameter. In spite of failing Grenander’s conditions for well-behaved data, a limiting normal distribution of the estimators centered at the true parameters is derived. Although this distribution is singular, it can nevertheless be used to draw inferences about joint restrictions by applying results from Andrews (1987) to show that Wald-type statistics remain valid when equipped with a pseudo-inverse. Monte-Carlo evidence confirms the accuracy of the asymptotic theory for the finite sample behaviour of estimators and test statistics discussed here.
C51|Efficient Forecasting of Electricity Spot Prices with Expert and LASSO Models|Recent electricity price forecasting (EPF) studies suggest that the least absolute shrinkage and selection operator (LASSO) leads to well performing models that are generally better than those obtained from other variable selection schemes. By conducting an empirical study involving datasets from two major power markets (Nord Pool and PJM Interconnection), three expert models, two multi-parameter regression (called baseline ) models and four variance stabilizing transformations combined with the seasonal component approach, we discuss the optimal way of implementing the LASSO. We show that using a complex baseline model with nearly 400 explanatory variables, a well chosen variance stabilizing transformation (asinh or N-PIT), and a procedure that recalibrates the LASSO regularization parameter once or twice a day indeed leads to significant accuracy gains compared to the typically considered EPF models. Moreover, by analyzing the structures of the best LASSO-estimated models, we identify the most important explanatory variables and thus provide guidelines to structuring better performing models.
C51|A note on averaging day-ahead electricity price forecasts across calibration windows|We propose a novel concept in energy forecasting and show that averaging day-ahead electricity price forecasts of a predictive model across 28- to 728-day calibration windows yields better results than selecting only one 'optimal' window length. Even more significant accuracy gains can be achieved by averaging over a few, carefully selected windows.
C51|Probabilistic electricity price forecasting with NARX networks: Combine point or probabilistic forecasts?|A recent electricity price forecasting (EPF) study has shown that the Seasonal Component Artificial Neural Network (SCANN) modeling framework, which consists of decomposing a series of spot prices into a trend-seasonal and a stochastic component, modeling them independently and then combining their forecasts, can yield more accurate point predictions than an approach in which the same non-linear autoregressive NARX-type neural network is calibrated to the prices themselves. Here, considering two novel extensions of the SCANN concept to probabilistic forecasting, we find that (i) efficiently calibrated NARX networks can outperform their autoregressive counterparts, even without combining forecasts from many runs, and that (ii) in terms of accuracy it is better to construct probabilistic forecasts directly from point predictions, however, if speed is a critical issue, running quantile regression on combined point forecasts (i.e., committee machines) may be an option worth considering. Moreover, we confirm an earlier observation that averaging probabilities outperforms averaging quantiles when combining predictive distributions in EPF.
C51|Selection of Calibration Windows for Day-Ahead Electricity Price Forecasting|We conduct an extensive empirical study on the selection of calibration windows for day-ahead electricity price forecasting, which involves six year-long datasets from three major power markets and four autoregressive expert models fitted either to raw or transformed prices. Since the variability of prediction errors across windows of different lengths and across datasets can be substantial, selecting ex-ante one window is risky. Instead, we argue that averaging forecasts across different calibration windows is a robust alternative and introduce a new, well-performing weighting scheme for averaging these forecasts.
C51|Understanding intraday electricity markets: Variable selection and very short-term price forecasting using LASSO|Using a unique set of prices from the German EPEX market we take a closer look at the fine structure of intraday markets for electricity with its continuous trading for individual load periods up to 30 minutes before delivery. We apply the least absolute shrinkage and selection operator (LASSO) to gain statistically sound insights on variable selection and provide recommendations for very short-term electricity price forecasting.
C51|“Tracking economic growth by evolving expectations via genetic programming: A two-step approach”|The main objective of this study is to present a two-step approach to generate estimates of economic growth based on agents’ expectations from tendency surveys. First, we design a genetic programming experiment to derive mathematical functional forms that approximate the target variable by combining survey data on expectations about different economic variables. We use evolutionary algorithms to estimate a symbolic regression that links survey-based expectations to a quantitative variable used as a yardstick (economic growth). In a second step, this set of empirically-generated proxies of economic growth are linearly combined to track the evolution of GDP. To evaluate the forecasting performance of the generated estimates of GDP, we use them to assess the impact of the 2008 financial crisis on the accuracy of agents' expectations about the evolution of the economic activity in 28 countries of the OECD. While in most economies we find an improvement in the capacity of agents' to anticipate the evolution of GDP after the crisis, predictive accuracy worsens in relation to the period prior to the crisis. The most accurate GDP forecasts are obtained for Sweden, Austria and Finland.
C51|Combining sign and parametric restrictions in SVARs by Givens Rotations|This paper develops a method for combining sign and parametric restrictions in SVARs by means of Givens matrices. The Givens matrix is used to rotate an initial set of orthogonal shocks in the SVAR. Parametric restrictions are imposed on the Givens matrix in a manner which utilises its properties. This gives rise to a system of equations which can be solved recursively for the ¡®angles¡¯ in the constituent Givens matrices to enforce the parametric restrictions. The method is applied to several identifications which involve a combination of sign restrictions, and long-run and/or contemporaneous restrictions in Peersman¡¯s (2005) SVAR for the US economy. The method is compared to the recently developed method of Aries, Rubio-Ramirez and Waggoner (2018) which combines zero and sign restrictions.
C51|An IV framework for combining sign and long-run parametric restrictions in SVARs|This paper develops a method to impose a long?run restriction in an instrumental variables (IV) framework in a SVAR which is comprised of both I(1) and I(0) variables when the shock associated with one of the I(0) variables is made transitory. This is the identification which is utilized in the small open economy SVAR that we take from the literature. The method is combined with a recently developed sign restrictions approach which can be applied in an IV setting. We then consider an alternate identification in this SVAR which makes the shocks associated with all of the I(0) variables transitory. In this case, we show that another method can be used to impose the long-run restrictions. The results from both methods are reported for the SVARs estimated with Canadian data.
C51|Chinese resource demand or commodity price shocks: Macroeconomic effects for an emerging market economy|This paper empirically addresses the hypothesis that of the external commodity based sector, Chinese resource demand is the most important driver of emerging market economy business cycles using Brazil as a representative case. Using a structural VAR to examine the effects of Chinese resource demand, world commodity prices and foreign output on domestic macroeconomic variables, we show that shocks to Chinese demand induce an expansion in Brazilian resource exports, the non-tradeable primary commodity sector and other domestic activity. Commodity price shocks are less favorable than Chinese resource demand shocks. Our findings identify the important role of the interest rate in amplifying the real effects of the commodity sector boom, in contrast to the role of the interest rate in developed countries. By incorporating Chinese resource demand in addition to commodity prices, commodity prices play a smaller role in explaining the variance of domestic output than found in previous literature.
C51|Uncertain Kingdom: Nowcasting GDP and its Revisions|We design a new econometric framework to nowcast macroeconomic data subject to revisions, and use it to predict UK GDP growth in real-time. To this aim, we assemble a novel dataset of monthly and quarterly indicators featuring over ten years of real-time data vintages. Successive monthly estimates of GDP growth for the same quarter are treated as correlated observables in a Dynamic Factor Model (DFM) that also includes a large number of mixed-frequency predictors, leading to the release-augmented DFM (RA-DFM). The framework allows for a simple characterisation of the stochastic process for the revisions as a function of the observables, and permits a detailed assessment of the contribution of the data flow in informing (i) forecasts of quarterly GDP growth; (ii) the evolution of forecast uncertainty; and (iii) forecasts of revisions to early released GDP data. By evaluating the real-time performance of the RA-DFM, we find that the model’s predictions have information about the latest GDP releases above and beyond that contained in the statistical office earlier estimates; predictive intervals are well-calibrated; and UK GDP growth real-time estimates are commensurate with professional nowcasters. We also provide evidence that statistical office data on production and labour markets, subject to large publication delays, account for most of the forecastability of the revisions.
C51|Bayesian estimation of DSGE models: Identification using a diagnostic indicator|Koop et al. (2013) suggest a simple diagnostic indicator for the Bayesian estimation of the parameters of a DSGE model. They show that, if a parameter is well identified, the precision of the posterior should improve as the (artificial) data size T increases, and the indicator checks the speed at which precision improves. As it does not require any additional programming, a researcher just needs to generate artificial data and estimate the model with increasing sample size, T. We apply this indicator to the benchmark Smets and Wouters (2007) DSGE model of the US economy, and suggest how to implement this indicator on DSGE models.
C51|Early retirement decisions: Lessons from a dynamic structural modelling| Early retirement has many causes according to economic and sociological literature. These causes may be the preference for leisure, nancial and health conditions, and social environment. In our paper, we aim to specify and estimate an econometric model to assess the early retirement decision-making process for aged workers. We specify a worker's utility function from which we derive worker's probability to retire earlier that depends on her health stock, estate value and preference for future. We also estimate an health production and an health consumption functions that are key factors in the individual's decision to retire earlier. Thus, we show that our model disentangles between three groups of workers: (i) those who choose early retirement, (ii) those who will never choose early retirement and (iii) those who are uncertain about early retirement. We also show that our predicted early retirement probability is a good predictor of early retirement as it is causal for observed early retirement.
C51|Cointegrated Dynamics for A Generalized Long Memory Process|Recent developments in econometric methods enable estimation and testing of general long memory process, which include the general Gegenbauer process. This paper considers the error correction model for a vector general long memory process, which encompasses the vector autoregressive fractionally-integrated moving average and general Gegenbauer process. We modify the tests for unit roots and cointegration, based on the concept of heterogeneous autoregression. The Monte Carlo simulations show that the finite sample properties of the modified tests are satisfactory, while the conventional tests suffer from size distortion. Empirical results for interest rates series for the U.S.A. and Australia indicate that: (1) the modified unit root test detected unit roots for all series, (2) after differencing, all series favour the general Gegenbauer process, (3) the modified test for cointegration found only two cointegrating vectors, and (4) the zero interest rate policy in the U.S.A. has no effect on the cointegrating vector for the two countries
C51|Measuring the Natural Rates of Interest in Germany and Italy|In this paper a semi-structural econometric model is implemented in order to estimate the natural rates of interest in two large economies of the Euro Area: Germany an Italy. The estimates suggest that after the financial crisis of 2007-2008 a decrease of the growth rate of potential output and the corresponding natural rate of interest was greater in Italy than in Germany which could have had important implications for the effectiveness of a common monetary policy. Unlike in other studies, it is found that the monetary policy stance was less expansionary in Italy as compared to Germany for the whole after-crisis period.
C51|“Tracking economic growth by evolving expectations via genetic programming: A two-step approach”|The main objective of this study is to present a two-step approach to generate estimates of economic growth based on agents’ expectations from tendency surveys. First, we design a genetic programming experiment to derive mathematical functional forms that approximate the target variable by combining survey data on expectations about different economic variables. We use evolutionary algorithms to estimate a symbolic regression that links survey-based expectations to a quantitative variable used as a yardstick (economic growth). In a second step, this set of empirically-generated proxies of economic growth are linearly combined to track the evolution of GDP. To evaluate the forecasting performance of the generated estimates of GDP, we use them to assess the impact of the 2008 financial crisis on the accuracy of agents' expectations about the evolution of the economic activity in 28 countries of the OECD. While in most economies we find an improvement in the capacity of agents' to anticipate the evolution of GDP after the crisis, predictive accuracy worsens in relation to the period prior to the crisis. The most accurate GDP forecasts are obtained for Sweden, Austria and Finland.
C51|“A regional perspective on the accuracy of machine learning forecasts of tourism demand based on data characteristics”|In this work we assess the role of data characteristics in the accuracy of machine learning (ML) tourism forecasts from a spatial perspective. First, we apply a seasonal-trend decomposition procedure based on non-parametric regression to isolate the different components of the time series of international tourism demand to all Spanish regions. This approach allows us to compute a set of measures to describe the features of the data. Second, we analyse the performance of several ML models in a recursive multiple-step-ahead forecasting experiment. In a third step, we rank all seventeen regions according to their characteristics and the obtained forecasting performance, and use the rankings as the input for a multivariate analysis to evaluate the interactions between time series features and the accuracy of the predictions. By means of dimensionality reduction techniques we summarise all the information into two components and project all Spanish regions into perceptual maps. We find that entropy and dispersion show a negative relation with accuracy, while the effect of other data characteristics on forecast accuracy is heavily dependent on the forecast horizon.
C51|“A geometric approach to proxy economic uncertainty by a metric of disagreement among qualitative expectations”|In this study we present a geometric approach to proxy economic uncertainty. We design a positional indicator of disagreement among survey-based agents' expectations about the state of the economy. Previous dispersion-based uncertainty indicators derived from business and consumer surveys exclusively make use of the two extreme pieces of information coming from the respondents expecting a variable to rise and to fall. With the aim of also incorporating the information coming from the share of respondents expecting a variable to remain constant, we propose a geometrical framework and use a barycentric coordinate system to generate a measure of disagreement, referred to as a discrepancy indicator. We assess its performance, both empirically and experimentally, by comparing it to the standard deviation of the share of positive and negative responses, which has been used by Bachman et al. (2013) as a proxy for economic uncertainty. When applied in sixteen European countries, we find that both time-varying metrics co-evolve in most countries for expectations about the country's overall economic situation in the present, but not in the future. Additionally, we obtain their simulated sampling distributions and we find that the proposed indicator gravitates uniformly towards the three vertices of the simplex representing the three answering categories, as opposed to the standard deviation, which tends to overestimate the level of uncertainty as a result of ignoring the no-change responses. Consequently, we find evidence that the information coming from agents expecting a variable to remain constant has an effect on the measurement of disagreement.
C51|“A new metric of consensus for Likert scales”|In this study we present a metric of consensus for Likert-type scales. The measure gives the level of agreement as the percentage of consensus among respondents. The proposed framework allows to design a positional indicator that gives the degree of agreement for each item and for any given number of reply options. In order to assess the performance of the proposed metric of consensus, in an iterated one-period ahead forecasting experiment we test whether the inclusion of the degree of agreement in consumers’ expectations regarding the evolution of unemployment improves out-of-sample forecast accuracy in eight European countries. We find evidence that the degree of agreement among consumers contains useful information to predict unemployment rates in most countries. The obtained results show the usefulness of consensus-based metrics to track the evolution of economic variables.
C51|Bi-Demographic Changes and Current Account using SVAR Modeling|The paper aims to explore the impacts of bi-demographic structure on the current account and growth. Using a SVAR modeling, we track the dynamic impacts between these underlying variables. New insights have been developed about the dynamic interrelation between population growth, current account and economic growth. The long-run net impact on economic growth of the domestic working population growth and demand labor for emigrants is positive, due to the predominant contribution of skilled emigrant workers. Besides, the positive long-run contribution of emigrant workers to the current account growth largely compensates the negative contribution from the native population, because of the predominance of skilled compared to unskilled workforce. We find that a positive shock in demand labor for emigrant workers leads to an increasing effect on native active age ratio. Thus, the emigrants appear to be more complements than substitutes for native workers.
C51|Sufficient Statistics for Unobserved Heterogeneity in Structural Dynamic Logit Models|We study the identification and estimation of structural parameters in dynamic panel data logit models where decisions are forward-looking and the joint distribution of unobserved heterogeneity and observable state variables is nonparametric, i.e., fixed-effects model. We consider models with two endogenous state variables: the lagged decision variable, and the time duration in the last choice. This class of models includes as particular cases important economic applications such as models of market entry-exit, occupational choice, machine replacement, inventory and investment decisions, or dynamic demand of differentiated products. The identification of structural parameters requires a sufficient statistic that controls for unobserved heterogeneity not only in current utility but also in the continuation value of the forward-looking decision problem. We obtain the minimal sufficient statistic and prove identification of some structural parameters using a conditional likelihood approach. We apply this estimator to a machine replacement model.
C51|Day-ahead electricity price forecasting with high-dimensional structures: Univariate vs. multivariate modeling frameworks|We conduct an extensive empirical study on short-term electricity price forecasting (EPF) to address the long-standing question if the optimal model structure for EPF is univariate or multivariate. We provide evidence that despite a minor edge in predictive performance overall, the multivariate modeling framework does not uniformly outperform the univariate one across all 12 considered datasets, seasons of the year or hours of the day, and at times is outperformed by the latter. This is an indication that combining advanced structures or the corresponding forecasts from both modeling approaches can bring a further improvement in forecasting accuracy. We show that this indeed can be the case, even for a simple averaging scheme involving only two models. Finally, we also analyze variable selection for the best performing high-dimensional lasso-type models, thus provide guidelines to structuring better performing forecasting model designs.
C51|Factor-Driven Two-Regime Regression|We propose a novel two-regime regression model where the switching between the regimes is driven by a vector of possibly unobservable factors. When the factors are latent, we estimate them by the principal component analysis of a much larger panel data set. Our approach enriches conventional threshold models in that a vector of factors may represent economy-wide shocks more realistically than a scalar observed random variable. Estimating our model brings new challenges as well as opportunities in terms of both computation and asymptotic theory. We show that the optimization problem can be reformulated as mixed integer optimization and present two alternative computational algorithms. We derive the asymptotic distributions of the resulting estimators under the scheme that the threshold effect shrinks to zero. In particular, with latent factors, not only do we establish the conditions on factor estimation for a strong oracle property, which are different from those for smooth factor augmented models, but we also identify semi-strong and weak oracle cases and establish a phase transition that describes the effect of first stage factor estimation as the cross-sectional dimension of panel data increases relative to the time-series dimension. Moreover, we develop a consistent factor selection procedure with a penalty term on the number of factors and present a complementary bootstrap testing procedure for linearity with the aid of efficient computational algorithms. Finally, we illustrate our methods via Monte Carlo experiments and by applying them to factor-driven threshold autoregressive models of US macro data.
C51|Oil price shocks and uncertainty: How stable is their relationship over time?|This paper investigates the time-varying relationship between economic/financial uncertainty and oil price shocks in the US. A structural VAR (SVAR) model and a time-varying parameter VAR (TVP-VAR) model are estimated, using six indicators that reflect economic and financial uncertainty. The findings of the study reveal that static frameworks (SVAR) do not show the full dynamics of the oil price shocks effects to the US economic/financial uncertainty. This is owing to the evidence provided by the time-varying framework (TVP-VAR), which convincingly shows that uncertainty responses to the three oil price shocks are heterogeneous both over time and over the different oil price shocks. In particular, uncertainty responses seem to experience a shift in the post global financial crisis period. Thus, the conventional findings that economic fundamentals response marginally, positively or negatively to supply-side, aggregate demand and oil specific demand shocks, respectively, do not necessarily hold at all periods. Rather, they are impacted by the prevailing economic conditions at each time period. The findings are important to policy makers and investors, as they provide new insights on the said relationships.
C51|The WTI/Brent oil futures price differential and the globalisation-regionalisation hypothesis|This study examines the globalisation-regionalisation hypothesis in the WTI/Brent crude oil futures price differential by considering a set of the potential determinants at 1, 3 and 6 months to maturity contracts. To this end, we employ monthly data over the period 1993:1-2016:12 for a set of crude oil-market specific (convenience yield, consumption, production) and oil-futures market specific (open interest, trading volume) determinants. Our results can be outlined as follows. First, the WTI/Brent convenience yield spread can drive a wedge between the WTI and Brent oil futures prices for the nearby month and 3-month contracts. Second, the WTI/Brent oil production spread is a significant determinant for the 1-month, the 3-month and the 6-month to maturity contracts, while the WTI/Brent oil consumption spread is significant for the 6-month contract. Third, the WTI/Brent open interest spread appears to influence the oil futures price variability between the WTI and Brent for the 3-month and the 6-month contracts, while the WTI/Brent trading volume spread lends predictive power for the 1-month and the 3-month contracts. Fourth, the oil futures market does not appear to be globalised in every time period. We provide evidence of a regionalised oil futures market in the short-run horizon. Fifth, our robustness analysis lends support to the above findings. The findings of this study provide valuable information to energy investors, traders and hedgers.
C51|State Correlation and Forecasting: A Bayesian Approach Using Unobserved Components Models|Implications to signal extraction that arise from specifying unobserved components (UC) models with correlated or orthogonal innovations have been well-investigated. In contrast, an analogous statement for forecasting evaluation cannot be made. This paper attempts to fill this gap in light of the recent resurgence of studies adopting UC models for forecasting purposes. In particular, four correlation structures are entertained: orthogonal, correlated, perfectly correlated innovations as well as a novel approach which combines features from two contrasting cases, namely, orthogonal and perfectly correlated innovations. Parameter space restrictions associated with different correlation structures and their connection with forecasting are discussed within a Bayesian framework. Introducing perfectly correlated innovations, however, reduces the covariance matrix rank. To accommodate that, a Markov Chain Monte Carlo sampler which builds upon properties of Toeplitz matrices and recent advances in precision-based algorithms is developed. Our results for several measures of U.S. inflation indicate that the correlation structure between state variables has important implications for forecasting performance as well as estimates of trend inflation.
C51|Characterizing the Canadian Financial Cycle with Frequency Filtering Approaches|In this note, I use two multivariate frequency filtering approaches to characterize the Canadian financial cycle by capturing fluctuations in the underlying variables with respect to a long-term trend. The first approach is a dynamically weighted composite, and the second is a stochastic cycle model. Applying the two approaches to Canada yields several findings. First, the Canadian financial cycle is more than twice as long as the business cycle, with an amplitude almost four times greater. Second, the overall Canadian financial cycle is most strongly associated with household credit and house prices. Third, while Canadian house prices are mostly associated with the financial cycle, they are also significantly tied to the business cycle. Lastly, house prices are found to lead the overall financial cycle. These results are generally in line with findings for other countries studied in literature. Additionally, I compare each approach’s proneness to revision and find that both are more reliable, when monitored in real time, than the Basel III total credit-to-GDP gap. Nonetheless, further work is encouraged to investigate more variable combinations and undertake a cross-country analysis since data on systemic financial stress in Canada are limited. It should be noted that since the approaches produce a measure of the financial cycle relative to trend, comparison with level indicators (as those monitored in the Bank of Canada’s Financial System Review) is not straightforward.
C51|Detecting exchange rate contagion using copula functions|We study exchange rate dependence for seven countries from four different regions of the world. Our sample includes two developed countries, the United Kingdom and Germany (representing the Euro Area), two large emerging Asian economies, South Korea and Indonesia, two Latin American countries, Brazil and Chile, and South Africa. The currencies of all of these countries are actively traded in global forex markets and all of them are important for large international portfolio composition and rebalancing. We construct multivariate copula functions using a regular vine copula approach, allowing for very flexible dependency structures. We find evidence of exchange rate contagion for our set of countries. However, important asymmetries are worth noting. First, contagion occurs only during periods of exchange rate appreciation of the different currencies with respect to the United States Dollar. Second, contagion is more frequent in pairs of countries that include either the United Kingdom or Germany. In fact, the largest tail dependence coefficient corresponds to the pair composed by these two countries’ exchange rates. Third, contagion occurs more within countries of a same region, for instance, between Brazil and Chile, and between Korea and Indonesia. This result shows that during episodes of large currency appreciation hedging strategies for global investors taking positions in large markets requires of regional diversification.
C51|Asymptotically unbiased inference for a panel VAR model with p lags|Panel dynamic estimators with fixed effects are biased due to the incidental parameters problem. At this regard, Hahn and Kuersteiner (2002) proposed an estimator to correct this issue. However, they only consider a panel VAR (PVAR) model with one lag. In this paper we extend this bias correction, its asymptotic and small sample properties for a more general case, a PVAR model with p lags. The simulation results indicate that the bias corrected estimator outperforms the OLS panel VAR estimator when sample size in time dimension is small, and when the persistence of the model is low. In these cases, the proposed estimator improves significantly in terms of both, the reduction of bias and mean square error. **** RESUMEN: Los estimadores de los parámetros de un modelo panel dinámico de efectos fijos son sesgados debido al problema de parámetros incidentales. Al respecto, Hahn y Kuersteiner (2002) proponen un estimador para corregir este problema. Sin embargo, ellos consideran únicamente un modelo panel VAR con un sólo un rezago. En este documento analizamos las propiedades asintóticas y de muestra pequeña del estimador corregido por sesgo para un caso más general, un modelo PVAR con p rezagos. Los resultados de las simulaciones indican que el estimador corregido por sesgo tiene un mejor desempeño con respecto al estimador panel VAR MCO cuando la dimensión temporal de la muestra (T) es pequeña, y cuando la persistencia del modelo es baja. En estos casos, el estimador propuesto presenta una disminución significativa en términos de sesgo, y de error cuadrático medio.
C51|Effects of Interest Rate Caps on Financial Inclusion|In this paper we study the liberalization of the microcredit usury rate in Colombia and its effects on loan expansion. Namely, in February 2007 the interest rate ceiling for microcredit loans was lifted and fixed to 33%, while the ceiling of all other loans remained unchanged and close to 20%. We perform a Difference-in-Diffeence analysis by comparing the expansion of microcredit loans (treatment group) with that of corporate loans (control group). Additionally, we narrow in on similar levels of both loan size and debtor's risk in order to make microcredit and corporate portfolios more comparable. Our results indicate that this policy encouraged and facilitated financial access to entrepreneurs. Specifically, we find that, on average, the amount lent by credit establishments increased between 21:5% and 42:4%, and the number of new loans increased between 25:1% and 47:8%. **** RESUMEN: En este trabajo estudiamos la liberalización de la tasa de usura de la cartera de microcrédito en Colombia y sus efectos sobre la expansión del crédito. Concretamente, en febrero de 2007 la tasa de usura para la cartera de microcrédito se elevó y se fijó en 33%, mientras que la tasa de usura de las otras carteras se mantuvo cerca al 20%. Realizamos un análisis de Diferencias en Diferencias al comparar la expansión de los créditos de microcrédito (grupo de tratamiento) con la de los créditos comerciales (grupo de control). Adicionalmente, analizamos niveles similares tanto del tamaño del crédito como del riesgo del deudor, con el objetivo de que ambas carteras sean más comparables. Nuestros resultados indican que esta política estimuló y facilitó el acceso financiero a las pequeñas y medianas empresas. Específicamente, encontramos que, en promedio, el monto prestado por los establecimientos de crédito aumentó entre 21.5% y 42.4%, y el número de nuevos créditos aumentó entre 25.1% y 47.8%.
C51|Nonlinear state and shock dependence of exchange rate pass through on prices|This paper examines the nature of the pass-through of exchange rate shocks on prices along the distribution chain, and estimates its short and long-term path. It uses monthly data from a small open economy and a smooth transition auto-regressive vector model estimated by Bayesian methods. The main finding is that exchange rate pass-through is nonlinear and state and shock dependent. There are two main policy implications of these findings. First, models used by central banks for policymaking should take into account the nonlinear and endogenous nature of the pass-through. Second, a specific rule on pass-through for monetary policy decisions should be avoided.
C51|Time series with interdependent level and second moment: statistical testing and applications with Greek external trade and simulated data|This work aims to fill an existing gap in the literature regarding the statistical testing for the existence and the identification of the character of time-varying second moment in its dependence on a non-constant mean level in time series. To this end a new statistical testing procedure is introduced with some considerable advantages over the existing ones. Amongst others it is argued that the existing statistical tests are insufficient and sometimes lead to biased results. Further the effect of the application of this methodology on some crucial elements of time series modelling such as outlier detection and seasonal adjustment is examined, through case studies conducted on a comparative basis using both the new methodology and an established one. The severe consequences of the improper treatment of the type of time-varying second moment dealt with in this work are evidenced and emphasized. The data set comprises time series on monthly external trade statistics for Greece. Overall, the resulting empirical evidence favours the new approach. Further supporting evidence is provided by the application of the new methodology to simulated data.
C51|Do Korean Exports Have Different Patterns over Different Regimes?: New Evidence from STAR-VECM|In this work we examine whether the relationships of Korean exports to global GDP and to the exchange rate change depending on whether exports are in their expansionary or their contractionary regimes. To this empirical end we incorporate the two distinct dynamic features of regime change and co-integration into a multi-variable smooth transition autoregressive vector error correction model (STAR-VECM). Our estimation results reveal asymmetries in the short-run relationships of Korea¡¯s exports to global GDP and to the exchange rate, between the contractionary and the expansionary export regimes, although their long-run relationships remain stable. Specifically, the positive effect of real global GDP on Korea¡¯s real exports is inelastic during contractionary regimes but changes to become elastic in expansionary regimes. The effect of the real effective exchange rate on Korea¡¯s real exports is positive and inelastic under contractionary regimes, but becomes negative and elastic under expansionary regimes. Our results suggest that the asymmetric properties of the relationship of Korea¡¯s exports to global GDP and the exchange rates across the different regimes should be taken into account in order to better understand the behavior of Korea¡¯s exports.
