C88|U.S. Macroeconomic Policy Evaluation in an Open Economy Context using Wavelet Decomposed Optimal Control Methods|It is widely recognized that the policy objectives of fiscal and monetary policymakers usually have different time horizons, and this feature may not be captured by traditional econometric techniques. In this paper, we first decompose U.S macroeconomic data using a time-frequency domain technique, namely discrete wavelet analysis. We then model the behavior of the U.S. economy over each wavelet frequency range and use our estimated parameters to construct a tracking model. To illustrate the usefulness of this approach, we simulate jointly optimal fiscal and monetary policy with different short-term targets: an inflation target, a money growth target, an interest rate target, and a real exchange rate target. The results determine the reaction in fiscal and monetary policy that is required to achieve an inflation target in a low inflation environment, and when both fiscal and monetary policy are concerned with meeting certain economic growth objectives. The combination of wavelet decomposition in an optimal control framework can also provide a new approach to macroeconomic forecasting.
C88|Calendar-based Graphics for Visualizing People's Daily Schedules|Calendars are broadly used in society to display temporal information and events. This paper describes a new calendar display for plotting data, that includes a layout algorithm with many options, and faceting functionality. The functions use modulus algebra on the date variable to restructure the data into a calendar format. The user can apply the grammar of graphics to create plots inside each calendar cell, and thus the displays synchronize neatly with ggplot2 graphics. The motivating application is studying pedestrian behavior in Melbourne, Australia, based on counts which are captured at hourly intervals by sensors scattered around the city. Faceting by the usual features such as day and month, is insufficient to examine the behavior. Making displays on a monthly calendar format helps to understand pedestrian patterns relative to events such as work days, weekends, holidays, and special events. The functions for the calendar algorithm are available in the R package sugrrants.
C88|A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data|"Mining temporal data for information is often inhibited by a multitude of formats: irregular or multiple time intervals, point events that need aggregating, multiple observational units or repeated measurements on multiple individuals, and heterogeneous data types. On the other hand, the software supporting time series modeling and forecasting, makes strict assumptions on the data to be provided, typically requiring a matrix of numeric data with implicit time indexes. Going from raw data to model-ready data is painful. This work presents a cohesive and conceptual framework for organizing and manipulating temporal data, which in turn flows into visualization, modeling and forecasting routines. Tidy data principles are extended to temporal data by: (1) mapping the semantics of a dataset into its physical layout; (2) including an explicitly declared index variable representing time; (3) incorporating a ""key"" comprising single or multiple variables to uniquely identify units over time. This tidy data representation most naturally supports thinking of operations on the data as building blocks, forming part of a ""data pipeline"" in time-based contexts. A sound data pipeline facilitates a fluent workflow for analyzing temporal data. The infrastructure of tidy temporal data has been implemented in the R package tsibble."
C88|Shape Factor Asymptotic Analysis I|"The shape factor defined as kurtosis divided by skewness squared K/S^2 is characterized as the only choice among all factors K/〖|S|〗^α ,α>0 which is greater than or equal to 1 for all probability distributions. For a specific distribution family, there may exists α>2 such that min⁡〖K/〖|S|〗^α 〗≥1. The least upper bound of all such α is defined as the distribution’s characteristic number. The useful extreme values of the shape factor for various distributions which are found numerically before, the Beta, Kumaraswamy, Weibull, and GB2 Distribution, are derived using asymptotic analysis. The match of the numerical and the analytical results can be considered prove of each other. The characteristic numbers of these distributions are also calculated. The study of the boundary value of the shape factor, or the shape factor asymptotic analysis, help reveal properties of the original shape factor, and reveal relationship between distributions, such as between the Kumaraswamy distribution and the Weibull distribution."
C88|Teaching computer programming for industrial engineering without teacher|"The C programming language is widely used in computer and industrial engineering. Because of that, such programming language is also widely used as a language to teach programming to industrial engineering students. In Spain, many universities use this language compulsory in the first year, or even in higher courses. Our experience shows that learning computer programming in four months is an arduous task, but curricula require it. Such learning process is also tough by the fact that many students can not attend classes regularly and, even if they attend, sometimes the class is no longer at the level they require. In this work we develop a series of files in ""presentation"" format (.ppsx) that allows students to see several explanations about the most complicated programming C topics: functions, arrays, structures, strings, arrays with structures... This multimedia material includes explanations (voice-over), and animations with examples. Students can watch and listen to the explanations whenever they want and wherever they want (tablet, PC, phone?). Surveys made to students reveal that it is also interesting for students who regularly attend classes, and they prefer to use this course material only at home, outside of regular classes."
C88|Machine Learning vs Traditional Forecasting Methods: An Application to South African GDP|This study employs traditional autoregressive and vector autoregressive forecasting models, as well as machine learning methods of forecasting, in order to compare the performance of each of these techniques. Each technique is used to forecast the percentage change of quarterly South African Gross Domestic Product, quarter-on-quarter. It is found that machine learning methods outperform traditional methods according to the chosen criteria of minimising root mean squared error and maximising correlation with the actual trend of the data. Overall, the outcomes suggest that machine learning methods are a viable option for policy-makers to use, in order to aid their decision-making process regarding trends in macroeconomic data. As this study is limited by data availability, it is recommended that policy-makers consider further exploration of these techniques.
C88|SSP Long Run Scenarios for European NUTS2 Regions|In this paper we illustrate the development of a modeling framework aimed at producing detailed quantitative estimates for economic variables, consistent with Shared Socio-economic Pathways, and their assumptions about national income and population. Our model not only provides information on industrial production levels, employment, consumption patterns, trade flows and other macroeconomic variables, but disaggregates them further at the sub-national level, for European NUTS2 regions. Estimates are produced by an especially designed dynamic general equilibrium model (G-RDEM), augmented with a regional down-scaling module. The latter takes into account the different sectoral composition of the regional economies, their endowments of primary resources, as well as the possible existence of structural and agglomeration externalities. After describing the methodology, the paper presents an illustrative sample of results produced by the model, focusing on Italian regions and the Shared Socio-economic Pathway 1 in the period 2011-2051.
C88|G-RDEM: A GTAP-Based Recursive Dynamic CGE Model for Long-Term Baseline Generation and Analysis|We motivate and detail the new GTAP-based recursive dynamic economic model (G-RDEM), a computable general equilibrium tool for long-term counterfactual analysis and baseline generation from given gross domestic product (GDP) and population projections. It encompasses an implicitly directly additive demand system (AIDADS) demand system with non-linear Engel curves, debt accumulation from foreign saving and introduces sector specific productivity changes, endogenous aggregate saving rates, as well as time-varying cost shares for value added and individual intermediates. Parameters for these relationships are econometrically estimated or taken from published work. The core of the model is derived from the Global Trade Anaylsis Project (GTAP) standard model and seamlessly incorporated into the modular and flexible CGEBox modelling platform, allowing for combined applications with various other extensions, such as GTAP-agro-ecological zones (AEZ) or GTAP-Water. G-RDEM maintains the flexible aggregation from the GTAP Data Base. It is open source, encoded in General Algebraic Modelling System (GAMS) and can be steered by a Graphical User Interface, which also encompasses a tool to analyse results with tables, graphs and maps. Existing GDP and population projections from the Socio-Economic Pathways (SSP) 1-5 can be directly incorporated for baseline construction. A comparison of the generated long-term structural composition of the economy against a simple recursive-dynamic variant, derived from the standard GTAP model, shows that G-RDEM brings about much more plausible results, as well as a more realistic, internally consistent representation of the economic structure in a hypothetical future.
C88|A network-based method to harmonize data classifications|A frequent problem in research is the harmonization of data to a common classification, whether that is in terms of ? to name a few examples ? industries, commodities, occupations, or geograph- ical areas. Statistical offices often provide concordance tables, to match data through time or with different classifications, but these concordance tables alone are often not sufficient to define a clear methodology on how the matching should be performed. In fact, the concordance tables have, in numerous occasions, a many-to-many mapping of classifications. The issue is exacerbated when two or more concordance tables are concatenated. In this Jupyter notebook, I discuss a network- based abstraction of this problem and propose, as a general solution, a method that identifies the network components (or the network communities) to make data converge to a new classification. The method simplifies the issue and reduces greatly conversion errors.
C88|Fake News and Indifference to Truth|State of the Union Addresses (SOUA) by two recent US Presidents, President Obama (2016) and President Trump (2018), and a series of recent of tweets by President Trump, are analysed by means of the data mining technique, sentiment analysis. The intention is to explore the contents and sentiments of the messages contained, the degree to which they dier, and their potential implications for the national mood and state of the economy. President Trump's 2018 SOUA and his sample tweets are identied as being more positive in sentiment than President Obama's 2016 SOUA. This is conrmed by bootstrapped t tests and non-parametric sign tests on components of the respective sentiment scores. The issue of whether overly positive pronouncements amount to self-promotion, rather than intrinsic merit or sentiment, is a topic for future research.
C88|Quality Assessment of Microsimulation Models The Case of EUROMOD|Assessing the quality of microsimulation models is an important contributing factor for motivating their use in both academic and policy environments. This is particularly relevant for EUROMOD, the tax-benefit microsimulation model for the European Union, because it is intended to be widely used. This paper explains how the quality of EUROMOD is assessed. It focusses on the validity and scope of results as particularly important dimensions of quality, and on the transparency with which this assessment is done. It also provides evidence on the extent and breadth of the use of EUROMOD. Some of the key trade-offs between different aspects of quality are identified and the paper concludes with a view on the appropriate division of responsibility for quality assessment, between model developers and users.
C88|The U.S. Syndicated Loan Market: Matching Data|We introduce a new software package for determining linkages between datasets without common identifiers. We apply these methods to three datasets commonly used in academic research on syndicated lending: Refinitiv LPC DealScan, the Shared National Credit Database, and S&P Global Market Intelligence Compustat. We benchmark the results of our match using results from the literature and previously matched files that are publicly available. We find that the company level matching is enhanced by careful cleaning of the data and considering hierarchical relationships. For loan level matching, a tailored approach based on a good understanding of the data can be better in certain dimensions than a more pure machine learning approach. The R package for the company level match can be found on Github at https://github.com/seunglee98/fedmatch.
C88|The U.S. Syndicated Loan Market : Matching Data|We introduce a new software package for determining linkages between datasets without common identifiers. We apply these methods to three datasets commonly used in academic research on syndicated lending: Refinitiv LPC DealScan, the Shared National Credit Database, and S&P Global Market Intelligence Compustat. We benchmark the results of our match using results from the literature and previously matched files that are publicly available. We find that the company level matching is enhanced by careful cleaning of the data and considering hierarchical relationships. For loan level matching, a tailored approach based on a good understanding of the data can be better in certain dimensions than a more pure machine learning approach. The R package for the company level match can be found on Github.
C88|The introduction of a social gradient in mortality in the Destinie 2 model|We use here the mortality tables by education level recently published by Blanpain (2016b) to significantly improve the projected differential mortality in the Destinie 2 model that was usually only based on Insee mortality projections broken down by age and sex categories. We show that the relational proportionality method better predicts the evolution of differential mortality over the recent past than the Brass relationality method, used in other microsimulation models. The introduction of heterogeneous mortality according to education level aims at better reproducing the positive correlation that exists beteen life expectancy and pension levels. To quantify it, we compare the elasticities of survival to the amount of the retirement pension, by sex and age, obtained by microsimulation with those estimated on the Échantillon Interrégimes de Retraités (EIR). The simulated elasticities are lower than the empirical elasticities, especially for women. For men, however, they are statistically significant and at least two-thirds of the empirically observed values. At the aggregate level, this introduction also slightly increases projected pension expenditure.
C88|Spatiotemporal distribution of inclusive wealth data: An illustrated guide|In this paper we develop an illustrated guide for IWR2017 data. Graphical representations aim to reveal the multi-layer nature of IWR data with self-explanatory schemes. There are four parts of the analysis. In the first part, we present the spatial distribution of the three types of capitals - natural, human and produced - associated to social well-being. In the second part, we illustrate capitals’ temporal variation over 1990-2014, on different geographical and economic backgrounds. We investigate the dynamic evolution of capital assets and capture the key trend among different geographical regions and among regions with different economic growth. The third part makes an additional focus on natural capital and its spatial distribution over different income levels and regions. The forth part examines the causal relation between pollution and wealth. All four research questions are confronted with ease, clarity, and accuracy, with digital methods for mapping. A variety of graphical styles and/or forms is employed to indicate the resource use, capital exploitation trends of countries of different economic integration, uncover policies per income level.
C88|Exploring the EMEP Input-Output model of air pollution|The primary objective of this paper is the structural analysis of source-receptor air pollution problems in the EU region. Two views are provided for the analysis: an emission-driven view and a deposition-driven view. Different visual schemes are used to reproduce the global pollution network and identify the biggest sources and sinks of pollution. Visual modelling helps to understand the linkages and interconnections in the transboundary pollution network. Our interactive outputs give the options to zoom in to specific areas of the global source-receptor air pollution scheme and highlight the top emitters or receptors of pollution. Ranking of countries in decreasing order of pollution responsibility and/or vulnerability using graph metrics is a main result. Data sources are emissions-depositions (or source-receptor) tables of air pollutants, available online from the data repository of the European Monitoring and Evaluation Program (EMEP) of the Long-Range Transmission of Air Pollutants in Europe. In our computer-based visual analysis, we employ solely open software.
C88|Public Relations Research in the Time of Big Data|Public relations research has been facing many challenges in a fast-changing media environment. How to measure public relations effects? This remains the key question for many scholars and communication professionals. In the time of big data, possibilities to measure different aspects of human activities seem accessible. However, the challenge of coping with 3Vs (Volume, Velocity, Variety) of big data seems as an exhaustible effort to get a whole picture and interpret the meaning of these data. Undoubtedly, big data research represents an interdisciplinary approach. In public relations research interdisciplinarity was always present and therefore scholars and public relations professionals are in search of possible tools, designs and solutions that can help in big data analysis. The aim of this paper is to present possible research designs and solutions for public relations research concerning big data and user-generated content (UGC). As communicative practices are increasingly changing and moving on to social media platforms, focus of public relations research is also moving online. The author is examining collection, aggregation, analysis and interpretation of data obtained from various online sources that are publicly available. In terms of big data, the analysis is focused on user-generated content as a potential manifestation of public relations activities. The author is analysing UGC with real-time sentiment analysis and other available tools.
C88|Mobile application for diabetes follow-up and analysis|It is very difficult for people with diabetes, one of the most common diseases of recent years, to follow factors that affect their disease. With the development of technology, patients have become able to measure their own blood sugar using tiny devices whenever they want, wherever they want, instead of going to hospitals to measure blood sugar. Considering that nowadays when smartphones are so widespread, diabetes can be easily followed using a mobile application. In this paper, design and development of a mobile application that allows a patient to save his/her diabetes related parameters at certain intervals and transmit the values to his/her doctor is explained. Since potential users of the designed application are Turkish people, the interfaces of the software application are in Turkish. The user interface in English is currently being developed.
C88|A simple application for network stegonagraphy|The Internet has changed the paradigm of traditional circuit switch network largely. The services and applications have been created by the network users themselves. This paradigm shift is one of the main sources of the tremendous success of the Internet. On the other hand, although the Internet has created many new possibilities and opportunities, communication through the Internet is subject to many security risks. In this paper, we propose a simple application to transfer sensitive data securely and present its details. The application we propose secures the exchange of sensitive data by relying on network steganography. When the users input data at the application?s interface, the data is hidden in IP packets before transmission so that it cannot be obtained by malicious users. Since the required libraries are more stable and can be easily found on the Linux platform, the application was developed for the Linux platform.
C88|Student Perceptions on the use of Student Response System in Higher Education in Hong Kong|The SRS (Student Response System) is a software tool that is designed to facilitate students to learn by making the lessons more interesting and interactive, quickly assessing their understanding of the subject, and inspiring discussions. While the traditional SRS makes use of custom-made devices called Clickers, recently there are many mobile phone-based SRSs developed and accessible through the Internet. However, many university teachers are hesitant to use SRS due to the lack of research about student perception of the application of SRS in higher education. Our study will report on the student perception of using mobile phone-based SRS in a self-financed higher education institution in Hong Kong. Data were collected from over 400 students using online surveys during the autumn semester in 2017.This paper will start a concise overview of the SRS technology. Then it will report the sampling method and survey procedure. Finally, it will show the analysis of the results using the Technology Acceptance Model (TAM). This research shows that the students have positive perceptions on the usefulness and ease of use of the SRS. However, the students in the early stage of study have a significantly more positive perception on the ease of use than the students in the final stage of study. All the students have positive intention to continue to use SRS. Hence, we recommend teachers should adopt SRS in their classroom teacher, with more attention in making questions easier to understand for final stage students.
C88|Fake News And Indifference To Truth: Dissecting Tweets And State Of The Union Addresses By Presidents Obama And Trump|State of the Union Addresses (SOUA) by two recent US Presidents, President Obama (2016) and President Trump (2018), and a series of recent of tweets by President Trump, are analysed by means of the data mining technique, sentiment analysis. The intention is to explore the contents and sentiments of the messages contained, the degree to which they differ, and their potential implications for the national mood and state of the economy. President Trump's 2018 SOUA and his sample tweets are identified as being more positive in sentiment than President Obama's 2016 SOUA. This is confirmed by bootstrapped t tests and non-parametric sign tests on components of the respective sentiment scores. The issue of whether overly positive pronouncements amount to self-promotion, rather than intrinsic merit or sentiment, is a topic for future research.
C88|Carpooling with heterogeneous users in the bottleneck model|"When drivers opt for carpooling, road capacity will be freed up, and this will reduce congestion. Therefore, carpooling is interesting for policy makers as a possible solution to congestion. We investigate the effects of carpooling in a dynamic equilibrium model of congestion, which captures various dimensions of heterogeneity: heterogeneity in preference for carpooling, ""ratio heterogeneity"" between the values of time and the values of schedule delay, and ""proportional heterogeneity"" that scales all values equally. We investigate three policy scenarios: no-toll, first-best pricing, and subsidization of carpooling. The optimal second-best subsidy equals each type’s heterogeneous marginal external benefit (MEB) of switching to carpooling. If such differentiation is impossible, the third-best subsidy is a weighted average of the MEBs, where the weights depend on the number of each type and their sensitivity to the subsidy. In our numerical example, we find that when increasing the degree of ""ratio heterogeneity"", the relative efficiency of the second-best subsidization first increases and then falls with the degree of heterogeneity and L type carpoolers benefit more than H type carpoolers. However, when increasing the degree of ""proportional heterogeneity"", H type users benefit more than L types for both solo drivers and carpoolers. Moreover, the relative efficiency of the second-best subsidization decreases throughout."
C88|Fake news and indifference to scientific fact: President Trump’s confused tweets on global warming, climate change and weather|Abstract A set of 115 tweets on climate change by President Trump, from 2011 to 2015, are analysed by means of the data mining technique, sentiment analysis. The intention is to explore the contents and sentiments of the messages contained, the degree to which they differ, and their implications about his understanding of climate change. The results suggest a predominantly negative emotion in relation to tweets on climate change, but they appear to lack a clear logical framework, and confuse short term variations in localised weather with long term global average climate change.
C88|Exploring Long Run Structural Change with a Dynamic General Equilibrium Model|In this paper we present a computable general equilibrium model (G-RDEM), specifically designed for the generation of long run scenarios of economic development, featuring a non-homothetic demand system, endogenous saving rates, differentiated industrial productivity growth, interest payments on foreign debt and time-varying input-output coefficients. To the best of our knowledge, this is the first model of this kind. We illustrate how parameters of the five modules of structural change have been estimated, and we test the model by comparing its results with those obtained by a more conventional recursive dynamic CGE model. Both models are driven by the same GDP and population data, exogenously provided by the IPCC Shared Socio-economic Pathway 3. GDP levels determine the endogenous productivity parameters. Population affects the definition of per capita income, which in turn affects the household demand system and the variation of input-output coefficients. Information on the demographic structure is also employed to modify the aggregate saving rate parameters. It is found that the two models do produce different findings, both globally and at the regional and industrial level. Understanding the origins of such differences sheds some light on how mechanisms of structural change operate in the long run.
C88|Web mining of firm websites: A framework for web scraping and a pilot study for Germany|Nowadays, almost all (relevant) firms have their own websites which they use to publish information about their products and services. Using the example of innovation in firms, we outline a framework for extracting information from firm websites using web scraping and data mining. For this purpose, we present an easy and free-to-use web scraping tool for large-scale data retrieval from firm websites. We apply this tool in a large-scale pilot study to provide information on the data source (i.e. the population of firm websites in Germany), which has as yet not been studied rigorously in terms of its qualitative and quantitative properties. We find, inter alia, that the use of websites and websites' characteristics (number of subpages and hyperlinks, text volume, language used) differs according to firm size, age, location, and sector. Web-based studies also have to contend with distinct outliers and the fact that low broadband availability appears to prevent firms from operating a website. Finally, we propose two approaches based on neural network language models and social network analysis to derive firm-level information from the extracted web data.
C88|Cost Analysis of Poor Quality Using a Software Simulation|The issues of quality, cost of poor quality and factors affecting quality are crucial to maintaining a competitiveness regarding to business activities. Use of software applications and computer simulation enables more effective quality management. Simulation tools offer incorporating the variability of more variables in experiments and evaluating their common impact on the final output. The article presents a case study focused on the possibility of using computer simulation Monte Carlo in the field of quality management. Two approaches for determining the cost of poor quality are introduced here. One from retrospective scope of view, where the cost of poor quality and production process are calculated based on historical data. The second approach uses the probabilistic characteristics of the input variables by means of simulation, and reflects as a perspective view of the costs of poor quality. Simulation output in the form of a tornado and sensitivity charts complement the risk analysis.
C88|Modelling a small open economy using a wavelet-based control model|This paper develops a wavelet-based control system model that can be used to simulate fiscal and monetary strategies in an open economy context in the time-frequency domain. As the emphasis on real exchange rate stability is increased, the model simulates the effects on both the aggregate and decomposed trade balance under both constant and depreciating real exchange rate targets, and also the effects on the real GDP expenditure components. This paper adds to recent research in this area by incorporating an external sector via the use of a real effective exchange rate as a driver of output. The research is also the first to analyze exchange rate effects within a time-frequency model with integrated fiscal and monetary policies in an open-economy applied wavelet-based optimal control setting. To demonstrate the usefulness of this model, we use post-apartheid South African macro data under a political targeting design for the frequency range weights, where we simulate jointly optimal fiscal and monetary policy under varying preferences for real exchange rate stability.
C88|Dimensions of Quality of Life in Germany: Measured by Plain Text Responses in a Representative Survey (SOEP)|"This paper demonstrates how quality of life can be measured by plain text in a representative survey, the German Socio Economic Panel Study (SOEP). Furthermore, the paper shows that problems that are difficult to monitor, especially problems like the state of the European Union, long-term climate change but also the national debt or problems with the quality of consumer goods (like food) and services (like medical treatment), are not issues of particular importance to the majority of people. Developments and risks that are difficult to monitor and only have long-term effects should be left primarily to the discourse conducted by experts and the politically-minded ""elites"", the avant garde. And in representative democracies it is ultimately the parliamentarians who must decide. Parliamentarians are likely able to make somewhat better decisions using modern representative surveys and national dialogues than they would be without these instruments of civic participation. Nevertheless, improved civic participation cannot replace parliaments."
C88|Obeying vs. resisting unfair laws. A structural analysis of the internalization of collective preferences on redistribution using classification trees and random forests|In this paper, we study whether individual normative preferences are affected by the knowledge of collective normative preferences. In a questionnaire-experimental framework, we study whether respondents obey, resist or are indifferent to a very unfair but legal distribution of an inheritance between a minimum wage-earner and a millionaire. In addition to regressions, we use classification trees and random forests to provide a full picture of how asymmetric combinations of self-interest and ideological factors may lead to identical individual redistributive preferences and law internalization attitudes. We find that sensitivity to procedural fairness and responsibility cut opinions are good predictors of individual redistributive preferences. We also find that law internalization is associated with the support of core normative values, but not with the support of fairness as procedures. This echoes Cooterâ€™s hypothesis of â€˜meta preferencesâ€™ triggering an expressive vs. backlash effects of laws. Lastly, we find that, among the law-sensitive, the social â€˜losersâ€™ tend to submit to the unfair but legal collective preference while the social â€˜winnersâ€™ tend to either be indifferent of voice their disagreement.
C88|Climate change effects and their interactions: An analysis aiming at policy implications|In this study we provide a computerized graph structure for synthesizing and displaying the data on a region’s ecosystem-economic system. By applying Mathematica-based graph modeling we create a causal network of the synergistic impact mechanism among certain climate related factors. Our computational approach identifies a climate factor that affects most immediately or most strongly the others. Important factors are indicated through the use of graph theoretical tools. Our graph-based approach and its computational aspects allow for factor ranking(s) according to their importance to the network both numerically and visually, for certain settlement types. Our contribution provides quantitative estimates of impacts and adaptation potentials of five potential effects of climate change (migration, flooding-landslides-fire, air and water pollution, human health and energy-water-other resources) which play a substantial role at the synergistic impact mechanism. By using graph visualization techniques, the structure of the synergistic impact mechanism is self-evident. Specifically, graph layouts are created to detect i) the causal relationships of the synergistic mechanism under study ii) the most influential factor(s) in the synergistic mechanism and iii) classify the factor’s roles (based on the degree of their impact) within the coping mechanism. Highlighting graph elements let information for policy implications stand out.
C88|Computational analysis of perfect-information position auctions|After experimentation with other designs, major search engines converged on weighted, generalized second-price auctions (wGSPs) for selling keyword advertisements. Theoretical analysis is still not able to settle the question of why they found this design preferable to other alternatives. We approach this question in a new way, adopting an analytical paradigm we dub “computational mechanism analysis.” Specifically, we sample position auction games from a given distribution, encode them in a computationally efficient representation language, compute their Nash equilibria, and calculate economic quantities of interest. We considered seven widely studied valuation models from the literature and three position auction variants. We found that wGSP consistently showed the best ads of any position auction, measured both by social welfare and expected number of clicks. In contrast, we found that revenue was extremely variable across auction mechanisms and was highly sensitive to equilibrium selection, the preference model, and the valuation distribution.
C88|On the gains of using high frequency data and higher moments in Portfolio Selection|In this paper we conduct an empirical analysis on the performance gains of using high frequency data in Portfolio Selection. Within a CRRA-utility maximization framework, we suggest the construction of two different portfolios: a low and a high frequency portfolio. For ten different risk aversion levels, we compare the performance of both portfolios in terms of several out-of-sample measures. Using data on fourteen stocks of the CAC 40 stock market index, from January 1999 to December 2003, we conclude that the “fight” is always “won” by the high frequency portfolio for all the considered performance evaluation measures.
C88|GitHub API based QuantNet Mining infrastructure in R|QuantNet being an online GitHub based organization is an integrated environment consisting of different types of statistics-related documents and program codes called Quantlets. The QuantNet Style Guide and the yamldebugger package allow a standardized audit and validation of YAML annotated software repositories within this organization. The behavior statistics of QuantNet users are measured with Web Metrics from Google Analytics. We show how the search queries obtained from Google’s metrics can be used in the test collections in order to calibrate and evaluate the information retrieval (IR) performance of QuantNet’s search engine called QuantNetXploRer. For that purpose, different text mining (TM) models will be examined by means of the new TManalyzer package. Further, we introduce the Validation Pipeline (Vali-PP) and apply it on the YAML data. Vali-PP is a functional multi-staged instrument for clustering analysis, providing multivariate statistical analysis of the co-occurrence distribution of driving factors of the pipeline. The new package rgithubS, which enables a GitHub wide search for code and repositories using the GitHub Search API and which is an essential element of the QuantNet Mining infrastructure, is briefly presented. The TManalyzer results show that for all considered single term queries the number of true positives is maximal in a latent semantic analysis model configuration (LSA50). The Vali-PP analysis indicates that the optimality of the combination LSA50 and hierarchical clustering (HC) applies to 70 − 90% of the cluster sizes for most of the considered quality indices. Further, we can infer that more accurate and comprehensive metadata increases the clustering quality. Subsequently, the findings of our experimental design are implemented into the QuantNetXploRer. The GitHub API driven QuantNetXploRer can be found and mined under http://www.quantlet.de
C88|A divide and conquer algorithm for exploiting policy function monotonicity|A divide and conquer algorithm for exploiting policy function monotonicity is proposed and analyzed. To solve a discrete problem with n states and n choices, the algorithm requires at most nlog2(n)+5n objective function evaluations. In contrast, existing methods for nonconcave problems require n2 evaluations in the worst case. For concave problems, the solution technique can be combined with a method exploiting concavity to reduce evaluations to 14n+2log2(n). A version of the algorithm exploiting monotonicity in two‐state variables allows for even more efficient solutions. The algorithm can also be efficiently employed in a common class of problems that do not have monotone policies, including problems with many state and choice variables. In the sovereign default model of Arellano (2008) and in the real business cycle model, the algorithm reduces run times by an order of magnitude for moderate grid sizes and orders of magnitude for larger ones. Sufficient conditions for monotonicity and code are provided.
C88|Conducting interactive experiments online|Abstract Online labor markets provide new opportunities for behavioral research, but conducting economic experiments online raises important methodological challenges. This particularly holds for interactive designs. In this paper, we provide a methodological discussion of the similarities and differences between interactive experiments conducted in the laboratory and online. To this end, we conduct a repeated public goods experiment with and without punishment using samples from the laboratory and the online platform Amazon Mechanical Turk. We chose to replicate this experiment because it is long and logistically complex. It therefore provides a good case study for discussing the methodological and practical challenges of online interactive experimentation. We find that basic behavioral patterns of cooperation and punishment in the laboratory are replicable online. The most important challenge of online interactive experiments is participant dropout. We discuss measures for reducing dropout and show that, for our case study, dropouts are exogenous to the experiment. We conclude that data quality for interactive experiments via the Internet is adequate and reliable, making online interactive experimentation a potentially valuable complement to laboratory studies.
C88|Dimensions of Quality of Life in Germany: Measured by Plain Text Responses in a Representative Survey (SOEP)|This paper demonstrates how quality of life can be measured by plain text in a representative survey, the German Socio Economic Panel study (SOEP). Furthermore, the paper shows that problems that are difficult to monitor, especially problems like the state of the European Union, long-term climate change but also the national debt or problems with the quality of consumer goods (like food) and services (like medical treatment), are not issues of particular importance to the majority of people. Developments and risks that are difficult to monitor and only have long-term effects should be left primarily to the discourse conducted by experts and the politically-minded “elites”, the avant garde. And in representative democracies it is ultimately the parliamentarians who must decide. Parliamentarians are likely able to make somewhat better decisions using modern representative surveys and national dialogues than they would be without these instruments of civic participation. Nevertheless, improved civic participation cannot replace parliaments. In diesem Beitrag wird gezeigt, dass es heutzutage gut möglich ist, die Wichtigkeitgesellschaftlicher Ziele und dem Stand der Lebensqualität in der Bevölkerung mit Hilfe einesrepräsentativen Surveys (hier: dem Sozio-oekonomischen Panel, SOEP) mit offenen Fragenund Klartextantworten zu erheben und sinnvoll auszuwerten. Dabei zeigt sich, dasslangfristig wichtige, aber zugleich aktuell wenig spürbare Themen wie Klimawandel,Staatsverschuldung oder die Europäische Unionkaum genannt werden. Wir ziehen dieSchlussfolgerung, dass langfristig wirkende Entwicklungen und Gefahren auch weiterhinvorwiegend dem Diskurs der Fachleute und der politisch denkenden „Avantgarde“zugewiesen werden sollten. Undam Ende müssen in einer repräsentativen Demokratie dieParlamente entscheiden. Auf Basis von modernen repräsentativen Erhebungen undBürgerdialogen können Parlamente vermutlich etwas besser entscheiden als ohne dieseInstrumente der Bürgerbeteiligung. Aber auch eine noch so effektive Bürgerbeteiligung kannParlamente nicht ersetzen.
C88|A Toolkit for Value Function Iteration|Abstract This article introduces a Toolkit for Value Function Iteration. The toolkit is implemented in Matlab and makes automatic use of the GPU and of parallel CPUs. Likely uses are Teaching, Testing Algorithms, Replication, and Research. I here provide a description of some of the main components and algorithms. I also describe the design philosophy underlying choices about how to structure the toolkit and which algorithms to use. Rather than provide simple examples, something best done online (links are given), I instead perform a replication of a classic paper from the real business cycle literature as a demonstration of the use of such a toolkit. The Toolkit, Documentation, Examples, Replications, and more can be found at vfitoolkit.com
C88|Energy, Economics & Replication|This article outlines recent developments in Markdown scripting languages that facilitate the production of replicable, publication quality, research. The approach is similar to that achieved by using, say, Sweave, R and LaTeX, but is written instead in simple Markdown syntax and not tied to any particular output format (e.g., MS Word) nor computational language (e.g., Python). The computational component can be written in C++, Python, SQL, Stan, Bash, or R by way of example. The Markdown script is seamlessly converted to any one of a number of output formats. The output format is essentially an afterthought, and could be rendered as a PDF (LaTeX or Beamer presentation), MS Word, HTML, EPUB, or gitbook document, by way of illustration. Conversion of the Markdown script to the desired output format is performed by pandoc (a universal document converter). These tools can dramatically reduce the amount of time required to complete a research project that can be trivially replicated. Recent enhancements to RStudio streamline the entire process of output format generation via a simple click of an icon or keystroke shortcut (the minimum requirement is R). Replicability is guaranteed by using the checkpoint package in R. This article was written using Markdown.
C88|Strategie, pubbliche e private, in azione per ri-costruire meglio. Analisi dei testi di quattro interviste|"In this paper we present the methodology and results of the automatic analysis of the transcripts of the interviews carried out under the project ""ideas and projects for better reconstruction"" (Esposito et al., 2017). Four strategic areas are considered: the Territorial Cohesion Agency, the Casa Italia Plan, the Civil Protection, and that of a community model of action in the community. The goal is twofold. On the one hand, it is intended to offer a systematic reading of the specificities and elements common to the various aspects considered in the interviews. On the other hand, we want to explore some methodological ideas in the automatic analysis of the texts on the following issues: integration of analytical tools, constraints resulting from the size of the corpus, interpretative potential of graphical representation of results, integration between automatic information retrieval, auto-coding of a corpus and expert reading."
C88|Strategie, pubbliche e private, in azione per ri-costruire meglio. Analisi dei testi di quattro interviste|"In this paper we present the methodology and results of the automatic analysis of the transcripts of the interviews carried out under the project ""ideas and projects for better reconstruction"" (Esposito et al., 2017). Four strategic areas are considered: the Territorial Cohesion Agency, the Casa Italia Plan, the Civil Protection, and that of a community model of action in the com-munity. The goal is twofold. On the one hand, it is intended to offer a systematic reading of the specificities and elements common to the various aspects considered in the interviews. On the other hand, we want to explore some methodological ideas in the automatic analysis of the texts on the following issues: integration of analytical tools, constraints resulting from the size of the corpus, interpretative potential of graphical representation of results, integration between automatic information retrieval, auto-coding of a corpus and expert reading."
C88|Blockchain Publique versus Blockchain Privée : Enjeux et Limites|La blockchain est un sujet très prisé dans le milieu bancaire et de l'assurance, de quoi s'agit-il ? La notion de blockchain émane de la cryptographie et il s'agit d'un protocole permettant de transmettre des informations de manière sécurisée. Nous distinguerons deux approches, l'approche publique décentralisée et l'approche privée centralisée. Le concept de blockchain est apparu grâce à l'émergence de crypto-monnaie et en particulier du Bitcoin. Si la blockchain doit devenir un outil important au sein des banques alors il est nécessaire d'avoir une connaissance assez juste des outils sous-jacents et des enjeux associés à cette nouvelle technologie. En effet, il apparait nécessaire d'identifier les risques qui y sont associés et de proposer des stratégies en vue de les contrôler
C88|Blockchain publique et contrats intelligents (Smart Contrats). Les possibilités ouvertes par Ethéreum... et ses limites|Ethéreum est un protocole d'échanges décentralisés qui ne produit pas seulement une crypto-monnaie, mais permet aussi la création par les utilisateurs de smart contrats. Mais si la plateforme laisse beaucoup de libertés aux acteurs en termes de développement d'applications, des questions de sécurité et de robustesse se posent encore concernant le protocole, les plateformes, les bugs dans le code des contrats
C88|Computational analysis of source receptor air pollution problems|This study introduces a method of graph computing for Environmental Economics. Different visualization modules are used to reproduce source-receptor air pollution schemes and identify their structure. Data resources are emissions-depositions tables, available online from the European Monitoring and Evaluation Program (EMEP) of the Long-Range Transmission of Air Pollutants in Europe. In network models of pollutants exchange, we quantify the responsibility of polluters by exploring graph measures and metrics. In a second step, we depict the size of the responsibility of EU countries. We create pollution schemes for ranking the blame for the change in pollutants in the extended EMEP area. Our approach considers both the activity and the amount of pollution for each polluter. To go a step further in qualitative analysis of pollution features, we cluster countries in communities, bonded with strong polluting-based relationships. The network framework and pollution pattern visualization in tabular representations is integrated in Mathematica computer software.
C88|The performance of four possible rules for selecting the Prime Minister after the Dutch Parliamentary elections of March 2017|Economic policy depends not only on national elections but also on coalition bargaining strategies. In coalition government, minority parties bargain on policy and form a majority coalition, and select a Prime Minister from their mids. In Holland the latter is done conventionally with Plurality, so that the largest party provides the chair of the cabinet. Alternative methods are Condorcet, Borda or Borda Fixed Point. Since the role of the Prime Minister is to be above all parties, to represent the nation and to be there for all citizens, it would enhance democracy and likely be optimal if the potential Prime Minister is selected from all parties and at the start of the bargaining process. The performance of the four selection rules is evaluated using the results of the 2017 Dutch Parliamentary elections. Plurality gives VVD. VVD is almost a Condorcet winner except for a tie with 50Plus. Borda and BordaFP give CU as the prime minister. The impossibility theorem by Kenneth Arrow (Nobel memorial prize in economics 1972) finds a crucially different interpretation.
C88|The K-Y Paradox: Problems in Creating a Centralised Sovereign Backed Cryptocurrency on a Decentralised Platform|Cryptocurrency networks and Blockchains are decentralized systems, functioning on distributed consensus. Fiat currencies on the other hand are issued, maintained and supervised by a sovereign central authority. RSBCs are Regulated And Sovereign Backed Cryptocurrencies (based on the K-Y Protocol) i.e. they are essentially decentralized cryptocurrencies floated by a central (sovereign) authority; it presents a paradox; known as the K-Y paradox. This paper explores the various dimensions of the K-Y paradox and its resolution.
C88|Creating HTML or Markdown documents from within Stata using webdoc|In this article, I discuss the use of webdoc for creating HTML or Markdown documents from within Stata. The webdoc command provides a way to embed HTML or Markdown code directly in a do-file and automate the inte- gration of results from Stata in the final document. The command can be used, for example, to create a webpage documenting your data analysis, including all Stata output and graphs. More generally, the command can be used to create and maintain a website that contains results computed by Stata.
C88|Automatic regrouping of strata in the chi-square test|Pearson´s chi-square test is widely employed in social and health science to analyze categorical data and contingency tables and to assess sample representativeness. For the test to be valid the sample size must be big enough to provide a minimum number of expected elements per category. If the researcher chooses to regroup the strata in order to solve the failure on the minimum size requirement, the existence of automatic re-grouping procedures in statistical software would be very useful, especially when tests are applied sequentially. After comprehensively reviewing the software that can carry out this test, we find that, with a few exceptions, there is no automatic regrouping of the strata to meet this requirement, although it would be very useful if this were available. This paper develops some functions for regrouping strata automatically no matter where they are located, thus enabling the test to be performed within an iterative procedure. The functions are written in Excel VBA (Visual Basic for Applications) and in Mathematica, so it would not be hard to implement them in other languages. The utility of these functions is shown by using three different datasets. Finally, the iterative use of the functions is applied to the Continuous Sample of Working Lives, a dataset that has been used in a considerable number of studies, especially on labor economics and the Spanish public pension system.
C88|Some Methods Of Quantile Regression For Analysis Of The Poverty In Iraq|World Bank has mentioned that approximately half of the world’s poor people live in countries with high income and many of these countries are oil producer countries. In this paper we study some of the economic variables (unemployment, average monthly per capita income, average monthly per capita spending on basic food, the rise in prices of these basic food goods and average taxes imposed on the Iraqi citizen) that impact on the increasing number of poor households in Iraq. We employ a regression model based on classical quantile regression for building the models which represent the relationship between the response variable and the covariates, through five quantile lines (0.16, 0.33, 0.50, 0.66, 0.83). We also use Bayes Lasso quantile regression for variable selection. The data were taken from an economic survey made by the Central Bureau of Statistics in 2007. We use R packages quantreg and bayesQR
C88|Impact of different irrigation systems on water quality in peri-urban areas of Gujarat, India| The ever-growing population of India, along with the increasing competition for water for productive uses in different sectors – especially irrigated agriculture and related local water systems and drainage – poses a challenge in an effort to improve water quality and sanitation. In rural and peri-urban settings, where agriculture is one of the main sources of livelihood, the type of water use in irrigated agriculture has complex interactions with drinking water and sanitation. In particular, the multi-purpose character of irrigation and drainage infrastructure creates several interlinks between water, sanitation (WATSAN) and agriculture and there is a competition for water quantity between domestic water use and irrigated agriculture. This study looks at the determinants of the microbiological quality of stored drinking water among households residing in areas where communities use different types of irrigation water. The study used multiple tube fermentation method ‘Most Probable Number (MPN) technique, a WHO recommended technique, to identify thermotolerant fecal coliforms and E. coli in water in the laboratory (WHO 1993). Overall, we found that the microbiological water quality was poor. The stored water generally had very high levels of Escherichia coli (E. coli) contamination, 80% of the households had water in storage that could not be considered potable as per the World Health Organization (WHO) standards, and 73% of the households were using a contaminated water source. The quality of household storage water was largely unaffected by the major household socioeconomic characteristics, such as wealth, education level or social status. Households using surface water for irrigation had poor drinking water quality, even after controlling for hygiene, behavioral and community variables. Drinking water quality was positively impacted by proper storage and water treatment practices, such as reverse osmosis. Hygiene and sanitation indicators had mixed impacts on the quality of drinking water, and the impacts were largely driven by hygiene behavior rather than infrastructures. Community open defaecation and high village-household density deteriorates household storage water quality.
C88|Risk Management In The Electronic Business|Risk should not be understood as a destructive phenomenon, but bear in mind that managers who know how to use it can lead to real opportunities. Manager must first recognize the existence of risk, namely to identify and then use specific methods to avoid or reduce the risk. The purpose of this paper is to enter the world, at all simple, of risk management, relatively easy concept to understand but not so easy to put into practice. Of course, the approach relates primarily at the risks inherent of the business in digital environments, but they not represent only a particular case of the risks they are exposed, in general, the companies. In the paper we put in evidence the significance in general business, risks in e-business, then we added a description of the types of security risks, an exemplification of these and a series of test scenarios, and finally to make a analysis of operational solutions of risk management
C88|Analysis of the balance between U.S. monetary and fiscal policy using simulated wavelet-based optimal tracking control|This paper uses wavelet-based optimal control to simulate fiscal and monetary strategies under different levels of policy restrictions. The model applies the Maximal Overlap Discrete Wavelet Transform (MODWT) to United States quarterly GDP data, and then uses the decomposed variables to build a large 80 dimensional state-space linear-quadratic tracking model. Using a political targeting design for the frequency range weights, we simulate jointly optimal fiscal and monetary policy where: (1) both fiscal and monetary policy are dually emphasized, (2) fiscal policy is unrestricted while monetary policy is restricted to achieving a steady increase in the market interest rate, and (3) only monetary policy is relatively active, while fiscal spending is restricted to achieving a target growth rate. The results show that fiscal policy must be more aggressive when the monetary authorities are not accommodating the fiscal expansion, and that the dual-emphasis policy leads a series of interest rate increases that are balanced between a steadily increasing target and a low, fixed rate. This research is the first to construct integrated fiscal and monetary policies in an applied wavelet-based optimal control setting using U.S. data.
C88|On the Stock–Yogo Tables|A standard test for weak instruments compares the first-stage F -statistic to a table of critical values obtained by Stock and Yogo (2005) using simulations. We derive a closed-form solution for the expectation from which these critical values are derived, as well as present some second-order asymptotic approximations that may be of value in the presence of multiple endogenous regressors. Inspection of this new result provides insights not available from simulation, and will allow software implementations to be generalised and improved. Finally, we explore the calculation of p -values for the first-stage F -statistic weak instruments test.
C88|Improving the Validity of Microsimulation Results: Lessons from Slovakia|This paper summarizes the lessons learned in the process of building a microsimulation tool tailored to country-specific conditions and involving a maximum degree of user control. The objective to construct a model useful in the process of budgeting and fiscal forecasting has been achieved by paying attention to policy simulation details as well as to the representativeness of the underlying micro-dataset. The validity of simulated results improved significantly after the input database sample has been reweighted in such a way that the new weights replicate, among other factors, the earned income distribution and selected age cohorts directly. Innovative approaches in bringing the model closer to legislation as well as data highlight the benefits of having more user control compared with standardized microsimulation tools.
C88|JAS-mine: A new platform for microsimulation and agent-based modelling|We introduce JAS-mine, a new Java-based computational platform that features tools to support the development of large-scale, data-driven, discrete-event simulations. JAS-mine is specifically designed for both agent-based and microsimulation modelling, anticipating a convergence between the two approaches. An embedded relational database management system provides tools for sophisticated input-output communications and data storage, allowing the power of relational databases to be used within an object-oriented framework. The JAS-mine philosophy encourages the separation of distinct concepts, objects and functionalities of the simulation model, and advocates and supports transparency, flexibility and modularity in model design. For instance, JAS-mine allows to store the list of regressors and their estimated coefficients externally to code, making it easy to change the specification of regression models used in the simulation and achieving a complete parallelisation between the tasks of the econometricians and those of the programmers. Moreover, tools for uncertainty analysis and search over the parameter space are also built in.
C88|The specifics of supply chain of medical kits product group in the context of using Enterprise Resource Planning class systems|Motivation: The article contains proposals of IT solutions supporting the management of selected links in the supply chain of Disposable Medical Kits product group. Aim: The aim of the article is the analysis of the distinctive features of the supply chain of Disposable Medical Kits product group (later referred to as DMK) in the context of managing such chain by means of IT solutions. Results: The first part of the article identifies distinctive features of the supply chain for this product group, which are of importance in the context of realizing the goals of social logistics of medical products supplies, as well as from the point of view of optimizing the management of the chain’s individual links. The second part of the article is devoted to presenting proposals of IT solutions supporting the management of supply chain, whose functionality are implied by the analysis conducted in the first part.
C88|Climate policy scenarios in Brazil: A multi-model comparison for energy|This paper assesses the effects of market-based mechanisms and carbon emission restrictions on the Brazilian energy system by comparing the results of six different energy-economic or integrated assessment models under different scenarios for carbon taxes and abatement targets up to 2050. Results show an increase over time in emissions in the baseline scenarios due, largely, to higher penetration of natural gas and coal. Climate policy scenarios, however, indicate that such a pathway can be avoided. While taxes up to 32US$/tCO2e do not significantly reduce emissions, higher taxes (from 50US$/tCO2e in 2020 to 162US$/tCO2e in 2050) induce average emission reductions around 60% when compared to the baseline. Emission constraint scenarios yield even lower reductions in most models. Emission reductions are mostly due to lower energy consumption, increased penetration of renewable energy (especially biomass and wind) and of carbon capture and storage technologies for fossil and/or biomass fuels. This paper also provides a discussion of specific issues related to mitigation alternatives in Brazil. The range of mitigation options resulting from the model runs generally falls within the limits found for specific energy sources in the country, although infrastructure investments and technology improvements are needed for the projected mitigation scenarios to achieve actual feasibility.
C88|Introducing MOZLEAP: An integrated long-run scenario model of the emerging energy sector of Mozambique|Since recently Mozambique is actively developing its large reserves of coal, natural gas and hydropower. Against this background, we present the first integrated long-run scenario model of the Mozambican energy sector. Our model, which we name MOZLEAP, is calibrated on the basis of recently developed local energy statistics, demographic and urbanization trends as well as cross-country based GDP elasticities for biomass consumption, sector structure, vehicle ownership and energy intensity. We develop four scenarios to evaluate the impact of the anticipated surge in natural resources exploration on aggregate trends in energy supply and demand, the energy infrastructure and economic growth in Mozambique. Our analysis shows that until 2030, primary energy production is likely to increase at least six-fold, and probably much more. This is roughly 10 times the expected increase in energy demand; most of the increase in energy production is destined for export. As a result, Mozambique may well become one of the leading global producers of natural gas and coal. We discuss the opportunities and challenges that this resource wealth poses for the country.
C88|Portfolio choice with high frequency data: CRRA preferences and the liquidity effect|Abstract This paper suggests a new approach for portfolio choice. In this framework, the investor, with CRRA preferences, has two objectives: the maximization of the expected utility and the minimization of the portfolio expected illiquidity. The CRRA utility is measured using the portfolio realized volatility, realized skewness and realized kurtosis, while the portfolio illiquidity is measured using the well-known Amihud illiquidity ratio. Therefore, the investor is able to make her choices directly in the expected utility/liquidity (EU/L) bi-dimensional space. We conduct an empirical analysis in a set of fourteen stocks of the CAC 40 stock market index, using high frequency data for the time span from January 1999 to December 2005 (seven years). The robustness of the proposed model is checked according to the out-of-sample performance of different EU/L portfolios relative to the minimum variance and equally weighted portfolios. For different risk aversion levels, the EU/L portfolios are quite competitive and in several cases consistently outperform those benchmarks, in terms of utility, liquidity and certainty equivalent.
C88|Q3-D3-Lsa|QuantNet 1 is an integrated web-based environment consisting of different types of statistics-related documents and program codes. Its goal is creating reproducibility and offering a platform for sharing validated knowledge native to the social web. To increase the information retrieval (IR) efficiency there is a need for incorporating semantic information. Three text mining models will be examined: vector space model (VSM), generalized VSM (GVSM) and latent semantic analysis (LSA). The LSA has been successfully used for IR purposes as a technique for capturing semantic relations between terms and inserting them into the similarity measure between documents. Our results show that different model configurations allow adapted similarity-based document clustering and knowledge discovery. In particular, different LSA configurations together with hierarchical clustering reveal good results under M3 evaluation. QuantNet and the corresponding Data-Driven Documents (D3) based visualization can be found and applied under http://quantlet.de. The driving technology behind it is Q3-D3-LSA, which is the combination of “GitHub API based QuantNet Mining infrastructure in R”, LSA and D3 implementation.
C88|New experiences in the production of business statistics: the construction of the “Frame SBS” and SBS - data warehouse|This paper describes the first experience of data integration using administrative sources to estimate Structural Business Statistics (SBS) aggregates. It briefly shows the process that led to the construction of the “Frame”, the “integrated” dataset that is the base to derive SBS estimates from the year 2012. This approach represents an innovative way to produce business statistics: the use of direct surveys is limited in favour of the use of administrative data. Moreover, this paper presents an innovative IT proposal on how to combine the integrated production model and the warehouse approach for the production of Structural Business Statistics.This consists in a metadata-driven data warehouse of integrated SBS information which is well-suited for supporting the management of modules in generic workflows. Such a modular approach can improve the efficiency of collaboration among different statistical experts on common data.
C88|Entering H $$^{\infty }$$ ∞ -Optimal Control Robustness into a Macroeconomic LQ-Tracking Model|This analysis explores robust designs for an applied macroeconomic discrete-time LQ tracking model with perfect state measurements. We develop a procedure that reframes the tracking problem as a regulator problem that is then used to simulate the deterministic, stochastic LQG, H-infinity, multiple-parameter minimax, and mixed stochastic/H-infinity control, for quarterly fiscal policy. We compare the results of the five different design structures within a closed-economy accelerator model using data for the United States for the period 1947–2012. When the consumption and investment tracking errors are more heavily emphasized, the H-infinity design renders the most aggressive fiscal policy, followed by the multiple-parameter minimax, mixed, LQG, and deterministic versions. When the control tracking errors are heavily weighted, the resulting fiscal policy is initially more aggressive under the multi-parameter specification than under the H-infinity and mixed designs. The results from both weighting schemes show that fiscal policy remains more aggressive under the robust designs than the deterministic model. The simulations show that the multi-parameter minimax and mixed designs provide a balancing compromise between the stochastic and robust methods when the worst-case concerns can be primarily limited to a subset of the state-space. Copyright Springer Science+Business Media New York 2016
C88|Analysis of Correlation Based Networks Representing DAX 30 Stock Price Returns|Abstract In this paper, we consider three methods for filtering pertinent information from a series of complex networks modelling the correlations between stock price returns of the DAX 30 stocks for the time period 2001–2012 using the Thomson Reuters Datastream database and also the FNA platform to create the visualizations of the correlation-based networks. These methods reduce the complete $$30\times 30$$ 30 × 30 correlation coefficient matrix to a simpler network structure consisting only of the most relevant edges. The chosen network structures include the minimum spanning tree, asset graph and the planar maximally filtered graph. The resulting networks and the extracted information are analysed and compared, looking at the clusters, cliques and connectivity. Finally, we consider some specific time periods (a) a period of crisis (October–December 2008) and (b) a period of recovery (May–August 2010) where we discuss the possible underlying economic reasoning for some aspects of the network structures produced. Overall, we find that network based representations of correlations within a broad market index are useful in providing insights about the growth dynamics of an economy.
C88|Version Control Systems to Facilitate Research Collaboration in Economics|Abstract Reliable and reproducible research is an important cornerstone of science, and version control systems not only make reproducible research possible in a rapid and easy way, but also provide a way of collaborating with co-authors. The purpose of this methodological paper is to present Git, a very successful version control system and how it can be used by economists working together on their papers and the accompanying computer code. Version control systems also make sharing the findings with the rest of the scientific community more easy and streamlined. To understand how version control systems came to be, one must be familiar with the history of free software. In the introduction, I will present free software and its philosophy and show how version control systems make free software possible. In the second section I present Git which is a widely used version control system. In the third section I show a basic usage of Git. In the fourth section, I conclude.
C88|Documentare e comunicare l'attività di trasferimento tecnologico. Analisi testuale della comunicazione dei poli di innovazione|"There is an increasing attention on the needs to support SMEs in enhancing their innovation op-portunities and capabilities. Through a policy measure to foster the regional innovation system, 12 innovation poles were active in Tuscany in the period 2011-2014 to provide to their members (af-filiation is needed) a range of knowledge-intensive services such as knowledge and technology mapping, R&D partnership formation, technical assistance in R&D projects, technology transfer. Each pole was created as a consortium of organizations operating as public or private research cen-tres and service centres (universities, innovation centres or technology transfer centres and firms). In this paper we adopt a statistical analysis of textual content produced by the innovation poles to identify distinctive or common elements in the various texts they produced in three years of activi-ty and to draw some assessment of their communication on their activities. Documents under analysis are of different types: designed as written texts (on Smart Specializa-tion Strategy and monitoring the activities of the poles), transcripts of spoken language (the re-cordings of interviews); web communication. Through and automatic analysis we propose a sys-tematic comparison of all these documents that would not be possible through direct reading of texts: on the whole it is over 56,000 graphic forms, for a total of over two million occurrences. To compare both the intra diversity across the same type of document and across the different types of documents, first we analyse each of the four body separately, in order to identify the specific con-tent and the four languages used by the poles of innovation: ""report"", texts structured in the format of the monitoring; ""design"", the documents on smart specialization strategy; ""reflection and analy-sis"", in the transcription of interviews; and ""communication"", that characterizes the web sites. For this analysis, each document is associated with one or more categories (such as, for example, pole' band category, date of the document) that allow us to group or isolate relevant content in different contexts. In this work we first introduce the set of processing of texts aimed at the selection of graphic forms on which we focus our analysis. Then, we present for each corpus the description of the analysed documents, the results of calculations performed for the treatment of the text and the analysis of the main components that explain the variability of language within each corpus. These analyses (represented by the factorial of two main components) interpret the selection of graphic forms be-ing analysed with respect to categorical variables, defined for each document in each of the corpo-ra. The analysis concludes"
C88|Documentare e comunicare l'attività di trasferimento tecnologico.Analisi testuale della comunicazione dei poli di innovazione|"There is an increasing attention on the needs to support SMEs in enhancing their innovation op-portunities and capabilities. Through a policy measure to foster the regional innovation system, 12 innovation poles were active in Tuscany in the period 2011-2014 to provide to their members (af-filiation is needed) a range of knowledge-intensive services such as knowledge and technology mapping, R&D partnership formation, technical assistance in R&D projects, technology transfer. Each pole was created as a consortium of organizations operating as public or private research cen-tres and service centres (universities, innovation centres or technology transfer centres and firms). In this paper we adopt a statistical analysis of textual content produced by the innovation poles to identify distinctive or common elements in the various texts they produced in three years of activi-ty and to draw some assessment of their communication on their activities. Documents under analysis are of different types: designed as written texts (on Smart Specializa-tion Strategy and monitoring the activities of the poles), transcripts of spoken language (the re-cordings of interviews); web communication. Through and automatic analysis we propose a sys-tematic comparison of all these documents that would not be possible through direct reading of texts: on the whole it is over 56,000 graphic forms, for a total of over two million occurrences. To compare both the intra diversity across the same type of document and across the different types of documents, first we analyse each of the four body separately, in order to identify the specific con-tent and the four languages used by the poles of innovation: ""report"", texts structured in the format of the monitoring; ""design"", the documents on smart specialization strategy; ""reflection and analy-sis"", in the transcription of interviews; and ""communication"", that characterizes the web sites. For this analysis, each document is associated with one or more categories (such as, for example, pole' band category, date of the document) that allow us to group or isolate relevant content in different contexts. In this work we first introduce the set of processing of texts aimed at the selection of graphic forms on which we focus our analysis. Then, we present for each corpus the description of the analysed documents, the results of calculations performed for the treatment of the text and the analysis of the main components that explain the variability of language within each corpus. These analyses (represented by the factorial of two main components) interpret the selection of graphic forms be-ing analysed with respect to categorical variables, defined for each document in each of the corpo-ra. The analysis concludes with some ideas for the modelling of regional system of innovation clusters in Tuscany."
C88|Specific Requirements for the Accounting Software in Budget Organizations|The accounting in budget organizations complies with the general legal requirements applicable to all organizations and businesses in Bulgaria, but also has its own specifics. Unfortunately, most of the software products in the field of accounting do not take sufficient account of these specifics. It is common practice to look for compromise solutions for customers in the public sector within the base functionality of existing accounting products, targeted primarily at business users. To find an adequate solution to the problem, it is necessary to examine the specificity of the data in the public sector. On this basis, proposed solutions in some of the most popular software products in the field of accounting used in budget organizations, can be analysed and to assess to what extent these solutions reflect the specifics of the data. In conclusion, can be made recommendations to improve the accounting software so as to satisfy the needs of users in the public sector.
C88|The Formalization of a Generic Trading Company Model Using Software Agents as Active Elements|Business environment simulation often requires unique knowledge based on the modeler’s experience. However, even experience based simulation models need some extent of abstraction and formalization in order to achieve results that are expected. Business process simulation models usually incorporate several essential components such as the model of trading functions that reflect customer behavior, procurement functions for modeling company inputs and the optimization of production & logistics functions. When modeling management decisions, a management function model with a loopback to company economic outputs is also needed. As a solid foundation of such complex business simulations, an abstract multi-agent architecture of a trading company model is proposed. The abstract model is inhabited with active entities – software agents and a method of registering their actions in a simulation run log is proposed. This approach combines the basic notions of abstract agent archi tectures with process mining methodology. Finally, the correctness of our software-agent system is verified and a validation is provided showing that the proposed system fits the real data company outputs.
C88|Measures of correlation and computer algebra|Our contribution in this work is to set the directions for specialized econometric computations in a free computer algebra system, Xcas. We focus on the programming of a routine dedicated to correlation criteria for multiple regression models. We program several operations for detecting and evaluating collinearity by applying the diagnostic techniques of linear regression analysis. Xcas could constitute a supplemental tool in a collinear data study. Its use is proposed complementary to established econometric software or as substitute software.
C88|Agent-Based Model for River-Side Land-living: Portrait of Bandung Indonesian Cikapundung Park Case Study|A city park has been built from the organic urban settlement in the Cikapundung River, Bandung, Indonesia. While the aim for the development is the revitalization of the river for being unhealthy from the waste coming from the settlement. A study on how Indonesian people, in general, treating water source, like river, lake, and ocean is revisited. Throwing waste into the river has actually become paradox with the collective mental understanding about water among Indonesians. Two scenarios of agent-based simulation is presented, to see the dynamics of organic settlement and life of the city park after being opened for public. The simulation is delivered upon the imagery of landscape taken from the satellite and drone. While experience for presented problems gives insights, the computational social laboratory also awaits for further theoretical explorations and endeavors to sharpen good policymaking.
C88|Assessing classical input output structures with trade networks: A graph theory approach|We present data structures from multiregional multisectoral trade activities from the perspective of networks. To illustrate our approach we make use of trade patterns taken from three classical input-output models. Unlike other conventional approaches by which networks statistics are evaluated, here, emphasis is given on recovering the structure architecture of interrelations in the input-output model. By self-explanatory visual outputs we display the interaction of the trading partners, the number of trade links and the density of interrelations. Connectivity and density are quantified by evaluating the node degrees. Our network approach traces the feedback loops among regions and activities. Some global structural properties are also examined. Programming in Mathematica allows for the creation of iterative schemes explaining aspects of the nature of trade and the evolution of spatial trading/production cycles in growing trading systems. Mathematica’s environment enables interactive visual schemes and infinite number of experiments.
C88|Feature Selection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets| The statistically equivalent signature (SES) algorithm is a method for feature selection inspired by the principles of constraint-based learning of Bayesian networks. Most of the currently available feature selection methods return only a single subset of features, supposedly the one with the highest predictive power. We argue that in several domains multiple subsets can achieve close to maximal predictive accuracy, and that arbitrarily providing only one has several drawbacks. The SES method attempts to identify multiple, predictive feature subsets whose performances are statistically equivalent. In that respect the SES algorithm subsumes and extends previous feature selection algorithms, like the max-min parent children algorithm. The SES algorithm is implemented in an homonym function included in the R package MXM, standing for mens ex machina, meaning 'mind from the machine' in Latin. The MXM implementation of SES handles several data analysis tasks, namely classification, regression and survival analysis. In this paper we present the SES algorithm, its implementation, and provide examples of use of the SES function in R. Furthermore, we analyze three publicly available data sets to illustrate the equivalence of the signatures retrieved by SES and to contrast SES against the state-of-the-art feature selection method LASSO. Our results provide initial evidence that the two methods perform comparably well in terms of predictive accuracy and that multiple, equally predictive signatures are actually present in real world data.
C88|Online labour index: Measuring the online gig economy for policy and research|Labour markets are thought to be in the midst of a dramatic transformation, where standard employment is increasingly supplemented or substituted by temporary work mediated by online platforms. Yet the scale and scope of these changes is hard to assess, because conventional labour market statistics and economic indicators are ill-suited to measuring this “online gig work”. We present the Online Labour Index (OLI), an experimental economic indicator that approximates the conventional labour market statistic of new open vacancies. It measures the utilization of online labour across countries and occupations by tracking the number of projects and tasks posted on major online gig platforms in near-real time. The purpose of this article is to introduce the OLI and describe the methodology behind it. We also demonstrate how it can be used to address previously unanswered questions about the online gig economy. To benefit policymakers, labour market researchers and the general public, our results are published in an interactive online visualisation which is updated daily.
C88|Climate change impacts: Understanding the synergetic interactions using graph computing|In this study we provide a computerized graph structure for synthesizing and displaying the data on a region’s ecosystem-economic system. By applying Mathematica-based graph modelling we create a causal network of the synergistic impact mechanism among certain climate related factors. Our computational approach identifies a climate factor that affects most immediately or most strongly the others. Important factors are indicated through the use of graph theoretical tools. Our graph-based approach and its computational aspects allow for factor ranking(s) according to their importance to the network both numerically and visually, for certain settlement types. Our contribution provides quantitative estimates of impacts and adaptation potentials of five potential effects of climate change (migration, flooding- landslides- fire, air and water pollution, human health and energy-water-other resources) which play a substantial role at the synergistic impact mechanism. Results allow having a picture of the structure of synergistic impact mechanism in a glimpse. Specifically, visual output is created to detect i) the causal relationships of the synergetic mechanism under study ii) the most influential factor(s) in the synergistic mechanism and iii) classify the factor’s roles (based on the degree of their impact) within the coping mechanism.
C88|Odhad parametrů rozšířeného Kaldorova modelu a analýza stability stacionárního řešení<BR>[An Inflation Analysis Using an Endogenous Business Cycle Model]|In this article we analyze the continuous inflation dynamics using a four-equation model. When constructing the model, the traditional Kaldorian two-equation model is extended by adding two other equations. One of them describes an adaptive inflation expectations and the other continuous dynamics of the money market. In this setting, the instability velocity of money circulation is assumed due to the effects of expected inflation on money circulation velocity. Then the parameters of the model are estimated using the real Czech economic data. As it is a non-linear model in its parameters, a non-linear estimation technique is used for this purpose. Further, the stationarity as well as the stability of the estimated model is thoroughly examined as its instability may indicate that the model can generate some complex dynamics.
C88|Data Editing for Complex Surveys in Presence Of Administrative Data: An Application to Fss 2013 Livestock Survey Data Based on The Joint Sequential Use Of Different R Packages|Data editing and imputation (E&I) in complex sample business surveys is a task which is usually split into two steps to gain efficiency in terms of time and human resources: first selective editing techniques are applied to the primary target estimates variables in order to identify a potential set of influential errors that require usually manual editing and a second part of automatic identification and imputation of inconsistencies and missing values. Within this framework, the present paper reviews the Italian top-down data editing strategy adopted and automated imputation showing the experience applied to 2013 Farm Structure Survey livestock data.In this edition this process has been entirely carried out in the R environment by means of different R packages.
C88|Use Of R in Statistics Lithuania|Recently R becoming more and more popular among official statistics offices. It can be used not even for research purposes, but also for a production of official statistics. Statistics Lithuania recently started an analysis of possibilities where R can be used and could it replace some other statistical programming languages or systems. For this reason a work group was arranged. In the paper we will present overview of the current situation on implementation of R in Statistics Lithuania, some problems we are chasing with and some future plans. At the current situation R is used mainly for research purposes. Looking for- ward a short courses on basic R was prepared and at the moment we are starting to use R for data analysis, data manipulation from Oracle data bases, some reports preparation, data editing, survey estimation. On the other hand we found some problems working with big data sets, also survey sampling as there are surveys with complex sampling designs. We are also analysing the running of R on our servers in order to have possibilities to use more random access memory (RAM). Despite the problems, we are trying to use R in more fields in production of official statistics.
C88|An R implementation of a Recurrent Neural Network Trained by Extended Kalman Filter|Nowadays there are several techniques used for forecasting with different performances and accuracies. One of the most performant techniques for time series prediction is neural networks. The accuracy of the predictions greatly depends on the network architecture and training method. In this paper we describe an R implementation of a recurrent neural network trained by the Extended Kalman Filter. For the implementation of the network we used the Matrix package that allows efficient vector-matrix and matrix-matrix operations. We tested the performance of our R implementation comparing it with a pure C++ implementation and we showed that R can achieve about 75% of the C++ programs. Considering the other advantages of R, our results recommend R as a serious alternative to classical programming languages for high performance implementations of neural networks.
C88|A Variance Estimation R-package for Repeated Surveys – Useful for Estimates of Changes in Quarterly and Annual Averages|This paper presents a newly developed R-package for calculation of variances of estimates based on data from several waves of a repeated survey with partly overlapping samples. Development of the package is a part of on-going work on quality improvements of the Labour Force Survey in Norway, which is quarterly and based on a rotating panel. The package can, for example, be used to calculate variances of net changes of annual averages of unemployment rates for persons aged 20-64. The methodology is based on linearly calibrated weights (as calculated by the packages ReGenesees and survey) and residuals from the corresponding regression modelling. These computations may be done separately for each wave. The functionality is generic and the user can specify any calibration model and any linear combination of (quarterly) estimates. Linearization is used to calculate variances of rates. The main method assumes that all relevant population totals can be computed from register data, but situations where totals are unknown for some of the calibration variables are also handled.
C88|Integrating R and Java for Enhancing Interactivity of Algorithmic Data Analysis Software Solutions|Conceiving software solutions for statistical processing and algorithmic data analysis involves handling diverse data, fetched from various sources and in different formats, and presenting the results in a suggestive,tailorable manner. Our ongoing research aims to design programming technics for integrating R developing environment with Java programming language for interoperability at a source code level. The goal is to combine the intensive data processing capabilities of R programing language, along with the multitude of statistical function libraries, with the flexibility offered by Java programming language and platform, in terms of graphical user interface and mathematical function libraries.Both developing environments are multiplatform oriented, and can complement each other through interoperability. R is a comprehensive and concise programming language, benefiting from a continuously expanding and evolving set of packages for statistical analysis, developed by the open source community. While is a very efficient environment for statistical data processing, R platform lacks support for developing user friendly, interactive, graphical user interfaces (GUIs). Java on the other hand, is a high level object oriented programming language, which supports designing and developing performant and interactive frameworks for general purpose software solutions, through Java Foundation Classes, JavaFX and various graphical libraries. In this paper we treat both aspects of integration and interoperability that refer to integrating Java code into R applications, and bringing R processing sequences into Java driven software solutions. Our research has been conducted focusing on case studies concerning pattern recognition and cluster analysis.
C88|Estimation of Household Waste in the Republic of Serbia using R software|This paper deals with the problem of estimation of annual amount of waste generated by households in Republic of Serbia. Waste generated by households is a part of municipal waste that also includes waste generated by trade and services activities as well as by tourists. In order to estimate pure household waste, regression analysis was preformed with reference to Cammarota et al. (2005) ”A proposal for the estimation of household waste”. In order to face this problem, regression models were constructed for municipal waste that are based on non domestic variables which are related to trade and services activities and tourism. The part of the municipal waste that could not be explained by a model based on non domestic variables was ascribed to pure household waste. In order to check validity of results, the model residuals were then related to domestic variables (usual population and the average number of inhabitants per occupied dwelling). The regression models were fitted using R software.
C88|Statistical Data Processing with R – Metadata Driven Approach|In recent years the Statistical Office of the Republic of Slovenia has put a lot of effort into re-designing its statistical process. We replaced the classical stove-pipe oriented production system with general software solutions, based on the metadata driven approach. This means that one general program code, which is parametrized with process metadata, is used for data processing for a particular survey. Currently, the general program code is entirely based on SAS macros, but in the future we would like to explore how successfully statistical software R can be used for this approach.Paper describes the metadata driven principle for data validation, generic software solution and main issues connected with the use of statistical software R for this approach.
C88|Two Decades of Research Collaboration: A Keyword Scopus Evaluation|One issue that has become more important over the years is to evaluate the capability for worldwide research networks on different areas of research, especially in the areas that are identified as being worldwide significant. The study investigated the research output, citations impact and collaborations on publications listed in Scopus authored by researchers all over the world, research published between 1999-2014, selected by a group of keywords identified by authors. The results of the analysis identified an increasing trend in scientific publications starting with 2006, especially on three of the analyzed keywords. We also found differences in the citations patterns for the Black Sea and Danube Delta keywords in the contributing countries. The results of this study revealed a steady increase of the collaboration output and an increasing trend in the collaboration behavior, both at the European and national level. Additionally, at the national level the study identified the collaboration network between Romanian institutions per counties.
C88|Likelihood Estimation of the Systemic Poison-Induced Morbidity in an Adult North Eastern Romanian Population|Purpose - Acute exposure to a systemic poison represents an important segment of medical emergencies. We aimed to estimate the likelihood of systemic poison-induced morbidity in a population admitted in a tertiary referral center from North East Romania, based on the determinant factors. Methodology - This was a prospective observational cohort study on adult poisoned patients. Demographic, clinical and laboratory characteristics were recorded in all patients. We analyzed three groups of patients, based on the associated morbidity during hospitalization. We identified significant differences between groups and predictors with significant effects on morbidity using multiple multinomial logistic regressions. ROC analysis proved that a combination of tests could improve diagnostic accuracy of poison-related morbidity. Main findings - Of the 180 patients included, aged 44.7 ± 17.2 years, 51.1% males, 49.4% had no poison-related morbidity, 28.9% developed a mild morbidity, and 21.7% had a severe morbidity, followed by death in 16 patients (8.9%). Multiple complications and deaths were recorded in patients aged 53.4 ± 17.6 years (p .001), with a lower Glasgow Coma Scale (GCS) score upon admission and a significantly higher heart rate (101 ± 32 beats/min, p .011). Routine laboratory tests were significantly higher in patients with a recorded morbidity. Multiple logistic regression analysis demonstrated that a GCS
C88|Secure Web-Based Communication Framework For Smart Home Systems Designed For The Elderly And Disabled|As a result of the increasing awareness about common daily problems of the elderly and disabled in recent years, novel technology solutions have been designed and developed. Thanks to smart homes which involve a set of various sensors and devices to make the daily lives of the elderly and disabled easier and provide remote monitoring ability to the family members, nowadays the elderly and disabled stay in their houses instead of assisted-living centres. On the other hand, elderly and disabled people living alone may be exposed to potential security threats. In this study, a framework is proposed in order to achieve secure communication over the Internet. In the proposed framework, information to be transferred is first encrypted using RSA algorithm and then sent to the recipient. For security reasons, after the recipient reads the encrypted information, traces of the communication should be removed. For this objective, before the transfer, the sender determines how many seconds the message will be available to the recipient via a link. When the recipient receives the link, he/she can open the message with the secret key. After the predefined time, the message is deleted from the database used by the framework. The framework can be used to enable confidential messaging between smart home inhabitants and people they would like to communicate. The framework can easily be integrated into smart home control panels or web-based interfaces of smart home systems.
C88|Design and Implementation of a Software Application for Secure Web-Based Communication for People with Speech Disorders|All people need to communicate with their families, friends, neighbours, and physicians through various communication means. However, there are common barriers to communication needs of mute or/and deaf people, most importantly in terms of the declining sensory and physical abilities. On the other hand, technological advances have led to the development of a variety of new tools and technologies through various communication medium including the Internet for assistive and augmentative communication for individuals with disabilities. Although communication through the Internet seems highly attractive, it poses privacy and security issues. In this paper a web-based secure message transfer application is proposed to address privacy related information security threats directed to software-based communication of mute and deaf people. The proposed application is practical and easy-to-implement, and is based on the integration of cryptology and steganography. With its easy-to-use graphical user interface suitable for different devices, it can easily be used by its potential users for ensuring the privacy and security of their message transfers. The main limitation of the proposed application is that the application requires a public web server and hence may be vulnerable against attacks directed to web servers if required precautions are not taken.
C88|Structural analysis of value creation in software service platforms|Abstract As software service platforms grow in number of users and variety of service offerings, it raises the question of how this phenomenon impacts the value obtained by users. This paper identifies system usability, service variety, and personal connectivity to be the major determinants that contribute to the value offered to users on mobile software service platforms. A structural equation model, which is based on utility theory, technology acceptance theory, and the theory of network externalities, has been constructed from seven observed constructs, reflecting the three determinants and the user value. The lower bound of user value is estimated through the user’s willingness-to-pay for services and the user’s willingness to spend time on using services. For the validation, a co-variance-based structural equation analysis has been conducted on online survey data of 210 users of mobile service platforms (e.g., Android, iOS). The results show that the number of services used and the number of active user connections were found to be the strongest constructs explaining user value. Perceived usefulness did not explain user value as much. In total, they can explain 49 % of the value that the user receives from the platform. The implication of this result is that users’ value from a software service platform cannot be explained by the technology acceptance model itself. Instead, an approach that, as used in this research, of integrating network externality theory, utility theory, and technology acceptance theory is necessary.
C88|Why can’t we settle again? Analysis of factors that influence agreement prospects in the post-settlement phase|Abstract Negotiators often settle for inefficient agreements and therefore potentially leave substantial value at the bargaining table. Scholars recommend the use of a post-settlement negotiation and its support to give negotiators the opportunity to engage in Pareto-improvement steps. While negotiators can only benefit from reaching a post-settlement agreement, prior research reports low agreement-rates in post-settlement negotiations. This study identifies factors that potentially influence the likelihood of reaching a post-settlement agreement. Based on a laboratory experiment, we show that the prospects of reaching a post-settlement agreement depend on the gender of the negotiators, their mutual understanding in terms of language proficiency, their engagement in the post-settlement phase and the ‘integrativeness’ of the initial agreement. Furthermore, our results show that during post-settlement activities, negotiators seem to focus rather on an extension than on a reallocation of welfare gains. The results of this study provide a better understanding of negotiation behavior, theoretical and practical implications and assistance to designers of negotiation support systems for improving the fit between users’ requirements and available support functionalities.
C88|Weighted Euclidean Biplots|Abstract We construct a weighted Euclidean distance that approximates any distance or dissimilarity measure between individuals that is based on a rectangular cases-by-variables data matrix. In contrast to regular multidimensional scaling methods for dissimilarity data, our approach leads to biplots of individuals and variables while preserving all the good properties of dimension-reduction methods that are based on the singular-value decomposition. The main benefits are the decomposition of variance into components along principal axes, which provide the numerical diagnostics known as contributions, and the estimation of nonnegative weights for each variable. The idea is inspired by the distance functions used in correspondence analysis and in principal component analysis of standardized data, where the normalizations inherent in the distances can be considered as differential weighting of the variables. In weighted Euclidean biplots, we allow these weights to be unknown parameters, which are estimated from the data to maximize the fit to the chosen distances or dissimilarities. These weights are estimated using a majorization algorithm. Once this extra weight-estimation step is accomplished, the procedure follows the classical path in decomposing the matrix and displaying its rows and columns in biplots.
C88|Effect of salvage market on strategic technology choice and capacity investment decision of firm under demand uncertainty| This paper examines the effect of salvage market on technology choice and capacity investment decision of two firms that compete on quantity under demand uncertainty. A game theoretic model applies such that firms choose their production technology between two alternatives: flexible versus inflexible production process. Then they decide on the amount of capacity investment: flexible firm makes decision about general and specific components and inflexible firm just about unified component. One stage forward both enter the primary market in which demand is uncertain and play a la Cournot and finally, flexible firm will be able to sell its unsold general components in the secondary market with a deterministic price. Numerical study was employed to observe equilibrium behavior of firms. Findings demonstrate that with symmetric parameterization there is a unique Nash equilibrium in which both firms choose inflexible technology while applying asymmetric parameters has the potential to form two types of equilibrium when both firms choose inflexible technology or only one firm chooses flexible technology. Moreover, it is shown that there is a cost threshold that could shift the equilibria.
C88|The Effect of Salvage Market on Strategic Technology Choice and Capacity Investment Decision of Firm under Demand Uncertainty|This paper examines the effect of salvage market on strategic technology choice (flexible vs. inflexible) and capacity investment (general, specific and unified components) decision of firms. A four-stage game theoretic model applies to capture strategic decisions of two competitors. In solving optimization problems of the model, we reach intractable equations that enforce us to employ numerical studies. Findings show that with symmetric parameterization there is a unique symmetric Nash equilibrium in which both firms choose inflexible technology while applying asymmetric parameters has the potential to form two types of equilibrium: 1. Both firms chooses inflexible technology or 2. Only one firm chooses flexible technology. Moreover it is shown that there is a specific unified cost threshold that could shift the equilibrium of the game. Finally we discuss on the case that there is no equilibrium and mention some managerial implications of the model.
C88|Creating LaTeX documents from within Stata using texdoc|I discuss the use of texdoc for creating LaTeX documents from within Stata. Specifically, texdoc provides a way to embed LaTeX code directly in a do-file and to automate the integration of results from Stata in the final document. One can use the command, for example, to assemble automatic reports, write a Stata Journal article, prepare slides for classes, or put together solutions for homework assignments. Copyright 2016 by StataCorp LP.
C88|The Challenge of Measuring Hunger through Survey|There is widespread interest in estimating the number of hungry people in the world as well as trends in hunger. Current global counts rely on combining each country’s total food balance with information on distribution patterns from household consumption expenditure surveys. Recent research has advocated for calculating hunger numbers directly from these same surveys, which are increasingly available in low-income countries. For either approach, embedded in this effort are a number of important details about how household surveys are designed and how these data are then used. Using a survey experiment in Tanzania, this study finds great fragility in hunger counts stemming from alternative survey designs. As such, caution should be taken in drawing inferences on hunger over time and space on the basis of household surveys.
C88|Developing An Algorithm For Generating Computerized Test Combinations|This article focuses on comp uterized testing as one the most effective methods of assessment in higher education. The study demonstrates problems of generating tests with random questions using computers. Proposed algorithm overcomes these difficulties taking into account predefined complexity of each question used in the test. Multiple tests of the productivity of the algorithm show the results of different scenarios and help assess its quality. The reason these findings are important is that they demonstrate an effective method for the computerized creation of tests that could be described as preadaptive.
C88|An Approach For Automatic Analysis Of Online Store Product And Services Reviews|One of the advantages of e-commerce systems is that they enable customers and merchants to become acquainted with product and services reviews. Currently in the most popular online stores there are hundreds and even thousands of reviews for certain goods, which contain valuable information about the quality of t he offered assortment. This is the reason to look for ways for their computer processing. The article proposes an approach for automated analysis of customer reviews, based on natural language processing technology and application of methods of machine learning. À model for analysis and its implementation with the software product RapidMiner are proposed.
C88|A Conceptual Model Of A Web Based System Designed For Data Mining Web Resource|This article discusses the disadvantages of the cur rent process of data mining web resources in e-commerce and the reasonable circumstances for the application of intelligent methods. The proposed web-based system aims to overcome existing draw backs in the process of web mining, bringing together the functions of three separate types of software into one. The key module in the system is a multi-agent system that performs local expertise and web mining based on data collected from various web resources. Using such a web system for analyzing a web resource provides the tools and knowledge necessary for the proper development of the e-store.
C88|Classroom Games: Trading in a Pit Market 2.0|We have developed a computerized version of Charles Holt’s classical market game that can be used even in classes with a large audience. The Pit market game gives students intuitive access to the interaction of supply and demand in real-world markets. Even though trade can take place at non-uniform prices in the classroom game, the average price and the quantity traded are usually very close to the equilibrium values predicted by supply and demand curves. The classroom game can also be used for a lively discussion about the efficiency of markets or to show the consequences of taxes and regulatory interventions.
C88|Aktuelle Forschung in der Gartenbauökonomie: Tagungsband zum 1. Symposium für Ökonomie im Gartenbau am 27. November 2013 in der Paulinerkirche Göttingen|[Vorwort] Das 1. Symposium für Ökonomie im Gartenbau fand am 27. November 2013 in der Paulinerkirche in Göttingen statt. Es wurde als Kooperation der Georg-August-Universität Göttingen, des Thünen- Instituts für Betriebswirtschaft und des Agrarkompetenznetzes WeGa durchgeführt. Die Veranstaltung erfreute sich eines regen Interesses. Dies gilt mit Blick sowohl auf die Referentinnen und Referenten als auch auf die Teilnehmerinnen und Teilnehmer. Als Zielgruppe des Symposiums war nicht nur die Wissenschaft angesprochen, sondern auch die Beratung, die Anbaupraxis, die Verwaltung und die Politik. Vertreterinnen und Vertreter aller genannten Gruppen nahmen an der Tagung teil. In zwei parallelen Sessions wurden bei dem Symposium insgesamt 20 angemeldete Vorträge gehalten. Überwiegend von Nachwuchswissenschaftlerinnen und Nachwuchswissenschaftlern wurden (Zwischen-)Ergebnisse aus Promotionsvorhaben und anderen Forschungsprojekten vorgestellt. Zusätzlich hielten Frau Prof. Dr. Vera Bitsch von der Technischen Universität München und Herr Prof. Dr. Wolfgang Bokelmann von der Humboldt-Universität zu Berlin Plenarvorträge. Die Summe aller Vorträge auf der Tagung bildete die Breite der gartenbauökonomischen Forschung in Deutschland sehr gut ab. Sie finden das Programm des Symposiums im Anhang. [...]
C88|Developing an online collaborative system within the domain of financial auditing|The research paper, focused on a rather technical approach, has the goal to design a system that brings together diverse audit stakeholders and investigates how an audit database available online can be implemented in SharePoint, as part of an on-line audit system which is collaborative and national. The online audit database covers various information needs for both financial auditors and the employees of the Chamber of Financial Auditors of Romania. For rapid deployment, we used various tools: Microsoft SQL Server 2008 R2, SharePoint Server 2010, SharePoint Designer 2010 and various implementation features: external content types, external lists, business data web parts etc. In this paper, we use two research methods: the first one is empiric, based on formulating a questionnaire and the interpretation of the results, while the second is the analysis of the implementation process by using a step-by-step approach. The online audit database stores information about the results of previous audits, the opinions issued as result of audits, the results of online electronic inspections, audit firms, audited entities, risks identified etc. The conclusion was that the online database, which is updated through Internet, is feasible to implement in SharePoint, for multiple audit stakeholders including financial auditors who can sell their financial audit services benefiting from the transparency that the system provides.
C88|Study On The Impact Of Quality Of Service On The Quality Of Experience In Multimedia Environments|Providing a guarantee for the customer satisfaction is essential nowadays in multiple domains. A user-centric approach has become a necessity since user satisfaction represents a key performance indicator for the providers; a metric, providers can use in order to improve their business. In multimedia environments, users are sensitive to factors as noise, echo or delay; it is a matter of seconds to quit using a service. To this purpose, QoS (Quality of Service) plays a crucial role. This paper focus is to provide insights on how QoS impacts the QoE (Quality of Experience) perceived by the end-user of multimedia networks. The literature review, the practical approaches and the experiments described in this paper aim at providing a better understanding of the QoS-QoE relationship when dealing with multimedia traffic.
C88|Identifying inliers|The problem of outliers is well-known in statistics: an outlier is a value that is far from the general distribution of the other observed values, and can often perturb the results of a statistical analysis. Various procedures exist for identifying outliers, in case they need to receive special treatment, which in some cases can be exclusion from consideration. An inlier, by contrast, is an observation lying within the general distribution of other observed values, generally does not perturb the results but is nevertheless non-conforming and unusual. For single variables, an inlier is practically impossible to identify, but in the multivariate case, thanks to interrelationships between variables, values can be identified that are observed to be more central in a distribution but would be expected, based on the other information in the data matrix, to be more outlying. We propose an approach to identify inliers in a data matrix, based on the singular value decomposition. An application is presented using a table of economic indicators for the 27 member countries of the European Union in 2011, where inlying values are identified for some countries such as Estonia and Luxembourg.
C88|Sound Auction Specification and Implementation|We introduce 'formal methods' of mechanized reasoning from computer science to address two problems in auction design and practice: is a given auction design soundly specified, possessing its intended properties; and, is the design faithfully implemented when actually run? Failure on either front can be hugely costly in large auctions. In the familiar setting of the combinatorial Vickrey auction, we use a mechanized reasoner, Isabelle, to first ensure that the auction has a set of desired properties (e.g. allocating all items at non-negative prices), and to then generate verified executable code directly from the specified design. Having established the expected results in a known context, we intend next to use formal methods to verify new auction designs.
C88|Euro area monetary and fiscal policy tracking design in the time-frequency domain|This paper first applies the MODWT (Maximal Overlap Discrete Wavelet Transform) to Euro Area quarterly GDP data from 1995 – 2014 to obtain the underlying cyclical structure of the GDP components. We then design optimal fiscal and monetary policy within a large state-space LQ-tracking wavelet decomposition model. Our study builds a MATLAB program that simulates optimal policy thrusts at each frequency range where: (1) both fiscal and monetary policy are emphasized, (2) only fiscal policy is relatively active, and (3) when only monetary policy is relatively active. The results show that the monetary authorities should utilize a strategy that influences the short-term market interest rate to undulate based on the cyclical wavelet decomposition in order to compute the optimal timing and levels for the aggregate interest rate adjustments. We also find that modest emphasis on active interest rate movements can alleviate much of the volatility in optimal government spending, while rendering similarly favorable levels of aggregate consumption and investment. This research is the first to construct joint fiscal and monetary policies in an applied optimal control model based on the short and long cyclical lag structures obtained from wavelet analysis.
C88|Fiscal policy tracking design in the time–frequency domain using wavelet analysis|In this paper discrete wavelet filtering techniques are applied to decompose macroeconomic data so that they can be simultaneously analyzed in both the time and frequency domains. The MODWT (Maximal Overlap Discrete Wavelet Transform) is applied to US quarterly GDP data to obtain the underlying cyclical structure of the GDP components. A MATLAB program is then used to design optimal fiscal policy within an LQ tracking model with wavelet decomposition, and the results are compared with an aggregate model with no frequency decomposition. The results show that fiscal policy is more active under the wavelet-based model, and that the consumption and investment trajectories under the aggregate model are misaligned. We also simulate FHEC (Frequency Harmonizing Emphasis Control) strategies that allow policymakers to concentrate the policy thrust on tracking frequencies that are optimally aligned with policy goals under different targeting priorities. These strategies are only available by using time–frequency analysis. This research is the first to construct fiscal policy in an applied optimal control model based on the short and long cyclical lag structures obtained from wavelet analysis. Our wavelet-based optimal control procedure allows the policymaker to construct a pragmatic tracking policy, avoid suboptimal policies gleaned from an aggregate model, and reduce the potential for destabilization that might otherwise result due to improper thrust and timing.
C88|On the uniqueness of solutions to rational expectations models|Klein (2000) advocates the use of the Schur decomposition of a matrix pencil to solve linear rational expectations models. Meanwhile his algorithm has become a center piece in several computer codes that provide approximate solutions to (non-linear) dynamic stochastic general equilibrium models. A subtlety not resolved by Klein is whether or not a certain Schur decomposition could fail to solve the model while a second one would provide a solution. We show that this cannot happen.
C88|Optimization of the Colombian biodiesel supply chain from oil palm crop based on techno-economical and environmental criteria|During the last years the worldwide legal framework has stimulated the biofuel development, causing their production and use to be exhaustively studied. Biodiesel has been mostly produced in European countries, but the agronomic potential of the Latin-American countries has led the effort toward the biodiesel production, via regulations. This is the case of Colombia, where the oil palm has been exploited as the major biodiesel feedstock. The acts, laws and regulations indicate the mandatory expansion of biodiesel usage. Nonetheless, the laws do not explain the way in that those expansion targets will be done. In this work, the optimal conditions of the supply chain biodiesel were studied via the techno-economic and environmental analysis. Using logistic restrictions, environmental assessment and cost minimization (all of them were estimated in this work), the optimal expansion conditions were decided, taking into account the minimal emissions and the effect of the Land Use Change (LUC) for the oil palm crop expansion. The results showed that the Middle Region is the most promising zone for biodiesel expansion, as well as the Eastern Region is the most adequate zone for expansion crops since the LUC impact is lowest. Finally, the results indicated that the biodiesel based industry must be addressed toward other feedstocks.
C88|What drives gold returns? A decision tree analysis|The behavior of gold as an investment asset has been researched extensively. For the very long run, that is several decades, gold does not outperform equities. However, for shorter periods, gold responds to fears of inflation, stock market corrections, currency crises and financial instabilities very vigorously. In this paper we follow a decision tree methodology to investigate the behavior of gold prices using both traditional financial variables such as equity returns, equity volatility, oil prices, and the euro. We also use the new Cleveland Financial Stress Index to investigate its effectiveness in explaining changes in gold prices. We find that gold returns depend on different determinants across various regimes.
C88|Forecasting the COMEX copper spot price by means of neural networks and ARIMA models|This paper examines the forecasting performance of ARIMA and two different kinds of artificial neural networks models (multilayer perceptron and Elman) using published data of copper spot prices from the New York Commodity Exchange, (COMEX). The empirical results obtained showed a better performance of both neural networks models over the ARIMA. The findings of this research are in line with some previous studies, which confirmed the superiority of neural networks over ARIMA models in relative research areas.
C88|SiSOB data extraction and codification: A tool to analyze scientific careers|This paper describes the methodology and software tool used to build a database on the careers and productivity of academics, using public information available on the Internet, and provides a first analysis of the data collected for a sample of 360 US scientists funded by the National Institute of Health (NIH) and 291 UK scientists funded by the Biotechnology and Biological Sciences Research Council (BBSRC). The tool’s structured outputs can be used for either econometric research or data representation for policy analysis. The methodology and software tool is validated for a sample of US and UK biomedical scientists, but can be applied to any countries where scientists’ CVs are available in English. We provide an overview of the motivations for constructing the database, and the data crawling and data mining techniques used to transform webpage-based information and CV information into a relational database. We describe the database and the effectiveness of our algorithms and provide suggestions for further improvements. The software developed is released under free software GNU General Public License; the aim is for it to be available to the community of social scientists and economists interested in analyzing scientific production and scientific careers, who it is hoped will develop this tool further.
C88|Research Ideas for the Journal of Informatics and Data Mining: Opinion|The purpose of this Opinion article is to discuss some ideas that might lead to papers that are suitable for publication in the Journal of Informatics and Data Mining. The suggestions include the analysis of citations databases, PI-BETA (Papers Ignored – By Even The Authors), model specification and testing, pre-test bias and data mining, international rankings of academic journals based on citations, international rankings of academic institutions based on citations and other factors, and case studies in numerous disciplines in the sciences and social sciences.
C88|Subjective Truths|__Abstract__ On the one hand, economists heavily rely on hard numbers: GDP, growth rate, and exchange rates. On the other hand, their explanations often rely on soft factors: executive confidence in the economy, consumer sentiment, and investor expectations. The hard numbers are objective, but the soft factors are subjective and depend on each individual. Economists increasingly recognize the need to study subjective factors. The first part of the lecture illustrates the key role of subjective truths in modern economics. For instance, measures of subjective well-being are now being proposed to replace or at least complement GDP. Economic policies often rely on subjective forecasting by experts. The second part of the lecture will show that even though they are subjective, the soft factors can still be studied objectively. We will see how to incentivize people to reveal their expectations about future events but also their confidence in their expectations. Finally, I will show how to make people reveal truths that are completely unverifiable.
C88|"Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say ""Usually Not"""|No abstract is available for this item.
C88|Efficient skewness/semivariance portfolios|Abstract This article proposes a flexible methodology for portfolio selection using a skewness/semivariance biobjective optimisation framework. The solutions of this biobjective optimisation problem allow the investor to analyse the efficient trade-off between skewness and semivariance. This methodology is used empirically on four data sets, collected from the Fama/French data library. The out-of-sample performance of the skewness/semivariance model was assessed by choosing three portfolios belonging to each in-sample Pareto frontier and measuring their performance in terms of skewness per semivariance ratio, Sharpe ratio and Sortino ratio. Both the in-sample and the out-of-sample performance analyses were conducted using three different target returns for the semivariance computations. The results show that the efficient skewness/semivariance portfolios are consistently competitive when compared with several benchmark portfolios.
C88|Portfolio Management With Higher Moments: The Cardinality Impact|In this paper we extend the study of the cardinality impact from the standard mean-variance scenario to higher moments, considering a utility maximization framework. For each scenario, we propose a bi-objective model that allows the investor to directly analyse the efficient trade-off between expected utility and cardinality. We study not only the effect of cardinality in each scenario but also the real gain of considering higher moments in portfolio management. This analysis is performed assuming that the investor has constant relative risk aversion (CRRA) preferences. For the data collected on the PSI20 index, the empirical results showed that there are no performance gains, in-sample, from the efficient mean-variance expected utility/cardinality portfolios to the efficient expected utility/cardinality portfolios when higher moments are considered. However, the out-of-sample performance of the efficient mean-variance-skewness expected utility/cardinality portfolios and of the efficient mean-variance-skewness-kurtosis expected utility/cardinality portfolios suggest the existence of real gains, especially when transaction costs are considered.
C88|Big Data Management: Relational Framework|Volumes of available digital data have been significantly expanding over the past decade. Alongside the volume, diversity and complexity of digital data have also been growing. Contemporary devices and systems are capable of generating data vastly exceeding capabilities of organizations and conventional information technologies to process it. Big, diverse and complex data presents novel challenges for organizationsâ€•but also opportunities. Big data enables tackling longstanding complex problems that would otherwise be out of reach. It also opens new scientific and commercial possibilities that could not exist without availability of data. Organizations utilizing large volumes of diverse data, however, face unique challenges. The challenges range from technological and processing issues to business and management matters. Organizations need to adopt appropriate management strategy in order to satisfactorily deal with the issues arising from utilization of big data. This necessitates understanding of relationships between the aspects of data and their managerial consequences. We examine the essential characteristics of big data and explore pertinent managerial implications.
C88|Three-Point Volatility Smile Classification: Evidence From The Warsow Stock Exchange During Volatile Summer 2011 / Clasificación De Las Sonrisas De Volatilidad Según Tres Puntos De Monetización: Evidencia Empírica Para La Bolsa De Varsovia Durante El Volátil Verano De 2011|This paper studies the behavior of the smile in the Warsaw Stock Exchange (WSE) during the volatile summer of 2011.We investigate the volatility smile derived from liquid call and put options on the Polish WIG20 index which option series expired on September 2011. In this period, the polish index has dropped about 20% in two weeks time. By linear interpolation, implied volatilities for moneyness points needed were calculated, then we construct 355 smile curves for calls and puts options to study and make some kind of smile-types classification. We propose seventeen types-smiles which represent all possible cases of three points (three moneynesses) graphical patterns. This classification is made basing upon relationship higher/equal/lower values of implied volatility for each of three points. Furthermore, we distinguish the convexity of pattern. We can note that smiles, smirks and ups are convex in shape, while reversed ones and downs are concave functions. / Este artículo analiza el comportamiento de la sonrisa de la volatilidad en la Bolsa de Varsovia (WSE) durante el volátil verano de 2011, derivada de las opciones más líquidas sobre el índice polaco WIG20, cuyas series expiraron en septiembre de 2011. En ese período, el índice había caído aproximadamente un 20% en tan sólo dos semanas. Mediante interpolación lineal construimos 355 curvas de sonrisas para poder estudiar una posible clasificación o tipología de las mismas. Proponemos 17 tipos de sonrisas, las cuales representan todos los casos posibles de tres monetizaciones de patrones gráficos. Esta clasificación se basa en la relación valores superiores/iguales/inferiores de la volatilidad implícita para cada uno de los tres puntos. Además, se distingue la convexidad de cada una. Destacamos que las sonrisas completas, las sonrisas asimétricas y las inclinadas hacia arriba son de forma convexa, mientras que las invertidas y las inclinadas hacia abajo son funciones cóncavas.
C88|Accessibility of internet services for visually impaired persons by means of Text-To-Speech technology. Benefits-costs perspective|Access of the persons with disabilities to services provided by the Internet is an important aspect of everyday existence of these people with special needs. In the particular case of blind people, the Internet offers unprecedented opportunities for an independent living, the fundamental challenge for any disabled person. In this paper we present some aspects of Internet accessibility via Text-To-Speech (TTS) technology, from a benefits-costs perspective. This assistive technology transforms into speech the texts gathered from accessing of web and email services of the Internet.
C88|Producing small area estimation using R in the Romanian official statistics|The purpose of this paper is to reveal the opportunities found in the Romanian official statistics to develop a long-way but strong implementation of the Small Area Estimation techniques, together with the use of R statistical software. The Small Area Estimation technique, a model-based approach to produce regional or even locality level data, has been already succesfully applied on the estimation of the international migration, which is often hard to estimate. In these circumstances, the official statistics in Romania face new challenges in data analysis tools. In the last two years, a small team of statisticians introduced R both in the official statistics and academia, as an opportunity for progress. The paper contains an overview of the small area techniques applied to the estimation of international migration and covers other possible applications of small area estimation methods, presenting the challenges currently considered in the official statistics
C88|A Dynamic Interface for Trade Pattern Formation in Multi-regional Multi-sectoral Input-output Modeling|This paper introduces a visual framework in computational environment for displaying multi-region, multi-sector classical models, associated with authors such Isard, Chenery, Moses, Leontief, Riefler and Tiebout. Based on the quantity and nature of trade data of each model, different conditions are imposed upon the matrix of trade coefficients $$T$$ T which result in various matrix partitioning schemes. Matrix $$T$$ T illustrates the interactions among interregional and intersectoral economic activities and is considered a key component in input-output modeling. Using MATHEMATICA as software tool we introduce a method to construct and present matrix $$T$$ T both graphically, with static and dynamic images, and analytically. The output produced enables understanding and/or teaching theoretical trade hypotheses. Furthermore, our computational approach produces random, structured matrices of trade coefficients, which makes possible infinite computer experiments with interregional input-output models of any size, without typing in input. The computer codes are fully presented and can be reproduced as they are in computational-based research practice and education. Copyright Springer Science+Business Media New York 2015
C88|Some Considerations Regarding the Design and Implementation of Data Warehouse in Insurance Broker Management|This paper describes a proposal for a data warehouse model, designed for the use in the management of insurance brokerage companies. The model aims to provide information to the leadership of such companies, beyond the classical knowledge drawn from current activity reports. Also, the design process took into consideration the characteristics of the business model analysed. The model is then “exploited” by making some analyses on the data loaded.
C88|Business Intelligence in Insurance Brokerage Companies – a Tool for Decision-Makers|This paper presents several applications of data analysis software in the insurance brokerage activity, particularly in matters related to the decision-making process. The study is based on a dataset loaded in a data warehouse, and several analysis procedures are presented, from the viewpoint of the managers of such company.
C88|Guyana: A Half a Century of Struggles with Planning, Growth, and Development|Guyana was a proving ground for both the capitalist and socialist development ideas in the latter half of the 20th century. We analyze the problems and solutions for those experiment from the point of views of appraising their performances. Traditional theories such as input-output models, sectorial models, and growth models form the windows for the appraisal. When the subject matter is judgmental, Analytic Network Process and (ANP) and Data Envelop Analysis (DEA) are engaged. When the subject matter is structural, the time series of gross investment to GDP is considered as a driver of growth. We subject that ratio to spectral and cross-spectral analysis for a half a century of data for Guyana vs. neighboring countries. The results indicate that Guyana has stepped into the long-run cumulative growth process after its struggles with development.
C88|Analisi lessico testuale delle ordinanze del Commissario Delegato alla ricostruzione in Emilia-Romagna: un contributo alla legge nazionale su emergenza e ricostruzione|In the three years after the 2012 earthquake in Emilia-Romagna, through the enactment of more than 350 ordinances, the Commissioner has structured interventions to cope with emergency and reconstruction. The intense law-making, essential to fill a legal vacuum, has enabled to overcome the uncertainties of the difficult phase of recovery. There is agreement among experts that a large number of those ordinances was due to the absence of national rules governing the urgent intervention in case of natural disasters. On the push of actions taken in Emilia-Romagna, the Italian Parliament has reopened the debate on a national law on emergency after natural disasters. Through a systematic content analysis of the corpus of ordinances issued in EmiliaRomagna, in this paper we propose a contribution in drafting a law on emergency. Two main strands of analysis have been developed. In the first one, an automatic text analysis, supported by Taltac2, has provided inputs for a factor analysis and a cluster analysis of the thematic areas covered by the ordinances. Four main topics have been singled out: grant criteria and contributions; management of allocation of resources; urgent works for municipalities, schools and churches buildings; interventions to support population. Having associated each ordinance to one of the four topics, a temporal analysis of the issues addressed during the emergency and reconstruction phase highlights the sequence of actions that were undertaken in Emilia. In a second step, the set of terms characterizing each cluster it is used to obtain a redefinition of disjunctive classification towards a fuzzy multi-class.
C88|Analisi lessico testuale delle ordinanze del Commissario Delegato alla ricostruzione in Emilia-Romagna: un contributo alla legge nazionale su emergenza e ricostruzione|In the three years after the 2012 earthquake in Emilia-Romagna, through the enactment of more than 350 ordinances, the Commissioner has structured interventions to cope with emergency and reconstruction. The intense law-making, essential to fill a legal vacuum, has enabled to overcome the uncertainties of the difficult phase of recovery. There is agreement among experts that a large number of those ordinances was due to the absence of national rules governing the urgent intervention in case of natural disasters. On the push of actions taken in Emilia-Romagna, the Italian Parliament has reopened the debate on a national law on emergency after natural disasters. Through a systematic content analysis of the corpus of ordinances issued in EmiliaRomagna, in this paper we propose a contribution in drafting a law on emergency. Two main strands of analysis have been developed. In the first one, an automatic text analysis, supported by Taltac2, has provided inputs for a factor analysis and a cluster analysis of the thematic areas covered by the ordinances. Four main topics have been singled out: grant criteria and contributions; management of allocation of resources; urgent works for municipalities, schools and churches buildings; interventions to support population. Having associated each ordinance to one of the four topics, a temporal analysis of the issues addressed during the emergency and reconstruction phase highlights the sequence of actions that were undertaken in Emilia. In a second step, the set of terms characterizing each cluster it is used to obtain a redefinition of disjunctive classification towards a fuzzy multi-class.
C88|Promozione del sistema dei poli di innovazione nello spazio web: analisi dei contenuti e delle reti di relazioni virtuali|In analysing and modelling the Tuscany regional system of innovation poles (www.poliinnova zione.unimore.it), we examine systematically available information on websites with two objec-tives: (i) to analyze the variety of language and content that characterize the poles in their online activities; (ii) examine the extent poles refer to the same institutions, enterprises, organizations, projects, and among these, organizations / or activities directly related to the (such as the compa-nies managing the poles laboratories, incubators, the adherents ). This paper offers a linguistic analysis of the websites produced by innovation poles in Tuscany, in order to provide a set of var-iables for the evaluation of their promotion strategies. The focus of the analysis is the evaluation of original vs. reported information, which is here used as indicator of the innovation poles' en-gagement in the promotion of their activities. The purpose is to quantify the level of engagement that each single network has bu ilt – on its website – through the publication of texts. Through Corpus Linguistics methodology, and using both a quantitative and a qualitative ap-proach (similar to the one proposed by the CADS framework), this papers looks at how the inno-vation poles taken into account have used a set of 24 terms related to the notions of innovation and promotion (business, businesses, centers, collaboration, knowledge, finance, management, manager, business, business, industrial, innovation, pole, poles, processes, projects, design, re-search, service, services, development, technology , technologies, territory). By looking at whether these terms appear in original or reported (produced by third-parties) texts, and at how their mean-ings and connotations are constructed in the former cases, this paper identifies how active have the innovation poles been in promoting their work. In addition to linguistic analysis, text analysis of web pages also allowed to identify main domains (urls) and complete links mentioned on the websites of poles. Adopting a perspective of social network analysis, with information on these virtual networks we analysed two issues relevant to modelling the system of innovation poles: through the quotes of major domains, we highlight the extent to which the web sites have reported the poles connections with each other and with those involved in technology transfer; through the analysis of complete links present on the websites of individual poles we can identify to what extent the poles refer to the same information space. The presentation is structured as follows: section 1 presents the data, tools and methodology used in linguistic analysis; section 2 presents linguistic analysis of the 24 selected terms; in section 3 vir-tual networks are analysed; section 4 concludes. Appendix 1 presents the technical details on the collection and subsequent data cleansing; Appendix 2 contains the list of the top link complete inside of the poles of the sites surveyed.
C88|Intelligent Tagging and Search as a Fully Automated System|Taxonomic structures and folksonomy tags are well-known mechanisms used in systems for organizing unstructured content. But they come from different worlds, taxonomies tend to be highly professional, both referring to their building process and their usage, while folksonomies are more user-oriented and therefore they suffer from the lack of professionalism. A new structure of terms called metataxonomy has been developed and the advantages of its implementation has been already demonstrated. This paper briefly presents the idea laying behind the concept for intelligent tagging and search based on this extended taxonomic structure in order to depict the missing part that will bring that system to the state of being thorough
C88|The Use of Payment Cards and the Prevention of Payment Card Fraud in Europe and Bulgaria|Payment cards are the primary and most widely used electronic payment instrument for retail pay­ment in Europe. Their relative importance among other payment tools is paramount. The analysis of ECB data shows that their use over the studied 14- year period of 2000-2013 is growing steadily both in absolute and in relative terms. At the same time, the issue of the security of payment card transactions remains relevant. Fol­lowing a certain reduction in the number and total value of payment card fraud in the years after their peak in 2008, they started to grow again in 2012. The primary type of fraud is the card-not-present type, which forms the majority of the total transac­tion value. There are different approaches possible to tackle this issue and a primary one is to make use of two-factor authentication of users available, especially in e-commerce. The importance of fraud committed through ATMs and POS devices decreases thanks to the almost complete migra­tion towards using chip cards in accordance with the EMV standard, which has established itself as the main counter-measure for card-present fraud. Despite the relatively weak development of the card market in Bulgaria, the significance of card fraud should not be underestimated. Legislative amendments are strongly advised in order to en­sure a reliable collection of data for their number and value and to help further with their effective counteraction
C88|Impact of a Financial Transaction Tax on a Financial Market|The aim of this paper is to investigate the impact of financial transaction tax (FTT) on the stability of financial market. The paper presents an agent-based financial market model and simulations, in which agents follow technical and fundamental trading rules to determine their speculative investment positions. The model developed by Westerhoff (2009) was chosen for the implementation and it was extended by FTT and arising transaction costs. As FTT may be defined variously, assets are understood as a tax object in this paper. The model includes direct interactions between speculators due to which they may decide to change their trading behaviour and deals with a technical and fundamental strategy of market participants. Results suggest that the modified model has a tendency to stabilize itself in a long-term if the fundamental trading rules overbear the technical trading method. This could be used, when the bubbles and the crashes occur in a financial market. Assets price would be stabilized, because its value targets near the fundamental value and the volatility would be also minimized. Substantial is setting the FTT at a low rate for market stabilization. If FTT and consequent transaction costs are too high, the financial system destabilizes and the price grows without limit.
C88|Theoretical Approaches For The Analysis Of Innovation Capacity As A Factor That Affects The Competitiveness Of Software Industry Of Jalisco|The aim of this work is to review the theoretical approach to analyze the innovation capacity of enterprises in the software industry of Jalisco. Based on a survey of the companies in the Software Center of the State, as well as evaluating the influence that has the capacity for innovation on competitiveness, seeking empirical evidence to answer the question. The main hypothesis for this research is that the ability to innovate is a factor that positively affects the performance of companies in the software industry, which is reflected in the competitiveness of the sector. The methods used in this research are three: innovativeness index (ICI), Linear Regression Model with OLS and Soft Computing using evolutionary algorithms: FUZZYCESAR, the latter something very new which puts us in the forefront of knowledge in the methods it is still.
C88|ActiveX Controls in Microsoft Excel|ActiveX controls can be placed in Word documents, Excel spreadsheets or PowerPoint slides. These controls are selected from the Toolbox and they allow user interaction. A control has adjustable attributes (properties) and it detects events (it is “alive”). Controls are associated with code sequences, in the form of certain procedures, called event procedures.
C88|Form Controls in Microsoft Excel|Form Controls are used to employ the data from cells without necessarily using VBA code. One can also run the macros associated with controls. Therefore, one can attach an existent macro to a control, one can write or register a new macro, and then, when a user clicks the control, the macro starts running.
C88|Study Design Requirements To Realize A Software For Document Archives (Gad)|This paper studies the software market for the management and storage of documents. They identified three categories of software products for this area: the computerized management of electronic records, computerized management of existing domentelor in organizations and computerized management of physical archives of documents. All three categories of software interfere with each other, but they each have specific functions well developed information, addressing only tangentially other specific categories. Author very poor market accounting software products for managing physical documents computerized archives. Due to this aim and it manages to define the specific requirements of this gategorii software products will subsequently address such a product design software. In conclusion decision variables are analyzed for the design format and define the most appropriate version to be addressed.
C88|oTree—An open-source platform for laboratory, online, and field experiments|oTree is an open-source and online software for implementing interactive experiments in the laboratory, online, the field or combinations thereof. oTree does not require installation of software on subjects’ devices; it can run on any device that has a web browser, be that a desktop computer, a tablet or a smartphone. Deployment can be internet-based without a shared local network, or local-network-based even without internet access. For coding, Python is used, a popular, open-source programming language. www.oTree.org provides the source code, a library of standard game templates and demo games which can be played by anyone.
C88|Fanning-Out or Fanning-In? Continuous or Discontinuous? Estimating Indifference Curves Inside the Marschak-Machina Triangle using Certainty Equivalents|This paper introduces a new method of estimating indifference curves in the Marschak-Machina triangle. The method involves posing questions about indifference. Contrary to previous attempts, where subjects were required to identify those lotteries to which they were indifferent vis-à-vis a given lottery, the subjects are here required to determine its certainty equivalent. The procedure is repeated for a large number of lotteries inside the triangle. Simple, linear interpolation of certainty equivalent values between adjacent points representing the lotteries under consideration allows any indifference curve inside the triangle to be plotted. The experimental results presented in the paper shed new light on the shape of indifference curves inside the Marschak-Machina triangle, where curve parallelism, fanning-out, fanning-in and boundary effects, including (possibly discontinuous) jumps, are all common. As shown, those decision-making models, which can predict jumps on the triangle legs, offer the best econometric fit of the indifference curves obtained in the study.
C88|Автоматизация Деятельности Страховой Компании<BR>[Automation of the insurance company]|In the article the questions of automation of the insurance company. Emphasized the use of information technology to the field of insurance. Described created using MS Access database
C88|Smets-Wouters '03 model revisited - an implementation in gEcon|This paper presents an implementation of the well-known Smets-Wouters 2003 model for Euro Area using the gEcon package - what we call the ``third generation'' DSGE modelling toolbox. Our exercise serves three goals. First, we show how gEcon can be used to implement an important - from both applications and historical perspective - model. Second, through rigorous exposition enforced by the gEcon’s block-agent paradigm we analyse all the Smets-Wouters model’s building blocks. Last, but not least, the implementation presented here serves as a natural starting point for important from applications point of view extensions, like opening the economy, introducing non-lump-sum taxes, or adding sectors to the model economy. Full model implementation is attached.
C88|A Portrait of Diversity In Indonesian Traditional Cuisine|The archipelagic geography and demography of Indonesian people due to the way people serve food and drinks on the table is analyzed. Statistically some properties about the food recipes are observed, while the analysis is followed by the methodology to see the clustering of the food and beverage due to their ingredients. The global mapping of all the food yields four classes of the food that is related to the way people conventionally prepare the cuisines, whether the recipes are on vegetables, fish and seafood, chicken and poultry, and meats. It is obvious that ingredient wise, the diversity of the food is emerged from traditional ways adding spices and herbs. For more insights, the analysis for food dressings and traditional drinks are also delivered. While the mappings exhibit the classes of food and beverages based on the purposes and styles of the service in the cuisines, some signatures of regional localities are also detected.
C88|The financial consequences of the ’Women & Men 40’ pension scheme concept in pay-as-you-go pension systems|With the help of five models, this paper analyses pension systems in general and the direct financial effects of the retirement-age-reducing concept mentioned in the title. Th e first part assumes a financially unchanged environment, when earnings are permanent, there is no interest and everyone lives for a predetermined length of time (deterministic models). Taking into consideration actual mortality rates (stochastic model) we give an idealised description of the current pay-as-you-go pension system, which, however, does not significantly diff er from real life. Th e most important consequence of taking mortality into consideration is the life insurance effect. The contribution of individuals who deceased prior to reaching a pensionable age, will increase the sources for the pension of survivors. Every forint the survivors paid themselves is worth 1.5–2 times as much on account of this life-insurance effect. In the second part of the paper incomes are assumed to increase and interest also accounted for. Here we highlight the advantage of funded pension schemes in that for the same amount of pension they require a third to half of the pension insurance contributions of the pay-as-you-go pension system, because half to two-thirds of pension funding comes from the returns on invested payments. Analysis of this reveals the hidden state deficit inherent in the pay-as-you-go pension scheme, and the fact that every active employee pays the interests on it on a monthly basis when paying two to three times more in pension insurance contribution than would be necessary. The third part demonstrates that if both women and men were to retire after 40 years of employment, it would entail pensions cuts of 9–12% for females and males respectively or a general pension cut of 19% for both sexes on average, if the present balance of contributions and pension payments are to be kept in the future.
C88|Estimating Gaussian Mixture Autoregressive model with Sequential Monte Carlo algorithm: A parallel GPU implementation|In this paper, we propose using Bayesian sequential Monte Carlo (SMC) algorithm to estimate the univariate Gaussian mixture autoregressive (GMAR) model. The prominent benefit of the Bayesian approach is that the stationarity restriction required by the GAMR model can be straightforwardly imposed via prior distribution. In addition, compared to MCMC (Markov Chain Monte Carlo) and other simulation based algorithms, the SMC is robust to multimodal posteriors, and capable of providing fast on-line estimation when new data is available. Furthermore, it has a linear computational complexity and is ready for parallelism. To demostrate the SMC, an empirical application with US GDP growth data is considered. After estimation, we conduct the Bayesian model selection to evaluate the empirical evidence for different GMAR models. To facilitate the realization of this compute-intensive estimation, we parallelize the SMC algorithm on a nVidia CUDA compatible Graphical Process Unit (GPU) card.
C88|Big Data In Business Environment|In recent years, dealing with a lot of data originating from social media sites and mobile communications among data from business environments and institutions, lead to the definition of a new concept, known as Big Data. The economic impact of the sheer amount of data produced in a last two years has increased rapidly. It is necessary to aggregate all types of data (structured and unstructured) in order to improve current transactions, to develop new business models, to provide a real image of the supply and demand and thereby, generate market advantages. So, the companies that turn to Big Data have a competitive advantage over other firms. Looking from the perspective of IT organizations, they must accommodate the storage and processing Big Data, and provide analysis tools that are easily integrated into business processes. This paper aims to discuss aspects regarding the Big Data concept, the principles to build, organize and analyse huge datasets in the business environment, offering a three-layer architecture, based on actual software solutions. Also, the article refers to the graphical tools for exploring and representing unstructured data, Gephi and NodeXL.
C88|Analyzing Social Networks From The Perspective Of Marketing Decisions|Nowadays, the Web became more than a space for product presentation, but also a capitalization market (e-commerce) and an efficient way to know the customer preferences and to meet their requirements. Large companies have the financial potential to use various marketing strategies and, in particular, digital-marketing. Instead, small businesses are looking for lower cost or no cost methods (also called guerrilla marketing). A small company can compete with a large company by approaching a particular range of products that excel in quality, and also by inventiveness in the marketing strategy. During 2010-2015 the potential of Information Technology and Communications (IT&C) sector was proved for the companies which aimed towards modernization of technologies and introduced new strategies in order to commercialize new products. An important challenge for companies was to be aware of the changes in customer behaviour, using social networks software. Finally, research centers have set up new IT&C services and improved marketing and communications following the crisis. More and more companies invest in analytic tools to monitor their marketing strategies and Big Data becomes extremely useful for this purpose, using information like customer demographics and spending habits, oscillation between simplicity, comfort and glamour. There are various tools that can transform in a very short time, massive amounts of data into real business value in a very short time, helping companies and retailers to understand, at any point in the product lifecycle, which trends are gaining and which are losing ground. These insights give them the possibility to reduce the risk of not selling their products by making adjustments to the design, production or promotional strategies, before putting the goods on the market. In this paper we aim to present the advantages of exploring customer requirements from social media for marketing strategy of an enterprise, by using SNA software Gephi and NodeXL and making a comparison of their features.
C88|Associative Analysis in Statistics|In the last years, the interest in technologies such as in-memory analytics and associative search has increased. This paper explores how you can use in-memory analytics and an associative model in statistics. The word “associative” puts the emphasis on understanding how datasets relate to one another. The paper presents the main characteristics of “associative” data model. Also, the paper presents how to design an associative model for labor market indicators analysis. The source is the EU Labor Force Survey. Also, this paper presents how to make associative analysis.
C88|Data Mining And Erp: An Application In Retail Sector|Many medium or large scale organizations with large databases invest on advance data collecting and managing systems. The main point of turning this data into your success is the difficulty of extracting knowledge about the system that you study from the collected data. Enterprise Resource Planning (ERP) software helps companies to put all previously separated data to in single software. ERP has several advantages. Storing whole data in a single place make it possible to analyze data from different business functions. Because a large scale of data are in the same place, new tools are needed to analyze them. In this study customer purchase records from the the biggest computer retailing firm?s data in Turkey were analyzed. Association Rules were used to determine the shopping behavior of the customers. According to the results, various rule sets are obtained. This rule sets can be used for purposes such as store layout, shelf arrangement in the store, products can be placed close together to increase sales and other promotional strategies.
C88|An analysis of mobile banking customers for a bank strategy and policy planning|Online banking is increasingly common. Financial institutions deliver online services via various electronic channels, subsequently diminishing the importance of conventional branch networks. This study proposed an integrated data mining and customer behavior scoring model to manage existing mobile banking users in an Iranian bank. This segmentation model was developed to identify groups of customers based on transaction history, recency, frequency, monetary background. It classified mobile banking users into six groups. This study demonstrated that identifying customers by a behavioral scoring facilitates marketing strategy assignment. Then the bank can develop its marketing actions. Thus, the bank can attract more customers, maintain its customers, and keep high customers' satisfaction.
C88|Ontology Construction for the Website Quality Evaluation (Budowa ontologii na potrzeby oceny jakosci serwisow internetowych)|In the paper the conception of integration of websites quality assessment methods was formulated. In this case the authors proposed using ontologies which will be created for those methods. For building ontologies, the Methontology methodology was used. The main problem was an evaluation of created ontology. The evaluation was done using a reasoner and description logic, which let indicate and repair errors in the modeled ontology.
C88|Challenges Emerging from Future Cloud Application Scenarios|The cloud computing paradigm encompasses several key differentiating elements and technologies, tackling a number of inefficiencies, limitations and problems that have been identified in the distributed and virtualized computing domain. Nonetheless, and as it is the case for all emerging technologies, their adoption led to the presentation of new challenges and new complexities. In this paper we present key application areas and capabilities of future scenarios, which are not tackled by current advancements and highlight specific requirements and goals for advancements in the cloud computing domain. We discuss these requirements and goals across different focus areas of cloud computing, ranging from cloud service and application integration, development environments and abstractions, to interoperability and relevant to it aspects such as legislation. The future application areas and their requirements are also mapped to the aforementioned areas in order to highlight their dependencies and potential for moving cloud technologies forward and contributing towards their wider adoption.
C88|Unpacking Big Systems - Natural Language Processing meets Network Analysis. A Study of Smart Grid Development in Denmark|Studies within the detection of technological trajectories and technology fore- casting tend traditionally to rely on patent or bibliometric data. The main drawback of these invention-focused approaches is their inability to account for many mainly non-technical factors related to the social and institutional framing of technology. Value driven policies, technological and institutional path dependencies or user expectations and routines have major impact on the technological outcomes in a particular context. This paper suggests a new method for the mapping and analysis of large (technical) systems and contained technological trajectories on a national level using a combination of methods from statistical natural language processing, vector space modelling and network analysis. The proposed approach does not aim at replacing the researcher or expert but rather offers the possibility to algorithmically structure and to some extent quantify unstructured text data. The utilized filtered corpora consist of two types of Danish text-documents: 99 R&DD project descriptions and 574 (initially before filtering 813) non-academic/industrial journal publications dealing with the development of the smart energy grid in Denmark. Results show that in the explored case it is not mainly new technologies and applications that are driving change but innovative re-combinations of old and new technologies.
C88|Matching and Winning? The Impact of Upper and Middle Managers on Team Performance|In this paper we investigate the impact of upper and middle level managers on firm performance by simultaneously estimating manager and match qualities for management pairings in Major League Baseball (MLB). We document the economic significance of managers at both organizational levels and illustrate the importance of accounting for match quality in evaluating manager impact. Our results suggest assortative matching as managerial quality is positively correlated across organizational levels. Higher match quality in a pairing is further associated with higher individual manager qualities and longer joint employment spells. Mismatches in educational attainment are linked to lower match quality.
C88|Research Ideas for the Journal of Informatics and Data Mining: Opinion|The purpose of this Opinion article is to discuss some ideas that might lead to papers that are suitable for publication in the Journal of Informatics and Data Mining. The suggestions include the analysis of citations databases, PI-BETA (Papers Ignored – By Even The Authors), model specification and testing, pre-test bias and data mining, international rankings of academic journals based on citations, international rankings of academic institutions based on citations and other factors, and case studies in numerous disciplines in the sciences and social sciences.
C88|Assessment Of Approaches To Building An Analytical Crm System|The article assesses the advantages and disadvantages of the approaches to building analytical CRM systems using the author's own system of criteria. The results of the comparative analysis and the assessment indicate that there cannot be identified only one approach which can be classified as most suitable. Having in mind the advantages, the disadvantages and the limitations connected with the individual approaches, the author proposes building an analytical CRM system through combining different approaches and substantiates the feasibility of the combined approach through the use of service-oriented architecture.
C88|Sources of Growth and Instability in Agricultural Production in Western Odisha, India| This paper analyzes the nature and sources of agricultural instability in the Bolangir district of Western Odisha, India. The nature of instability in agricultural production is examined by determining the agricultural instability index (AII) of variables such as area, production and yield of food grains and paddy, irrigation coverage, and annual rainfall. The period covered by the study (1984–2009), which is characterized by greater technology dissemination, is categorized into two sub- periods: (1984–1993) and (1994–2009). The effects of a change in major inputs on the variability of crop productivity are assessed using a double-log model. The yield decomposition analysis is used to examine the role of drought risk factors and the amount and productivity of inputs in crop yield growth. The extent of instability in agricultural production and productivity in the region is found to be quite high on account of the high level of rainfall variability and the low irrigation coverage. The level of instability in food grain production is much larger during the second sub-period. The decomposition analysis reveals that about 84.4 percent of the total change in paddy yield growth is due to drought risk factors such as rainfall failure, rainfall variability, high temperature, and drought-induced pest attack, while the remaining change in paddy yield is due to the change in amount and productivity of major agricultural inputs such as labor, fertilizer, pesticides, and irrigation.
C88|New Issues Of Organizational Processes Security In Romanian Commercial Banks|The technological revolution has led to a reassessment of human perception of the surrounding world and the explosion of information technology has increased the number of communication methods between individuals. Romania is becoming more attractive to hackers and Internet fraud seem to stop anyone bypass. Last year, Romania has experienced forty two millions computer security incidents and was affected approximately two millions IP address. Most have focused on financial and banking system, public institutions and NGOs.
C88|Financial Analysis Management of Companies in a Region of Mexico: the Need of a Financial Ratios Annual Directory|Decisions of the organizational policies by business managers, based on ideal financial indicators, can be taken with the support of the analysis of financial ratios like predictors of business solvency, profitability and growth. This is an important financial tool to support decision making and for understanding the economic contexts where a company operates. For these reasons, the objective of this research is to identify the financial ratio usage level and to determine the relevance of elaborating an annual directory as support in defining the future of companies in Mexico. A qualitative methodology with a descriptive cross-sectional approach was used. To obtain the information a questionnaire with 43 items was successfully applied to 120 entrepreneurs in Culiac¨¢n, Sinaloa, Mexico. In this context it can be affirmed that the usage level of financial ratios is related to the firm¡¯s size. That is to say, the bigger the size of the company, the higher the level of usage is. The results support the relevance of elaborating the directory with advantages: its availability to businesses of all sizes at low cost.
C88|Software Solutions Adopted By Smes: Case Study Regarding Romanian Smes| During the last decades, the ICT industry imposed new tendencies concerning the communization of software platforms and solutions; thus a continuously growing phenomenon has been developed, namely that of open source software. In the present context, when companies have to manage large databases and exploit them to develop a business intelligence allowing them to create important competitive advantages within the global economy, open source software solutions become viable alternatives to be adopted by any company. Taking into account the global trends in open source software development, and also the increase in the level of utilization of this type of software in companies, the present study assesses the types of software solutions used by Romanian SMEs, identifying comparative aspects of use and perception of SMEs regarding the two important categories of software: open source and commercial. An SME profile is also identified from point of view of SMEs as final consumers of software products used to support their business processes. The results of the study can be considered as an important source of information for the business environment, starting with the companies’ decision makers responsible for the elaboration of the development strategy for their company, continuing with the commercial software providers and open source software communities and developers
C88|Predictive Inference on Finite Populations Segmented in Planned and Unplanned Domains|In this paper, we develop a new model-based method to inference on totals and averages of nite populations segmented in planned domains or strata. Within each stratum, we decompose the total as the sum of its sampled and unsampled parts, making inference on the unsampled part using Bayesian nonparametric methods. Additionally, we extend this method to make inference on totals of unplanned domains simultaneously modelling, within each stratum, the underlying uncertainty about the composition of the population and the totals across unplanned domains. Making inference on population averages is straightforward in both frameworks. To illustrate these methods, we develop a simulation exercise and evaluate the uncertainty surrounding the gender wage gap in Mexico.
C88|A Formal Proof of Vickrey's Theorem by Blast, Simp, and Rule|Formal methods use computers to verify proofs or even discover new theorems. Interest in applying formal methods to problems in economics has increased in the past decade, but - to date - none of this work has been published in economics journals. This paper applies formal methods to a familiar environment - Vickrey's theorem on second-price auctions - and provides, as background, an introduction to formal methods.
C88|Smart Residential Buildings as Learning Agent Organizations in the Internet of Things|Background: Smart buildings are one of the major application areas of technologies bound to embedded systems and the Internet of things. Such systems have to be adaptable and flexible in order to provide better services to its residents. Modelling such systems is an open research question. Herein, the question is approached using an organizational modelling methodology bound to the principles of the learning organization. Objectives: Providing a higher level of abstraction for understanding, developing and maintaining smart residential buildings in a more human understandable form. Methods/Approach: Organization theory provides us with the necessary concepts and methodology to approach complex organizational systems. Results: A set of principles for building learning agent organizations, a formalization of learning processes for agents, a framework for modelling knowledge transfer between agents and the environment, and a tailored organizational structure for smart residential buildings based on Nonaka’s hypertext organizational form. Conclusions: Organization theory is a promising field of research when dealing with complex engineering systems
C88|Web Technology Convergente With Expert Systems|From the first applications to this day, expert systems have obtained remarkable results in many areas, by using knowledge extracted and transferred from human experts. The main reason behind the advent of expert systems was the potential to provide recommendations to a large number of users. In today’s world, the spread of the Internet and web applications generated an exponential growth of the quantity of information that needs to be processed by an increasing number of users. Under these circumstances emerged the convergence of web technologies with expert systems, resulting in the category of web based expert systems. The potential of traditional expert systems was harnessed by using web technologies, offering new ways of disseminating expertise and knowledge to a mass audience. In this paper we will first review base concepts and features of traditional expert systems and then point out the benefits brought by the development and the use of web based expert systems.
C88|Teaching the multiplier: The value of a quantitative approach|To create an engaging and motivating learning environment, we have developed software which is based on estimated parameters for the UK economy. The program allows students to both simulate the effects of economic policy on national income and its components as well as the flexibility to vary key parameters of interest in order to assess the impact on economic performance. In this paper we present the main features of the software and the model on which it is based. We discuss the potential uses of the software within a class-room context and consider two simulations. The first of these is a fiscal expansion designed to bring down unemployment and the second is an exogenous shock to consumption affecting the parameters of the consumption–income relationship. This paper advocates that students’ understanding of theoretical models can be greatly enhanced by the addition of practical examples that can be used in lectures and tutorials as well as for independent study.
C88|A Geospatial Dynamic Microsimulation Model for Household Population Projections|Forecasting Populations (FPOP) is a microsimulation model (MSM) that is the demographic core of an extensible modeling framework. The framework, with FPOP at its core, enables the geospatial projection of a population under purely demographic processes or under the additional influence of exogenous factors such as disease, policy changes and prevention programs, or environmental stressors. Empirically-derived transition probabilities of life events such as birth, death, marriage, divorce and migration, captured in lookup table format, drive the simulation. These transition probabilities can be modified dynamically by external user-defined functions or other external MSMs. The use of MSM structures and methodologies enables FPOP to portray the impact of heterogeneity in the geospatial dimension (e.g., distribution of environmental factors or distribution of intervention programs), as well as the social dimension (e.g., household or social network correlates), on the projections. POP is designed and structured to: enable linking with external MSMs of any kind; support inclusion or configuration of more detailed transition probabilities; be scalable to millions of agents; use either an existing baseline synthetic population or a custom synthetic population of the userÂ’s design; and, run under computing environments that donÂ’t require a high degree of specialized software or hardware. In this paper we describe the design and structure of FPOP and then apply FPOP first under purely demographic processes and, secondly, in conjunction with an external disease model of obesity.The objective of FPOP is to provide a demographically realistic projection of the size, structure, and movement of populations and households decades into the future.
C88|Metodi di Forward Search per la ricerca di outlier: un’applicazione ai dati Istat sui matrimoni nel 2011|Forward Search (FS) is an iterative method to detect the presence of groups of outliers in the case of regression or multivariate data. In the present paper, FS is applied to Istat data on marriages celebrated in 2011 and to the characteristics of partners. The aim is to identify Provinces and Municipalities with significantly different characteristics from the other Italian Provinces and Municipalities. The analysis was conducted referring to three FS models: the multivariate, the compositional and the regression model. The comparison of the results obtained highlights the importance of the reference framework to define an observation as outlier. The analysis was performed using SiRiO (Outlier Research System), a software developed in Istat to make the FS analysis available for non- expert users.
C88|Managing census complexity through highly integrated web systems|During 2011 General Population and Housing Census methodological innovations towards the planning of a register-based census were introduced. Such innovations allowed to reach a satisfactory cost-benefit balance but increased the survey complexity and the risk of errors.Istat focus on web technologies was aimed at performing an innovative census, both in terms of methodology and costs and of data dissemination timeliness. This paper shows how a highly integrated web information system was used to manage a census workflow involving several actors and multiple phases and integrating different data sources. The success of 2011 Census led Istat to adopt such technological infrastructure as the core of the ‘Continuous General Population and Housing Census’.
C88|A Scoring Tool for Websites – A Case of Sustainable Organizations|This paper introduces a scoring tool to analyse company sustainability marketing efforts. We identify the expected scores for the companies selected on Corporate Responsibility Magazine’s list of 100 Best Corporate Citizens of the year. The scoring tool is based on the aspects of sustainability and website quality and is divided into three categories: a) user friendly, b) transparency, and c) content. The automation of the scoring tool benefits from a sustainability taxonomy to extract and evaluate the sustainability concepts and efforts mentioned by the companies. The tool scores the selected companies websites to determine the extent and quality of a company’s marketing of sustainability efforts. The result of applying the scoring tool shows that all companies in the list scored 8 to 14 in the user friendly section. In the transparency section, they scored 5 to 7, and in the content section they scored 6 to 10.
C88|Flexible Generation of E-Learning Exams in R: Moodle Quizzes, OLAT Assessments, and Beyond| The capabilities of the package exams for automatic generation of (statistical) exams in R are extended by adding support for learning management systems: As in earlier versions of the package exam generation is still based on separate Sweave files for each exercise – but rather than just producing different types of PDF output files, the package can now render the same exercises into a wide variety of output formats. These include HTML (with various options for displaying mathematical content) and XML specifications for online exams in learning management systems such as Moodle or OLAT. This flexibility is accomplished by a new modular and extensible design of the package that allows for reading all weaved exercises into R and managing associated supplementary files (such as graphics or data files). The manuscript discusses the readily available user interfaces, the design of the underlying infrastructure, and how new functionality can be built on top of the existing tools.
C88|Viable Stabilising Non-Taylor Monetary Policies for an Open Economy|We extend Krawczyk and Kim (Macroecon Dyn 13(1):46–80, 1999 ) and apply a viability approach to a small-open economy where the exchange rate works as an additional monetary policy transmission channel. A continuous-time version of the model presented in Batini and Haldane (In: Monetary policy rules. National Bureau of Economic Research, Cambridge, pp. 157–202, 1999a ) is used. The model comprises the IS equation, a supply curve and the interest parity condition. We modify the third equation to capture an impact of a domestic interest-rate hike on the speedy appreciation of local currency. We calibrate this modified model using available literature results and apply specialised software (VIKAASA) to compute the open-economy viability kernel that is a set of economic states, from which the central bank can control the economy so that it remains within a nominal constraint set. We then analyse the kernel topology and show a few stablising policies that keep the economy within the constraint set. We also discuss the robustness of such polices to shocks and parameter uncertainty and observe that viability-based policies come from models, which do not require explicit weights on the variables of interest of a central bank. We also contend that in general, viability-based policies are less likely to do damage, if the policy-maker is wrong about some aspects of the environment. Copyright Springer Science+Business Media New York 2014
C88|Heterogeneous Computing in Economics: A Simplified Approach|This paper shows the potential of heterogeneous computing in solving dynamic equilibrium models in economics. We illustrate the power and simplicity of C++ Accelerated Massive Parallelism (C++ AMP) recently introduced by Microsoft. Starting from the same exercise as Aldrich et al. (J Econ Dyn Control 35:386–393, 2011 ) we document a speed gain together with a simplified programming style that naturally enables parallelization. Copyright Springer Science+Business Media New York 2014
C88|Software for continuous game experiments|ConG is software for conducting economic experiments in continuous and discrete time. It allows experimenters with limited programming experience to create a variety of strategic environments featuring rich visual feedback in continuous time and over continuous action spaces, as well as in discrete time or over discrete action spaces. Simple, easily edited input files give the experimenter considerable flexibility in specifying the strategic environment and visual feedback. Source code is modular and allows researchers with programming skills to create novel strategic environments and displays. Copyright Economic Science Association 2014
C88|Standardisation in the Retail Banking Sector Designing Functions for an Individualised Asset Allocation Advisory|This article is about individualising the process of giving advice to a retail customer in the field of asset allocation. With regard to this process, two main contributions are made by answering two questions. First, which objectives are relevant for a customer (beyond return and risk) and which functions are adequate to evaluate portfolios of investment alternatives with regard to these objectives? Based on empirical literature on customers’ goals, the four objectives liquidity, variability, comprehensiveness, and manageability are identified as relevant. The background of each objective is discussed in order to formulate desirable properties of the objective functions. These properties are then used to axiomatically identify particular functions from fuzzy theory suitable for the given context. The second question is: which selection function is adequate to select a particular portfolio out of a set of portfolios? To answer this question, again an axiomatic approach is chosen: Several properties are discussed and stated which shall reflect the customer’s decision calculus. By requiring these properties, the selection function can be exactly specified. The results can help financial services providers in two ways: On the one hand, they can provide their customers with a higher quality of their advisory services by taking into account more objectives than return and risk. On the other hand, as the derived functions are standardised, they can be used in software applications to support the advisory process which can then be offered at lower costs and thereby even to retail customers.
C88|A Decision Support System for Business Location Based on Open GIS Technology and Data|This article presents the architecture, features, and operating mode of a DSS (Decision Support System) aiming to assist entrepreneurs and managers in the process of location decision making. The research assembled concepts derived from theory, findings of empirical studies, together with open GIS (Geographical Information System) software and data, and modelled them into a DSS software tool, according to an original methodology and design. The users are guided step-by-step to input information on their businesses into the DSS (industry, preferences for land-use areas and facility types, weights of key location factors), and are returned two sets of results: one based on own options, and another one aggregate for the industry they operate in. The results consist in the top five locations for the user's firm, as well as for the industry, depicted both in a graphical report (map) and a text report (explanation of results).
C88|Dynamic microsimulation: general principles and examples in R|Dynamic microsimulation allows modelling complex systems where the variability of individual characteristics and behaviours plays a dominant role. Its basic principle is extremely simple but its implementation raises some methodological issues that are not always documented in descriptions of existing models. In addition, a major question for model builders is the choice of a programming language. Some dedicated platforms now exist. The simplicity of the method also allows starting from scratch with generalist languages and this is the choice that had been retained for the two successive versions of the Destinie model developed at Insee since the mid 1990s. A third option is to rely on a statistical package. This document provides a general introduction to this modelling technique and to its few methodological difficulties, and some examples of this third programming style, written in R, including a partial adaptation, in R, of the core of Destinie’s demographic module.
C88|Model For Designing An Information System With High Reliability|In the context of the generalization process of computerization in all areas of economic and social necessity author presents the design of information systems at risk as small operation. This paper defines the concept of reliability of systems, its basic components and create a model for accomplishing it. Reliability is a key factor considered by the author to reduce the risk of operating systems. The problem is even more important for embedded computing systems. The paper is designed and presented an original design of systems with high reliability. According to the author, the cost of preparing such a system is a factor that determines the level of reliability designed paper presents various design alternatives based on costs..
C88|Demand function and its role in a business simulator|Business simulations are useful tools due to the fact that it eases management decision making. No doubt there are many processes which must be considered and simulated. Therefore, such business simulator is often composed of many processes and contains many agents and interrelations as well. Since the business simulator based on multi-agent system is characterized by many interrelations within, this article deals with a specific part of the business simulator only – a demand function and its modeling. The aim of this partial research is to suggest demand function which would be most suitable for the business simulation. In this paper a new approach for customer decision function in business process simulation was presented. The decision of the customer is based on Marshallian demand function and customer utility function using Cobb-Douglas preferences. The results obtained by means of the MAREA simulation environment proved that this approach yields correct simulation results.
C88|Analyzing and visualizing the synergistic impact mechanisms of climate change related costs|One climate related phenomenon could affect many more. The direct costs associated to climate related factors pass to a number of other climate related costs through the indirect economic consequences of climate change. In this paper we propose a mathematical model which aims to provide forecasts of the distribution of the costs caused by the synergistic mechanism of environmental effects. The model is created to be directly applied to situations where the primary costs associated to climate related factors can be specified. It is expressed in matrix terms and is programmed using Mathematica’s matrix functions. We provide the framework for efficient computation of this model, covering possible linear and nonlinear functions of the impact mechanism for costs and, infinite direct cost scenarios. Some directions for the quantitative estimation of impact indicators and adaptation potentials of the costs incurred by certain climate related factors are included, in order to apply the proposed model using real socioeconomic data.
C88|Nonlinear time series analysis of annual temperatures concerning the global Earth climate|This paper presents results concerning the nonlinear analysis of the mean annual value temperature time series corresponding to the Earth’s global climate for the time period of 713 – 2004. The nonlinear analysis consists of the application of several filtering methods, the estimation of geometrical and dynamical characteristics in the reconstructed phase space, techniques of discrimination between nonlinear low dimensional and linear high dimensional (stochastic) dynamics and tests for serial dependence and nonlinear structure. All study results converge to the conclusion of nonlinear stochastic and complex nature of the global earth climate.
C88|Computer system for farms (SITEFA) - an opportunity for performant agricultural management|"Any modern agricultural unit, regardless of profile, size, ownership and socio-economic space in which they operate, requires a management style based on flexibility, dynamism and foresight, which is inconceivable without a complex, operative and quality information, to underpin decision making. Therefore, any farmer needs objective, relevant, reliable, timely, useful, concerning: market demand, new products and technologies, the position of competitors, suppliers and customers, their performance, etc., so that their analysis to directly influence and as the competitiveness of the farm in a particular market or market segment. Based on the theme of the ""Determination of economic indicators of crop production technologies and animal applied in order to increase environmental performance (costs, productivity, profitability, gross margin)"" from the Sector Plan ADER 2020 was developed computer system SITEFA- a product developed and designed program-technical-economic analysis of the performances of farm economic and efficient use of production factors in classical operating conditions."
C88|Использование Средств Информатизации Образования При Обучении Математике В Экономическом Вузе<BR>[The Usage of Information Support Means in Teaching Mathematics in Finance University]|The analysis of the content of kinds of performance of future Bachelor of Economics has been conducted to detail the means of competency development, to identify common cultural and professional competencies which are focused on the usage of information technologies in teaching Mathematics. Through the example of the solution of the profession-oriented problem in the context of the discipline “Method of seeking optimal decisions” the authors investigate the possibilities to use information support means. They demonstrate the advisability of the usage of spreadsheet MS Excel (build-in mathematical instrument “Finding Solutions”) to solve the problem of linear programming that in its turn provides automation of bloated mathematical calculation and allows to carry out the benchmarking study of different scenarios of the practical situation
C88|Perspectives on integrating a computer algebra system into advanced calculus curricula|We introduce a topic in the intersection of symbolic mathematics and computation, concerning topics in multivariable Optimization and Dynamic Analysis. Our computational approach gives emphasis to mathematical methodology and aims at both symbolic and numerical results as implemented by a powerful digital mathematical tool, CAS software Xcas. This work could be used as guidance to develop course contents in advanced calculus curricula, to conduct individual or collaborative projects for programming related objectives, as Xcas is freely available to users and institutions. Furthermore, it could assist educators to reproduce calculus methodologies by generating automatically, in one entry, abstract calculus formulations.
C88|Parallel Computing in Economics - An Overview of the Software Frameworks|This paper discusses problems related to parallel computing applied in economics. It introduces the paradigms of parallel computing and emphasizes the new trends in this field - a combination between GPU computing, multicore computing and distributed computing. It also gives examples of problems arising from economics where these parallel methods can be used to speed up the computations
C88|Acceso a Recursos de Cómputo de Alto Rendimiento Mediante Correo Electrónico (An email-based platform for accessing High Performance Computing resources)| Spanish abstract. El cómputo de alto rendimiento es una necesidad para el desarrollo de investigaciones con grandes volúmenes de datos. La creciente demanda de este tipo de resultados ha impulsado a varios centros de investigación a poner en funcionamiento recursos de cómputo de alto rendimiento. En Cuba no existe una solución definitiva que permita a todos los centros de investigación disponer de los recursos de cómputo necesarios para desarrollar sus proyectos. Este trabajo propone el empleo de un clúster de computadoras de la Universidad de Griffith a través de una interfaz basada en el correo electrónico. Esta solución permite disponer de recursos de cómputo de alto rendimiento sin necesidad de una alta conectividad. Como caso de estudio se analizan los resultados obtenidos en un proyecto de optimización global en grandes dimensiones desarrollado en la Universidad de La Habana. Para experimentos con un mes de duración (en una computadora estándar) los resultados muestran que al utilizar el recurso de alto rendimiento es posible alcanzar un incremento en el rendimiento relativo superior al 1300%. English abstract. Research with large volumes of data usually require access to High Performance Computing. The increasing demand for this kind of research has led many institutions to develop their own computer clusters. However, in Cuba there is no definitive solution for the High Performance Computing requirements of institutions such as the University of Havana. The expenses of building a computer cluster disallows many institutions to have their own, while the low connectivity limits the use of international high performance computing services. This research presents an alternative solution based on the development of an email-based platform for accessing a computer cluster at Griffith University in Australia. This new communication interface has been successfully used on a Large Scale Optimization research project at the University of Havana. Computational results show that by using the remote cluster it is possible to improve upon the performance of a personal computer by up to 1300%.
C88|Integrated Computer Systems. Enterprise Resource Planning (Erp)|At the beginning of the XXI century society, knowledge based society, the management of economic organizations can only be achieved through optimal IT systems. They can be seen as an extension of increasingly complex information systems and provide effective leadership only if they are integrated in the economic system of the organization. We have previously shown some of the features that recommend integrated IT systems to be controlled and used, as well as main principles for building the integrated computer systems, strategies that can be applied in the designing of this type of IT system. Advantages of management integrated IT systems can be best supported by examples, and therefore we intend to present a special category, but increasingly used, of integrated IT systems: Enterprise Resource Planning (ERP). They are “distributed IT systems based on client / server and developed for the processing of transactions and facilitating the integration of business processes with suppliers, customers and other business partners.”
C88|Statistical Data Analysis via R and PHP: A Case Study Of the Relationship Between GDP and Foreign Direct Investments for the Republic Of Moldova|This paper provides an overview over a way of integrating R with PHP scripting language in order to analyze statistical data (time series).We analyze the relationship between the foreign direct investments and GDP of the Republic of Moldova over 1992-2012 time period.
C88|The Progress of R in Romanian Official Statistics|The present paper exposes an overview of the state-of-the-art of R statistical software in the official statistics in Romania, predominantly in the social statistics. Examples on data analysis and econometric models of Small Area Estimation successfully completed are given.The scientific approach includes also a summary of the applications of R in other statistical offices around the world. Other countries like United Kingdom or Netherlands are truly experienced in the use of R.We conclude with a series of proposals on the future research opportunities and other potential analysis procedures of R in the social statistics.
C88|R – a Global Sensation in Data Science|The main objective of this paper is to expose the evolution of R, as the most used data analysis tool among statisticians and the academic researchers. Its flexibility and complexity simply gained the statisticians and data scientists. The paper examines some of the reasons behind the popularity of R, using tools like SWOT analysis. R software environment offers integrated tools for a very large area of data analysis, from computations and data mining to high-effects visualization. As an example, we performed in this paper an illustration of 3D plotting.
C88|Integrating R and Hadoop for Big Data Analysis|Analyzing and working with big data could be very difficult using classical means like relational database management systems or desktop software packages for statistics and visualization. Instead, big data requires large clusters with hundreds or even thousands of computing nodes. Official statistics is increasingly considering big data for deriving new statistics because big data sources could produce more relevant and timely statistics than traditional sources. One of the software tools successfully and wide spread used for storage and processing of big data sets on clusters of commodity hardware is Hadoop. Hadoop framework contains libraries, a distributed file-system (HDFS), a resource-management platform and implements a version of the MapReduce programming model for large scale data processing. In this paper we investigate the possibilities of integrating Hadoop with R which is a popular software used for statistical computing and data visualization. We present three ways of integrating them: R with Streaming, Rhipe and RHadoop and we emphasize the advantages and disadvantages of each solution.
C88|Sources of Growth and Instability in Agricultural Production in Western Odisha, India|This paper analyzes the nature and sources of agricultural instability in the Bolangir district of Western Odisha, India. The nature of instability in agricultural production is examined by determining the agricultural instability index (AII) of variables such as area, production and yield of food grains and paddy, irrigation coverage, and annual rainfall. The period covered by the study (1984â€“2009), which is characterized by greater technology dissemination, is categorized into two sub- periods: (1984â€“1993) and (1994â€“2009). The effects of a change in major inputs on the variability of crop productivity are assessed using a double-log model. The yield decomposition analysis is used to examine the role of drought risk factors and the amount and productivity of inputs in crop yield growth. The extent of instability in agricultural production and productivity in the region is found to be quite high on account of the high level of rainfall variability and the low irrigation coverage. The level of instability in food grain production is much larger during the second sub-period. The decomposition analysis reveals that about 84.4 percent of the total change in paddy yield growth is due to drought risk factors such as rainfall failure, rainfall variability, high temperature, and drought-induced pest attack, while the remaining change in paddy yield is due to the change in amount and productivity of major agricultural inputs such as labor, fertilizer, pesticides, and irrigation.
C88|GRID for model structure discovering in high dimensional regression|Given a nonparametric regression model, we assume that the number of covariates $d\rightarrow\infty$ but only some of these covariates are relevant for the model. Our goal is to identify the relevant covariates and to obtain some information about the structure of the model. We propose a new nonparametric procedure, called GRID, having the following features: (a) it automatically identifies the relevant covariates of the regression model, also distinguishing the nonlinear from the linear ones (a covariate is defined linear/nonlinear depending on the marginal relation between the response variable and such a covariate); (b) the interactions between the covariates (mixed effect terms) are automatically identified, without the necessity of considering some kind of stepwise selection method. In particular, our procedure can identify the mixed terms of any order (two way, three way, ...) without increasing the computational complexity of the algorithm; (c) it is completely data-driven, so being easily implementable for the analysis of real datasets. In particular, it does not depend on the selection of crucial regularization parameters, nor it requires the estimation of the nuisance parameter $\sigma^2$ (self scaling). The acronym GRID has a twofold meaning: first, it derives from Gradient Relevant Identification Derivatives, meaning that the procedure is based on testing the significance of a partial derivative estimator; second, it refers to a graphical tool which can help in representing the identified structure of the regression model. The properties of the GRID procedure are investigated theoretically.
C88|Bias-corrected inference for multivariate nonparametric regression: model selection and oracle property|The local polynomial estimator is particularly affected by the curse of dimensionality. So, the potentialities of such a tool become ineffective for large dimensional applications. Motivated by this, we propose a new estimation procedure based on the local linear estimator and a nonlinearity sparseness condition, which focuses on the number of covariates for which the gradient is not constant. Our procedure, called BID for Bias-Inflation-Deflation, is automatic and easily applicable to models with many covariates without any additive assumption to the model. It simultaneously gives a consistent estimation of a) the optimal bandwidth matrix, b) the multivariate regression function and c) the multivariate, bias-corrected, confidence bands. Moreover, it automatically identify the relevant covariates and it separates the nonlinear from the linear effects. We do not need pilot bandwidths. Some theoretical properties of the method are discussed in the paper. In particular, we show the nonparametric oracle property. For linear models, the BID automatically reaches the optimal rate $Op(n^{-1/2})$, equivalent to the parametric case. A simulation study shows a good performance of the BID procedure, compared with its direct competitor.
C88|MATLAB for Economics and Econometrics A Beginners Guide|This beginners' guide to MATLAB for economics and econometrics is an updated and extended version of Frain (2010). The examples and illustrations here are based on Matlab version 8.3 (R2014a). It describes the new MATLAB Desktop, contains an introductory MATLAB session showing elementary MATLAB operations, gives details of data input/output, decision and loop structures, elementary plots, describes the LeSage econometrics toolbox and shows how to do maximum likelihood estimation. Various worked examples of the use of MATLAB in economics and econometrics are also given. I see MATLAB not only as a tool for doing economics/econometrics but as an aid to learning economics/econometrics and understanding the use of linear algebra there. This document can also be seen as an introduction to the MATLAB on-line help, manuals and various specialist MATLAB books.
C88|Combination of Multiple Bipartite Ranking for Multipartite Web Content Quality Evaluation|Web content quality evaluation is crucial to various web content processing applications. Bagging has a powerful classification capacity by combining multiple classifiers. In this study, similar to Bagging, multiple pairwise bipartite ranking learners are combined to solve the multipartite ranking problems for web content quality evaluation. Both encoding and decoding mechanisms are used to combine bipartite rankers to form a multipartite ranker and, hence, the multipartite ranker is called MultiRank.ED. Both binary encoding and ternary encoding extend each rank value to an L ? 1 dimensional vector for a ranking problem with L different rank values. Predefined weighting and adaptive weighting decoding mechanisms are used to combine the ranking results of bipartite rankers to obtain the final ranking results. In addition, some theoretical analyses of the encoding and the decoding strategies in the MultiRank.ED algorithm are provided. Computational experiments using the DC2010 datasets show that the combination of binary encoding and predefined weighting decoding yields the best performance in all four combinations. Furthermore, this combination performs better than the best winning method of the DC2010 competition.
C88|"""Mathematics and Archaeology"" rediscovered"|"The book ""Mathematics and Archaeology"", consisting of 25 chapters by a range of international scholars in archaeology, will be published by Chapman & Hall in 2014. The present document, written as an invited Epilogue to the book, recounts the rediscovery of the book 275 years later by an archaeolinguist. The remnants of the book have been found in the Universitat Pompeu Fabra’s dilapidated library, which fell into disuse after books were abandoned in favour of electronic publishing. The archaeolinguist explains how statistical methods found in old texts on quantitative archaeology helped to piece together the basic content of this book, using the words in the chapters as artefacts."
C88|Size and shape in the measurement of multivariate proximity|Most methods of multivariate analysis rely on a measure of proximity between individual cases or samples to quantify inter-sample differences. The choice of this measure is fundamental to the method and its subsequent results. For example, when data are abundance counts of a set of species at several sampling locations, some approaches rely on the Bray-Curtis dissimilarity measure between samples, while other approaches rely on the chi-square distance. A set of observed species abundances at a location has both size, in the form of the overall levels of the species counts, and shape, in the form of the relative values of the counts. The aim of this report is to clarify how much the chosen proximity measure is capturing differences in size between samples as opposed to differences in shape. After motivating the idea using physical morphometric data, the study is extended to nonnegative data in general, with special focus on abundance counts and biomass estimates, which are ubiquitous in ecological research.
C88|Measurement error in occupational coding:an analysis on SHARE data| This article studies the potential measurement errors when coding occupational data. The quality of occupational data is important but often neglected. We recoded open-ended questions on occupation for last and current job in the Dutch SHARE data, using the CASCOT ex-post coding software. The disagreement rate, defined as the percentage of observations coded differently in SHARE and CASCOT, is high even when compared at ISCO 1-digit level (33.7% for last job and 40% for current job). This finding is striking, considering our conservative approach to exclude vague and incomplete answers. The level of miscoding should thus be considered as a lower bound of the �true� miscoding. This highlights the complexity of occupational coding and suggests that measurement error due to miscoding should be taken into account when making statistical analysis or writing econometric models. We tested whether the measurement error is random or correlated to individual or job-related characteristics, and we found that the measurement error is indeed more evident in ISCO-88 groups 1 and 3 and is more pronounced for higher educated individuals and males. These groups may be sorted in occupations that are intrinsically more difficult to be classified, or education and gender may affect the way people describe their jobs.
C88|Software For Energy Audit Of Households|Nowadays Russian government pays a lot of attention to solutions of the high energy intensive production system in the country. The key factors of low efficiency of regional energy saving programs is the lack of approved standards and algorithms in the area of energy audit. In practice, both technical and economical parameters of energy audit fluctuate in a very large range. It's due not only to different complexity of the audit but also the immaturity of the market. General population in Russia on its own has not yet embraced energy efficiency as a social value, therefore not many people are interested to invest in increasing energy efficiency of their homes and, therefore, to use algorithms for home energy audit suggested by well-known western companies such as Energy Star and others. New energy efficient home appliances can significantly reduce the energy consumption, however, the extent to which the theoretical reduction potential can be realized highly depends on individual decision processes. In this situation most of the small and middle-size companies as well as government organizations are getting more and more interested in some 'do-it-yourself' tools that can help to make very first steps in introduction of energy management systems and reduce the cost of professional energy audit. In this paper we present a simple interactive calculator, which can be used in individual flats and houses, office buildings and educational institutes such as colleges and schools, for assessment of electricity use by different categories of equipment. The user indicates only type of equipment and its approximate time of work. The program outputs the structure of energy consumption in graphic format. The results of test procedures and the ways for improvement of the program are discussed. The program is realised using the C++ programming language following the C++03 standard. It uses Qt libraries and provides graphical output in the form of a histogram, showing which energy appliances consume the most energy.
C88|Comparative analysis of regional development: Exploratory space-time data analysis and open source implementation|This paper aims to make contributions to comparative analysis of regional economic dynamics in China and the US from the methodological perspective. More specifically, some recent advances in exploratory space-time data analysis (ESTDA) will be suggested and implemented to conduct this task in an open source environment. China and the US have been the subject of much discussion about the patterns and trends of regional development, because of their importance in the world economic system. Despite the rich and growing empirical literature on numerous case studies at both national and sub-national scales, comparative space time analysis between and within these two economic systems has just begun to catch attentions. Additionally, ESTDA and its open source implementation can facilitate comparative studies of regional development.
C88|Reorganization in Multi-Agent Architectures: An Active Graph Grammar Approach|Background: Organizational architecture is a holistic approach to design of humane organizations and studies an organization from five perspectives: structure, culture, processes, strategy and individuals. In this paper, the concept of organizational architecture is firstly formalized using the fractal principle and then applied to multi-agent systems’ (MAS) organizations. Objectives: Providing a holistic framework for modeling all aspects of MAS reorganization. Methods/Approach: MAS organizations are formalized using graph theoryand a new active graph rewriting formalism inspired by active database theory is intoduced. Results: The newly developed framework is graphical, event-driven and applicable in a distributed MAS environment. Conclusions: By defining organizational units, processes, strategies and cultural artefacts in a recursive way, it is shown that labeled graphs andhypergraphs can be used to model various levels of organizational architecture while active graph grammars allow one to model reorganization of each of the architectural perspectives.
C88|Plotting regression coefficients and other estimates in Stata|Graphical presentation of regression results has become increasingly popular in the scientific literature, as graphs are much easier to read than tables in many cases. In Stata such plots can be produced by the -marginsplot- command. However, while -marginsplot- is very versatile and flexible, it has two major limitations: it can only process results left behind by -margins- and it can only handle one set of results at the time. In this article I introduce a new command called -coefplot- that overcomes these limitations. It plots results from any estimation command and combines results from several models into a single graph. The default behavior of -coefplot- is to plot markers for coefficients and horizontal spikes for confidence intervals. However, -coefplot- can also produce various other types of graphs. The capabilities of -coefplot- are illustrated in this article using a series of examples. A shorter version of this paper has been published in The Stata Journal 14(4): 708-737 (see: http://www.stata-journal.com/article.html?article=gr0059).
C88|JAS 2: a new Java platform for AB modeling and dynamic microsimulation|We present JAS 2, a new Java platform which aims at providing a unique simulation tool for discrete-event simulations, including agent-based and dynamic microsimulation models. JAS 2 is not released as a self-contained stand-alone application for model development. With the aim to develop large-scale, computationally resource-intensive models, the main architectural choice of JAS 2 is to use whenever possible standard, open-source tools aready available in the software development community. This decision entails a longer learning curve for the user than a dedicated, language-specific software package, but it allows for a more professional approach to simulation modeling compared to specific proprietary environments. There are two main advantages of using an open architecture where external libraries can be added and utilized: one is the possibility to easily integrate external functions that can be added as plug-ins; the other is the possibility to freely extend and modify the functions available on the platform, possibly within a cooperative effort of the community of users. Moreover, an open source application makes it easier to share and test the models. Building on the vast number of software solutions available, JAS 2 allows the user to separate data representation and management from the implementation of processes and behavioral algorithms. It also allows the user, within an object-oriented paradigm, to model the real-life system naturally and intuitively: each category of individuals or objects that populate the model is represented by a specific class, with its own variables and methods. The main value added of the platform lies in the potential for integration with RDBMS (relational database management) tools through ad-hoc microsimulation Java libraries. The management of input data persistence layers and simulation results is performed using standard database management tools, and the platform takes care of the automatic translation of the relational model (which is typical of a database) into the object-oriented simulation model.
C88|Metodologii De Dezvoltare A Programelor Informatice - Studiu Comparativ|Mutaţiile din domeniul tehnologiei informaţionale şi al metodelor de abordare a sistemelor s-au reflectat şi în ciclul de viaţă al dezvoltării sistemelor, fie prin schimbarea etapelor acestuia, fie prin modificarea opticii de parcurgere a lor. Spre exemplu, odată cu abordarea orientată-obiect a sistemelor, s-au lansat şi noi modele ale ciclului de viaţă. Prin parcurgerea materialelor de specialitate, se poate constata că numărul fazelor/etapelor unui ciclu de viață variază de la trei (de exemplu analiza, proiectarea, implementarea) la peste douăzeci. Există multe metodologii de dezvoltare a sistemelor, și fiecare dintre ele este unică în funcție de ordinea și importanța fiecărei faze din ciclul de dezvoltare. Unele metodologii reprezintă standarde formale folosite de agenții guvernamentale, în timp ce altele au fost dezvoltate de firme de consultanță pentru a fi vândute clienților. Multe organizații au metodologiile lor interne care au fost rafinate de-a lungul anilor. Lucrarea aceasta îsi propune să facă un scurt inventar al principalelor metodologii de dezvoltarea a programelor informatice, evidențiind diferențele și asemănările dintre ele, ca în final să ofere sugestii cu privirea la oportunitatea folosirii unei metode sau alta în funcție de tipul proiectului.
C88|Using Statistical Methods For Estimating Students' Opinion On The Purpose Of Human Life|Currently, two anthropological models are dominant in Romania. The first is the atheistic-evolutionary model and the second is the Christian model. The purpose of this article is to assess the students’ opinion on the two anthropological models and, in particular, on the purpose of human life. To that effect, there have been used the coefficients of association between the main aspects of the two models. Starting from the two outlined approaches, this paper aims to investigate the opinions of the students from two different specializations within Ovidius University of Constanta – the Faculty of Economics and the Faculty of Theology – on the issues mentioned above
C88|Evaluación y medición de la calidad de gestión en las cajas rurales Alcance y sustentabilidad|Resumen El presente artículo destaca el beneficio y buen funcionamiento del programa microfi- nanciero denominado Cajas Rurales. Además de un estudio comparativo, para observar las fortalezas y debilidades que ellas presentan, se desarrolla la construcción del modelo “Evaluación y medición de la calidad de gestión de las Cajas Rurales” (EMCGCR). El diagnóstico preliminar indica que las cajas son una organización microfinanciera sus- tentable en los sectores rurales. Los resultados han desmoronado el mito: “los pobres no pagan”; reportan tasas de morosidad muy bajas. Para reforzar cuantitativamente esta conclusión se emplearon alternativamente técnicas de análisis multivariante y de inteligencia artificial. Se comprobó la eficiencia de las cajas y además, que ambas técnicas permitieron clasificar con alto desempeño la sustentabilidad de las cajas estudiadas.
