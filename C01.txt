C01|Resuscitating the co-fractional model of Granger (1986)|We study the theoretical properties of the model for fractional cointegration proposed by Granger (1986), namely the FVECM d,b. First, we show that the stability of any discrete time stochastic system of the type II(L)Yt = E t can be assessed by means of the argument principle under mild regularity condition on II (L) where L is the lag operator. Second, we prove that, under stability, the FVECMd,b allows for a representation of the solution that demonstrates the fractional and co-fractional properties and we find a closed-form expression for the impulse response functions. Third, we prove that the model is identifed for any combination of number of lags and cointegration rank, while still being able to generate polynomial co-fractionality. Finally, we show that the asymptotic properties of the maximum likelihood estimator reconcile with those of the FCVARd,b model studied in Johansen and Nielsen (2012).
C01|Demand and Welfare Analysis in Discrete Choice Models with Social Interactions|Many real-life settings of consumer choice involve social interactions, causing targeted policies to have spillover effects. This paper develops novel empirical tools for analyzing demand and welfare effects of policy interventions in binary choice settings with social interactions. Examples include subsidies for health product adoption and vouchers for attending a high-achieving school. We establish the connection between econometrics of large games and Brock-Durlauf-type interaction models, under both I.I.D. and spatially correlated unobservables. We develop new convergence results for associated beliefs and estimates of preference parameters under increasing domain spatial asymptotics. Next, we show that even with fully parametric specifications and unique equilibrium, choice data, that are sufficient for counterfactual demand prediction under interactions, are insufficient for welfare calculations. This is because distinct underlying mechanisms producing the same interaction coefficient can imply different welfare effects and deadweight-loss from a policy intervention. Standard index-restrictions imply distribution-free bounds on welfare. We illustrate our results using experimental data on mosquito-net adoption in rural Kenya.
C01|Uniform Consistency of Marked and Weighted Empirical Distributions of Residuals|A uniform weak consistency theory is presented for the marked and weighted empirical distribution function of residuals. New and weaker sufficient conditions for uniform consistency are derived. The theory allows for a wide variety of regressors and error distributions. We apply the theory to 1-step Huber-skip estimators. These estimators describe the widespread practice of removing outlying observations from an intial estimation of the model of interest and updating the estimation in a second step by applying least squares to the selected observations. Two results are presented. First, we give new and weaker conditions for consistency of the estimators. Second, we analyze the gauge, which is the rate of false detection of outliers, and which can be used to decide the cut-off in the rule for selecting outliers.
C01|Models where the Least Trimmed Squares and Least Median of Squares estimators are maximum likelihood|The Least Trimmed Squares (LTS) and Least Median of Squares (LMS) estimators are popular robust regression estimators. The idea behind the estimators is to find, for a given h, a sub-sample of h good observations among n observations and estimate the regression on that sub-sample. We find models, based on the normal or the uniform distribution respectively, in which these estimators are maximum likelihood. We provide an asymptotic theory for the location-scale case in those models. The LTS estimator is found to be sqrt(h) consistent and asymptotically standard normal. The LMS estimator is found to be h consistent and asymptotically Laplace.
C01|Demand and Welfare Analysis in Discrete Choice Models with Social Interactions|Many real-life settings of consumer-choice involve social interactions, causing targeted policies to have spillover-effects. This paper develops novel empirical tools for analyzing demand and welfare-effects of policy-interventions in binary choice settings with social interactions. Examples include subsidies for healthproduct adoption and vouchers for attending a high-achieving school. We establish the connection between econometrics of large games and Brock-Durlauf-type interaction models, under both I.I.D. and spatially correlated unobservables. We develop new convergence results for associated beliefs and estimates of preference-parameters under increasing-domain spatial asymptotics. Next, we show that even with fully parametric specifications and unique equilibrium, choice data, that are sufficient for counterfactual demand - prediction under interactions, are insufficient for welfare-calculations. This is because distinct underlying mechanisms producing the same interaction coefficient can imply different welfare-effects and deadweightloss from a policy-intervention. Standard index-restrictions imply distribution-free bounds on welfare. We illustrate our results using experimental data on mosquito-net adoption in rural Kenya.
C01|Tail index estimation: quantile driven threshold selection|The selection of upper order statistics in tail estimation is notoriously difficult. Most methods are based on asymptotic arguments, like minimizing the asymptotic mse, that do not perform well in finite samples. Here we advance a data driven method that minimizes the maximum distance between the fitted Pareto type tail and the observed quantile. To analyse the finite sample properties of the metric we organize a horse race between the other methods. In most cases the finite sample based methods perform best. To demonstrate the economic relevance of choosing the proper methodology we use daily equity return data from the CRSP database and find economic relevant variation between the tail index estimates.
C01|Bayesian Inference for Markov-switching Skewed Autoregressive Models|We examine Markov-switching autoregressive models where the commonly used Gaussian assumption for disturbances is replaced with a skew-normal distribution. This allows us to detect regime changes not only in the mean and the variance of a specified time series, but also in its skewness. A Bayesian framework is developed based on Markov chain Monte Carlo sampling. Our informative prior distributions lead to closed-form full conditional posterior distributions, whose sampling can be efficiently conducted within a Gibbs sampling scheme. The usefulness of the methodology is illustrated with a real-data example from U.S. stock markets.
C01|Random Models for the Joint Treatment of Risk and Time Preferences|We develop a simple, tractable and sound stochastic framework for the joint treatment of risk and time preferences, in order to facilitate the estimation of risk and time attitudes. In so doing we: (i) study deterministic models of risk and time preferences paying special attention to their comparative statics, (ii) embed the deterministic models and their comparative statics within the random utility framework, and (iii) show how to estimate them, illustrating this exercise on several experimental datasets.
C01|Multicollinearity in the Presence of Errors-in-Variables Can Increase the Probability of Type-I Error|Multicollinearity, especially in combination with errors-in-variables, can increase the likelihood of a Type-I error by inflating the value of the estimated coefficients by more than it magnifies their standard errors, thereby increasing the likelihood of obtaining statistically significant results. This anomalous result may be due to an interaction effect between errors-in-variables and multicollinearity.
C01|Synthetic Difference In Differences|We present a new perspective on the Synthetic Control (SC) method as a weighted least squares regression estimator with time fixed effects and unit weights. This perspective suggests a generalization with two way (both unit and time) fixed effects, and both unit and time weights, which can be interpreted as a unit and time weighted version of the standard Difference In Differences (DID) estimator. We find that this new Synthetic Difference In Differences (SDID) estimator has attractive properties compared to the SC and DID estimators. Formally we show that our approach has double robustness properties: the SDID estimator is consistent under a wide variety of weighting schemes given a well-specified fixed effects model, and SDID is consistent with appropriately penalized SC weights when the basic fixed effects model is misspecified and instead the true data generating process involves a more general low-rank structure (e.g., a latent factor model). We also present results that justify standard inference based on weighted DID regression. Further generalizations include unit and time weighted factor models.
C01|Impacto de la implementación del canal de atención a clientes mineros de la ANM sobre la inversión minera en Colombia|El presente estudio busca medir el impacto de la implementación del Canal de Atención a Clientes Mineros del Grupo de Promoción de la Agencia Nacional de Minería (ANM) sobre la inversión en el sector minero colombiano. Teniendo en cuenta que el uso del canal es voluntario y libre, se optó por aplicar la metodología de emparejamiento PSM, con el fin de corregir el posible sesgo de autoselección causado por la diferencia de las empresas participantes y no participantes. Se encuentra que el canal de atención tiene impacto sobre las inversiones en el sector minero colombiano, toda vez que las empresas con presencia en el país que lo usan invierten en promedio $1.3 millones de pesos más que aquellas empresas que no se benefician de estos canales de atención. Como parte de las recomendaciones derivadas de este ejercicio, se considera que, si la ANM está interesada en determinar la eficacia de otras estrategias de promoción, se debe contar con el listado de las empresas beneficiadas por otro tipo de intervenciones.
C01|Which Sanctions Matter? Analysis of the EU/Russian Sanctions of 2014|In this paper we use a natural experiment of reciprocal imposition of trade sanctions by Russia and the EU since 2014. Using UNCTAD/BACI bilateral flows data we take this unique opportunity to analyse both sanctions. In particular, we study the effectiveness of narrow versus broadly defined sanctions, and differences in the effectiveness of sanctions imposed on exports and imports. We show that the Russian sanctions imposed on European and American food imports resulted in about 8 times stronger decline in trade flows than those imposed by the EU and the US on exports of extraction equipment. These results do not appear to be driven by diversion of trade flows via non-sanctioning countries. Hence the difference in sanctions' effectiveness can be attributed to the broader range of sanctioned goods and potentially to a stronger position of enforcement of sanctions on imports rather than exports.
C01|istinguishing Incentive from Selection Effects in Auction-Determined Contracts|This paper develops a novel approach to estimate how contract and principal-agent characteristics influence an ex-post performance outcome when the matching between agents and principals derives from an auction process. We propose a control-function approach to account for the endogeneity of contracts and matching. This consists of, first, estimating the primitives of an interdependent values auction model - which is shown to be non-parametrically identified from the bidding data - second, constructing control functions based on the distribution of the unobserved private signals conditional on the auction outcome. A Monte Carlo study shows that our augmented outcome equation corrects well of the endogeneity biases, even in small samples. We apply our methodology to a labor market application: we estimate the effect of sports players’ auction-determined wages on their individual performance.
C01|The Cowles Commission and Foundation for Research in Economics: Bringing Mathematical Economics and Econometrics from the Fringes of Economics to the Mainstream|Founded in 1932 by a newspaper heir disillusioned by the failure of forecasters to predict the Great Crash, the Cowles Commission promoted the use of formal mathematical and statistical methods in economics, initially through summer research conferences in Colorado and through support of the Econometric Society (of which Alfred Cowles was secretary-treasurer for decades). After moving to the University of Chicago in 1939, the Cowles Commission sponsored works, many later honored with Nobel Prizes but at the time out of the mainstream of economics, by Haavelmo, Hurwicz and Koopmans on econometrics, Arrow and Debreu on general equilibrium, Yntema and Mosak on general equilibrium in international trade theory, Arrow on social choice, Koopmans on activity analysis, Klein on macroeconometric modelling, Lange, Marschak and Patinkin on macroeconomic theory, and Markowitz on portfolio choice, but came into intense methodological, ideological and personal conflict with the emerging “Chicago school.” This conflict led the Cowles Commission to move to Yale in 1955 as the Cowles Foundation, directed by James Tobin (who had declined to move to Chicago to direct it). The Cowles Foundation remained a leader in the more technical areas of economics, notably with Tobin’s “Yale school” of monetary theory, Scarf’s computable general equilibrium, Shubik in game theory, and later Phillips and Andrews in econometric theory but as formal methods in economic theory and econometrics pervaded the discipline of economics, Cowles (like the Econometric Society) became less distinct from the rest of economics.
C01|Fed’s Unconventional Monetary Policy and Risk Spillover in the US Financial Markets|This study examines volatility spillover dynamics among the S&P 500 index, the US 10-year Treasury yield, the US dollar index futures and the commodity price index. The focus of the study is to analyze effects of Fed’s unconventional monetary policy on the US financial markets. We use realized volatility measures based on daily data covering the period from December 29, 1996 to November 12, 2018. To address nonlinear and asymmetric spillover dynamics in low and high volatility states, we propose a new regime-dependent spillover index based on a smooth transition vector autoregressive (STVAR) model, extending the study of Diebold and Yilmaz (2009,2012) to regime switching models. When applied to US financial data, we find strong evidence that the US financial market risk structure changes after the announcement of quantitively easing (QE) through the portfolio balance channel. The risk spillover moves from purchased assets to non-purchased assets after the QE announcements.
C01|Who Marries Whom? The Role of Identity, Cognitive and Noncognitive Skills in Marriage|I estimate a structural model of marriage sorting on a representative sample of British individuals. The paper first investigates the importance of numerical skills in the selection of the partner and the role of identity for marriage matching on a British sample. The findings show that identity is among the most important attributes, together with education and physical characteristics, in marriage sorting. Cognitive skills are both direct and indirect determinants of marriage matching. Personality traits are also relevant in the choice of the partner: conscientiousness and openness to experience play, in addition to risk propensity, a direct and an indirect role, while agreeableness, extraversion and neuroticism matter only indirectly. Interesting findings, robust to both alternative specifications and a sensitivity analysis, and heterogeneous preferences between males and females emerge from the analysis.
C01|DyMH_LU: a simple tool for modelling and simulating the health status of the Luxembourgish elderly in the longer run|We are facing one of the most important demographic events of the last decades in Europe: the population ageing process. This process will have significant economic effects particularly on health. As most diseases are age-related, this process might imply a proportionally higher share of individuals with declining health. Being able to forecast the health status of the population can help to deal with concerns about the financial and social sustainability of several public policies including health. In this paper, we present the DyMH_LU model, a dynamic microsimulation model focused exclusively on the health status of the Luxembourgish population. One of its major characteristics is that it simulates more than sixty different diseases and limitations in the activities of daily living. All this simulated information can be aggregated in order to compute, for each period, the overall health status of each individual, the marginal distribution of each disease among the total population and the global health status of the entire population. The starting point of the DyMH_LU model is the information collected in 2015 in the Wave 6 of the SHARE database that targets individuals aged 51 or older. The simulation period covers 2017 until 2045.
C01|Uniform Consistency of Marked and Weighted Empirical Distributions of Residuals|A uniform weak consistency theory is presented for the marked and weighted empirical distribution function of residuals. New and weaker sufficient conditions for uniform consistency are derived. The theory allows for a wide variety of regressors and error distributions. We apply the theory to 1-step Huber-skip estimators. These estimators describe the widespread practice of removing outlying observations from an intial estimation of the model of interest and updating the estimation in a second step by applying least squares to the selected observations. Two results are presented. First, we give new and weaker conditions for consistency of the estimators. Second, we analyze the gauge, which is the rate of false detection of outliers, and which can be used to decide the cut-off in the rule for selecting outliers.
C01|Models where the Least Trimmed Squares and Least Median of Squares estimators are maximum likelihood|The Least Trimmed Squares (LTS) and Least Median of Squares (LMS) estimators are popular robust regression estimators. The idea behind the estimators is to fi?nd, for a given h; a sub-sample of h '?good' ?observations among n observations and estimate the regression on that sub-sample. We fi?nd models, based on the normal or the uniform distribution respectively, in which these estimators are maximum likelihood. We provide an asymptotic theory for the location-scale case in those models. The LTS estimator is found to be h1/2 consistent and asymptotically standard normal. The LMS estimator is found to be h consistent and asymptotically Laplace.
C01|Body Mass Index and Social Interactions from Adolescence to Adulthood|We apply a dynamic linear-in-means model to analyze the importance of social ties for the body-weight-related behavior of US youth. Our methodology shows how to estimate peer effects free of the “reflection problem” in a dynamic context where individual- and group-specific unobservable effects are controlled for. Our results show that the main drivers for the body-weight-related behavior are past and peer effects. For individuals who were normal-weight or obese during adolescence, past and peer effects are shown to be both relevant. Peer effects, instead, explain more the variation in the BMI for individuals who were over-weight during adolescence, showing in this way the importance of social interactions for body-weight-related behavior.
C01|Taming the Factor Zoo: A Test of New Factors|We propose a model-selection method to systematically evaluate the contribution to asset pricing of any new factor, above and beyond what a high-dimensional set of existing factors explains. Our methodology explicitly accounts for potential model-selection mistakes, unlike the standard approaches that assume perfect variable selection, which rarely occurs in practice and produces a bias due to the omitted variables. We apply our procedure to a set of factors recently discovered in the literature. While most of these new factors are found to be redundant relative to the existing factors, a few — such as profitability — have statistically significant explanatory power beyond the hundreds of factors proposed in the past. In addition, we show that our estimates and their significance are stable, whereas the model selected by simple LASSO is not.
C01|Synthetic Difference in Differences|We present a new perspective on the Synthetic Control (SC) method as a weighted least squares regression estimator with time fixed effects and unit weights. This perspective suggests a generalization with two way (both unit and time) fixed effects, and both unit and time weights, which can be interpreted as a unit and time weighted version of the standard Difference In Differences (DID) estimator. We find that this new Synthetic Difference In Differences (SDID) estimator has attractive properties compared to the SC and DID estimators. Formally we show that our approach has double robustness properties: the SDID estimator is consistent under a wide variety of weighting schemes given a well-specified fixed effects model, and SDID is consistent with appropriately penalized SC weights when the basic fixed effects model is misspecified and instead the true data generating process involves a more general low-rank structure (e.g., a latent factor model). We also present results that justify standard inference based on weighted DID regression. Further generalizations include unit and time weighted factor models.
C01|The Time Variation in Risk Appetite and Uncertainty|We develop measures of time-varying risk aversion and economic uncertainty that are calculated from financial variables at high frequencies. We formulate a dynamic no-arbitrage asset pricing model for equities and corporate bonds. The joint dynamics among asset-specific cash flows, macroeconomic fundamentals and risk aversion feature heteroskedasticity and non-Gaussianity. Variance risk premiums on equity are very informative about risk aversion, whereas credit spreads and corporate bond volatility are highly correlated with economic uncertainty. Model-implied risk premiums outperform standard instruments for predicting excess returns on equity and corporate bonds. A financial proxy to our economic uncertainty predicts output growth significantly negatively.
C01|Ensemble Methods for Causal Effects in Panel Data Settings|In many prediction problems researchers have found that combinations of prediction methods (“ensembles”) perform better than individual methods. A simple example is random forests, which combines predictions from many regression trees. A striking, and substantially more complex, example is the Netflix Prize competition where the winning entry combined predictions using a wide variety of conceptually very different models. In macro-economic forecasting researchers have often found that averaging predictions from different models leads to more accurate forecasts. In this paper we apply these ideas to synthetic control type problems in panel data setting. In this setting a number of conceptually quite different methods have been developed, with some assuming correlations between units that are stable over time, others assuming stable time series patterns common to all units, and others using factor models. With data on state level GDP for 270 quarters, we focus on three basic approaches to predicting missing values, one from each of these strands of the literature. Rather than try to test the different models against each other and find a true model, we focus on combining predictions based on each of the separate models using ensemble methods. For the ensemble predictor we focus on a weighted average of the three individual methods, with non-negative weights determined through out-of-sample cross-validation.
C01|Identification of Causal Effects with Multiple Instruments: Problems and Some Solutions|Empirical researchers often combine multiple instruments for a single treatment using two stage least squares (2SLS). When treatment effects are heterogeneous, a common justification for including multiple instruments is that the 2SLS estimand can still be interpreted as a positively-weighted average of local average treatment effects (LATEs). This justification requires the well-known monotonicity condition. However, we show that with more than one instrument, this condition can only be satisfied if choice behavior is effectively homogenous. Based on this finding, we consider the use of multiple instruments under a weaker, partial monotonicity condition. This condition is implied by standard choice theory and allows for richer heterogeneity. First, we show that the weaker partial monotonicity condition can still suffice for the 2SLS estimand to be a positively-weighted average of LATEs. We characterize a simple sufficient and necessary condition that empirical researchers can check to ensure positive weights. Second, we develop a general method for using multiple instruments to identify a wide range of causal parameters other than LATEs. The method allows researchers to combine multiple instruments to obtain more informative empirical conclusions than one would obtain by using each instrument separately.
C01|The Perry Preschoolers at Late Midlife: A Study in Design-Specific Inference|This paper presents the first analysis of the life course outcomes through late midlife (around age 55) for the participants of the iconic Perry Preschool Project, an experimental high-quality preschool program for disadvantaged African-American children in the 1960s. We discuss the design of the experiment, compromises in and adjustments to the randomization protocol, and the extent of knowledge about departures from the initial random assignment. We account for these factors in developing conservative small-sample hypothesis tests that use approximate worst-case (least favorable) randomization null distributions. We examine how our new methods compare with standard inferential methods, which ignore essential features of the experimental setup. Widely used procedures produce misleading inferences about treatment effects. Our design-specific inferential approach can be applied to analyze a variety of compromised social and economic experiments, including those using re-randomization designs. Despite the conservative nature of our statistical tests, we find long-term treatment effects on crime, employment, health, cognitive and non-cognitive skills, and other outcomes of the Perry participants. Treatment effects are especially strong for males. Improvements in childhood home environments and parental attachment appear to be an important source of the long-term benefits of the program. The appendix to this paper may be found here.
C01|On Testing Continuity and the Detection of Failures|Estimation of discontinuities is pervasive in applied economics: from the study of sheepskin effects to prospect theory and “bunching” of reported income on tax returns, models that predict discontinuities in outcomes are uniquely attractive for empirical testing. However, existing empirical methods often rely on assumptions about the number of discontinuities, the type, the location, or the underlying functional form of the model. We develop a nonparametric approach to the study of arbitrary discontinuities — point discontinuities as well as jump discontinuities in the nth derivative, where n = 0,1,... — that does not require such assumptions. Our approach exploits the development of false discovery rate control methods for lasso regression as proposed by G’Sell et al. (2015). This framework affords us the ability to construct valid tests for both the null of continuity as well as the significance of any particular discontinuity without the computation of nonstandard distributions. We illustrate the method with a series of Monte Carlo examples and by replicating prior work detecting and measuring discontinuities, in particular Lee (2008), Card et al. (2008), Reinhart and Rogoff (2010), and Backus et al. (2018b).
C01|Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics|In this essay I discuss potential outcome and graphical approaches to causality, and their relevance for empirical work in economics. I review some of the work on directed acyclic graphs, including the recent “The Book of Why,” ([Pearl and Mackenzie, 2018]). I also discuss the potential outcome framework developed by Rubin and coauthors, building on work by Neyman. I then discuss the relative merits of these approaches for empirical work in economics, focusing on the questions each answer well, and why much of the work in economics is closer in spirit to the potential outcome framework.
C01|Uniform Consistency of Marked and Weighted Empirical Distributions of Residuals|A uniform weak consistency theory is presented for the marked and weighted emÂ­pirical distribution function of residuals. New and weaker sufficient conditions for uniform consistency are derived. The theory allows for a wide variety of regressors and error distributions. We apply the theory to 1-step Huber-skip estimators. These estimators describe the widespread practice of removing outlying observations from an intial estimation of the model of interest and updating the estimation in a second step by applying least squares to the selected observations. Two results are presented. First, we give new and weaker conditions for consistency of the estimators. Second, we analyze the gauge, which is the rate of false detection of outliers, and which can be used to decide the cut-off in the rule for selecting outliers.
C01|Random models for the joint treatment of risk and time preferences|We develop a simple, tractable and sound stochastic framework for the joint treatment of risk and time preferences, in order to facilitate the estimation of risk and time attitudes. In so doing we: (i) study deterministic models of risk and time preferences paying special attention to their comparative statics, (ii) embed the deterministic models and their comparative statics within the random utility framework, and (iii) show how to estimate them, illustrating this exercise on several experimental datasets.
C01|Dividend payout ratio follows a Tweedie distribution: International evidence|Dividend policy is still a largely discussed issue in corporate finance literature. One of the main indicators used in analysing the dividend policy is the dividend payout ratio. Using a database consisting of 12,085 companies operating in 73 countries, for the period 2008-2014, the authors found that the dividend payout ratio follows a Tweedie distribution, and not a normal one. This distribution is stable over time for the entire analysed period. In addition, it describes the case of almost all the countries included in the sample. Thus, a better estimation of the probability that dividend payout ratio is lower or higher than a benchmark can be provided. Also, an analysis of dividend policy, distinctly considering payer versus non-payer companies, can offer additional important information for both practitioners and academics.
C01|Measuring competitive balance in Formula One Racing|The Formula One Championship (F1) is one of the biggest sports businesses in the world. But, however, it seems to astonish that only very few scholarly articles analyze the F1 business. The aim of this study is to contribute to closing two gaps in the existing literature: it contributes (1) to the (sports) economic analysis of the F1 business and (2) to the literature on competitive balance in non-team sports. Like competitive balance in team sport leagues, also for F1 racing three dimensions can be distinguished: (a) race-specific competitive balance, (b) within-season competitive balance, and (c) between-season competitive balance. In addition to classical tools and data, some new and F1 specific indicators, like average lead changes or leading distance, are employed. Also, pitfalls induced especially by the used data source or calculation method are highlighted.
C01|Using Value-at-Risk for effective energy portfolio risk management|It is evident that the prediction of future variance through advanced GARCH type models is essential for an effective energy portfolio risk management. Still it fails to provide a clear view on the specific amount of capital that is at risk on behalf of the investor or any party directly affected by the price fluctuations of specific or multiple energy commodities. Thus, it is necessary for risk managers to make one further step, determining the most robust and effective approach that will enable them to precisely monitor and accurately estimate the portfolio’s Value-at-Risk, which by definition provides a good measure of the total actual amount at stake. Nevertheless, despite the variety of the variance models that have been developed and the relative VaR methodologies, the vast majority of the researchers conclude that there is no model or specific methodology that outperforms all the others. On the contrary, the best approach to minimize risk and accurately forecast the future potential losses is to adopt that specific methodology that will be able to take into consideration the particular characteristic features regarding the trade of energy products.
C01|“The determinants of students' achievement: a difference between OECD and not OECD countries”|This paper investigates on the determinants of school performance measured by the average value of students’ tests score (math, reading and science) at school level. PISA data from 2000 to 2012 are used in order to explore this relationship. A multivariate regression is assessed considering the different channels (funds, computers connected to internet, parental education, student teacher ratio, number of girls and ownership) and controlling for time and country fixed effects. The analysis is done both allowing for the total sample and grouping for OECD countries and NO-OECD countries. The most important results show that, considering the all sample and the only OECD countries, school performances are positively driven by the student fees, presence of girls and computers; also the mother’s education plays an important role, while the father’s one is notable only at high level, otherwise is negative. Moreover, differently from that the improvement of the student achievement in NO-OECD countries is encouraged from charity funds, the presence of girls, and the parent’s education level.
C01|Bitcoin's return behaviour: What do We know so far?|In this paper we study the daily return behavior of Bitcoin digital currency. We propose the use of generalized hyperbolic distributions (GH) to model Bitcoin's return. Our, results show that GH is a very good candidate to model this return.
C01|New Approach to Estimating Gravity Models with Heteroscedasticity and Zero Trade Values|This paper proposes new estimation techniques for gravity models with zero trade values and heteroscedasticity. We revisit the standard PPML estimator and we propose an improved version. We also propose various Heckman estimators with different distributions of the residuals, nonlinear forms of both selection and measure equations, and various process of the variance. We add to the existent literature alternative estimation methods taking into account the non-linearity of both the variance and the selection equation. Moreover, because of the unavailability of pre-set package in the econometrics software (Stata, Eviews, Matlab, etc.) to perform the estimation of the above-mentioned Heckman versions, we had to code it in Matlab using a combination of fminsearch and fminunc functions. Using numerical gradient matrix G, we report standard errors based on the BHHH technique. The proposed new Heckman version could be used in other applications. Our results suggest that previous empirical studies might be overestimating the contribution of the GDP of both import and export countries in determining the bilateral trade.
C01|Impact de risque de crédit et de liquidité sur la stabilité bancaire<BR>[Impact of liquidity and credit risks on the bank stability]|The global ﬁnancial crisis has induced a series of failures of most conventional banks. This study investigates the main sources of banking fragility. We use a sample of 49 banks operating in the Tunisian over the period 2006-2015 to analyze the relationship between credit risk and liquidity risk and its impact on bank stability. Our results show that credit risk and liquidity risk do not have an economically meaningful reciprocal contemporaneous or time-lagged relationship. However, both risks separately affect bank stability and their interaction contributes to bank instability. These ﬁndings provide bank managers with more understanding of bank risk and serve as an underpinning for recent regulatory efforts aimed at strengthening the joint risk management of liquidity and credit risks.
C01|Les normes prudentielles : étude d’impact sur la solvabilité bancaire<BR>[Prudential standards: impact study on bank solvency]|The evolution of the banking regulatory environment in recent years raises many questions about the effectiveness of prudential measures and the relevance of the legal system in this new landscape. The Cooke ratio, replaced in 2003 by the Mc Donough ratio, has since become an international benchmark for banks. Banks that are less risky and comply with prudential standards are solvent. Thus, with their compliance with prudential standards, Tunisian commercial banks are relatively safe from risks. From a sample of 10 commercial banks between 2007 and 2015, we studied the impact of compliance with prudential standards on the solvency of the banking institution. To do this, we based on the studies of Kefi and Maraghni (2011). Indeed, we have made estimates on panel data, these results show that the ratio of liquidity, interest rate risk ratio and Return on assets have positive and significant effects on the risk coverage ratio.
C01|Les normes prudentielles : étude d’impact sur la solvabilité bancaire<BR>[Prudential standards: impact study on bank solvency]|The evolution of the banking regulatory environment in recent years raises many questions about the effectiveness of prudential measures and the relevance of the legal system in this new landscape. The Cooke ratio, replaced in 2003 by the Mc Donough ratio, has since become an international benchmark for banks. Banks that are less risky and comply with prudential standards are solvent. Thus, with their compliance with prudential standards, Tunisian commercial banks are relatively safe from risks. From a sample of 10 commercial banks between 2007 and 2015, we studied the impact of compliance with prudential standards on the solvency of the banking institution. To do this, we based on the studies of Kefi and Maraghni (2011). Indeed, we have made estimates on panel data, these results show that the ratio of liquidity, interest rate risk ratio and Return on assets have positive and significant effects on the risk coverage ratio.
C01|Forecasting with many predictors using message passing algorithms|Machine learning methods are becoming increasingly popular in economics, due to the increased availability of large datasets. In this paper I evaluate a recently proposed algorithm called Generalized Approximate Message Passing (GAMP) , which has been very popular in signal processing and compressive sensing. I show how this algorithm can be combined with Bayesian hierarchical shrinkage priors typically used in economic forecasting, resulting in computationally efficient schemes for estimating high-dimensional regression models. Using Monte Carlo simulations I establish that in certain scenarios GAMP can achieve estimation accuracy comparable to traditional Markov chain Monte Carlo methods, at a tiny fraction of the computing time. In a forecasting exercise involving a large set of orthogonal macroeconomic predictors, I show that Bayesian shrinkage estimators based on GAMP perform very well compared to a large set of alternatives.
C01|Fostering innovation in South Asia: Evidence from FMOLS and Causality analysis|Innovation is at the core of fourth industrial revolution which is already under way. Both Sustainable growth and development depend on technological innovation. Traditional economic models/theories are now undermined because of new technologies like AI, automation,3D printing, robotics etc. Lack of innovation creates major socio-economic problems such as inequality, unemployment, poverty and many more. Therefore, in this competitive world, a country needs innovative people with innovative ideas to go forward. The aim of this study is to explain and critically examine the determinants of technological innovation across 5 South Asian countries using yearly data for 1980-2015 period. This paper employs several econometric techniques such as Cross sectional dependence to see if shocks that occur in one country affect another, Panel unit root test to check the stationary of the data and Panel Cointegration test to check long run relationship among the variables. This study also applies Fully Modified OLS to estimate long run coefficients and Dumitrescu and Hurlin panel causality test (2012) to see the causality between the variables. The findings suggest that democracy and human capital are negatively related to innovation, contrary to popular belief. The analysis also reveals that trade openness positively and significantly affects innovation and there exists a nonlinear, in particular an inverted U shaped relationship between innovation and financial development in South Asia. Findings from the Causality test reveals that there is bidirectional causality between total patent application and trade openness and also between financial development and human capital. This study, therefore, has several policy implications for South Asian countries.
C01|Momentum and Disposition Effect in the stock market of USA|This paper analyze whether momentum effect drives disposition effect and vice versa during the period of January 1963 to 2017 in the stock market of USA. To examine the relationship, Fama and Macbeth (1973) cross sectional regressions are performed in the study. The results show that disposition effect drives momentum but not the other way around. Furthermore, this relationship is also examined for three sub-samples, and we find that relationship between momentum and disposition effect varies over the time and one possible reason could be crisis as sample is divided on the basis of the dot-com bubble and global financial crisis. Another finding of the study is that along with the disposition effect, size also has an impact on the momentum effect. To further analyze the impact of size on momentum and disposition effect, we test the relationship between momentum and disposition effect on the basis of size deciles. The results demonstrate that relationship does not vary significantly over the size of stocks but it does have an impact on momentum and disposition effect as past cumulative returns, and capital gain varies monotonically with the increase in the size of stocks.
C01|Explaining the gender gap in waiting times for scheduled surgery in the Portuguese National Health Service|This study examines waiting times for all patients submitted to surgical treatment in the Portuguese National Health Service (NHS), from 2011 to 2015 (amounting to more than 2.6 million observations) and it tries to evaluate whether there is gender discrimination in access to scheduled surgery. We show that men have 10% shorter waiting times than women. We then estimate a regression model on waiting times that controls for multiple sources of observed and unobserved heterogeneity. Some of these controls are high-dimensional fixed effects. The results still indicate an unexplained differential where men have 3.1% shorter waiting times than women. To understand the contribution of the controls to the explained gender gap we use Gelbach’s decomposition. The results indicate that the patient’s initial priority is the variable that contributes most to the explained gap (-5.8 log points), followed by hospital specific fixed-effects (-1.7 log points). In addition to noticing the resource allocation is not efficiently provided, decision-makers should pay more attention to a pattern that seems to disadvantage women. Overall, we consider that our approach provides a more informative assessment of the sources of the gender gap than previous literature.
C01|Trade Openness and Economic Growth in SADC Countries|In spite of the wave of liberalisation studied during the past decades, the debate still remains open on the issue of the trade openness and economic growth nexus. The paper reviews the relationship between trade openness and economic growth for 11 SADC countries for the period between 1990 and 2016. Investments, labour and inflation are incorporated in the model to form a multivariate framework. The study employed the ARDL-bounds test approach and the Pooled Mean Group (PMG) model to estimate the long-run relationship among the variables. The evidence suggests that co-integration is detected at the 1% level in all countries with the exception of Malawi, Mauritius, Swaziland and Tanzania. Co-integration is only detected at the 10% level in Tanzania while Malawi, Mauritius and Swaziland the null of no co-integration is not rejected. Furthermore, the PMG results revealed that trade openness has a negative impact on economic growth in the long-run. A positive relationship between the variables was found only in the short-run. Apertura commerciale e crescita economica nei paesi SADC Nonostante l’onda liberista che ha caratterizzato gli studi degli ultimi decenni, il dibattito sulla relazione tra apertura commerciale e crescita economica è ancora aperto. Questo articolo rivede detta relazione su un campione di 11 paesi SADC (Southern African Development Cooperation) nel periodo 1990-2016. Investimenti, lavoro e inflazione sono incorporati nel modello. Lo studio impiega i test ARDL-bound (Autoregressive Distributed Lag) e il modello PMG (Pooled Mean Group) per valutare la relazione di lungo periodo tra le variabili. Vi sono evidenze di cointegrazione all’1% in tutti i paesi ad eccezione di Malawi, Muritius, Swaziland e Tanzania. La cointegrazione è significativa solo al 10% in Tanzania mentre per Malawi, Mauritius e Swaziland non si rifiuta il valore zero di non cointegrazione. Inoltre, i risultati PGM rivelano che l’apertura commerciale ha un impatto negativo sulla crescita economica nel lungo periodo. Una relazione positiva tra le due variabili è stata riscontrata solo nel breve periodo.
C01|A Class of Generalized Dynamic Correlation Models|This paper proposes a class of parametric correlation models that apply a two-layer autoregressive-moving-average structure to the dynamics of correlation matrices. The proposed model contains the Dynamic Conditional Correlation model of Engle (2002) and the Varying Correlation model of Tse and Tsui (2002) as special cases and offers greater flexibility in a parsimonious way. Performance of the proposed model is illustrated in a simulation exercise and an application to the U.S. stock indices.
C01|Trade Balance Analysis in Zimbabwe: Import and Export Examination Using Vector Auto-Regression Model|Zimbabwe, just like many other developing nations have failed to register a positive trade balance for the past decade. Zimbabwe, is then labelled a net-importer or a permanent net-importer, since imports have always been greater than exports. Despite differences in value of imports and exports, quality is also essential to determine the country’s development path. The trade balance is affected by both international and domestic events. Chief exports for Zimbabwe remains primary products which are unprocessed, while the country imports finished products which have been value-added. The study seeks to analyse the trade balance over the years 1980 to 2017, paying particular attention to the periodic trends. The study also explore the relationship between the trade balance components, being imports and exports. The study employed a trend analysis and a statistical analysis to attain its study objectives. The study noted a general rise in both exports and imports, however imports significantly above imports for the entire study period, whether for merchandise or non-merchandise. ADF unit root tests were applied to time series data and variables were found to be integrated of order one. Imports have been found to Granger cause exports while exports Granger cause imports as well. Johansen Cointegration test shows that exports and imports are cointegrated, however using the VAR model, the error correction term was insignificant, discarding the existence of a longrun relationship. Exports levels are affected by its past values and also past values of imports significantly. Imports are also affected by historical exports significantly. Improvement in export policy is critical, value addition to exports, market fetching through regionalism and import substitution is essential to manage the trade balance.
C01|Health and economic growth in Vista countries: An ARDL bounds test approach|The present study examined the relationship between health and economic growth in the VISTA countries (Vietnam, Indonesia, South Africa, Turkey and Argentina). The study employed time series data covering the period between 1990 and 2016. Labor and capital were incorporated in the model to form a multivariate framework. The ARDL bounds test approach was used to determine the presence of the long run relationship among the variables. The findings posited that there is long run relationship between economic growth, health, capital and labour in all the countries except for Argentina. There were mix results in terms of the long run and short estimates. It was established that in Vietnam, Indonesia and South Africa, there is evidence of a long run positive and significant relationship between economic growth and health while in Turkey a negative relationship was established. Therefore, the findings of the study have different implications for the different countries.
C01|Can we beat the Random Walk? The case of survey-based exchange rate forecasts in Chile|We examine the accuracy of survey-based expectations of the Chilean exchange rate relative to the US dollar. Our out-of-sample analysis reveals that survey-based forecasts outperform the Driftless Random Walk (DRW) in terms of Mean Squared Prediction Error at several forecasting horizons. This result holds true even when comparing the survey to a more competitive benchmark based on a refined information set. A similar result is found when precision is measured in terms of Directional Accuracy: survey-based forecasts outperform a “pure luck” benchmark at several forecasting horizons. Differing from the traditional “no predictability” result reported in the literature for many exchange rates, our findings suggest that the Chilean peso is indeed predictable.
C01|L’impact De L’investissement Des Revenus Pétroliers Sur La Croissance, L’inflation Et Le Chômage : Cas D’Algérie (2000-2015)<BR>[The Impact of Oil Revenue Investment on Growth, Inflation and Unemployment: The Case of Algeria (2000-2015)]|This paper investigates the short-run and long-run relationships between four main Algerian macroeconomic variables, the investment of oil revenues, economic growth, unemployment rate, inflation rate, using the Johansen multivariate cointegration techniques as well as VAR model for the period 2000-2015. The results indicate that there is not a long relationship between these four macroeconomic variables. The impulse functions and the variance decomposition from the stationary VAR show that the investment of oil revenues is very important to short run dynamics of the Algerian economy, when there is a shock in investment of oil revenues, GDP responds positively (13%) while the unemployment rate responds negatively (11%), in the long term. This is in line with the Algerian government's investment strategy, increasing GDP and reducing the unemployment rate.
C01|Regime heteroskedasticity in Bitcoin: A comparison of Markov switching models|Markov regime-switching (MRS) models, also known as hidden Markov models (HMM), are used extensively to account for regime heteroskedasticity within the returns of financial assets. However, we believe this paper to be one of the first to apply such methodology to the time series of cryptocurrencies. In light of Molnar and Thies (2018) demonstrating that the price data of Bitcoin contained seven distinct volatility regimes, we will �fit a sample of Bitcoin returns with six m-state MRS estimations, with m between 2 and 7. Our aim is to identify the optimal number of states for modelling the regime heteroskedasticity in the price data of Bitcoin. Goodness-of-�fit will be judged using three information criteria, namely: Bayesian (BIC); Hannan-Quinn (HQ); and Akaike (AIC). We determined that the restricted 5-state model generated the optimal estimation for the sample. In addition, we found evidence of volatility clustering, volatility jumps and asymmetric volatility transitions whilst also inferring the persistence of shocks in the price data of Bitcoin.
C01|Effective energy commodities’ risk management: Econometric modeling of price volatility|The current study emphasizes on the importance of the development of an effective price risk management strategy regarding energy products, as a result of the high volatility of that particular market. The study provides a thorough investigation of the energy price volatility, through the use of GARCH type model variations and the Markov-Switching GARCH methodology, as they are presented in the most representative academic researches. A large number of GARCH type models are exhibited together with the methodology and all the econometric procedures and tests that are necessary for developing a robust and precise forecasting model regarding energy price volatility. Nevertheless, the present research moves another step forward, in an attempt to cover also the probability of potential shifts in the unconditional variance of the models due to the effect of economic crises and several unexpected geopolitical events into the energy market prices.
C01|Retrato de la pobreza económica en España diez años después del inicio de la crisis: Un análisis descriptivo, bivariante y logístico de la insuficiencia de ingresos en 2018<BR>[Economic poverty in Spain after a decade of crisis: A descriptive, bivariate and logistic analysis of the income insufficiency in 2018]|"Given the social and academic relevance recently aroused by the social stratification, this paper aims to carry out a synchronic analysis that will serve as a ""picture"" of the factors associated with economic poverty in Spain, after ten years have elapsed since beginning of the Great Depression. With this purpose, we start from a relative conception of poverty, which allows us to treat the relative income insufficiency in terms of “poor” and “non-poor”. Therefore, a profile of individuals with low income in Spain is obtained, to later expose the socioeconomic and demographic factors related to this phenomenon through a bivariate analysis and several logistic regression models. Ultimately, our results show that there is no direct relationship between poverty and residence in the north or in the south of Spain, although the data show an association between it and other individual factors such as age, educational level, professional and employment situation or productive sector."
C01|Economic impacts of Political Uncertainty in Thailand|This paper aims to analyze political uncertainty in Thailand by looking at various dimensions of political uncertainty and quantifying the economic impacts. Based on keyword search in Thai-language newspapers, the paper proposes five measures related to different aspects of political uncertainty. These are: (1) political protest (2) official measures in dealing with political violence (3) coup d'tat (4) parliament dissolution or election and (5) political structural reform, including the aggregate index of political uncertainty. We find that the overall political uncertainty in Thailand has been in the rising trend during the past 20 years. In particular, during the past 10 years, the main source of Thai political uncertainty comes from uncertainty related to political structural reform. Based on various econometric specifications, rising political uncertainty is found to have significant negative impacts on the Thai economy both in the short run ? particularly, private investment ? and economic growth in the long run. Nevertheless, we find that the degree of the economic impact and statistical significance on different components of macroeconomy is quite varied, reflecting complicated interaction between political factors and economic outcome.
C01|Volatility persistence and asymmetry under the microscope: the role of information demand for gold and oil|This study explores the relationship between Google search activity and the conditional volatility of oil and gold spot market returns. By aggregating the volume of queries related to the two commodity markets in the spirit of Da et al. (), we construct a weekly Searching Volume Index (SVI) for each market as proxy of households and investors information demand. We employ a rolling EGARCH framework to reveal how the significance of information demand has evolved through time. We find that higher information demand increases conditional volatility in gold and oil spot market returns. Information flows from Google SVI's reduce the proportion of the significant volatility asymmetry produced by negative shocks in both commodity markets. The latter is more profound in the gold market.
C01|Sraffa’s Silenced Revival of the Classical Economists and of Marx|The standpoint of the old classical economists as well as of Marx “has been submerged and forgotten since the advent of the ‘marginal’ method” – to borrow Sraffa’s own words. The neoclassical (or ‘marginal’) paradigm, in fact, triumphantly dominated over the twentieth century (and is still dominating even now). A serious step towards the rehabilitation of the paradigm of the old classical economists was made by Sraffa (1951) with his remarkable ‘Introduction’ to Ricardo’s Principles, his seminal 1960 book Production of Commodities by Means of Commodities (PCMC) followed a few years later, as a logical completion of his long-standing work. The paper here proposed argues that Sraffa’s 1960 contribution has so far been mainly interpreted and used as a highly powered tool for destroying the foundations of neoclassical theory from a logical point of view, with the confident belief that attacking the logical side of the theory would have been sufficient to bring about its definite dismissal, which, instead, did not happen. As a consequence of all this, the revival of the classical economists and of Marx – which is one of the most characterizing feature of Sraffa’s 1960 contribution – was automatically silenced and this very fact precluded Sraffa’s theoretical framework from being used in a constructive way as a real alternative ‘vision’ to that proposed by the neoclassical market-centered paradigm. The aim of the paper is to underscore the crucial importance of Sraffa’s revival referred to above, by emphasizing its usefulness in providing a genuine alternative perspective and a radically different representation of the economy, compared with that provided by neoclassical theory. An attempt will be made to show the main features of the Sraffian framework in providing such an alternative ‘vision’ which, it will be argued, is now much needed, not least for suggesting far more sensible alternative economic policies than those so far pursued in the ceaseless turmoil of present day world economies.
C01|Where does “dirty” money go? A gravity analysis|The applied literature on illicit (“dirty”) money flows looks at the gravity equation as a natural strategy to control for characteristics of both “source” and “destination” countries. Unfortunately, the existing empirical literature on dirty money flows lacks solid theoretical underpinnings. We contribute to this literature in several ways: i) we exploit the theory of portfolio investment flows to estimate robust parameters (by applying both LSDV PPML techniques) on financial flows on a global scale ii) we derive a global picture of anomalous (i.e. unpredicted) money flows over time; ii) we identify possible correlations between these anomalous flows and “dirty money” determinants. Our results show that theoretical based gravity models provide a more robust way to look at “dirty” money flows than their ad hoc counterparts and open the ground to key policy implications: i) instead of spending time and effort tracing the exact path of “dirty” money flows, a global map of anomalous financial activities can be built by exploiting official statistics; ii) rather than focusing only on “off-shore financial centers”, we could assess the attractiveness to dirty money flows of each destination to a given origin by exploiting bilateral flows and the characteristics of their dyadic relationship.
C01|Economic Growth in Transition Economies: Does investments matter?|Many countries from the beginning of transition period tried to find answer of Adam Smith fundamental question, ?how countries get rich??. Thus, main goals of all economic reforms, since 1989-1990 years until now in countries with transition economic for stimulation economic growth. All transition states have two way of development ?Shock Therapy? and ?Gradualism? for faster implementation free market based economic system.Countries that are characterized as transition economies, with the inefficient political or economical system investment will play a less prominent role in stimulating the economy than in the developed countries. Given these peculiarities, investments have lesser role in stimulation economic growth in countries with transition than macroeconomic policies, structural reforms, protection of property right.Countries? from the start of the transition period or from the time of became independent, economic policy focus on attracting investments for stimulation economic growth, but the height role of investment is not confirmed in examples of many countries, which indicates that the economic growth in countries of transition is not related to the number of invitations attracted by the country. Our research is developed based on Oleh Havrylyshyn, Ivailo Izvorski, Ron van Rooden study ?Recovery and Growth in Transition Economies 1990-97: A Stylized Regression Analysis?. IMF, 1998. Which include inflation rate, structural reform, share of government expenditure in GDP; investment; price liberalization index; competition index; exchange rate and privatization index as an independent variables, dependent variable is real economic growth (GDP). 31 countries as transition economic are selected based on World Economic Outlook, October 2000, IMF. Dates (1997-2014) used in econometric models came from different publications of IMF, EBRD and WB collected by author.Statistical characteristics of 12 models satisfy the necessary requirements for the evaluation, namely R2 are presented in 0.21 (minimum) and 0.43 (maximum) interval. Other statistics are also used for assessing the model: Akaike info criterion; Schwarz criterion; Hannan-Quinn Criter; Durban-Watson and F-statistics. In those models are analyzed different combination of independent variables.Panel model where the analyzed period is divided into two parts (1997-2004; 2004-2014), It is important to note that in these models all variables of investment are characterized by negative correlation with economic growth, statistical characteristics of models analyzed during the 2004-2014 data are considerably improved compared to the previous model.Inflation, governmental expenditures, price liberalization index, competition policy, exchange regime, investment has negative impact on economic growth, but structural reforms in important factor for stimulation economic growth rate in all models.
C01|Analyzing Credit Risk Transmission to the Non-Financial Sector in Europe: A Network Approach|A high-dimensional network of European CDS spreads is modeled to assess the transmission of credit risk to the non-financial corporate sector in Europe. We build on a network connectedness approach that uses variance decompositions in vector autoregressions (VARs) to characterize the dependence structure in the panel of CDS spreads. Our main findings suggest a sectoral clustering in the CDS network, where financial institutions are located in the center of the network and non-financial as well as sovereign CDS are grouped around the financial center. The network has a geographical component re flected in differences in the magnitude and direction of real-sector risk transmission across European countries. We identify an increase in the transmission of financial and sovereign credit risk to the non-financial sector during the global financial crisis and the European debt crisis. By contrast, we find that the transmission of risk within the non-financial sector remains largely unaffected by crisis events.
C01|Statistical Inference on the Canadian Middle Class|Conventional wisdom says that the middle classes in many developed countries have recently suffered losses, in terms of both the share of the total population belonging to the middle class, and also their share in total income. Here, distribution-free methods are developed for inference on these shares, by means of deriving expressions for their asymptotic variances of sample estimates, and the covariance of the estimates. Asymptotic inference can be undertaken based on asymptotic normality. Bootstrap inference can be expected to be more reliable, and appropriate bootstrap procedures are proposed. As an illustration, samples of individual earnings drawn from Canadian census data are used to test various hypotheses about the middle-class shares, and confidence intervals for them are computed. It is found that, for the earlier censuses, sample sizes are large enough for asymptotic and bootstrap inference to be almost identical, but that, in the twenty-first century, the bootstrap fails on account of a strange phenomenon whereby many presumably different incomes in the data are rounded to one and the same value. Another difference between the centuries is the appearance of heavy right-hand tails in the income distributions of both men and women.
C01|The Wall’s Impact in the Occupied West Bank: A Bayesian Approach to Poverty Dynamics Using Repeated Cross-Sections|In 2002, the Israeli government decided to build a wall inside the occupied West Bank. The wall had a marked effect on the access to land and water resources as well as to the Israeli labour market. It is difficult to include the effect of the wall in an econometric model explaining poverty dynamics as the wall was built in the richer region of the West Bank. So a diff-in-diff strategy is needed. Using a Bayesian approach, we treat our two-period repeated cross-section data set as an incomplete data problem, explaining the income-to-needs ratio as a function of time invariant exogenous variables. This allows us to provide inference results on poverty dynamics. We then build a conditional regression model including a wall variable and state dependence to see how the wall modified the initial results on poverty dynamics. We find that the wall has increased the probability of poverty persistence by 58 percentage points and the probability of poverty entry by 18 percentage points.
C01|Recent Developments in Macro-Econometric Modeling: Theory and Applications|Developments in macro-econometrics have been evolving since the aftermath of the Second World War.[...]
C01|Econometrics and Income Inequality|It is well-known that, after decades of non-interest in the theme, economics has experienced a proper surge in inequality research in recent years. [...]
C01|Fair and unfair educational inequality in a developing country: The role of pupil’s effort|Inequality of opportunity builds upon the distinction between fair inequality related to responsibility variables and unfair inequality related to circumstances. This distinction is meaningful as long as responsibility variables are not fully determined by circumstances. We attempt to check the magnitude of the correlation between child effort and family background when measuring inequalities of opportunity in education using a purposefully designed survey on secondary-school education in rural Bangladesh. The analysis comprises decomposition exercises of the predicted variance of school performance in mathematics and English by source and subgroup based on parametric estimates of educational production functions. Pupils’ effort, preferences, and talents contribute between a third and 40\% of the total predicted variances in performance scores. The correlation between overall effort and circumstances does not matter much since the contribution of overall effort only falls by 10\% when the correlation is taken into account. All in all, these results cast doubt on the common practice of reducing education to a circumstance when estimating inequality of opportunity in income attainment.
C01|Moral Nimby-ism? Understanding Societal Support for Monetary Compensation to Plasma Donors in Canada|The growing demand for plasma, especially for the manufacture of therapeutic products, creates an urgent need for a careful discussion on the relative merits of different procurement and allocation systems in a way that addresses the increasing demand while abiding by the prevailing moral values in a society. We conducted a randomized survey experiment with a representative sample of 826 Canadian residents to assess attitudes toward legalizing the compensation to plasma donors, a practice that is illegal in several Canadian provinces. We found no evidence of widespread societal opposition to payments to plasma donors, because over 70% of respondents reported that they would support compensation. The support was higher for paying plasma donors in Australia and the United States than in Canada, but the differences were small, suggesting a weak role for “moral NIMBY-ism” or moral relativism in explaining the findings. Moral concerns were the highest-rated reason that respondents gave for being against payment, together with concerns for the safety of plasma supplied by compensated donors, although most of the plasma in Canada does come from compensated American donors. Among those in favor of legalizing compensation for donors (in Canada as well as in Australia), the highest-rated motive was to guarantee a higher domestic supply. Finally, roughly half of those who declared to be against payments reported that they would reconsider their position if the domestic supply and imports were insufficient to meet domestic demand. Most Canadians, therefore, seem to espouse a consequentialist view to issues related to the procurement of plasma.
C01|The Forcasting Performance of Dynamic Factor Models with Vintage Data|"We present a comparative analysis of the forecasting performance of two dynamic factor models, the Stock and Watson (2002a, b) model and the Forni, Hallin, Lippi and Reichlin (2005) model, based on vintage data. Our dataset contains 107 monthly US ""first release"" macroeconomic and financial vintage time series, spanning the 1996:12 to 2017:6 period with monthly periodicity, extracted from the Bloomberg database. We compute real-time one-month-ahead forecasts with both models for four key macroeconomic variables: the month-on-month change in industrial production, the unemployment rate, the core consumer price index and the ISM Purchasing Managers' Index. First, we find that both the Stock and Watson and the Forni, Hallin, Lippi and Reichlin models outperform simple autoregressions for industrial production, unemployment rate and consumer prices, but that only the first model does so for the PMI. Second, we find that neither models always outperform the other. While Forni, Hallin, Lippi and Reichlin's beats Stock and Watson's in forecasting industrial production and consumer prices, the opposite happens for the unemployment rate and the PMI."
C01|Mapping The Stocks In Micex: Who Is Central To The Moscow Stock Exchange?|In this article we use partial correlations to derive bidirectional connections between major firms listed in the Moscow Stock Exchange. We obtain coefficients of partial correlation from the correlation estimates of the Constant Conditional Correlation GARCH (CCC-GARCH) and the consistent Dynamic Conditional Correlation GARCH (cDCC-GARCH) models. We map the graph of partial correlations using the Gaussian Graphical Model and apply network analysis to identify the most central firms in terms of both shock propagation and connectedness with others. Moreover, we analyze some network characteristics over time and based on these we construct a measure of system vulnerability to external shocks. Our findings suggest that during the crisis interconnectedness between firms strengthens and becomes polarized and the system becomes more vulnerable to systemic shocks. In addition, we found that the most connected firms are the state-owned firms Sberbank and Gazprom and the private oil company Lukoil, while in the top most central in terms of systemic risk contributors Sberbank gave its place to NLMK Group.
C01|The Technological Elements of Artificial Intelligence|We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together – each targeting a straightforward prediction task – to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.
C01|Statistical Non-Significance in Empirical Economics|Significance tests are probably the most common form of inference in empirical economics, and significance is often interpreted as providing greater informational content than non-significance. In this article we show, however, that rejection of a point null often carries very little information, while failure to reject may be highly informative. This is particularly true in empirical contexts that are typical and even prevalent in economics, where data sets are large (and becoming larger) and where there are rarely reasons to put substantial prior probability on a point null. Our results challenge the usual practice of conferring point null rejections a higher level of scientific significance than non-rejections. In consequence, we advocate a visible reporting and discussion of non-significant results in empirical practice.
C01|On Heckits, LATE, and Numerical Equivalence|Structural econometric methods are often criticized for being sensitive to functional form assumptions. We study parametric estimators of the local average treatment effect (LATE) derived from a widely used class of latent threshold crossing models and show they yield LATE estimates algebraically equivalent to the instrumental variables (IV) estimator. Our leading example is Heckman's (1979) two‐step (“Heckit”) control function estimator which, with two‐sided non‐compliance, can be used to compute estimates of a variety of causal parameters. Equivalence with IV is established for a semiparametric family of control function estimators and shown to hold at interior solutions for a class of maximum likelihood estimators. Our results suggest differences between structural and IV estimates often stem from disagreements about the target parameter rather than from functional form assumptions per se. In cases where equivalence fails, reporting structural estimates of LATE alongside IV provides a simple means of assessing the credibility of structural extrapolation exercises.
C01|Quantifier Elimination for Deduction in Econometrics|When combined with the logical notion of partially interpreted functions, many nonparametric results in econometrics and statistics can be understood as statements about semi-algebraic sets. Tarski’s quantifier elimination (QE) theorem therefore guarantees that a universal algorithm exists for deducing such results from their assumptions. This paper presents the general framework and then applies QE algorithms to Jensen’s inequality, omitted variable bias, partial identification of the classical measurement error model, point identification in discrete choice models, and comparative statics in the nonparametric Roy model. This paper also discusses the computational complexity of real QE and its implementation in software used for program verification, logic, and computer algebra. I expect that automation will become as routine for abstract econometric reasoning as it already is for numerical matrix inversion.
C01|Partial Identification of the Distribution of Treatment Effects with an Application to the Knowledge Is Power Program (KIPP)|We bound the distribution of treatment effects under plausible and testable assumptions on the joint distribution of potential outcomes, namely that potential outcomes are mutually stochastically increasing. We show how to test the empirical restrictions implied by those assumptions. The resulting bounds substantially sharpen bounds based on classical inequalities. We apply our method to estimate bounds on the distribution of effects of attending a Knowledge is Power Program (KIPP) charter school on student academic achievement, and find that a substantial majority of students' math achievement benefitted from attendance, especially those who would have fared poorly in a traditional classroom.
C01|A Causal Bootstrap|The bootstrap, introduced by Efron (1982), has become a very popular method for estimating variances and constructing confidence intervals. A key insight is that one can approximate the properties of estimators by using the empirical distribution function of the sample as an approximation for the true distribution function. This approach views the uncertainty in the estimator as coming exclusively from sampling uncertainty. We argue that for causal estimands the uncertainty arises entirely, or partially, from a different source, corresponding to the stochastic nature of the treatment received. We develop a bootstrap procedure that accounts for this uncertainty, and compare its properties to that of the classical bootstrap.
C01|Design-based Analysis in Difference-In-Differences Settings with Staggered Adoption|In this paper we study estimation of and inference for average treatment effects in a setting with panel data. We focus on the setting where units, e.g., individuals, firms, or states, adopt the policy or treatment of interest at a particular point in time, and then remain exposed to this treatment at all times afterwards. We take a design perspective where we investigate the properties of estimators and procedures given assumptions on the assignment process. We show that under random assignment of the adoption date the standard Difference-In-Differences estimator is is an unbiased estimator of a particular weighted average causal effect. We characterize the proeperties of this estimand, and show that the standard variance estimator is conservative.
C01|Matrix Completion Methods for Causal Panel Data Models|"In this paper we study methods for estimating causal effects in settings with panel data, where a subset of units are exposed to a treatment during a subset of periods, and the goal is estimating counterfactual (untreated) outcomes for the treated unit/period combinations. We develop a class of matrix completion estimators that uses the observed elements of the matrix of control outcomes corresponding to untreated unit/periods to predict the ""missing"" elements of the matrix, corresponding to treated units/periods. The approach estimates a matrix that well-approximates the original (incomplete) matrix, but has lower complexity according to the nuclear norm for matrices. From a technical perspective, we generalize results from the matrix completion literature by allowing the patterns of missing data to have a time series dependency structure. We also present novel insights concerning the connections between the matrix completion literature, the literature on interactive fixed effects models and the literatures on program evaluation under unconfoundedness and synthetic control methods."
C01|Real‐time forecast combinations for the oil price|Baumeister and Kilian (Journal of Business and Economic Statistics, 2015, 33(3), 338–351) combine forecasts from six empirical models to predict real oil prices. In this paper, we broadly reproduce their main economic findings, employing their preferred measures of the real oil price and other real‐time variables. Mindful of the importance of Brent crude oil as a global price benchmark, we extend consideration to the North Sea‐based measure and update the evaluation sample to 2017:12. We model the oil price futures curve using a factor‐based Nelson–Siegel specification estimated in real time to fill in missing values for oil price futures in the raw data. We find that the combined forecasts for Brent are as effective as for other oil price measures. The extended sample using the oil price measures adopted by Baumeister and Kilian yields similar results to those reported in their paper. Also, the futures‐based model improves forecast accuracy at longer horizons.
C01|White heteroscedasticty testing after outlier removal|Abstract Given the effect that outliers can have on regression and specification testing, a vastly used robustification strategy by practitioners consists in: (i) starting the empirical analysis with an outlier detection procedure to deselect atypical data values; then (ii) continuing the analysis with the selected non-outlying observations. The repercussions of such robustifying procedure on the asymptotic properties of subsequent specification tests are, however, underexplored. We study the effects of such a strategy on the White test for heteroscedasticity. Using weighted and marked empirical processes of residuals theory, we show that the White test implemented after the outlier detection and removal is asymptotically chi-square if the underlying errors are symmetric. Under asymmetric errors, the standard chi-square distribution will not always be asymptotically valid. In a simulation study, we show that - depending on the type of data contamination - the standard White test can be either severely undersized or oversized, as well as have trivial power. The statistic applied after deselecting outliers has good finite sample properties under symmetry but can suffer from size distortions under asymmetric errors.
C01|Linkages Between Oil Price Shocks and Stock Returns Revisited|In this paper, we revisit the debate on the relationship between oil price shocks and stock market returns by replicating the quantile-on-quantile (QQ) regression model for the US stock market in Sim and Zhou (2015, Journal of Banking and Finance), and extending it to 15 countries. The classification of these countries as oil importers or oil exporters depends on their net position in crude oil trade. Our results indicate that the finding by Sim and Zhou (2015) that large negative oil price shocks can bolster stock returns when markets are performing well is only partially supported by the three largest oil importers in our sample-China, Japan and India-during the period 1988:1-2007:12. However, when extending the study to more recent data (period 1988:1-2016:12), we find that China and India experience higher returns when markets perform well and there is a large positive oil price shock. Also, large positive oil price shocks often lead to higher stock market returns when markets perform well for both oil exporting countries-Canada, Russia, Norway-and moderately oil dependent countries-such as Malaysia, Philippines and Thailand. These findings highlight that the relationship between the distributions of oil price shocks and stock market returns is not stable over time in most countries studied. Furthermore, the asymmetric effect of oil price shocks observed in the US market by Sim and Zhou (2015) is less evident in most countries for both the baseline and extended periods.
C01|Spurious Seasonality Detection: A Non-Parametric Test Proposal|This paper offers a general and comprehensive definition of the day-of-the-week effect. Using symbolic dynamics, we develop a unique test based on ordinal patterns in order to detect it. This test uncovers the fact that the so-called “day-of-the-week” effect is partly an artifact of the hidden correlation structure of the data. We present simulations based on artificial time series as well. While time series generated with long memory are prone to exhibit daily seasonality, pure white noise signals exhibit no pattern preference. Since ours is a non-parametric test, it requires no assumptions about the distribution of returns, so that it could be a practical alternative to conventional econometric tests. We also made an exhaustive application of the here-proposed technique to 83 stock indexes around the world. Finally, the paper highlights the relevance of symbolic analysis in economic time series studies.
C01|Design-based Analysis in Difference-In-Differences Settings with Staggered Adoption|In this paper we study estimation of and inference for average treatment effects in a setting with panel data. We focus on the setting where units, e.g., individuals, firms, or states, adopt the policy or treatment of interest at a particular point in time, and then remain exposed to this treatment at all times afterwards. We take a design perspective where we investigate the properties of estimators and procedures given assumptions on the assignment process. We show that under random assignment of the adoption date the standard Difference-In-Differences estimator is an unbiased estimator of a particular weighted average causal effect. We characterize the properties of this estimand, and show that the standard variance estimator is conservative.
C01|Challenges in Implementing Worst-Case Analysis|Worst-case analysis is used among financial regulators in the wake of the recent financial crisis to gauge the tail risk. We provide insight into worst-case analysis and provide guidance on how to estimate it. We derive the bias for the non-parametric heavy-tailed order statistics and contrast it with the semi-parametric extreme value theory (EVT) approach. We find that if the return distribution has a heavy tail, the non-parametric worst-case analysis, i.e. the minimum of the sample, is always downwards biased and hence is overly conservative. Relying on semi-parametric EVT reduces the bias considerably in the case of relatively heavy tails. But for the less-heavy tails this relationship is reversed. Estimates for a large sample of US stock returns indicate that this pattern in the bias is indeed present in financial data. With respect to risk management, this induces an overly conservative capital allocation if the worst case is estimated incorrectly.
C01|Characterizing the Canadian Financial Cycle with Frequency Filtering Approaches|In this note, I use two multivariate frequency filtering approaches to characterize the Canadian financial cycle by capturing fluctuations in the underlying variables with respect to a long-term trend. The first approach is a dynamically weighted composite, and the second is a stochastic cycle model. Applying the two approaches to Canada yields several findings. First, the Canadian financial cycle is more than twice as long as the business cycle, with an amplitude almost four times greater. Second, the overall Canadian financial cycle is most strongly associated with household credit and house prices. Third, while Canadian house prices are mostly associated with the financial cycle, they are also significantly tied to the business cycle. Lastly, house prices are found to lead the overall financial cycle. These results are generally in line with findings for other countries studied in literature. Additionally, I compare each approach’s proneness to revision and find that both are more reliable, when monitored in real time, than the Basel III total credit-to-GDP gap. Nonetheless, further work is encouraged to investigate more variable combinations and undertake a cross-country analysis since data on systemic financial stress in Canada are limited. It should be noted that since the approaches produce a measure of the financial cycle relative to trend, comparison with level indicators (as those monitored in the Bank of Canada’s Financial System Review) is not straightforward.
C01|Asymmetric Risks to the Economic Outlook Arising from Financial System Vulnerabilities|When financial system vulnerabilities are elevated, they can give rise to asymmetric risks to the economic outlook. To illustrate this, I consider the economic outlook presented in the Bank of Canada’s October 2017 Monetary Policy Report in the context of two key financial system vulnerabilities: high levels of household indebtedness and housing market imbalances. Uncertainty on the profile of consumption by indebted households—and, therefore, risks to growth in gross domestic product (GDP)—arises from higher interest rates and from recent changes to the Office of the Superintendent of Financial Institutions’ B-20 mortgage underwriting guideline. I use non-linear Bayesian techniques to capture the potential amplification of negative shocks in a vulnerable environment. I find that the materialization of larger-than-expected impacts on consumption from higher interest rates and/or the tighter mortgage qualifying criteria would imply asymmetric risks to GDP growth.
C01|Inflation Dynamics under Fiscal Deficit Regime Switching in Mexico|We explore the dynamics of inflation, inflation expectations, and seigniorage-financed fiscal deficits in Mexico. To do so, we estimate the model in Sargent, Williams, and Zha (2009) using Mexican CPI inflation data. This model features dual expected inflation equilibriums and regime switching in the mean and variance of the fiscal deficit probability density function. We examine the dynamics of inflation and mean fiscal deficit regimes. In addition, we comment on the extent to which our results match to key economic events. Mexico has successfully stabilized inflation expectations for the past decades, an achievement for which fiscal policy has been fundamental. Nevertheless, this does not preclude the possibility of an increase in the expected price level or a switch to a regime in which inflation and its expectations become unstable.
C01|A Stochastic Latent Moment Model for Electricity Price Formation|The wide range of models needed to support the various short-term operations for electricity generation demonstrates the importance of accurate specifications for the uncertainty in market prices. This is becoming increasingly challenging, since electricity hourly price densities exhibit a variety of shapes, with their characteristic features changing substantially within the day and over time, and the in ux of renewable power, wind and solar in particular, has amplified these effects. A general-purpose, analytically tractable representation of the stochastic price formation process would have considerable value for operations control and trading, but existing empirical approaches or the application of standard density functions are unsatisfactory. We develop a general four parameter stochastic model for hourly prices, in which the four moments of the density function are dynamically estimated as latent state variables and furthermore modelled as functions of several plausible exogenous drivers. This provides a transparent and credible model that is suffciently exible to capture the shape-shifting effects, particularly with respect to the wind and solar output variations causing dynamic switches in the upside and downside risks. Extensive testing on German wholesale price data, benchmarked against quantile regression and other models in out-of-sample backtesting, validated the approach and its analytical appeal.
C01|Objective vs. Subjective Fuel Poverty and Self-Assessed Health|Policies towards fuel poverty often use relative or absolute measures. The effectiveness of the official indicators in identifying fuel poor households and assessing its impact on health is an emerging social policy issue. In this paper we analyse objective and perceived fuel poverty as determinants of self-assessed health in Spain. In 2014, 5.1 million of her population could not afford to heat their homes to an adequate temperature. We propose a latent class ordered probit model to analyse the influence of fuel poverty on self-reported health in a sample of 25,000 individuals in 11,000 households for the 2011-2014 period. This original approach allows us to include a ‘subjective’ measure of fuel poverty in the class membership probabilities and purge the influence of the ‘objective’ measure of fuel poverty on self-assessed health. The results show that poor housing conditions, fuel poverty, and material deprivation have a negative impact on health. Also, individuals who rate themselves as fuel poor tend to report poorer health status. The effect of objective fuel poverty on health is stronger when unobserved heterogeneity of individuals is controlled for. Since objective measures alone may not fully capture the adverse effect of fuel poverty on health, we advocate the use of approaches that allow a combination of objective and subjective measures and its application by policy-makers. Moreover, it is important that policies to tackle fuel poverty take into account the different energy vectors and the prospects of a future smart and integrated energy system.<br><small>(This abstract was borrowed from another version of this item.)</small>
C01|Government Expenditure Ceiling and Public Debt Dynamics in a Demand-led Macromodel|This article explores some aspects of the debate about the efficacy of a fiscal rule that sets a government expenditure ceiling for the stabilisation of the public debt-to-output ratio. We develop a demand-led macromodel that assumes a closed economy operating with excess capacity and show that a fiscal rule that sets a limit for government spending, excluding the payment of interests, may not ensure a non-explosive trajectory of the public debt-to-output ratio. Our model allows us to map out different outcomes in terms of the stabilisation of the public debt stemming from the process of fiscal consolidation and conclude that the commitment of the fiscal authority to comply with the ceiling by cutting government spending is less likely to stabilise the public debt-to-output ratio in economies enduring excessively high interest rates accompanied by more regressive taxation systems. Our model also suggests that a more progressive tax structure may increase the likelihood of public debt stabilisation in the long run.
C01|Time-invariant Regressors under Fixed Effects: Identification via a Proxy Variable|Identification of a coefficient associated with a time-invariant regressor (TIR) often relies on the assumption that the TIR is uncorrelated with the unobserved heterogeneity across panel units. We derive an estimator which avoids the random-effects assumption by employing a proxy for the unobserved heterogeneity thus extending the existing results on proxy variables from the cross-sectional literature. In addition, we quantify the sensitivity of the estimates to potential violations of the random-effects assumption when no proxy is available. The utility of this approach is illustrated on the problem of implausibly high distance elasticity produced by gravity models of international trade.
C01|The Endo-Exo Problem in High Frequency Financial Price Fluctuations and Rejecting Criticality|The endo-exo problem lies at the heart of statistical identification in many fields of science, and is often plagued by spurious strong-and-long memory due to improper treatment of trends, shocks and shifts in the data. A class of models that has shown to be useful in discerning exogenous and endogenous activity is the Hawkes process. This class of point processes has enjoyed great recent popularity and rapid development within the quantitative finance literature, with particular focus on the study of market microstructure and high frequency price fluctuations. We show that there are important lessons from older fields like time series and econometrics that should also be applied in financial point process modelling. In particular, we emphasize the importance of appropriately treating trends and shocks for the identification of the strength and length of memory in the system. We exploit the powerful Expectation Maximization (EM) algorithm and objective statistical criteria (BIC) to select the flexibility of the deterministic background intensity. With these methods, we strongly reject the hypothesis that the considered financial markets are critical at univariate and bivariate microstructural levels.
C01|Riesgo De Crédito Y Ciclos Del Crecimiento Económico|En este documento se estima la relación entre los diferenciales de riesgo de impago y el ciclo económico en Estados Unidos. Se emplea una muestra mensual que va desde noviembre del 2008 hasta agosto del 2017. Para la estimación se utiliza la metodología de cambios de régimen planteada por Hamilton (1989), con la cual se puede hacer uso del supuesto de que hay dos estados en la economía (uno de expansión y otro de recesión) y observa la forma en la que cambian las dinámicas económicas respecto al riesgo de impago. Se encuentra un efecto de la caída del sector financiero sobre el sector real, donde este último también cae.
C01|Tendencias actuales en la evaluación de políticas públicas|No abstract is available for this item.
C01|The Forecasting Performance of Dynamic Factor Models with Vintage Data|We present a comparative analysis of the forecasting performance of two dynamic factor models, the Stock and Watson (2002a, b) model and the Forni, Hallin, Lippi and Reichlin (2005) model, based on vintage data. Our dataset that contains 107 monthly US “first release” macroeconomic and financial vintage time series, spanning the 1996:12 to 2017:6 period with monthly periodicity, extracted from the Bloomberg database†. We compute real-time one-month-ahead forecasts with both models for four key macroeconomic variables: the month-on-month change in industrial production, the unemployment rate, the core consumer price index and the ISM Purchasing Managers’ Index. First, we find that both the Stock and Watson and the Forni, Hallin, Lippi and Reichlin models outperform simple autoregressions for industrial production, unemployment rate and consumer prices, but that only the first model does so for the PMI. Second, we find that neither models always outperform the other. While Forni, Hallin, Lippi and Reichlin’s beats Stock and Watson’s in forecasting industrial production and consumer prices, the opposite happens for the unemployment rate and the PMI.
C01|Analyzing credit risk transmission to the non-financial sector in Europe: a network approach|We use a factor model and elastic net shrinkage to model a high-dimensional network of European CDS spreads. Our empirical approach allows us to assess the joint transmission of bank and sovereign risk to the non-financial corporate sector. Our findings identify a sectoral clustering in the CDS network, where financial institutions are in the center and non-financial entities as well as sovereigns are grouped around the financial center. The network has a geographical component reflected in different patterns of real-sector risk transmission across countries. Our framework also provides dynamic estimates of risk transmission, a useful tool for systemic risk monitoring. JEL Classification: C32, C38, C55, F3, G01, G15
C01|Gender responses to competitive pressure in college: a regression discontinuity design|The proliferation of competitive college groups in Spain capturing highly qualified students has opened an interesting debate, motivating the study of how students react in such competitive environments. In this paper we provide empirical answers to this issue by comparing high achievement groups (in particular, International Business and Law and Business) with standard groups (Business Administration) at the University of Valencia, Spain. The co-existence of the two kind of groups sharing similar academic programs and the fact that they are separated by a particular value of the access-to-university score each year provide a suitable data source that allows us to identify the causal effect of peers by using a (fuzzy) regression discontinuity design. We implement this methodology to analyze peers’ influences in terms of learning externalities, competitive pressure, or requirement standards, making special emphasis in gender disparities. Our results suggest that peer effects in college are negative and significant for students at the threshold, that is, for those who are ranked at the bottom of the high achievement groups. These findings are more remarkable for women and in International Business, where the level of competitive pressure is expected to be the highest among the three groups considered. We conclude that competitive pressure exerts a negative impact on threshold student`s grades, particularly women, a result that contributes to the recent literature documenting the lower preference of women for competitive contexts.
C01|The Transmission of an Interest Rate Shock, Standard Mitigants and Household Behavior|We analyze the transmission of an interest rate shock to households in the context of a stress-test module. We examin standard mitigants, such as delays due to a future interest-rate-reset-date, tax deduction of the interest paid on mortgages, the amortization of different mortgage types and conjunctural factors. We also include the possibility of behavioral responses, where households can alleviate the effect of a shock by reducing debt using voluntary repayments. We estimate a Cragg log-normal hurdle model on loan-level data for the Dutch mortgage market. We simulate debt 30 years into the future under different scenarios for the development of the interest rate and simulating both contractual and voluntary amortization. This study finds a significant dampening role of voluntary repayments on the effects of an interest rate shock.
C01|Nonlinear Relationship between Exchange Rate Volatility and Economic Growth|In this paper, we challenge the traditional assumption of a linear relationship between exchange rate volatility and economic growth in South Africa. By using data collected from 1970 to 2016 applied to a smooth transition regression (STR) model, we are able to prove that the exchange rate-economic growth correlation is indeed nonlinear within the sampled time period. In particular, we find that regime switching behaviour is facilitated by government size in which exchange rate volatility positively and significantly influences economic growth when growth in government spending is below 6 percent. Above this 6 percent threshold, volatility exerts an insignificant effect on economic growth. In light of the adoption of a free floating exchange rate regime by the Reserve Bank, our results emphasize the importance of the role which fiscal authorities play on the extent to which exchange rate movements affect economic growth.
C01|Objective vs. Subjective Fuel Poverty and Self-Assessed Health|Policies towards fuel poverty often use relative or absolute measures. The effectiveness of the official indicators in identifying fuel poor households and assessing its impact on health is an emerging social policy issue. In this paper we analyse objective and perceived fuel poverty as determinants of self-assessed health in Spain. In 2014, 5.1 million of her population could not afford to heat their homes to an adequate temperature. We propose a latent class ordered probit model to analyse the influence of fuel poverty on self-reported health in a sample of 25,000 individuals in 11,000 households for the 2011-2014 period. This original approach allows us to include a ‘subjective’ measure of fuel poverty in the class membership probabilities and purge the influence of the ‘objective’ measure of fuel poverty on self-assessed health. The results show that poor housing conditions, fuel poverty, and material deprivation have a negative impact on health. Also, individuals who rate themselves as fuel poor tend to report poorer health status. The effect of objective fuel poverty on health is stronger when unobserved heterogeneity of individuals is controlled for. Since objective measures alone may not fully capture the adverse effect of fuel poverty on health, we advocate the use of approaches that allow a combination of objective and subjective measures and its application by policy-makers. Moreover, it is important that policies to tackle fuel poverty take into account the different energy vectors and the prospects of a future smart and integrated energy system.
C01|Testing Various Forms of Consumption Function for the Romanian Economy|In the paper we propose to test various forms of the consumer function for the Romanian economy: the linear function and the consumption function constructed in the hypothesis of the present inertial phenomenon. The analysis was carried out over a period of ten years between 2008 and 2017; the data, on total income and total expenditure, allow us to assess the opportunity of estimating a linear - model; the results of the model are analyzed using: (Econometric Views 10) & (Excel 2016).
C01|Semiparametric Identification in Panel Data Discrete Response Models|This paper studies semiparametric identification in linear index discrete response panel data models with fixed effects. Departing from the classic binary response static panel data model, this paper examines identification in the binary response dynamic panel data model and the ordered response static panel data model. It is shown that under mild distributional assumptions on the fixed effect and the time-varying unobservables, point-identification fails but informative bounds on the regression coefficients can still be derived. Partial identification is achieved by eliminating the fixed effect and discovering features of the distribution of the unobservable time-varying components that do not depend on the unobserved heterogeneity. Numerical analyses illustrate how the identified set changes as the support of the explanatory variables varies.
C01|Classifying Firms with Text Mining|Statistics on the births, deaths and survival rates of firms are crucial pieces of information, as they enter as an input in the computation of GDP, the identification of each sectorâ€™s contribution to the economy, and the assessment of gross job creation and destruction rates. Official statistics on firm demography are made available only several months after data collection and storage, however. Furthermore, unprocessed and untimely administrative data can lead to a misrepresentation of the life-cycle stage of a firm. In this paper we implement an automated version of Eurostatâ€™s algorithm aimed at distinguishing true startup endeavors from the resurrection of pre-existing but apparently defunct firms. The potential gains from combining machine learning, natural language processing and econometric tools for pre- processing and analyzing granular data are exposed, and a machine learning method predicting reactivations of deceptively dead firms is proposed.
C01|Prediction Regions for Interval-valued Time Series|We approximate probabilistic forecasts for interval-valued time series by offering alternative approaches to construct bivariate prediction regions of the interval center and range (or lower/upper bounds). We estimate a bivariate system of the center/log-range, which may not be normally distributed. Implementing analytical or bootstrap methods, we directly transform prediction regions for center/log-range into those for center/range and upper/lower bounds systems. We propose new metrics to evaluate the regions performance. Monte Carlo simulations show bootstrap methods being preferred even in Gaussian systems. For daily SP500 low/high return intervals, we build joint conditional prediction regions of the return level and return volatility.
C01|A guide for the evaluation of programs of human capital training for science, technology and innovation|We provide a practical guide for impact evaluation of Training and Human Capital programs in Science Technology and Innovation (STI). This document addresses specific challenges that arise when evaluating this type of programs, discussing its logic, the advantages and drawbacks of the different sources of information, the strategies which may be appropriate for evaluation, and the suitability of applying the different experimental and quasi-experimental available methods. For each technique, the document highlights the characteristics and assumptions, the strengths and weaknesses, and the practical issues related to their application to programs of human capital training for STI. Also, some specific issues, as for example the time after which the effects and externalities are expected to materialize, are discussed. Discussion is based on specific examples of existing evaluations.
C01|A new baseline model for estimating willingness to pay from discrete choice models|We show a substantive problem exists with the widely-used ratio of coefficients approach to calculating willingness to pay (WTP) from discrete choice models. The correctly calculated standard error for WTP using this approach is shown to be undefined. This occurs because the cost parameter's standard error implies some possibility the true parameter value is arbitrarily close to zero. We propose a simple yet elegant way to overcome this problem by reparameterizing the (negative) cost variable's coefficient using an exponential transformation to enforce the theoretically correct positive coefficient. With it the confidence interval for WTP is now finite and well behaved.
C01|Theory and Practice of Testing for a Single Structural Break in Stata|The major objective of this paper is to demonstrate, theoretically and empirically, the test of a single structural break/change. Failure to address a structural break can lead to forecasting errors and the general unreliability of a model. Three approaches of testing for structural change are discussed using data from Johnston et al. (1997, p.130) on Stata 14 software. The first approach assesses whether there is a structural break in parameters (slope and intercept) while the second and third assess whether there is a break in slope and intercept respectively. The Residual Sum of Squares (RSS) for the restricted and unrestricted models are established to necessitate the use of an F-test in making inferences. According to the first approach, a structural break exists at 5% level of significance. This result is confirmed by the Chow test. The second and third approaches establish that the structural break is from the intercept and not the slope. These results are also affirmed by the Chow test. Furthermore, all these results, from the first to the third approach, are confirmed by an alternative approach which relies on the knowledge that . Therefore, the dependent variable is not affected by the policy change on the explanatory variable but it is mainly affected by the basic unobserved qualitative characteristics of the two sub-periods. For further analysis, it is recommended that a unit root test be conducted using the Zivot-Andrews test. This test has been established as the panacea for the interplay between unit root and structural changes.
C01|Smooth Transition Spatial Autoregressive Models|This paper introduces a new model for spatial time series in which cross-sectional dependence varies nonlinearly over space by means of smooth transitions. We refer to our model as the Smooth Transition Spatial Autoregressive (ST-SAR). We establish consistency and asymptotic Gaussianity for the MLE under misspecification and provide additional conditions for geometric ergodicity of the model. Simulation results justify the use of limit theory in empirically relevant settings. The model is applied to study spatio-temporal dynamics in two cases that differ in spatial and temporal extent. We study clustering in urban densities in a large number of neighborhoods in the Netherlands over a 10-year period. We pay particular focus to the advantages of the ST-SAR as an alternative to linear spatial models. In our second study, we apply the ST-SAR to monthly long term interest rates of 15 European sovereigns over 25-year period. We develop a strategy to assess financial stability across the Eurozone based on attraction of individual sovereigns toward the common stochastic trend. Our estimates reveal that stability attained a low during the Greek sovereign debt crisis, and that the Eurozone has remained to struggle in attaining stability since the onset of the financial crisis. The results suggest that the European Monetary System has not fully succeeded in aligning the economies of Ireland, Portugal, Italy, Spain, and Greece with the rest of the Eurozone, while attraction between other sovereigns has continued to increase. In our applications linearity of spatial dependence is overwhelmingly rejected in terms of model fit and forecast accuracy, estimates of control variables improve, and residual correlation is better neutralized.
C01|Finite Sample Optimality of Score-Driven Volatility Models|We study optimality properties in finite samples for time-varying volatility models driven by the score of the predictive likelihood function. Available optimality results for this class of models suffer from two drawbacks. First, they are only asymptotically valid when evaluated at the pseudo-true parameter. Second, they only provide an optimality result `on average' and do not provide conditions under which such optimality prevails. We show in a finite sample setting that score-driven volatility models have optimality properties when they matter most. Score-driven models perform best when the data is fat-tailed and robustness is important. Moreover, they perform better when filtered volatilities differ most across alternative models, such as in periods of financial distress. These results are confirmed by an empirical application based on U.S. stock returns.
C01|Extreme Returns and Intensity of Trading|Asymmetric information models of market microstructure claim that variables like trading intensity are proxies for latent information on the value of financial assets. We consider the interval-valued time series (ITS) of low/high returns and explore the relationship between these extreme returns and the intensity of trading. We assume that the returns (or prices) are generated by a latent process with some unknown conditional density. At each period of time, from this density, we have some random draws (trades) and the lowest and highest returns are the realized extreme observations of the latent process over the sample of draws. In this context, we propose a semiparametric model of extreme returns that exploits the results provided by extreme value theory. If properly centered and standardized extremes have well defined limiting distributions, the conditional mean of extreme returns is a highly nonlinear function of conditional moments of the latent process and of the conditional intensity of the process that governs the number of draws. We implement a two-step estimation procedure. First, we estimate parametrically the regressors that will enter into the nonlinear function, and in a second step, we estimate nonparametrically the conditional mean of extreme returns as a function of the generated regressors. Unlike current models for ITS, the proposed semiparametric model is robust to misspecification of the conditional density of the latent process. We fit several nonlinear and linear models to the 5-min and 1-min low/high returns to seven major banks and technology stocks, and find that the nonlinear specification is superior to the current linear models and that the conditional volatility of the latent process and the conditional intensity of the trading process are major drivers of the dynamics of extreme returns.<br><small>(This abstract was borrowed from another version of this item.)</small>
C01|Testing the lag length of vector autoregressive models: A power comparison between portmanteau and Lagrange multiplier tests|In this paper we provide an asymptotic theoretical power comparison in the Bahadur sense, between the portmanteau and Breusch-Godfrey Lagrange Multiplier (LM) tests for the goodness-of-fit checking of vector autoregressive (VAR) models. The merits and the drawbacks of the studied tests are illustrated using Monte Carlo experiments.
C01|T20 resilience and inclusive growth|Severe recessions and financial crises are frequent. Their effect on the economy is persistent and often exceeds initial projections. They can also be a strong driver of widening inequality. Therefore it is important that measures be taken to minimize the risk of such events while strengthening the potential for economies to innovate and prosper (Phelps, Mass Flourishings: How Grassroots Innovation Created Jobs, Challenge and Change, 2013). An economy's resilience to crises and recessions can also be strengthened. Minimizing risks requires the accurate monitoring of home-grown vulnerabilities in real-time; coping with the consequences means identifying and putting in place policy settings and mechanisms that can help absorb the impact of a severe downturn and facilitate a swift rebound of economic activity. Strengthening resilience will also provide a key contribution to solving the global problems of rising populism, nationalism and protectionism.
C01|A database for investigating foreign direct investment and regional trade|This short paper introduces a new database combining data on international trade, Foreign Direct Investment, and regional trade agreements. The bilateral data covers 1980-2010 and includes gravity model variables and is appropriate for empirical analysis in a wide variety of contexts.
C01|Is my rental price overestimated? A small area index for Germany|Real estate prices are central to a range of themes that are, e.g., relevant for monetary policy, community development, environmental valuation, and economic planning more generally. This paper developes a real estate index based on apartment offer prices on the post code level for Germany, taking into accout apartment heterogeneity and small sample sizes within regional areas as well as spatial and temporal dependencies. In a first step, a hedonic price function is estimated. In a second step, the residuals calculated from the hedonic function are used as direct estimates in a small area estimation (SAE). This technique is designed to yield estimates with a smaller variance in the context of small samples. The results show similarities between the estimates obtained from the residuals and SAE estimates. But the SAE models show non-negligible gains in accuracy for the coefficient of variance, i.e. the estimates are stabilized.
C01|Business Cycle Estimation with High-Pass and Band-Pass Local Polynomial Regression|Filters constructed on the basis of standard local polynomial regression (LPR) methods have been used in the literature to estimate the business cycle. We provide a frequency domain interpretation of the contrast filter obtained by the difference of a series and its long-run LPR component and show that it operates as a kind of high-pass filter, so that it provides a noisy estimate of the cycle. We alternatively propose band-pass local polynomial regression methods aimed at isolating the cyclical component. Results are compared to standard high-pass and band-pass filters. Procedures are illustrated using the US GDP series.
C01|A Note on Identification of Bivariate Copulas for Discrete Count Data|Copulas have enjoyed increased usage in many areas of econometrics, including applications with discrete outcomes. However, Genest and Nešlehová (2007) present evidence that copulas for discrete outcomes are not identified, particularly when those discrete outcomes follow count distributions. This paper confirms the Genest and Nešlehová result using a series of simulation exercises. The paper then proceeds to show that those identification concerns diminish if the model has a regression structure such that the exogenous variable(s) generates additional variation in the outcomes and thus more completely covers the outcome domain.
C01|Structural Breaks, Inflation and Interest Rates: Evidence from the G7 Countries|This study reconsiders the common unit root/co-integration approach to test for the Fisher effect for the economies of the G7 countries. We first show that nominal interest and inflation rates are better represented as I(0) variables. Later, we use the Bai–Perron procedure to show the existence of structural changes in the Fisher equation. After considering these breaks, we find very limited evidence of a total Fisher effect as the transmission coefficient of the expected inflation rates to nominal interest rates is very different than one.
C01|Testing for a Structural Break in a Spatial Panel Model|We consider the problem of testing for a structural break in the spatial lag parameter in a panel model (spatial autoregressive). We propose a likelihood ratio test of the null hypothesis of no break against the alternative hypothesis of a single break. The limiting distribution of the test is derived under the null when both the number of individual units N and the number of time periods T is large or N is ﬁxed and T is large. The asymptotic critical values of the test statistic can be obtained analytically. We also propose a break-date estimator that can be employed to determine the location of the break point following evidence against the null hypothesis. We present Monte Carlo evidence to show that the proposed procedure performs well in ﬁnite samples. Finally, we consider an empirical application of the test on budget spillovers and interdependence in ﬁscal policy within the U.S. states.
C01|Goodness-of-Fit Tests for Copulas of Multivariate Time Series|In this paper, we study the asymptotic behavior of the sequential empirical process and the sequential empirical copula process, both constructed from residuals of multivariate stochastic volatility models. Applications for the detection of structural changes and specification tests of the distribution of innovations are discussed. It is also shown that if the stochastic volatility matrices are diagonal, which is the case if the univariate time series are estimated separately instead of being jointly estimated, then the empirical copula process behaves as if the innovations were observed; a remarkable property. As a by-product, one also obtains the asymptotic behavior of rank-based measures of dependence applied to residuals of these time series models.
C01|Accuracy and Efficiency of Various GMM Inference Techniques in Dynamic Micro Panel Data Models|Studies employing Arellano-Bond and Blundell-Bond generalized method of moments (GMM) estimation for linear dynamic panel data models are growing exponentially in number. However, for researchers it is hard to make a reasoned choice between many different possible implementations of these estimators and associated tests. By simulation, the effects are examined in terms of many options regarding: (i) reducing, extending or modifying the set of instruments; (ii) specifying the weighting matrix in relation to the type of heteroskedasticity; (iii) using (robustified) 1-step or (corrected) 2-step variance estimators; (iv) employing 1-step or 2-step residuals in Sargan-Hansen overall or incremental overidentification restrictions tests. This is all done for models in which some regressors may be either strictly exogenous, predetermined or endogenous. Surprisingly, particular asymptotically optimal and relatively robust weighting matrices are found to be superior in finite samples to ostensibly more appropriate versions. Most of the variants of tests for overidentification and coefficient restrictions show serious deficiencies. The variance of the individual effects is shown to be a major determinant of the poor quality of most asymptotic approximations; therefore, the accurate estimation of this nuisance parameter is investigated. A modification of GMM is found to have some potential when the cross-sectional heteroskedasticity is pronounced and the time-series dimension of the sample is not too small. Finally, all techniques are employed to actual data and lead to insights which differ considerably from those published earlier.
C01|A Simple Test for Causality in Volatility|An early development in testing for causality (technically, Granger non-causality) in the conditional variance (or volatility) associated with financial returns was the portmanteau statistic for non-causality in the variance of Cheng and Ng (1996). A subsequent development was the Lagrange Multiplier (LM) test of non-causality in the conditional variance by Hafner and Herwartz (2006), who provided simulation results to show that their LM test was more powerful than the portmanteau statistic for sample sizes of 1000 and 4000 observations. While the LM test for causality proposed by Hafner and Herwartz (2006) is an interesting and useful development, it is nonetheless arbitrary. In particular, the specification on which the LM test is based does not rely on an underlying stochastic process, so the alternative hypothesis is also arbitrary, which can affect the power of the test. The purpose of the paper is to derive a simple test for causality in volatility that provides regularity conditions arising from the underlying stochastic process, namely a random coefficient autoregressive process, and a test for which the (quasi-) maximum likelihood estimates have valid asymptotic properties under the null hypothesis of non-causality. The simple test is intuitively appealing as it is based on an underlying stochastic process, is sympathetic to Granger’s (1969, 1988) notion of time series predictability, is easy to implement, and has a regularity condition that is not available in the LM test.
C01|Regime Switching Vine Copula Models for Global Equity and Volatility Indices|For nearly every major stock market there exist equity and implied volatility indices. These play important roles within finance: be it as a benchmark, a measure of general uncertainty or a way of investing or hedging. It is well known in the academic literature that correlations and higher moments between different indices tend to vary in time. However, to the best of our knowledge, no one has yet considered a global setup including both equity and implied volatility indices of various continents, and allowing for a changing dependence structure. We aim to close this gap by applying Markov-switching R -vine models to investigate the existence of different, global dependence regimes. In particular, we identify times of “normal” and “abnormal” states within a data set consisting of North-American, European and Asian indices. Our results confirm the existence of joint points in a time at which global regime switching between two different R -vine structures takes place.
C01|Consistency of Trend Break Point Estimator with Underspecified Break Number|This paper discusses the consistency of trend break point estimators when the number of breaks is underspecified. The consistency of break point estimators in a simple location model with level shifts has been well documented by researchers under various settings, including extensions such as allowing a time trend in the model. Despite the consistency of break point estimators of level shifts, there are few papers on the consistency of trend shift break point estimators in the presence of an underspecified break number. The simulation study and asymptotic analysis in this paper show that the trend shift break point estimator does not converge to the true break points when the break number is underspecified. In the case of two trend shifts, the inconsistency problem worsens if the magnitudes of the breaks are similar and the breaks are either both positive or both negative. The limiting distribution for the trend break point estimator is developed and closely approximates the finite sample performance.
C01|Fractional Unit Root Tests Allowing for a Structural Change in Trend under Both the Null and Alternative Hypotheses|This paper considers testing procedures for the null hypothesis of a unit root process against the alternative of a fractional process, called a fractional unit root test. We extend the Lagrange Multiplier (LM) tests of Robinson (1994) and Tanaka (1999), which are locally best invariant and uniformly most powerful, to allow for a slope change in trend with or without a concurrent level shift under both the null and alternative hypotheses. We show that the limit distribution of the proposed LM tests is standard normal. Finite sample simulation experiments show that the tests have good size and power. As an empirical analysis, we apply the tests to the Consumer Price Indices of the G7 countries.
C01|Between Institutions and Global Forces: Norwegian Wage Formation Since Industrialisation|This paper reviews the development of labour market institutions in Norway, shows how labour market regulation has been related to the macroeconomic development, and presents dynamic econometric models of nominal and real wages. Single equation and multi-equation models are reported. The econometric modelling uses a new data set with historical time series of wages and prices, unemployment and labour productivity. Impulse indicator saturation is used to achieve robust estimation of focus parameters, and the breaks are interpreted in the light of the historical overview. A relatively high degree of constancy of the key parameters of the wage setting equation is documented, over a considerably longer historical time period than earlier studies have done. The evidence is consistent with the view that the evolving system of collective labour market regulation over long periods has delivered a certain necessary level of coordination of wage and price setting. Nevertheless, there is also evidence that global forces have been at work for a long time, in a way that links real wages to productivity trends in the same way as in countries with very different institutions and macroeconomic development.
C01|Acknowledgement to Reviewers of Econometrics in 2016|The editors of Econometrics would like to express their sincere gratitude to the following reviewers for assessing manuscripts in 2016.[...]
C01|Endogeneity, Time-Varying Coefficients, and Incorrect vs. Correct Ways of Specifying the Error Terms of Econometric Models|Using the net effect of all relevant regressors omitted from a model to form its error term is incorrect because the coefficients and error term of such a model are non-unique. Non-unique coefficients cannot possess consistent estimators. Uniqueness can be achieved if; instead; one uses certain “sufficient sets” of (relevant) regressors omitted from each model to represent the error term. In this case; the unique coefficient on any non-constant regressor takes the form of the sum of a bias-free component and omitted-regressor biases. Measurement-error bias can also be incorporated into this sum. We show that if our procedures are followed; accurate estimation of bias-free components is possible.
C01|A Fast Algorithm for the Computation of HAC Covariance Matrix Estimators|This paper considers the algorithmic implementation of the heteroskedasticity and autocorrelation consistent (HAC) estimation problem for covariance matrices of parameter estimators. We introduce a new algorithm, mainly based on the fast Fourier transform, and show via computer simulation that our algorithm is up to 20 times faster than well-established alternative algorithms. The cumulative effect is substantial if the HAC estimation problem has to be solved repeatedly. Moreover, the bandwidth parameter has no impact on this performance. We provide a general description of the new algorithm as well as code for a reference implementation in R .
C01|Dutch Disease in Central and Eastern European Countries|Bulgaria, Croatia, Estonia, Latvia, Lithuania, Hungary, Poland, Czech Republic, Romania, Slovenia, and Slovakia have all benefited from an increase of European Union capital transfers of funds since the demand for European integration. At the same time, foreign direct investments have risen, mainly due to the liberalisation of capital movements. The effects of those funds and the reduction of financial costs can be considered as analogous to the phenomenon known as Dutch Disease. That is to say, the inflow of financial transfers is also considered a curse. In order to eliminate this curse we must take into account the two effects associated with Dutch Disease: the ‘spending effect’ and the ‘resource movement effect’. Public policies have not been appropriate and have not prevented the real exchange rate appreciation, thereby contributing to a poor performance in terms of competitiveness and economic growth. After a descriptive analysis of some variables, we estimate a set of equations that take account of the direct and indirect effects of European Union funds and financial costs on the economy where the effects on the real exchange rate play the major role.
C01|Demanda de energía eléctrica en Bolivia: un modelo SARIMA-GARCH & ARN. Demand of electric energy in Bolivia: a model SARIMA-GARCH & ARN|The forecast of demand for electric power for the National Interconnected System (SIN), is highly relevant for the planning of electric power generation and thus to be able to anticipate in an efficient and anticipated way the projects for the future generation of electric power so Of avoiding imbalances in the Wholesale Energy Market (MEM), it is also a priority of the State to know the demand for electric energy, which is set out in Agenda 2025. In this document, the monthly electricity demand of the SIN is modeled. The SARIMA model is defined, verifying that the model is correct. And to control the variation of the conditional dispersion, an ARCH model is also proposed. In order to compare the prognosis, a model of Artificial Neural Networks (ARN) is elaborated. Finally, to see the FIR of the electric energy demand against a shick in the index of conomica IGAE activity, a model of Self-Regressive Vectors is elaborated.
C01|How Can Contingent Valuation Inform the Bioethics Debate? Evidence from a Survey on Hereditary Cancers in France|BRCA1/2 carriers have a higher risk of developing breast and ovarian cancer at a younger age.?Preimplantation genetic diagnosis (PGD) and prenatal diagnosis (PND) are two of the few options available to avoid transmitting the mutation.?To inform the bioethics debate about authorization, a contingent valuation survey elicited preferences regarding access to PGD and PND from a sample of 460 unaffected by cancer BRCA1/2 carriers (GENEPSO cohort).?We find that the respondents can be classified into three groups: one opposed to PGD/PND (28.3%), one strongly in favour of PGD/PND (45.8%), and one in an intermediate position (25.9%).?We look for the determinants of these preferences, especially of the willingness to pay for PGD/PND.?Overall, we find that BRCA1/2 carriers support access to PGD/PND, which has implications for recommendations to decision-makers. JEL Codes: C01, C83, D04, D79, I19.
C01|Activity diversification and performance of Islamic banks in Malaysia|The current paper analyzes the performance and the choice of portfolio in Islamic banks. We consider a sample of 8 Malaysian universal Islamic banks between 2004 and 2008. We use the Herfindahl-Hirschman Index (HHI) as an indicator of the degree of diversification. The performance of the banks is measured by the return on assets ratio (ROA) and the Risk Adjusted Return On Capital ratio (RAROC). Finally, we use the Modern Portfolio Theory (MPT) of Markowitz to define the efficient frontier and the optimal portfolio.The results show that the corporate and investment activity increases significantly returns on assets. However, retail and commercial activity improves the results and performance of these banks. We find evidence that the level of diversification is not too high and recommend that they become concentrated on just one type of these activities. Finally, the MPT supports the idea that Islamic banks are not efficient.
C01|Measurement of Validity of Corruption Indices|Corruption is an intrinsically latent phenomenon, which makes it a challenging task to measure it and requires the use of indirect indicators. The academic community and non-government organizations have proposed various indices that differ in terms of their methodology, data and coverage. In this paper, we estimate construct validity of the most widely used indices of corruption: The Corruption Perceptions Index, The Control of Corruption Index, The Bribing and Corruption Index, The Corruption Index, and The Rule of Law: Absence of Corruption. In this paper we show that Corruption Index by the International Country Risk Guide and Absence of Corruption Index are not constructively valid and, therefore, are not suitable for the use in scholarly research. We also show that all indices provide poor estimates of a corruption level in the highly corrupted group of countries
C01|Some Remarks on the Causal Inference for Historical Persistence|A growing body of literature examines the relationships between historical events and contemporary economic outcomes. Recent studies estimate the causal effects using detailed historical data and contemporary microdata of individuals and/or households. In this paper, we discuss conceptual and econometric issues inherent in the causal inference following the potential outcomes framework. Using an empirical example, we also discuss a simple alternative approach to avoid these issues that is coherent with the potential outcomes framework.
C01|Tail event driven networks of SIFIs|The interdependence, dynamics and riskiness of financial institutions are the key features frequently tackled in financial econometrics. We propose a Tail Event driven Network Quantile Regression (TENQR) model which addresses these three aspects. More precisely, our framework captures the risk propagation and dynamics in terms of a panel quantile autoregression involving network effects that are quantified through a time-varying adjacency matrix. To reflect the risk content in stress situations the construction of the adjacency matrix is suggested to include tail events. More precisely we employ the conditional expected shortfall as risk profile. Based on the similarity of the risk profiles we create a positive and a negative network factor, which capture the effects of risk contagion and risk diversification respectively. The developed joint spacings variance ratio test supports the suggested methodology. The TENQR technique is evaluated using the SIFIs (systemically important financial institutions) identified by the Financial Stability Board (FSB). The risk decomposition of the resulting network identifies the systemic importance of SIFIs and thus provides measures for the required level of additional loss absorbency. It is discovered that the positive network effect, as a function of the tail probability level, becomes more profound in stress situations and varies in its impact to SIFIs located in different geographic regions.
C01|Investing with cryptocurrencies - A liquidity constrained investment approach|Cryptocurrencies have left the dark side of the finance universe and become an object of study for asset and portfolio management. Since they have a low liquidity compared to traditional assets, one needs to take into account liquidity issues when one puts them into the same portfolio. We propose use a LIquidity Bounded Risk-return Optimization (LIBRO) approach, which is a combination of the Markowitz framework under the liquidity constraints. The results show that cryptocurrencies add value to a portfolio and the optimization approach is even able to increase the return of a portfolio and lower the volatility risk. The codes used to obtain the results in this paper are available via www.quantlet.de
C01|Double/debiased machine learning for treatment and structural parameters|We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.
C01|Estimating dynamic panel models: backing out the Nickell Bias| We propose a new estimator for the dynamic panel model, which solves the failure of strict exogeneity by calculating the bias in the first-order conditions as a function of the autoregressive parameter and solving the resulting equation. The estimator does well in a wide variety of situations where other estimators do not perform well: stationary initial condition, predetermined but not strictly exogenous regressors, and the presence of correlation between the error terms and the fixed effects. We also propose a general method for including predetermined variables infixed-effects panel regressions.
C01|Inequality Indices as Tests of Fairness|"Standard income inequality indices can be interpreted as a measure of welfare loss entailed in departures from equality of outcomes, for egalitarian social welfare functions defined on the distribution of outcomes. But such a welfare interpretation has been criticized for a long time on the grounds that these indices are snap shot outcomes-based measures which do not take into account the process generating the observed distribution. Rather than focusing on outcomes, it is argued, we should be interested in whether the underlying process is ""fair."" Following this line of argument, this paper develops statistical tests for fairness within well-defined income distribution generating processes and a well specified notion of fairness. We find that the likelihood ratio (LR) test for fairness versus unfairness within two such processes are proportional to Theil's first and second inequality indices respectively. The LR values may either be used as a test statistic or to approximate a Bayes factor that measures the posterior probabilities of the fair version of the processes over that of the unfair. The answer to the process versus outcomes critique is thus not to stop calculating inequality measures, but to interpret their values differently – to compare them to critical values for a test of the null hypothesis of fairness, or to use them directly as a measure of the chance that the process was fair relative to the chance it was unfair. We also apply this perspective to measurement of ""inequality of opportunity."""
C01|Inequality Indices as Tests of Fairness|"Standard income inequality indices can be interpreted as a measure of welfare loss entailed in departures from equality of outcomes, for egalitarian social welfare functions defined on the distribution of outcomes. But such a welfare interpretation has been criticized for a long time on the grounds that these indices are snap shot outcomes-based measures which do not take into account the process generating the observed distribution. Rather than focusing on outcomes, it is argued, we should be interested in whether the underlying process is ""fair"". Following this line of argument, this paper develops statistical tests for fairness within well define income distribution generating processes and a well specified notion of fairness. We find that the likelihood ratio (LR) test for fairness versus unfairness within two such processes are proportional to Theil's first and second inequality indices respectively. The LR values may either be used as a test statistic or to approximate a Bayes factor that measures the posterior probabilities of the fair version of the processes over that of the unfair. The answer to the process versus outcomes critique is thus not to stop calculating inequality measures, but to interpret their values differently - to compare them to critical values for a test of the null hypothesis of fairness, or to use them directly as a measure of the chance that the process was fair relative to the chance it was unfair. We also apply this perspective to measurement of ""inequality of opportunity""."
C01|Identifying and measuring economic discrimination|Differences in wages between men and women, white and black workers, or any two distinct groups are a controversial feature of the labor market, raising concern about discrimination by employers. Decomposition methods shed light on those differences by separating them into: (i) composition effects, which are explained by differences in the distribution of observable variables, e.g. education level; and (ii) structural effects, which are explained by differences in the returns to observable and unobservable variables. Often, a significant structural effect, such as different returns to education, can be indicative of discrimination.
C01|Alternative GMM estimators for spatial regression models| Using approximations of the score of the log-likelihood function, we derive moment conditions for estimating spatial regression models, starting with the spatial error model. Our approach results in computationally simple and robust estimators, such as a new moment estimator derived from the first-order approximation obtained by solving a quadratic moment equation, and performs similarly to existing generalized method of moments (GMM) estimators. Our estimator based on the second-order approximation resembles the GMM estimator proposed by Kelejian and Prucha in 1999. Hence, we provide an intuitive interpretation of their estimator. Additionally, we provide a convenient framework for computing the weighting matrix of the optimal GMM estimator. Heteroskedasticity robust versions of our estimators are also proposed. Furthermore, a first-order approximation for the spatial autoregressive model is considered, resulting in a computationally simple method of moment estimator. The performance of the considered estimators is compared in a Monte Carlo study.
C01|Season. Mathematica Packages for Seasonal Adjustment|This Zip-Archive provides Mathematica packages and documentation for the seasonal adjustment method proposed by Schlicht and Pauly (1983) and Schlicht (1984) covering Mathematica versions 5 to 11. The method makes use of non-parametric splines. It decomposes a time series into a trend, a seasonal component, and an irregular component. The method combines the trend filter proposed by Leser (1961) (also known as the HP-Filter), the seasonal filter proposed by Schlicht and Pauly (1983) and the orthogonal parametrization proposed by Schlicht (1984). In contrast to prevailing methods, it is based on an explicit statistical model (state-space) and estimates the smoothing parameters by a maximum-likelihood method.
C01|Incidental parameters, initial conditions and sample size in statistical inference for dynamic panel data models|We use a quasi-likelihood function approach to clarify the role of initial values and the relative sample size of the cross-section dimension N and the time series dimension T on the asymptotic properties of estimators for dynamic panel data models with the presence of individual-specific effects. We show that a properly specified quasi-likelihood estimator (QMLE) that uses the Mundlak–Chamberlain approach to condition the unobserved effects and initial values on the observed strictly exogenous covariates is asymptotically unbiased if N goes to infinity whether T is fixed or goes to infinity. Monte Carlo studies are conducted to demonstrate the importance of properly treating initial values in getting valid statistical inference. The simulation results also suggest that to deal with the incidental parameters issues arising from the presence of individual-specific effects or initial values, following the Mundlak’s (1978) suggestion to condition on the time series average of individual’s observed regressors performs better than conditioning on each observed variable at all different time periods.
C01|To Pool or Not to Pool: Revisited|This paper provides a new comparative analysis of pooled least squares and fixed effects (FE) estimators of the slope coefficients in the case of panel data models when the time dimension (T) is fixed while the cross section dimension (N) is allowed to increase without bounds. The individual effects are allowed to be correlated with the regressors, and the comparison is carried out in terms of an exponent coefficient, Î´, which measures the degree of pervasiveness of the FE in the panel. The use of Î´ allows us to distinguish between poolability of small N dimensional panels with large T from large N dimensional panels with small T. It is shown that the pooled estimator remains consistent so long as Î´
C01|Empirical Methods for the Law|Normative legal argument based on empirical evidence is not necessarily best served by standards from the social sciences. Precautionary concern for false negatives may call for an adjustment of the significance level. That all legal choice is historically contingent, that legal problems tend to be ill-defined, and that strategic actors have an incentive to bias the generation of evidence create further challenges. Yet the law can capitalize on the adversarial principle. Competition among interested parties helps contain the strategic element and spurs the creative search for better evidence. This leads to suggestive, but institutionally contained, empirical evidence.
C01|Angus Deaton, prix à la mémoire d'Alfred Nobel 2015 : un maître de l'économie appliquée|The Nobel prize which has been awarded to Angus Deaton in 1995 proved his exceptional contribution to applied microeconomics, microeconometric methods and development studies. The four books he published offer a large view on these domains and prove the importance taken vy new statistical methods and data in applied micro- and macroeconomics
C01|Hedonic Recommendations: An Econometric Application on Big Data|This work will demonstrate how economic theory can be applied to big data analysis. To do this, I propose two layers of machine learning that use econometric models introduced into a recommender system. The reason for doing so is to challenge traditional recommendation approaches. These approaches are inherently biased due to the fact that they ignore the final preference order for each individual and under-specify the interaction between the socio-economic characteristics of the participants and the characteristics of the commodities in question. In this respect, our hedonic recommendation approach proposes to first correct the internal preferences with respect to the tastes of each individual under the characteristics of given products. In the second layer, the relative preferences across participants are predicted by socio-economic characteristics. The robustness of the model is tested with the MovieLens (100k data consists of 943 users over 1682 movies) run by GroupLens. Our methodology shows the importance and the necessity of correcting the data set by using economic theory. This methodology can be applied for all recommender systems using ratings based on consumer decisions
C01|Estimating global bank network connectedness|We use LASSO methods to shrink, select, and estimate the high‐dimensional network linking the publicly traded subset of the world's top 150 banks, 2003–2014. We characterize static network connectedness using full‐sample estimation and dynamic network connectedness using rolling‐window estimation. Statically, we find that global bank equity connectedness has a strong geographic component, whereas country sovereign bond connectedness does not. Dynamically, we find that equity connectedness increases during crises, with clear peaks during the Great Financial Crisis and each wave of the subsequent European Debt Crisis, and with movements coming mostly from changes in cross‐country as opposed to within‐country bank linkages.
C01|Human Decisions and Machine Predictions|Can machine learning improve human decision making? Bail decisions provide a good test case. Millions of times each year, judges make jail-or-release decisions that hinge on a prediction of what a defendant would do if released. The concreteness of the prediction task combined with the volume of data available makes this a promising machine-learning application. Yet comparing the algorithm to judges proves complicated. First, the available data are generated by prior judge decisions. We only observe crime outcomes for released defendants, not for those judges detained. This makes it hard to evaluate counterfactual decision rules based on algorithmic predictions. Second, judges may have a broader set of preferences than the variable the algorithm predicts; for instance, judges may care specifically about violent crimes or about racial inequities. We deal with these problems using different econometric strategies, such as quasi-random assignment of cases to judges. Even accounting for these concerns, our results suggest potentially large welfare gains: one policy simulation shows crime reductions up to 24.7% with no change in jailing rates, or jailing rate reductions up to 41.9% with no increase in crime rates. Moreover, all categories of crime, including violent crimes, show reductions; these gains can be achieved while simultaneously reducing racial disparities. These results suggest that while machine learning can be valuable, realizing this value requires integrating these tools into an economic framework: being clear about the link between predictions and decisions; specifying the scope of payoff functions; and constructing unbiased decision counterfactuals.
C01|Shock Restricted Structural Vector-Autoregressions|It is well known that the covariance structure of the data alone is not enough to identify an SVAR, and the conventional approach is to impose restrictions on the parameters of the model based on a priori theoretical considerations. This paper suggests that much can be gained by requiring the properties of the identified shocks to agree with major economic events that have been realized. We first show that even without additional restrictions, the data alone are often quite informative about the quantitatively important shocks that have occurred in the sample. We propose shrinking the set of solutions by imposing two types of inequality constraints on the shocks. The first restricts the sign and possibly magnitude of the shocks during unusual episodes in history. The second restricts the correlation between the shocks and variables external to the SVAR. The methodology provides a way to assess the validity of assumptions imposed as equality constraints. The effectiveness and limitations of this approach are exemplified with three applications.
C01|Additive Nonparametric Instrumental Regressions: A Guide to Implementation|We present a review on the implementation of regularization methods for the estimation of additive nonparametric regression models with instrumental variables. We consider various versions of Tikhonov, Landweber-Fridman and Sieve (Petrov-Galerkin) regularization. We review data-driven techniques for the sequential choice of the smoothing and the regularization parameters. Through Monte Carlo simulations, we discuss the finite sample properties of each regularization method for different smoothness properties of the regression function. Finally, we present an application to the estimation of the Engel curve for food in a sample of rural households in Pakistan, where a partially linear specification is described that allows one to embed other exogenous covariates.
C01|A forecasting performance comparison of dynamic factor models based on static and dynamic methods|We present a comparison of the forecasting performances of three Dynamic Factor Models on a large monthly data panel of macroeconomic and financial time series for the UE economy. The first model relies on static principal-component and was introduced by Stock and Watson in [1], [2]. The second is based on generalized principal components and it was introduced by Forni, Hallin, Lippi and Reichlin in [3], [4]. The last model has been recently proposed by Forni, Hallin, Lippi and Zaffaroni in [5], [6]. The data panel is split into two parts: the calibration sample, from February 1986 to December 2000, is used to select the most performing specification for each class of models in a in-sample environment, and the proper sample, from January 2001 to November 2015, is used to compare the performances of the selected models in an out-of-sample environment. The metholodogical approach is analogous to [7], but also the size of the rolling window is empirically estimated in the calibration process to achieve more robustness. We find that, on the proper sample, the last model is the most performing for the Inflation. However, mixed evidencies appear over the proper sample for the Industrial Production.
C01| Has the Gender Wage Gap been Reduced during the 'Peruvian Growth Miracle?' A Distributional Approach| Between 2004 and 2014 the Peruvian economy experienced a noticeable growth which surpassed most of Latin American countries during that period, leading some to quote this episode as the Peruvian Growth Miracle. Yet, growth of wages would not have been accompanied by an equally marked reduction in wage differentials between men and women despite government efforts to address this issue. Consequently, this study analyzes and decomposes the gender wage gap in Peru for 2004 and 2014 using the Machado and Mata (2005) decomposition method correcting for sample selection bias in the context of quantile regression (Albrecht et al. 2009). This allows to decompose the differential in terms of the endowment and treatment effect at each point of the income distribution instead of, as has been customary in previous studies for Peru, only at the average of the distribution. Using data from the National Household Survey, we find that unconditional and conditional gaps, which favour men, have deepened between 2004 and 2014 at every point of the distribution, while there is evidence of sticky floors and glass ceilings in both years. Decompositions consistently reveal that, for both years, discrimination against women is the most important factor behind gender gaps at each percentile even though the effect of endowments plays in favor of those. All in all, this raise doubts about the aggregate effectiveness of pro-equity policies applied in recent years. JEL Classification-JEL: C01, J08, J16, O12
C01|The key factors of export intensity in Tunisia: A Logistic regression with random effect model|Given the growing international competition and globalization being characterized by the massive reduction of institutional barriers, opening new markets for consumer goods, the birth of many trade agreements and the establishment of the World Trade Organization, it is imperative for companies wishing to grow, the possibility to internationalize. Consequently, one of the first modes of internationalization of a firm is export. Indeed, the success of export can be measured by various factors that depend on company's goal against the use of export strategy. Such factors are grouped into two categories namely: external and internal factors to the company. This paper will focus on exploring and analyzing the key factors that affect the export intensity of Tunisian companies. Thus, our study was conducted at the micro-economic level. Indeed, as the available data, we will try to find out the factors of export activity for a sample of Tunisian companies and this through a Logit model with random effects applied to panel data from 1997 to 2003. Indeed, the main factors that positively affect the probability of exporting in Tunisia are: Capital intensity; the company age and size. Furthermore, among the main factors that negatively affect the probability of exporting, we state labor cost.
C01|Dependence of stock markets with gold and bonds under bullish and bearish market states|This paper examines the dependence of gold and benchmark bonds with ten stock markets, including five larger developed markets (e.g., the USA, the UK, Japan, Canada and Germany) and five Eurozone peripheral GIPSI countries’ (Greece, Ireland, Portugal, Spain and Ireland) stock markets. We use a novel quantile-on-quantile (QQ) approach to construct the dependence estimates of the quantiles of gold and bonds with the quantiles of stock markets. The QQ approach, recently developed by Sim and Zhou (2015), captures the dependence between the entire distributions of financial assets and uncovers some nuance features of the relationship. The empirical findings primarily show that gold is a strong hedge and diversifier for the stock portfolio except when both markets are under stress. Furthermore, the flight to safety phenomenon is short-lived because national benchmark bonds exhibit a positive dependence with their respective countries’ stock indices at various quantiles. In contrast to the existing literature, the QQ approach suggests that bonds act as safe havens for the stock portfolio, but gold does not. Our findings also suggest that the dependence between stock-gold and stock-bond pairs is not uniform, and this relationship is market state (e.g., bearish, mild bearish, optimistic or bullish) and country specific.
C01|Information Flow Interpretation of Heteroskedasticity for Capital Asset Pricing: An Expectation-based View of Risk|The Heteroskedastic Mixture Model (HMM) of Lamoureux, and Lastrapes (1990) is extended, relaxing the restriction imposed on the mean i.e. μt-1=0 . Instead, an exogenous variable rm, along with its vector βm, that predicts return rt is introduced to examine the hypothesis that the volume is a measure of speed of evolution in the price change process in capital asset pricing. The empirical findings are documented for the hypothesis that ARCH is a manifestation of time dependence in the rate of information arrival, in line with the observations of Lamoureux, and Lastrapes (1990). The linkage between this time dependence and the expectations of market participants is investigated and the symmetric behavioural response is documented. Accordingly, the tendency of revision of expectation in the presence of new information flow whose frequency as measured by ‘volume clock’ is observed. In the absence of new information arrival at the market, investors tend to follow the market on average. When new information is available, the expectations of investors are revised in the same direction as a symmetric response to the flow of new information arrival at the market.
C01|A New Nonlinearity Test to Circumvent the Limitation of Volterra Expansion with Applications|In this paper, we propose a quick, efficient, and easy method to examine whether a time series Yt possesses any nonlinear feature. The advantage of our proposed nonlinearity test is that it is not required to know the exact nonlinear features and the detailed nonlinear forms of Yt. We find that our proposed test can be used to detect any nonlinearity for the variable being examined and detect GARCH models in the innovations. It can also be used to test whether the hypothesized model, including linear and nonlinear, to the variable being examined is appropriate as long as the residuals of the model being used can be estimated. Our simulation study shows that our proposed test is stable and powerful. We apply our proposed statistic to test whether there is any nonlinear feature in the sunspot data and whether the S&P 500 index follows a random walk model. The conclusion drawn from our proposed test is consistent those from other tests.
C01|Forward Ordinal Probability Models for Point-in-Time Probability of Default Term Structure|Common ordinal models, including the ordered logit model and the continuation ratio model, are structured by a common score (i.e., a linear combination of a list of given explanatory variables) plus rank specific intercepts. Sensitivity with respect to the common score is generally not differentiated between rank outcomes. In this paper, we propose an ordinal model based on forward ordinal probabilities for rank outcomes. The forward ordinal probabilities are structured by, in addition to the common score and intercepts, the rank and rating (for a risk-rated portfolio) specific sensitivity. This rank specific sensitivity allows a risk rating to respond to its migrations to default, downgrade, stay, and upgrade accordingly. An approach for parameter estimation is proposed based on maximum likelihood for observing rank outcome frequencies. Applications of the proposed model include modeling rating migration probability for point-in-time probability of default term structure for IFRS9 expected credit loss estimation and CCAR stress testing. Unlike the rating transition model based on Merton model, which allows only one sensitivity parameter for all rank outcomes for a rating, and uses only systematic risk drivers, the proposed forward ordinal model allows sensitivity to be differentiated between outcomes and include entity specific risk drivers (e.g., downgrade history or credit quality changes for an entity in last two quarters can be included). No estimation of the asset correlation is required. As an example, the proposed model, benchmarked with the rating transition model based on Merton model, is used to estimate the rating migration probability and probability of default term structure for a commercial portfolio, where for each rating the sensitivity is differentiated between migrations to default, downgrade, stay, and upgrade. Results show that the proposed model is more robust.
C01|Practical considerations for questionable IVs|In this article, we examine several techniques that allow for the con- struction of bounds estimates based on instrumental variables, even when the instruments are not valid. We introduce the plausexog and imperfectiv com- mands, which implement methods described by Conley, Hansen, and Rossi (2012, Review of Economics and Statistics 94: 260–272) and Nevo and Rosen (2012b, Review of Economics and Statistics 94: 659–671). We examine the performance of these bounds under a range of circumstances, which leads to several practical results related to the informativeness of the bounds in different situations.
C01|Analyse empirique de la relation entre les décisions de renouvellement des brevets et les montants d’annuités<BR>[Empirical analysis of the relationship between patent renewal decisions and the amounts of annuities]|This study aims to empirically analyze the effect of scales of fees fixed by the Patent Office on renewal decisions. The objective is to discuss the empirical validity of the hypothesis, which is the heart of all renewal models, namely the renewal decisions are based on economic criteria: agents will renew their patents if the value of patent held during an additional year exceeds the cost of renewal. Indeed, we test on French data for the period from 1970 to 2000, the relationship between the proportions of renewed patents and amounts of annuities. The results indicate a statistically significant negative effect between these two variables. This confirms the theoretical assumption that the effective life of the patent can be influenced by the amount and profile of these annuities. The analysis used can improve our understanding in favor of the patent system as a means of action on innovation through the renewal system.
C01|Technology and Business Cycles: A Schumpeterian Investigation for the USA|The purpose of this paper is to deal with questions of instability and economic crisis, deriving theoretical arguments from Schumpeter’s works and presenting relevant empirical evidence for the case of the US economy by sector of economic activity in the time period 1957-2006, just before the first signs of the global recession made their appearance. More precisely, we make an attempt to interpret the economic fluctuations in the US economy by sector of economic activity and find causal relationships between the crucial variables dictated by Schumpeterian theory. In this context, a number of relevant techniques have been used, such as cointegration analysis, periodograms, Granger causality tests as well as stepwise bi-directional causality test a la Dufour and Renault. Our findings seem to give credit to certain aspects of the Schumpeterian theory of business cycles. The results are discussed in a broader context, related to the US sectoral economy.
C01|Should we drop covariate cells with attrition problems?|It is well known that sample attrition can lead to inconsistent treatment effect estimators even in randomized control trials. Standard solutions to attrition problems either rely on strong assumptions on the attrition mechanisms or consider the estimation of bounds, which may be uninformative if attrition problems are severe. In this paper, we analyze strategies of focusing the analysis on subsets of the data with less observed attrition problems. We show that these strategies are asymptotically valid when the number of observations in each covariate cell goes to infinity. However, they can lead to important distortions when the number of observations per covariate cell is finite.
C01|Financial development and total factors productivity channel: Evidence from Africa|We explore the links between financial development and economic growth through Total Factors Productivity canal in African economies. First, we use a composite index to gauge the levels of financial development in 40 African economies during the period 2004-2014. Second, we study the Finance-Total Factors Productivity (TFP) relationship in a panel of 22 economies classified by their income level. The main results of our study show that financial development in Africa promotes economic growth, improves the allocation investment, and stimulates total factors productivity, but affects negatively saving mobilization. Results by group of countries show that financial development does not promote total factors productivity in low-income and upper-middle-income countries. For low-income countries, this is due to the inadequacy of financial services available to the needs of economic agents. For the second category of countries, this result is probably due to the fact that the financial system is biased toward the formal sector, which does not make enough efforts to increase TFP. The Finance-TFP relationship is significantly positive in the lower middle-income countries. the reforms of African financial systems have to be designed and directed to increase the adequacy of financial services to the needs of each economy and its development level. Financial sectors should encourage the accumulation of inputs in factors-driven economies, improve the reallocation of resources to high-productivity sectors in efficiency-driven economies, and finance Innovation in innovation-driven economies.
C01|Institutional Quality and Economic Performance in West Africa|This study empirically accessed the impact of institutional quality on economic performance in West Africa. The study employed the control of corruption, government effectiveness, regulatory quality and rule of law as institutional quality indicators as provided by the World Governance Indicators, WGI (2017). A panel data set of 12 West African countries from 1996 to 2015 was estimated using the fixed effect model, the random effect model and the panel two-stage least square technique. The result showed that all the indicators of institutional quality employed in the study have positive and significant impact on economic performance in West Africa when the fixed and random effect model estimation technique was employed but only government effectiveness was significant after taking account of endogeneity using the panel two-stage least square technique. The study concludes that economic performance in West Africa would be enhanced in the presence of improved institutions with more consideration to government effectiveness.
C01|Performance of Markov-Switching GARCH Model Forecasting Inflation Uncertainty|This paper seeks to uncover the non-linear characteristics of uncertainty underlying the US inflation rates over the period 1971-2015 within a regime-switching framework. Accordingly, we employ two variants of a Markov regime-switching GARCH model, one with normally distributed errors (MS-GARCH-N) and another with t-distributed errors (MS-GARCH-t), and compare their relative in-sample as well as out-of-sample performances with those of their standard single-regime counterparts. Consistent with the findings in existing studies, both of our regime-switching models are successful in identifying the year 1984 as the breakpoint in inflation volatility. Among other interesting results is a new finding that the process of switching to the low volatility regime started around April, 1979 and continued until mid 1983. This time frame is matched with the period of aggressive monetary policy changes implemented by the then Fed chairman Paul Volcker. As regards the performance in forecasting uncertainty, for shorter horizons spanning 1 to 5 months, MS-GARCH-N forecasts are found to outperform all other models whereas for 8 to 12-month ahead forecasts MS-GARCH-t appears superior.
C01|Do The Countries’ Monetary Policies Have Spatial Impact?|Nowadays, not land border but economic cooperation and borders determine the neighborhood and closeness by globalization. No doubt, any economic event happens in any country affects other partners more and less according to economic relationship in globalization process. The desire of measuring of this interaction make occur spatial econometrics. Initially, in spatial models take into account land borders. Subsequently, studies about spatial econometric models allow economic interactions and relationships. After the global economic crises in 2008 Central Banks have started to vary monetary policy tool to ensure economic and financial stability. It is estimated that which tool will be implemented by following the policies of the central banks in which they are closely related. The spatial effect of monetary policy can be not only geographical but also economic or social. Different spatial models have set up to examine whether any spatial effect on monetary policy. Unlike other studies in this study not only geographic weight matrix but also economic weight matrix have been used in the spatial models. Different weight matrix models results have been compared and construed. Our preliminary findings reveal that there is a spatial effect on monetary policy between OECD, EU and G-20 countries. And also, economic weight matrix effect is more than geographic weight matrix.
C01|The demand for cigarette in Tanzania: A temporal approach|The study presented here attempts to estimate the effect of increased price of cigarettes on the corresponding consumption in Tanzania. The study is based on household panel survey data that were conducted over three periods in 2008, 2010 and 2012. For each period price is estimated indirectly. The price increase was substantial and higher than the corresponding aggregate Consumer Price Index (CPI). An attempt is made to relate the increase in price with a possible decrease in cigarette consumption. The methods of analysis are descriptive tabular statistics and the «two-equation econometrics» approach. Over the study period, the descriptive approach showed a substantial increase in price but a modest decrease in cigarette consumption prevalence rate, intensity rate and per capita consumption. When a more robust and powerful econometric method was applied, the results showed a substantial decrease in consumption. There was a significant reduction in intensity compared to participation. Between 2008 and 2010 total price elasticity of demand decreased from –0.686 to –0.727. In 2012 the value was –1.139. The reduction in consumption was much higher among the very poor and the poor (–2.387 and –1.967 respectively). When cross classified by age group the youth (20 years or less) showed a high elasticity of –1.672. The results suggest that in a typical African country smokers (who are relatively poor) will react to substantial price increase in a predictable manner. Price elasticity of demand greater than absolute unity need not be unusual.
C01|Wage Convergence across European Regions : Do International Borders Matter?|This study focuses on wage convergence among the member states of the European Union by addressing three important questions. First, is there average wage convergence in European Union regions? Second, if there is wage onvergence, are regional wage levels converging to a single, steady state level (unconditional convergence) or to their own steady state level (conditional convergence)? Third, do international borders matter for average wage convergence? By using a panel data set covering 203 Nomenclature of Territorial Units for Statistics-2 level regions from 1996 to 2006, the present study finds wage convergence for internal regions (regions within the same country) but no evidence of convergence for border regions (neighboring regions across international borders). These results imply that wage convergence is somehow restricted by international borders. These results are robust with both parametric and non-parametric approaches of testing convergence.
C01|Modelling Crypto-Currencies Financial Time-Series|This paper studies the behaviour of crypto{currencies financial time{series of which Bitcoin is the most prominent example. The dynamic of those series is quite complex displaying extreme observations, asymmetries and several nonlinear characteristics which are difficult to model. We develop a new dynamic model able to account for long{memory and asymmetries in the volatility process as well as for the presence of time{varying skewness and kurtosis. The empirical application, carried out on a large set of crypto{currencies, shows evidence of long memory and leverage effect that has a substantial contribution in the volatility dynamic. Going forward, as this new and unexplored market will develop, our results will be important for investment and risk management purposes.
C01|Forecasting Mortality: Some Recent Developments|Forecasting mortality has been a vital issue in demography and actuarial science. It also has profound implications for pension plan and long-term economic forecasts of the nation. In the present paper we examine various forecasting methods for mortality in the framework of cointegrated time series analysis. The Lee-Carter (LC) method has been regarded as the benchmark for forecasting mortality. However, its forecasting accuracy has been known to be particularly poor for short-term forecasts, while it is well for long-term forecasts. Recently, a new methods called the multivariate time series variance component (MTV) method has been proposed which explicitly satisfies cointegration restrictions of the series. It overcomes weak points of the LC method. In the present paper we propose two new methods. The first one is the modified MTV (mMTV) method which modifies the MTV method in order to get more accurate forecast of the trend component of the method. The second is the all-component Lee-Carter (LCA) method which generalizes the Lee-Carter method, by using all principal components, in order to improve short-term forecasts of the LC method. However, it may be noted that the LCA method does not satisfy cointegration restrictions. We analytically compare forecasting accuracy of the proposed methods with the Lee-Carter method and the MTV method in the framework of cointegrated time series. We further compare them in a Monte Carlo experiment and in an empirical application of forecasting mortality for Japanese male. It is shown that the mMTV method is generally the most accurate in the Monte Carlo experiment and in Japanese data. The MTV method works almost as well. However, since the drift estimator is inefficient, it is slightly less accurate than the mMTV method in some occasions. The forecast accuracy of the LCA method is reasonably high and can be equivalent to the mMTV method in some occasions, but is generally inferior to the MTV method and the mMTV method. As expected, the LC method is the worst among methods examined in the present study. The mMTV method is recommended for practical use.
C01|Effects of Fiscal Policy Shocks on the Economy: Evidence from Selected CEE Countries|The impact of fiscal policy on the economy is a subject of special interest to the EU countries outside the Eurozone, mainly due to their position of ?countries with a derogation? and their future access to the Euro Area. In this context it seems appropriate to investigate the impact of fiscal policy shocks on the economy in the short-run in these countries.The aim of this study is to analyze the effectiveness of fiscal policy shocks in selected CEE countries. In accordance with the goal, the empirical fiscal SVAR models have been prepared. The study is based on a quarterly data for six CEE countries: Bulgaria, Croatia, Czech Republic, Hungary, Poland and Romania. The empirical model for each country includes three variables: GDP, government spending and net taxes. The identification scheme is based on the Blanchard and Perotti (2002) approach. According to the estimated results the impact response of GDP to government spending shock is positive (and statistically significant in most analyzed countries), whereas the response of GDP to the net tax shock is negative or positive (positive in the case of two countries: Croatia and Poland) however statistically insignificant in analyzed countries.The dynamic responses are presented by impulse response functions investigated for each country. The analysis of these functions demonstrates the effects of structural shocks on the economy over horizon considered for the fiscal IRF. The results show differences in GDP responses on structural shocks in analyzed CEE countries.
C01|Inter-Regional Migration In Cz And Sk: The Empirical Study Of Panel Data At Nuts3 Level|The aim of this paper is to define the relationship between migration, income, and unemployment rates, and therefore estimate these relationships using vector autoregression and the Granger causality test. This study focused on inter-regional migration at NUTS3 level in the Czech and Slovak Republics. The analysed period is from the year 2004 to 2013, and the final panel data is set for one variable, and therefore contains a total of 220 observations. According to the results, the regional migration in the Czech and Slovak Republics was determined by income differences and it is in accordance with the neoclassical theory. The causal relation was not confirmed for differences in unemployment rate. The changes of income and unemployment rates in the Czech Republic and Slovakia were not caused by migration. These results do not support conclusions of the neoclassical model of migration.
C01|Gelişmiş ve Gelişmekte Olan Ülkelerde Ar&Ge Harcamaları ve Ekonomik Büyüme Arasındaki İlişki: Panel Veri Analizi|The main purpose of this paper is to investigate the relationship between research and development (R&D) expenditures and economic growth, based on 26 some developed and developing countries (Belgium, Bulgaria, Czech Republic, Denmark, Germany, Estonia, Ireland, Greece, Spain, France, Italy, Cyprus, Hungary, Latvia, Netherland, Austria, Poland, Portugal, Romania, Slovenia, Slovakia, Finland, Sweden, United Kingdom, Iceland, Norway). For this purpose, we used endogenous growth model of Romer (1990), which claims that technological change is the major source of productivity growth in the long run. Inthiscontext, usingannualdatawhich has taken from Eurostat; an empirical model has beendeveloped in the scope of the dynamic panel data analysis forth-period 1996-2014. The model includes real gross domestic product per capita (GDPRC) and gross domestic expenditures on R&D as an indicator of technological change (innovation). At the empirical part, firstly heterogeneity of the variables were investigate during the Delta test (Pesaran ve Yamagata, 2008), and then the existence of dependency between cross-sectional units that make series were examined by the CADF and Hadri-Kurozumi tests. After proving cointegration between series with Westurlund ECM test; Dumitrescu-Hurlin (2012) and Emirmahmutoğlu-Köse (2011) panel causality tests were applied. According to the empirical results, in the long term there is one way causal relationship from R&D expenditures to economic growth for some selected developed and developing countries.
C01|Econometric estimation of second-hand shipping markets using panel data analysis|Panel data analysis is becoming increasingly popular in shipping markets since it enables the employment of a wider source of variation which allows a more efficient estimation of a model’s parameters. This study applies an econometric analysis on a balanced panel data set of tankers’ second-hand prices for five different vessel types as cross-section identifiers. Empirical analysis investigates the existence of second-hand prices’ differentiation according to the vessel size using monthly observations for over a forty years time period. The key question concerns relationships among second-hand prices, spot rates and newbuilding prices and their dependence on whether vessel sizes experience low or higher rates of interdependence. Analysis focuses on the aspect of heterogeneity among variables, which is due to the effects of unobserved variables. The models estimate fixed and random effects and examine both cross section and time effects. Unit root and cointegration tests are performed in order to check for stationarity and for the existence of any longrun equilibrium relationships among variables. Also, Hausman test is adopted to test the existence of correlated random effects. Empirical results lead to conclusions and implications regarding the use of spot rates and newbuilding prices as intermediate means for the prediction of second-hand prices.
C01|Rural Land Transfer and Financial Impact: Evidence from China| Land is the most valuable capital that farmers own. Land transfer can improve income of farmers through an optimal allocation of factors of production. The land transfer is not only transferring of the ownership but also transferring of the management rights. Chinese rural land system has its unique characteristics: ownership, contract, and management rights. Ownership rights are owned by collectives, farmers have contract management rights which are divided into management transfer rights and contract rights. Since 2008, farmers are provided with both land contract rights and land management transfer rights. This reform has provided farmers with financial opportunities to obtain revenues using different channels. Transferring-in land needs additional capital thereby causing demand effects, but transferring-out land allows farmers to earn income thereby causing supply effects. Land transfer also changes farmer’s agricultural investment and insurance behavior. This paper uses 2014 data from nine Chinese provinces to test farmers’ financial behavior change between land management rights transfer-out and transfer-in. Results from doubly-robust estimator with inverse probability weighting estimator, regression-adjustment, and propensity score matching indicate a significant difference of financial selection between land transfer-in farmers and land transfer-out farmers. Land market gives an unblocked transmission channel to rural financial market through mechanism innovation.
C01|Efficient asymptotic variance reduction when estimating volatility in high frequency data|This paper shows how to carry out efficient asymptotic variance reduction when estimating volatility in the presence of stochastic volatility and microstructure noise with the realized kernels (RK) from Barndorff-Nielsen et al. (2008) and the quasi-maximum likelihood estimator (QMLE) studied in Xiu (2010). To obtain such a reduction, we chop the data into B blocks, compute the RK (or QMLE) on each block, and aggregate the block estimates. The ratio of asymptotic variance over the bound of asymptotic efficiency converges as B increases to the ratio in the parametric version of the problem, i.e. 1.0025 in the case of the fastest RK Tukey-Hanning 16 and 1 for the QMLE. The impact of stochastic sampling times and jump in the price process is examined carefully. The finite sample performance of both estimators is investigated in simulations, while empirical work illustrates the gain in practice.
C01|Testing if the market microstructure noise is fully explained by the informational content of some variables from the limit order book|In this paper, we build tests for the presence of residual noise in a model where the market microstructure noise is a known parametric function of some variables from the limit order book. The tests compare two distinct quasi-maximum likelihood estimators of volatility, where the related model includes a residual noise in the market microstructure noise or not. The limit theory is investigated in a general nonparametric framework. In the presence of residual noise, we examine the central limit theory of the related quasi-maximum likelihood estimation approach.
C01|The R Package MitISEM: Efficient and Robust Simulation Procedures for Bayesian Inference| This paper presents the R package MitISEM (mixture of t by importance sampling weighted expectation maximization) which provides an automatic and flexible two-stage method to approximate a non-elliptical target density kernel - typically a posterior density kernel - using an adaptive mixture of Student t densities as approximating density. In the first stage a mixture of Student t densities is fitted to the target using an expectation maximization algorithm where each step of the optimization procedure is weighted using importance sampling. In the second stage this mixture density is a candidate density for efficient and robust application of importance sampling or the Metropolis-Hastings (MH) method to estimate properties of the target distribution. The package enables Bayesian inference and prediction on model parameters and probabilities, in particular, for models where densities have multi-modal or other non-elliptical shapes like curved ridges. These shapes occur in research topics in several scientific fields. For instance, analysis of DNA data in bio-informatics, obtaining loans in the banking sector by heterogeneous groups in financial economics and analysis of education's effect on earned income in labor economics. The package MitISEM provides also an extended algorithm, 'sequential MitISEM', which substantially decreases computation time when the target density has to be approximated for increasing data samples. This occurs when the posterior or predictive density is updated with new observations and/or when one computes model probabilities using predictive likelihoods. We illustrate the MitISEM algorithm using three canonical statistical and econometric models that are characterized by several types of non-elliptical posterior shapes and that describe well-known data patterns in econometrics and finance. We show that MH using the candidate density obtained by MitISEM outperforms, in terms of numerical efficiency, MH using a simpler candidate, as well as the Gibbs sampler. The MitISEM approach is also used for Bayesian model comparison using predictive likelihoods.
C01|High Frequency vs. Daily Resolution: the Economic Value of Forecasting Volatility Models - 2nd ed|Forecasting volatility models typically rely on either daily or high frequency (HF) data and the choice between these two categories is not obvious. In particular, the latter allows to treat volatility as observable but they suffer from many limitations. HF data feature microstructure problem, such as the discreteness of the data, the properties of the trading mechanism and the existence of bid-ask spread. Moreover, these data are not always available and, even if they are, the asset’s liquidity may be not sufficient to allow for frequent transactions. This paper considers different variants of these two family forecasting-volatility models, comparing their performance (in terms of Value at Risk, VaR) under the assumptions of jumps in prices and leverage effects for volatility. Findings suggest that daily-data models are preferred to HF-data models at 5% and 1% VaR level. Specifically, independently from the data frequency, allowing for jumps in price (or providing fat-tails) and leverage effects translates in more accurate VaR measure.
C01|Testing Distributional Assumptions Using a Continuum of Moments|We propose specification tests for parametric distributions that compare theoretical and empirical characteristic functions. Our proposal is the continuum of moment conditions analogue to the usual overidentifying restrictions test, which takes into account the correlation between influence functions for different argument values. We derive its asymptotic distribution for fixed regularization parameter and when this vanishes with the sample size. We show its consistency against any deviation from the null, study its local power and compare it with existing tests. An extensive Monte Carlo exercise confirms that our proposed tests display good power in finite samples against a variety of alternatives.
C01|Determinantes de las relaciones reales de intercambio de España con Alemania (1970-2010). Un análisis econométrico de la ventaja absoluta de costo intrasectorial|El presente trabajo tiene como objeto analizar los determinantes de las relaciones reales de intercambio de España con Alemania, tomando como período 1970-2010. Para tal propósito, se lleva a cabo un análisis de series temporales aplicando pruebas de raíces unitarias y un test de cointegración, siendo el marco teórico de referencia el enfoque de Anwar Shaikh de la ventaja absoluta de costo intrasectorial. Finalmente, el análisis econométrico constata que el tipo de cambio real efectivo entre España y Alemania se encuentra sujeto en el largo plazo a los costos laborales unitarios reales relativos de la industria manufacturera de ambos países. ***** The main aim of this paper is to analyse the determinants of the real terms of trade between Spain and Germany during the 1970-2010 period. In order to do so, we have used unit root and cointegration tests, in which our theoretical framework is the absolute cost advantage that was developed by Anwar Shaikh. Finally, our econometric analysis discovered that the real effective rate exchange between Spain and Germany is, in the long term, subject to the real unit labour costs of manufacturing sectors.
C01|Evaluación de pronósticos de modelos lineales y no lineales de la tasa de cambio de Colombia|El presente trabajo de grado explora las predicciones de los modelos Markov Switching y ARIMA para la tasa de cambio de Colombia, junto con la combinación de pronósticos de los dichos modelos. El modelo Markov Switching pertenece a una clase de modelos de cambio de regímenes, cuyo supuesto principal es que una variable no observada que sigue un proceso de Markov, gobierna el régimen de la serie. La estimación del modelo MS se lleva a cabo por el algoritmo «Expected Maximization». Los pronósticos de los modelos individuales y sus combinaciones se evalúan por fuera de muestra en la serie diaria de la tasa representativa de mercado (TRM) de Colombia. Se realizan pronósticos para un horizonte de 25 días y pronósticos recursivos para el mismo horizonte y los resultados se comparan con las predicciones de un paseo aleatorio. El modelo MS obtuvo la mejor puntuación en la raíz del error cuadrático medio y en el error absoluto medio cuando se efectúa el pronóstico a horizonte 25. Al hacer el pronóstico recursivo, el modelo ARIMA obtuvo los mejores resultados. En el apéndice se desarrolla la derivación matemática para la estimación del modelo MS y las pruebas de especi?cación. También se muestra el código de programación desarrollado en el software estadístico R, para la estimación del modelo Markov Switching por el algoritmo EM y el código con el que se efectuó la combinación de pronóstico.
C01|Análisis de la relación rentabilidad-riesgo en el mercado accionario internacional para un mundo parcialmente integrado|No abstract is available for this item.
C01|Efectos del desarrollo financiero sobre el crecimiento económico de Colombia y Chile, 1982-2014|Este artículo analiza la validez y relevancia de las variables propuestas por King y Levine (1993) en el caso específico de Colombia y Chile; para ello, utiliza un modelo de datos de panel como una estrategia para encontrar posibles efectos significativos de las variables financieras sobre el crecimiento en ambos países, no solo considerando su comportamiento en el tiempo, sino también la existencia de heterogeneidades entre ambos. Los resultados muestran de manera parcial cómo algunas de sus variables tienen efectos distinguibles en el desempeño y crecimiento de la economía.******This article analyzes the validity and relevance of the variables proposed by King and Levine (1993) in the specific case of Colombia and Chile; for this effect, it uses a panel data model as a strategy to find possible significant impacts of the financial variables on growth in both countries, not only considering their behavior over time, but also the existence of heterogeneities among them. The results partially show how some of the variables have distinguishable impacts on economic performance and growth.******Este artigo analisa a validade e a relevância das variáveis propostas por King y Levine (1993) no caso específico da Colômbia e do Chile; para isso, utiliza um modelo de dados em painel como estratégia para encontrar possíveis efeitos significativos das variáveis financeiras sobre o crescimento em ambos os países, não somente considerando seu comportamento no tempo, mas também a existência de heterogeneidades entre eles. Os resultados mostram, de maneira parcial, como algumas de suas variáveis têm efeitos distinguíveis no desempenho e no crescimento da economia.
C01|Identification and Estimation in Non-Fundamental Structural VARMA Models|The basic assumption of a structural VARMA model (SVARMA) is that it is driven by a white noise whose components are independent and can be interpreted as economic shocks, called “structural” shocks. When the errors are Gaussian, independence is equivalent to noncorrelation and these models face two kinds of identi?cation issues. The ?rst identi?cation problem is “static” and is due to the fact that there is an in?nite number of linear transformations of a given random vector making its components uncorrelated. The second identi?cation problem is “dynamic” and is a consequence of the fact that the SVARMA process may have a non invertible AR and/or MA matrix polynomial but, still, has the same second-order properties as a VARMA process in which both the AR and MA matrix polynomials are invertible (the fundamental representation). Moreover the standard Box-Jenkins approach [Box and Jenkins (1970)] automatically estimates the fundamental representation and, therefore, may lead to misspeci?ed Impulse Response Functions. The aim of this paper is to explain that these dif?culties are mainly due to the Gaussian assumption, and that both identi?cation challenges are solved in a non-Gaussian framework. We develop new simple parametric and semi-parametric estimation methods when there is non-fundamentalness in the moving average dynamics. The functioning and performances of these methods are illustrated by applications conducted on both simulated and real data.
C01|Discovering pervasive and non-pervasive common cycles|The objective of this paper is to propose a strategy to exploit short-run commonalities in the sectoral components of macroeconomic variables to obtain better models and more accurate forecasts of the aggregate and of the components. Our main contribution concerns cases in which the number of components is large, so that traditional multivariate approaches are not feasible. We show analytically and by Monte Carlo methods that subsets of components in which all the elements share a single common cycle can be discovered by pairwise methods. As the procedure does not rely on any kind of cross-sectional averaging strategy: it does not need to assume pervasiveness, it can deal with highly correlated idiosyncratic components and it does not need to assume that the size of the subsets goes to infinity. Nonetheless, the procedure works both with fixed N and T going to infinity, and with T and N both going to infinity.
C01|Smoothed Estimating Equations For Instrumental Variables Quantile Regression|The moment conditions or estimating equations for instrumental variables quantile regression involves the discontinuous indicator function. We instead use smoothed estimating equations, with bandwidth h. This is known to allow higher-order expansions that justify bootstrap refinements for inference. Computation of the estimator also becomes simpler and more reliable, especially with (more) endogenous regressors. We show that the mean squared error of the vector of estimating equations is minimized for some h > 0, which also reduces the mean squared error of the parameter estimators. The same h also minimizes higher-order type I error for a χ2 test, leading to improved size-adjusted power. Our plug-in bandwidth consistently reproduces all of these properties in simulations.<br><small>(This abstract was borrowed from another version of this item.)</small>
C01|Identification And Inference On Regressions With Missing Covariate Data|This paper examines the problem of identification and inference on a conditional moment condition model with missing data, with special focus on the case when the conditioning covariates are missing. We impose no assumption on the distribution of the missing data and we confront the missing data problem by using a worst case scenario approach. We characterize the sharp identified set and argue that this set is usually too complex to compute or to use for inference. Given this difficulty, we consider the construction of outer identified sets (i.e. supersets of the identified set) that are easier to compute and can still characterize the parameter of interest. Two different outer identification strategies are proposed. Both of these strategies are shown to have non-trivial identifying power and are relatively easy to use and combine for inferential purposes.<br><small>(This abstract was borrowed from another version of this item.)</small>
C01|A Simple R-Estimation Method for Semiparametric Duration Models|Modeling nonnegative financial variables (e.g. durations between trades, traded volumes or asset volatilities) is central to a number of studies across financial econometrics, and, despite the efforts, still poses several statistical challenges. Among them, the efficiency aspects of semiparametric estimation. In this paper, we concentrate on estimation problems in Autoregressive Conditional Duration (ACD) models with unspecified innovation densities. Exponential quasi-likelihood estimators (QMLE) are the usual practice in that context. The efficiency of those QMLEs (the only Fisher-consistent QMLEs) unfortunately rapidly deteriorates away from the reference exponential density—a phenomenon that has been emphasized earlier by Drost and Werker (2003), who propose various semiparametrically efficient procedures to palliate that phenomenon. Those procedures rely on a general semiparametric approach which typically requires kernel estimation of the underlying innovation density. We propose rank-based estimators (R-estimators) as a substitute. Just as the QMLE, R-estimators remain root-n consistent irrespective of the underlying density, and rely on the choice of a reference density under which they achieve semiparametric efficiency; that density, however, needs not be the exponential one. Contrary to the semiparametric estimators proposed by Drost and Werker (2003), R-estimators neither require tangent space calculations nor kernel-based density estimation. Numerical results moreover indicate that R-estimators based on exponential reference densities uniformly outperform the exponential QMLE under such families of innovations as the Weibull or Burr densities. A real data example about modeling the price range of the Swiss stock market index concludes the paper.
C01|Estimation of integrated quadratic covariation with endogenous sampling times|When estimating high-frequency covariance (quadratic covariation) of two arbitrary assets observed asynchronously, simple assumptions, such as independence, are usually imposed on the relationship between the prices process and the observation times. In this paper, we introduce a general endogenous two-dimensional nonparametric model. Because an observation is generated whenever an auxiliary process called observation time process hits one of the two boundary processes, it is called the hitting boundary process with time process (HBT) model. We establish a central limit theorem for the Hayashi–Yoshida (HY) estimator under HBT in the case where the price process and the observation price process follow a continuous Itô process. We obtain an asymptotic bias. We provide an estimator of the latter as well as a bias-corrected HY estimator of the high-frequency covariance. In addition, we give a consistent estimator of the associated standard error.
C01|The performance of tests on endogeneity of subsets of explanatory variables scanned by simulation|Tests for classification as endogenous or predetermined of arbitrary subsets of regressors are formulated as significance tests in auxiliary IV regressions and their relationships with various more classic test procedures are examined and critically compared with statements in the literature. Then simulation experiments are designed by solving the data generating process parameters from salient econometric features, namely: degree of simultaneity and multicollinearity of regressors, and individual and joint strength of external instrumental variables. Next, for various test implementations, a wide class of relevant cases is scanned for flaws in performance regarding type I and II errors. Substantial size distortions occur, but these can be cured remarkably well through bootstrapping, except when instruments are relatively weak. The power of the subset tests is such that they establish an essential addition to the well-known classic full-set DWH tests in a data based classification of individual explanatory variables. This is also illustrated in an empirical example supplemented with hints for practitioners.
C01|Do financial reforms help stabilize inequality?|We explore the relationship between financial reforms and income inequality using a panel of 29 countries in 1975–2005. We extend panel unit root tests to allow for the presence of some financial-reform covariates and further suggest an associated but novel, semi-parametric approach. Results demonstrate that although both gross and net Gini indices follow a unit root process, this picture can change when financial reform indices are accounted for. In particular, while gross Gini coefficients are generally not stabilized by financial reforms, net measures are (more likely to be). Thus financial reforms enacted in the presence of a strong safety net would seem preferable.
C01|An Empirical Test for Costs Subadditivity in the Fishery Sector|The seminal work by Baumol et al. (1982) has highlighted the importance of analyzing firms’ costs structure. This allows to design proper policy measures and to understand the impacts of those policies in markets. The note presents an original method and an application for testing costs subadditivity in the fishery sector, by using a system of supply functions under strict conditions and assumptions. The method is practical, though robust, and can be applied in the absence of detailed data on costs structures. Under stringent hypothesis (that delimit the application) they can be inferred from the supply functions. Subadditivity in costs, in fact, is a more proper economic definition (and methodological approach) than traditional economies of scale in fishery. The latter, in fact, does not depend from the vessel technology, but on the degree of quantity and variety of fish species in the ocean.
C01|The impact of post-procedural complications on reimbursement, length of stay and mechanical ventilation among patients undergoing transcatheter aortic valve implantation in Germany|BACKGROUND: The impact of various post-procedural complications after transcatheter aortic valve implantation (TAVI) on resource use and their consequences in the German reimbursement system has still not been properly quantified. METHODS: In a retrospective observational study, we use data from the German DRG statistic on patient characteristics and in-hospital outcomes of all isolated TAVI procedures in 2013 (N = 9147). The impact of post-procedural complications on reimbursement, length of stay and mechanical ventilation was analyzed using both unadjusted and risk-adjusted linear and logistic regression analyses. RESULTS: A total of 235 (2.57%) strokes, 583 (6.37%) bleeding events, 474 (5.18%) cases of acute kidney injury and 1428 (15.61%) pacemaker implantations were documented. The predicted reimbursement of an uncomplicated TAVI procedure was €33,272, and bleeding events were associated with highest additional reimbursement (€12,839, p 48 h: OR 6.93, p 48 h: OR 5.73, p
C01|Undergraduate Econometrics Instruction: Through Our Classes, Darkly|"The past half-century has seen economic research become increasingly empirical, while the nature of empirical economic research has also changed. In the 1960s and 1970s, an empirical economist's typical mission was to ""explain"" economic variables like wages or GDP growth. Applied econometrics has since evolved to prioritize the estimation of specific causal effects and empirical policy analysis over general models of outcome determination. Yet econometric instruction remains mostly abstract, focusing on the search for ""true models"" and technical concerns associated with classical regression assumptions. Questions of research design and causality still take a back seat in the classroom, in spite of having risen to the top of the modern empirical agenda. This essay traces the divergent development of econometric teaching and empirical practice, arguing for a pedagogical paradigm shift."
C01|The Economic Cost of Carbon Abatement with Renewable Energy Policies|This paper exploits the randomness and exogeneity of weather conditions to identify the economic cost of decarbonization through renewable energy (RE) support policies. We find that both the aggregate cost and the distribution of cost between energy producers and consumers vary significantly depending on which type of RE technology is promoted reflecting substantial heterogeneity in production cost, temporal availability of natural resources, and market conditions (i.e., time-varying demand, carbon intensity of installed production capacities, and opportunities for cross-border trade). We estimate that the cost for reducing one ton of CO2 emissions through subsidies for solar are EUR 500-1870. Subsidizing wind entails significantly lower cost, which can even be slightly negative, ranging from EUR 5-230. While the economic rents for energy producers always decrease, consumers incur three to five times larger costs when solar is promoted but gain under RE policies promoting wind.
C01|Mapping the Stocks in MICEX: Who Is Central in Moscow Stock Exchange?|In this article we use partial correlations to derive bidirectional connections between the major firms listed in MICEX. We obtain the coefficients of partial correlation from the correlation estimates of constant conditional correlation GARCH (CCC-GARCH) and consistent dynamic conditional correlation GARCH (cDCC-GARCH) models. We map the graph of partial correlations using the Gaussian graphical model and apply network analysis to identify the most central firms in terms of shock propagation and in terms of connectedness with others. Moreover, we analyze some macro characteristics of the network over time and measure the system vulnerability to external shocks. Our findings suggest that during the crisis interconnectedness between firms strengthen and system becomes more vulnerable to systemic shocks. In addition, we found that the most connected firms are Sberbank and Lukoil while most central in terms of systemic risk are Gazprom and FGC UES.
C01|Retrieving Risk-Neutral Densities Embedded in VIX Options: a Non-Structural Approach|We propose a non-structural pricing method to retrieve the risk-neutral density implied by options contracts on the CBOE VIX. The method is based on orthogonal polynomial expansions around a kernel density and yields the risk-neutral density of the underlying asset without the need for modeling its dynamics. The method imposes only mild regularity conditions on shape of the density. The approach can be thought of as an alternative to Hermite expansions where the kernel has positive support. .e family of Laguerre kernels is extended to include the GIG and the generalized Weibull densities, which, due to their flexible rate of decay, are better suited at modeling the density of the VIX. Based on this technique, we propose a simple and robust way to estimate the expansion coefficients by means of a principal components analysis. We show that the proposed methodology yields an accurate approximation of the risk-neutral density also when the no-arbitrage and efficient option prices are contaminated by measurement errors. A number of numerical illustrations support the adequacy and the flexibility of the proposed expansions in a large variety of cases.
C01|Intrinsic Liquidity in Conditional Volatility Models|Until recently the liquidity of financial assets has typically been viewed as a second-order consideration. Liquidity was frequently associated with simple transaction costs that impose ? temporary if any ? effect on asset prices, and whose shocks could be easily diversified away. Yet the evidence ? especially the recent liquidity crisis ? suggests that liquidity is now a primary concern. This paper aims at disentangling market risk and liquidity risk in the context of conditional volatility models. Our approach allows the isolation of the intrisic liquidity of any asset, and thus makes it possible to deduce a liquidity risk even when volumes are not observed.
C01|Gauging Liquidity Risk in Emerging Market Bond Index Funds|ETFs and index funds have grown at very rapid rates in recent years. Originally launched to track some large liquid indices in developed markets, they now also concern less liquid asset classes such as emerging market bonds. Illiquidity certainly affects the quality of the replication, and in particular, liquidity might increase the tracking error of any index fund, i.e., the difference between the fund and the benchmark returns. The tracking error is then the first characteristic that investors consider when they select index funds. In this paper, we begin from the CDS-bond basis to simulate the tracking error (TE) of a hypothetical well-diversified fund investing in the emerging market bond universe. We compute the CDS-bond basis and the tracking error for 9 emerging market sovereign entities: Brazil, Chile, Hungary, Mexico, Poland, Russia, South Africa, Thailand and Turkey. All of these countries are included in the MSCI Emerging Market Debt in Local Currency index. Our sample period ranges from January 1, 2007 to March 26, 2012. Using a Regime Switching for Dynamic Correlations (RSDC) model, we show that the country-by-country tracking error is reduced by the diversification at the fund level. Moreover, we show that this diversification effect is less effective during crisis periods. This loss of diversification benefits is the main risk of index funds when they are designed to create a liquid exposure to illiquid asset classes.
C01|Assessing the Contribution of Agricultural Productivity to Food Security levels in Sub-Saharan African countries| The study investigates the effect of agricultural productivity on different food security measures in Sub-Saharan Africa (SSA). We identify food security indicators with per capita total food available in tonnes and per capita nutrient supply (e.g., calories and proteins), while agricultural–value-added per hectare and cereal production per hectare are taken as measures of agricultural productivity in the study. Using a panel data covering 41 countries from 1980-2009, we employ both the dynamic and linear models. The empirical results from both models show that an increase in agricultural productivity contributes positively and significantly to all measures of food security considered in the study. Thus suggesting that the key to improving food security is by boosting the current level of agricultural productivity growth in SSA. Accordingly, we contend that policies geared toward increasing government investment in agricultural research and development (R&D) would likely raise agricultural productivity and subsequently food security levels in the region.
