C52|Modelling and Forecasting WIG20 Daily Returns|The purpose of this paper is to model daily returns of the WIG20 index. The idea is to consider a model that explicitly takes changes in the amplitude of the clusters of volatility into account. This variation is modelled by a positive-valued deterministic component. A novelty in specification of the model is that the deterministic component is specified before estimating the multiplicative conditional variance component. The resulting model is subjected to misspecification tests and its forecasting performance is compared with that of commonly applied models of conditional heteroskedasticity.
C52|Inference from the futures: ranking the noise cancelling accuracy of realized measures|We consider the log-linear relationship between futures contracts and their underlying assets and show that in the classical Brownian semi-martingale (BSM) framework the two series must, by no-arbitrage, have the same integrated variance. We then introduce the concept of noise cancelling and propose a generally applicable methodology to assess the performance of realized measures when the variable of interest is latent, overcoming the problem posed by the lack of a true value for the integrated variance. Using E-mini index futures contracts, we carry out formal testing of several realized measures in the presence of noise. Moreover, a thorough simulation analysis is employed to evaluate the estimators' sensitivity to different price and noise processes, and sampling frequencies.
C52|Forecasting dynamically asymmetric fluctuations of the U.S. business cycle|The generalized smooth transition autoregression (GSTAR) parametrizes the joint asymmetry in the duration and length of cycles in macroeconomic time series by using particular generalizations of the logistic function. The symmetric smooth transition and linear autoregressions are nested in the GSTAR. A test for the null hypothesis of dynamic symmetry is presented. Two case studies indicate that dynamic asymmetry is a key feature of the U.S. economy. The GSTAR model beats its competitors for point forecasting, but this superiority becomes less evident for density forecasting and in uncertain forecasting environments.
C52|Consistent Inference for Predictive Regressions in Persistent VAR Economies|This paper studies the properties of standard predictive regressions in model economies, characterized through persistent vector autoregressive dynamics for the state variables and the associated series of interest. In particular, we consider a setting where all, or a subset, of the variables may be fractionally integrated, and note that this induces a spurious regression problem. We then propose a new inference and testing procedure - the local spectrum (LCM) approach - for the joint significance of the regressors, which is robust against the variables having different integration orders. The LCM procedure is based on (semi-)parametric fractional-filtering and band spectrum regression using a suitably selected set of frequency ordinates. We establish the asymptotic properties and explain how they differ from and extend existing procedures. Using these new inference and testing techniques, we explore the implications of assuming VAR dynamics in predictive regressions for the realized return variation. Standard least squares predictive regressions indicate that popular financial and macroeconomic variables carry valuable information about return volatility. In contrast, we find no significant evidence using our robust LCM procedure, indicating that prior conclusions may be premature. In fact, if anything, our results suggest the reverse causality, i.e., rising volatility predates adverse innovations to key macroeconomic variables. Simulations are employed to illustrate the relevance of the theoretical arguments for finite-sample inference.
C52|Models with Multiplicative Decomposition of Conditional Variances and Correlations|Univariate and multivariate GARCH type models with multiplicative decomposition of the variance to short and long run components are surveyed. The latter component can be either deterministic or stochastic. Examples of both types are studied.
C52|The Shifting Seasonal Mean Autoregressive Model and Seasonality in the Central England Monthly Temperature Series, 1772-2016|In this paper we introduce an autoregressive model with seasonal dummy variables in which coefficients of seasonal dummies vary smoothly and deterministically over time. The error variance of the model is seasonally heteroskedastic and multiplicatively decomposed, the decomposition being similar to that in well known ARCH and GARCH models. This variance is also allowed to be smoothly and deterministically time-varying. Under regularity conditions, consistency and asymptotic normality of the maximum likelihood estimators of parameters of this model is proved. A test of constancy of the seasonal coefficients is derived. The test is generalised to specifying the parametric structure of the model. A test of constancy over time of the heteroskedastic error variance is presented. The purpose of building this model is to use it for describing changing seasonality in the well-known monthly central England temperature series. More specifically, the idea is to find out in which way and by how much the monthly temperatures are varying over time during the period of more than 240 years, if they do. Misspecification tests are applied to the estimated model and the findings discussed.
C52|Assessing predictive accuracy in panel data models with long-range dependence|This paper proposes tests of the null hypothesis that model-based forecasts are uninformative in panels, allowing for individual and interactive fixed effects that control for cross-sectional dependence, endogenous predictors, and both short-range and long-range dependence. We consider a Diebold-Mariano style test based on comparison of the model-based forecast and a nested nopredictability benchmark, an encompassing style test of the same null, and a test of pooled uninformativeness in the entire panel. A simulation study shows that the encompassing style test is reasonably sized in finite samples, whereas the Diebold-Mariano style test is oversized. Both tests have non-trivial local power. The methods are applied to the predictive relation between economic policy uncertainty and future stock market volatility in a multi-country analysis.
C52|Inclusive development in environmental sustainability in sub‐Saharan Africa: Insights from governance mechanisms|This research examines the relevance of inclusive development in modulating the role of governance on environmental degradation. The study focuses on 44 countries in sub‐Saharan Africa for the period 2000–2012. The generalised method of moments is employed as the empirical strategy, and CO2 emissions per capita is used to measure environmental pollution. Bundled and unbundled governance dynamics are employed, notably political governance (consisting of political stability/no violence and “voice and accountability”), economic governance (encompassing government effectiveness and regulation quality), institutional governance (entailing corruption‐control and the rule of law), and general governance (a composite measure of political governance, economic governance, and institutional governance). The following main findings are established. First, the underlying net effect in the moderating role of inclusive development in the governance‐CO2 emissions nexus is not significant in regressions pertaining to political governance and economic governance. Second, there are positive net effects from the relevance of inclusive development in modulating the effects of regulation quality, economic governance, and general governance on CO2 emissions. The significant and insignificant effects are elucidated. Policy implications are discussed.
C52|Economic Development Thresholds for a Green Economy in Sub-Saharan Africa|This study investigates how increasing economic development affects the green economy in terms of CO2 emissions, using data from 44 countries in the SSA for the period 2000-2012. The Generalised Method of Moments (GMM) is used for the empirical analysis. The following main findings are established. First, relative to CO2 emissions, enhancing economic growth and population growth engenders a U-shaped pattern whereas increasing inclusive human development shows a Kuznets curve. Second, increasing GDP growth beyond 25% of annual growth is unfavorable for a green economy. Third, a population growth rate of above 3.089% (i.e. annual %) has a positive effect of CO2 emissions. Fourth, an inequality-adjusted human development index (IHDI) of above 0.4969 is beneficial for a green economy because it is associated with a reduction in CO2 emissions. The established critical masses have policy relevance because they are situated within the policy ranges of adopted economic development dynamics.
C52|Governance,CO2 emissions and inclusive human development in Sub-Saharan Africa|This study investigates the relevance of government quality in moderating the incidence of environmental degradation on inclusive human development in 44 sub-Saharan African countries for the period 2000-2012. Environmental degradation is measured with CO2 emissions and the governance dynamics include: political stability, voice and accountability, government effectiveness, regulation quality, the rule of law and corruption-control. The empirical evidence is based on the Generalised Method of Moments. Regulation quality modulates CO2 emissions to exert a net negative effect on inclusive development. Institutional governance (consisting of corruption-control and the rule of law) modulates CO2 emissions to also exert a net negative effect on inclusive human development. Fortunately, the corresponding interactive effects are positive, which indicates that good governance needs to be enhanced to achieve positive net effects. A policy threshold of institutional governance at which institutional governance completely dampens the unfavourable effect of CO2 emissions on inclusive human development is established. Other policy implications are discussed.
C52|Inclusive development in environmental sustainability in sub-Saharan Africa: insights from governance mechanisms|This research examines the relevance of inclusive development in modulating the role of governance on environmental degradation. The study focuses on forty-four countries in sub-Saharan Africa for the period 2000-2012. The Generalised Method of Moments is employed as the empirical strategy and CO2 emissions per capita is used to measure environmental pollution. Bundled and unbundled governance dynamics are employed, notably: political governance (consisting of political stability/no violence and â€œvoice and accountabilityâ€ ), economic governance (encompassing government effectiveness and regulation quality), institutional governance (entailing corruption-control and the rule of law), and general governance (a composite measure of political governance, economic governance and institutional governance). The following main findings are established. First, the underlying net effect in the moderating role of inclusive development in the governance-CO2 emissions nexus is not significant in regressions pertaining to political governance and economic governance. Second, there are positive net effects from the relevance of inclusive development in modulating the effects of regulation quality, economic governance and general governance on CO2 emissions. The significant and insignificant effects are elucidated. Policy implications are discussed.
C52|Economic Development Thresholds for a Green Economy in Sub-Saharan Africa|This study investigates how increasing economic development affects the green economy in terms of CO2 emissions, using data from 44 countries in the SSA for the period 2000-2012. The Generalised Method of Moments (GMM) is used for the empirical analysis. The following main findings are established. First, relative to CO2 emissions, enhancing economic growth and population growth engenders a U-shaped pattern whereas increasing inclusive human development shows a Kuznets curve. Second, increasing GDP growth beyond 25% of annual growth is unfavorable for a green economy. Third, a population growth rate of above 3.089% (i.e. annual %) has a positive effect of CO2 emissions. Fourth, an inequality-adjusted human development index (IHDI) of above 0.4969 is beneficial for a green economy because it is associated with a reduction in CO2 emissions. The established critical masses have policy relevance because they are situated within the policy ranges of adopted economic development dynamics.
C52|Economic Development Thresholds for a Green Economy in Sub-Saharan Africa|This study investigates how increasing economic development affects the green economy in terms of CO2 emissions, using data from 44 countries in the SSA for the period 2000-2012. The Generalised Method of Moments (GMM) is used for the empirical analysis. The following main findings are established. First, relative to CO2 emissions, enhancing economic growth and population growth engenders a U-shaped pattern whereas increasing inclusive human development shows a Kuznets curve. Second, increasing GDP growth beyond 25% of annual growth is unfavorable for a green economy. Third, a population growth rate of above 3.089% (i.e. annual %) has a positive effect of CO2 emissions. Fourth, an inequality-adjusted human development index (IHDI) of above 0.4969 is beneficial for a green economy because it is associated with a reduction in CO2 emissions. The established critical masses have policy relevance because they are situated within the policy ranges of adopted economic development dynamics.
C52|Linkages between Globalisation, Carbon dioxide emissions and Governance in Sub-Saharan Africa|This study investigates linkages between environmental degradation, globalisation and governance in 44 countries in Sub-Saharan Africa using data for the period 2000-2012. The Generalised Method of Moments is employed as empirical strategy. Environmental degradation is proxied by carbon dioxide emissions whereas globalisation is appreciated in terms of trade openness and net foreign direct investment inflows. Bundled and unbundled governance indicators are used, namely: political governance (consisting of political stability/no violence and â€œvoice & accountabilityâ€ ), economic governance (encompassing government effectiveness and regulation quality), institutional governance (entailing corruption-control and the rule of law) and general governance (a composite measurement of political governance, economic governance and institutional governance). The following main finding is established. Trade openness modulates carbon dioxide emissions to have positive net effects on political stability, economic governance, the rule of law and general governance.
C52|The persistence of global terrorism|This study investigates persistence of global terrorism in a panel of 163 countries for the period 2010 to 2015. The empirical evidence is based on Generalised Method of Moments. The following findings are established. First, persistence in terrorism is a decreasing function of income levels because it consistently increases from high income (through upper middle income) to lower middle income countries. Second, compared to Christian-oriented countries, terrorism is more persistent in Islam-oriented nations. Third, landlocked countries also reflect a higher level of persistence relative to their coastal counterparts. Fourth, Latin American countries show higher degrees of persistence when compared with Middle East and North African (MENA) countries. Fifth, the main determinants of the underlying persistence are political instability and weapons import. The results are discussed to provide answers to four main questions which directly pertain to the reported findings. These questions centre on why comparative persistence in terrorism is based on income levels, religious orientation, landlockedness and regions.
C52|FDI in Selected Developing Countries: Evidence from Bundling and Unbundling Governance|The objective of this study is to assess governance drivers of FDI in a panel of BRICS and MINT countries for the period 2001-2011. We bundle and unbundle governance determinants using a battery of contemporary and non-contemporary estimation techniques. Our findings reveal the following: Firstly, for both contemporary and non-contemporary specifications, while the majority of our governance determinants of Gross FDI are significant, they are overwhelmingly insignificant for Net FDI. Secondly, the significance of the governance dynamics in increasing order of magnitude are general governance, political governance, economic governance, political stability, regulation quality and government effectiveness. Thirdly, for non-contemporary specifications, the significance of governance variables is as follows in ascending order of magnitude: economic governance, institutional governance, general governance, corruption-control, political governance and political stability. The importance of combining governance indicators is captured by the effects of political governance, economic governance and institutional governance. The results indicate that the simultaneous implementation of the various components of governance clarifies a countryâ€™s attractiveness for FDI location. Policy implications are discussed with particular emphasis on the timing of FDI and its targeting.
C52|Environmental Pollution, Economic Growth and Institutional Quality: Exploring the Nexus in Nigeria|The interaction between environmental pollution and economic growth determines the achievement of the green growth objective of developing economies. An economy turns around the inverted U-shaped Environmental Kuznets Curve (EKC) when pollution is effectively dampened by social, political and economic factors as such economy grows. Thus, this study examines the EKC considering the impact of institutional quality on six variables of environmental pollution [carbon dioxide (CO2), Nitrous Oxide (N2O), Suspended Particulate Maters (SPM), Rainfall, Temperature and Total Green House Emission (TGH)] using the case of Nigeria. The EKC model includes population density, education expenditure, foreign direct investment, and gross domestic investment as control variables, and it was analysed using the Auto Regressive Distribution Lag (ARDL) econometric technique, which has not been applied in the literature on Nigeria. The results, inter alia, indicate that there is EKC for CO2 and SPM. This implies that the green growth objective can be pursued in Nigeria with concerted efforts. Other environmental pollution indicators did not exert significant influence on economic growth. Therefore, it is recommended that Nigeria’s institutional quality be strengthened to limit environmental pollution in light of economic growth.
C52|Publish and Perish: Creative Destruction and Macroeconomic Theory|A number of macroeconomic theories, very popular in the 1980s, seem to have completely disappeared and been replaced by the dynamic stochastic general equilibrium (DSGE) approach. We will argue that this replacement is due to a tacit agreement on a number of assumptions, previously seen as mutually exclusive, and not due to a settlement by ‘nature’. As opposed to econometrics and microeconomics and despite massive progress in the access to data and the use of statistical software, macroeconomic theory appears not to be a cumulative science so far. Observational equivalence of different models and the problem of identification of parameters of the models persist as will be highlighted by examining two examples: one in growth theory and a second in testing inflation persistence.
C52|The Trend Unemployment Rate in Canada: Searching for the Unobservable|In this paper, we assess several methods that have been used to measure the Canadian trend unemployment rate (TUR). We also considerimprovements and extensions to some existing methods. The assessment is based on four criteria: (i) the extent to which methods provide explanations for changes in trend unemployment; (ii) whether revisions to unemployment gap (UGAP, the difference between the actual unemployment rate and TUR) estimates are well behaved; (iii) if UGAPs provide information about future inflation; and (iv) if UGAPs help explain historical data about wages and consumer price inflation. In our assessment of conformity to the second and third criteria, we use real-time data, i.e., the data available to policymakers at the time of making decisions. We find that while all methods we consider have both strengths and weaknesses, those based on variables thought to determine TUR provide better interpretation and tend to do at least as well as others against the other criteria. These are most promising for future work. Nevertheless, there is considerable uncertainty about the value of TUR, which suggests it would be prudent to use a range of models in research or policy work. While estimates of TUR have declined since the mid-1990s, it is assessed to range between 5.6 and 6.7 per cent in 2018Q4.
C52|Assessing financial stability risks from the real estate market in Italy: an update|We provide an update of the analytical framework to assess financial stability risks arising from the real estate sector in Italy. The enhancement concerns the definition of a new vulnerability indicator, measured in terms of the flow of total non-performing loans (NPLs) and not, as done previously, in terms of bad loans only. We focus separately on households (as an approximation for residential real estate, RRE) and on firms engaged in construction, management and investment services in the real estate sector (as an approximation for commercial real estate, CRE). Two early warning models are estimated using the new vulnerability indicator for RRE and CRE, respectively, as dependent variable. Both models exhibit good forecasting performances: the median predictions fit well the new vulnerability indicators in out-of-sample forecasts. Overall, models’ projections indicate that potential risks for banks stemming from the real estate sector will remain contained in the next few quarters.
C52|VAR-based Granger-causality test in the presence of instabilities|In this article, we review Granger-causality tests robust to the presence of instabilities in a Vector Autoregressive framework. We also introduce the gcrobustvar command, which illustrates the procedure in Stata. In the presence of instabilities, the Granger-causality robust test is more powerful than the traditional Granger-causality test.
C52|The finer points of model comparison in machine learning: forecasting based on russian banks’ data|We evaluate the forecasting ability of machine learning models to predict bank license withdrawal and the violation of statutory capital and liquidity requirements (capital adequacy ratio N1.0, common equity Tier 1 adequacy ratio N1.1, Tier 1 capital adequacy ratio N1.2, N2 instant and N3 current liquidity). On the basis of 35 series from the accounting reports of Russian banks, we form two data sets of 69 and 721 variables and use them to build random forest and gradient boosting models along with neural networks and a stacking model for different forecasting horizons (1, 2, 3, 6, 9 months). Based on the data from February 2014 to October 2018 we show that these models with fine-tuned architectures can successfully compete with logistic regression usually applied for this task. Stacking and random forest generally have the best forecasting performance comparing to the other models. We evaluate models with commonly used performance metrics (ROC-AUC and F1) and show that, depending on the task, F1-score could be better at defining the model’s performance. Comparison of the results depending on the metrics applied and types of cross-validation used illustrate the importance of choosing the appropriate metric for performance evaluation and the cross-validation procedure, which accounts for the characteristics of the data set and the task under consideration. The developed approach shows the advantages of non-linear methods for bank regulation tasks and provides the guidelines for the application of machine learning algorithms to these tasks.
C52|Predicción de precios de vivienda: Aprendizaje estadístico con datos de oferta y transacciones para la ciudad de Montevideo|En este trabajo se presentan modelos predictivos para el precio de un activo de difícil valuación como la vivienda. Se utilizan dos fuentes de datos para la ciudad de Montevideo: una proveniente de sitios web (a través de web scraping) y otra de registros administrativos de transacciones. Se implementan tres modelos fácilmente replicables: modelo lineal, árbol de regresión y bosques aleatorios. Los resultados arrojan una mejor performance del modelo de bosques aleatorios respecto al modelo lineal hedónico, ampliamente difundido en la literatura. Se busca incorporar al análisis de predicción de precios una metodología aún escasamente difundida a nivel nacional, implementada en el software R y poner a disposición una nueva base de datos.
C52|Shapley regressions: a framework for statistical inference on machine learning models|Machine learning models often excel in the accuracy of their predictions but are opaque due to their non-linear and non-parametric structure. This makes statistical inference challenging and disqualifies them from many applications where model interpretability is crucial. This paper proposes the Shapley regression framework as an approach for statistical inference on non-linear or non-parametric models. Inference is performed based on the Shapley value decomposition of a model, a pay-off concept from cooperative game theory. I show that universal approximators from machine learning are estimation consistent and introduce hypothesis tests for individual variable contributions, model bias and parametric functional forms. The inference properties of state-of-the-art machine learning models — like artificial neural networks, support vector machines and random forests — are investigated using numerical simulations and real-world data. The proposed framework is unique in the sense that it is identical to the conventional case of statistical inference on a linear model if the model is linear in parameters. This makes it a well-motivated extension to more general models and strengthens the case for the use of machine learning to inform decisions.
C52|Predicting systemic financial crises with recurrent neural networks|We consider predicting systemic financial crises one to five years ahead using recurrent neural networks. The prediction performance is evaluated with the Jorda-Schularick-Taylor dataset, which includes the crisis dates and relevant macroeconomic series of 17 countries over the period 1870-2016. Previous literature has found simple neural network architectures to be useful in predicting systemic financial crises. We show that such predictions can be greatly improved by making use of recurrent neural network architectures, especially suited for dealing with time series input. The results remain robust after extensive sensitivity analysis.
C52|Estimated human capital externalities in an endogenous growth framework|To better understand the quantitative implications of human capital externalities at the aggregate level, we estimate a two-sector endogenous growth model with knowledge spill-overs. To achieve this, we account for trend growth in a model consistent fashion and employ a Markov-chain Monte-Carlo (MCMC) algorithm to estimate the model's posterior parameter distributions. Using U.S. quarterly data from 1964-2017, we find significant positive externalities to aggregate human capital. Our analysis further shows that eliminating this market failure leads to sizeable increases in education-time, endogenous growth and aggregate welfare.
C52|Mostly Harmless Simulations? Using Monte Carlo Studies for Estimator Selection|We consider two recent suggestions for how to perform an empirically motivated Monte Carlo study to help select a treatment effect estimator under unconfoundedness. We show theoretically that neither is likely to be informative except under restrictive conditions that are unlikely to be satisfied in many contexts. To test empirical relevance, we also apply the approaches to a real-world setting where estimator performance is known. Both approaches are worse than random at selecting estimators which minimise absolute bias. They are better when selecting estimators that minimise mean squared error. However, using a simple bootstrap is at least as good and often better. For now researchers would be best advised to use a range of estimators and compare estimates for robustness.
C52|A Theory of Scenario Generation|We show how distributions can be reduced to low-dimensional scenario trees. Applied to intertemporal distributions, the scenarios and their probabilities become time-varying factors. From S&P 500 options, two or three time-varying scenarios suffice to forecast returns, implied variance or skewness on par, or better, than extant multivariate stochastic volatility jump-diffusion models, while reducing the computational effort to fractions of a second.
C52|Saddlepoint Approximations for Spatial Panel Data Models|We develop new higher-order asymptotic techniques for the Gaussian maximum likelihood estimator of the parameters in a spatial panel data model, with fixed effects, time-varying covariates, and spatially correlated errors. We introduce a new saddlepoint density and tail area approximation to improve on the accuracy of the extant asymptotics. It features relative error of order O(m to the power of -1) for m = n(T -1) with n being the cross-sectional dimension and T the time-series dimension. The main theoretical tool is the tilted-Edgeworth technique. It yields a density approximation that is always non-negative, does not need resampling, and is accurate in the tails. We provide an algorithm to implement our saddlepoint approximation and we illustrate the good performance of our method via numerical examples. Monte Carlo experiments show that, for the spatial panel data model with fixed effects and T = 2, the saddlepoint approximation yields accuracy improvements over the routinely applied first-order asymptotics and Edgeworth expansions, in small to moderate sample sizes, while preserving analytical tractability. An empirical application on the investment-saving relationship in OECD countries shows disagreement between testing results based on first-order asymptotics and saddlepoint techniques, which questions some implications based on the former.
C52|Arbitrage Free Dispersion|We develop a theory of arbitrage-free dispersion (AFD) that characterizes the testable restrictions of asset pricing models. AFD measures Jensen’s gap in the cumulant generating function of pricing kernels and returns. It implies a wide family of model-free dispersion constraints, which extend dispersion and co-dispersion bounds in the literature and are applicable with a unifying approach in multivariate and multiperiod settings. Empirically, the dispersion of stationary and martingale pricing kernel components in the benchmark long-run risk model yields a counterfactual dependence of short- vs. long-maturity bond returns and is insufficient for pricing optimal portfolios of market equity and short-term bonds.
C52|Estimation of Large Dimensional Conditional Factor Models in Finance|This chapter provides an econometric methodology for inference in large-dimensional conditional factor models in finance. Changes in the business cycle and asset characteristics induce time variation in factor loadings and risk premia to be accounted for. The growing trend in the use of disaggregated data for individual securities motivates our focus on methodologies for a large number of assets. The beginning of the chapter outlines the concept of approximate factor structure in the presence of conditional information, and develops an arbitrage pricing theory for large-dimensional factor models in this framework. Then we distinguish between two different cases for inference depending on whether factors are observable or not. We focus on diagnosing model specification, estimating conditional risk premia, and testing asset pricing restrictions under increasing cross-sectional and time series dimensions. At the end of the chapter, we review some of the empirical findings and contrast analysis based on individual stocks and standard sets of portfolios. We also discuss the impact on computing time-varying cost of equity for a firm, and summarize differences between results for developed and emerging markets in an international setting.
C52|Efectos de las variaciones del IPC en las decisiones financieras|En este documento se desarrolló un análisis de mercado que derivó en un modelo econométrico con miras a determinar el comportamiento del Índice de Precios al Consumidor en un horizonte de tiempo de 2 años, para dar apoyo a la toma de decisiones financieras de inversión y financiamiento. En el análisis se tuvieron en cuenta las Encuestas a expertos y los Pronósticos a entidades financieras. Sin embargo, al enfrentar dicha información con el IPC observado, se concluyó que los pronósticos y encuestas mencionadas no tenían una capacidad de predicción a dos años confiable. Debido a que el mercado no permitió cumplir con el objetivo propuesto, fue necesario desarrollar un modelo econométrico de tipo ARIMA con datos mensuales desde entre enero de 2010 y diciembre de 2018. En la construcción del modelo se determinó que la volatilidad del IPC estaba fuertemente influida por el precio de los alimentos, y por ende serían el fenómeno del niño y los paros de transporte las variables idóneas en la conformación del modelo. Como resultado, se obtuvo una proyección del IPC a dos años. No obstante, el pronóstico presentó una desviación estándar considerable y creciente en el tiempo que redujo la efectividad del modelo a un año. En el desarrollo del modelo como paso a seguir, se plantea necesario realizar una función impulso respuesta de las variables Dummy y adaptar el modelo a la nueva metodología del IPC propuesta por el DANE para 2019. *** In this document, a market analysis was developed which derived in an econometric model with a view to determining the behaviour of the Consumer Price Index in a time horizon of 2 years, to give support to the financial decision making of investment and financing. The analysis took into account the Surveys to experts and the Forecasts to financial entities. However, when facing said information with the observed CPI, it was concluded that the forecasts and surveys mentioned did not have a reliable two-year prediction capacity. Since the market did not comply with the proposed objective, it was necessary to develop an ARIMA-type econometric model with monthly data from January 2010 to December 2018. In the construction of the model, it was determined that the volatility of the CPI was strongly influenced by the food price, and therefore the El Niño phenomenon and transport stoppages would be the ideal variables in shaping the model. As a result, a two-year CPI projection was obtained. However, the forecast presented a considerable and increasing standard deviation over time that reduced the effectiveness of the model to one year. In the development of the model as a step to follow, it is necessary to carry out a response impulse function of the Dummy variables and to adapt the model to the new CPI methodology proposed by DANE for 2019.
C52|Una breve aplicación a la predicción de la fragilidad de empresas colombianas, mediante el uso de modelos estadísticos|Resumen: Este trabajo estima diferentes modelos estadísticos para medir la probabilidad de riesgo de quiebra empresarial e identificar cuál de ellos presenta un mejor desempeño predictivo. Para lograr este objetivo, se emplean los estados financieros de las empresas colombianas para el 2015 con el fin construir indicadores financieros como variables explicativas en los modelos empleados. Las variables más relevantes para medir la probabilidad de quiebra fueron la rentabilidad del patrimonio y el nivel de endeudamiento. Entre los modelos estimados (logístico, logístico heterocedástico, logístico robusto y logístico mixto), el logístico mixto fue el que presentó el mejor desempeño para predecir la fragilidad empresarial. / Abstract : This manuscript estimates different statistical models to measure the probability of risk business failure, and identifies which one has better predicting performance. In order to achieve this objective, the financial statements of Colombian companies in 2015 are used in order to build financial indicators as explanatory variables in the used models. The most relevant variables to measure the probability of business failure were the return on equity and debt ratio. Among the estimated models (logistic, heteroscedastic logistic, robust logistic and mixed logistic), the mixed logistic has the best performance in predicting the corporate fragility.
C52|Marginal jobs and job surplus: a test of the efficiency of separations|"We present a sharp test for the efficiency of job separations. First, we document a dramatic increase in the separation rate – 11.2ppt (28%) over five years – in response to a quasi-experimental extension of UI benefit duration for older workers. Second, after the abolition of the policy, the ""job survivors"" in the formerly treated group exhibit exactly the same separation behavior as the control group. Juxtaposed, these facts reject the ""Coasean"" prediction of efficient separations, whereby the UI extensions should have extracted marginal (low-surplus) jobs and thereby rendered the remaining (high-surplus) jobs more resilient after its abolition. Third, we show that a formal model of predicted efficient separations implies a piece-wise linear function of the actual control group separations beyond the missing mass of marginal matches. A structural estimation reveals point estimates of the share of efficient separations below 4%, with confidence intervals rejecting shares above 13%. Fourth, to characterize the marginal jobs in the data, we extend complier analysis to difference-in-difference settings such as ours. The UI-indiced separators stemmed from declining firms, blue-collar jobs, with a high share of sick older workers, and firms more likely to have works councils – while their wages were similar to program survivors. The evidence is consistent with a ""non-Coasean"" framework building on wage frictions preventing efficient bargaining, and with formal or informal institutional constraints on selective separations."
C52|The missing link: monetary policy and the labor share|The textbook New-Keynesian (NK) model implies that the labor share is pro-cyclical conditional on a monetary policy shock. We present evidence that a monetary policy tightening robustly increased the labor share and decreased real wages and labor productivity during the Great Moderation period in the US, the Euro Area, the UK, Australia, and Canada. We show that this is inconsistent not only with the basic NK model, but with a wide variety of NK models commonly used for monetary policy analysis and where the direct link between the labor share and the markup can be broken down.
C52|The Promise and Pitfalls of Conflict Prediction: Evidence from Colombia and Indonesia|"Policymakers can take actions to prevent local conflict before it begins, if such violence can be accurately predicted. We examine the two countries with the richest available sub-national data: Colombia and Indonesia. We assemble two decades of finegrained violence data by type, alongside hundreds of annual risk factors. We predict violence one year ahead with a range of machine learning techniques. Models reliably identify persistent, high-violence hot spots. Violence is not simply autoregressive, as detailed histories of disaggregated violence perform best. Rich socio-economic data also substitute well for these histories. Even with such unusually rich data, however, the models poorly predict new outbreaks or escalations of violence. ""Best case"" scenarios with panel data fall short of workable early-warning systems."
C52|Testing Constancy in Varying Coefficient Models|This article proposes tests for constancy of coefficients in semi-varying coefficients models. The testing procedure resembles in spirit the union-intersection parameter stability tests in time series, where observations are sorted according to the explanatory variable responsible for the coefficients varying. The test can be applied to model specification checks of interactive effects in linear regression models. Because test statistics are not asymptotically pivotal, critical values and p-values are estimated using a bootstrap technique. The finite sample properties of the test are investigated by means of Monte Carlo experiments, where the new proposal is compared to existing tests based on smooth estimates of the unrestricted model. We also report an application to returns of education modeling
C52|Score-driven time series models with dynamic shape : an application to the Standard & Poor's 500 index|We introduce new dynamic conditional score (DCS) volatility models with dynamic scale and shape parameters for the effective measurement of volatility. In the new models, we use the EGB2 (exponential generalized beta of the second kind), NIG (normal-inverse Gaussian) and Skew-Gen-t (skewed generalized-t) probability distributions. Those distributions involve several shape parameters that control the dynamic skewness, tail shape and peakedness of financial returns. We use daily return data from the Standard & Poor's 500 (S&P 500) index for the period of January 4, 1950 to December 30, 2017. We estimate all models by using the maximum likelihood (ML) method, and we present the conditions of consistency and asymptotic normality of the ML estimates. We study those conditions for the S&P 500 and we also perform diagnostic tests for the residuals. The statistical performances of several DCS specifications with dynamic shape are superior to the statistical performance of the DCS specification with constant shape. Outliers in the shape parameters are associated with important announcements that affected the United States (US) stock market. Our results motivate the application of the new DCS models to volatility measurement, pricing financial derivatives, or estimation of the value-at-risk (VaR) and expected shortfall (ES) metrics.
C52|Co-integration and common trends analysis with score-driven models : an application to the federal funds effective rate and US inflation rate|Co-integration and common trends are studied for time series variables, by introducing the new t-QVARMA (quasi-vector autoregressive moving average) model. t-QVARMA is an outlier-robust nonlinear score-driven model for the multivariate t-distribution. In t-QVARMA, the I(0) and I(1) components of the variables are separated in a way that is similar to the Granger-representation of VAR models. The relationship between the co-integrated federal funds effective rate and United States (US) inflation rate variables is studied for the period of July 1954 to January 2019. The in-sample statistical and out-of-sample forecasting performances of t-QVARMA are superior to those of the classical Gaussian-VAR model
C52|Maximum likelihood estimation of score-driven models with dynamic shape parameters : an application to Monte Carlo value-at-risk|Dynamic conditional score (DCS) models with time-varying shape parameters provide a exible method for volatility measurement. The new models are estimated by using the maximum likelihood (ML) method, conditions of consistency and asymptotic normality of ML are presented, and Monte Carlo simulation experiments are used to study the precision of ML. Daily data from the Standard & Poor's 500 (S&P 500) for the period of 1950 to 2017 are used. The performances of DCS models with constant and dynamic shape parameters are compared. In-sample statistical performance metrics and out-of-sample value-at-risk backtesting support the use of DCS models with dynamic shape.
C52|Implications of partial information for econometric modeling of macroeconomic systems|Representative models of the macroeconomy (RMs), such as DSGE models, frequently contain unobserved variables. A finite-order VAR representation in the observed variables may not exist, and therefore the impulse responses of the RMs and SVAR models may differ. We demonstrate this divergence often is: (i) not substantial; (ii) reflects the omission of stock variables from the VAR; and (iii) when the RM features I (1) variables can be ameliorated by estimating a latent-variable VECM. We show that DSGE models utilize identifying restrictions stemming from common factor dynamics reflecting statistical, not economic, assumptions. We analyze the use of measurement error, and demonstrate that it may result in unintended consequences, particularly in models featuring I (1) variables.
C52|Monetary Policy, Inflation Target and the Great Moderation: An Empirical Investigation|This paper compares the empirical fit of a Taylor rule featuring constant versus time-varying inflation target by estimating a Generalized New Keynesian model under positive trend inflation while allowing for indeterminacy. The estimation is conducted over two different periods covering the Great Inflation and the Great Moderation. We find that the rule embedding time variation in target inflation turns out to be empirically superior and determinacy prevails in both sample periods. Counterfactual simulations point toward both `good policy' and `good luck' as drivers of the Great Moderation. We find that better monetary policy, both in terms of a more active response to inflation gap and a more anchored inflation target, has resulted in the decline in inflation gap volatility and predictability. In contrast, the reduction in output growth variability is mainly explained by reduced volatility of technology shocks.
C52|Asymmetric conjugate priors for large Bayesian VARs|Large Bayesian VARs are now widely used in empirical macroeconomics. One popular shrinkage prior in this setting is the natural conjugate prior as it facilitates posterior simulation and leads to a range of useful analytical results. This is, however, at the expense of modelling exibility, as it rules out cross-variable shrinkage – i.e. shrinking coefficients on lags of other variables more aggressively than those on own lags. We develop a prior that has the best of both worlds: it can accommodate cross-variable shrinkage, while maintaining many useful analytical results, such as a closed-form expression of the marginal likelihood. This new prior also leads to fast posterior simulation - for a BVAR with 100 variables and 4 lags, obtaining 10,000 posterior draws takes less than half a minute on a standard desktop. In a forecasting exercise, we show that a data-driven asymmetric prior outperforms two useful benchmarks: a data-driven symmetric prior and a subjective asymmetric prior.
C52|Minnesota-type adaptive hierarchical priors for large Bayesian VARs|Large Bayesian VARs with stochastic volatility are increasingly used in empirical macroeconomics. The key to make these highly parameterized VARs useful is the use of shrinkage priors. We develop a family of priors that captures the best features of two prominent classes of shrinkage priors: adaptive hierarchical priors and Minnesota priors. Like the adaptive hierarchical priors, these new priors ensure that only ‘small’ coefficients are strongly shrunk to zero, while ‘large’ coefficients remain intact. At the same time, these new priors can also incorporate many useful features of the Minnesota priors, such as cross-variable shrinkage and shrinking coefficients on higher lags more aggressively. We introduce a fast posterior sampler to estimate BVARs with this family of priors - for a BVAR with 25 variables and 4 lags, obtaining 10,000 posterior draws takes about 3 minutes on a standard desktop. In a forecasting exercise, we show that these new priors outperform both adaptive hierarchical priors and Minnesota priors.
C52|Turning point and oscillatory cycles|The early history of cycles research involved locating turning points in the data. Later, the development of methods such as spectral analysis led to a focus on oscillations. A distinction between cycles and oscillations is needed - both imply turning points, but turning points do not necessarily imply oscillations. Comin and Gertler (2006) argue that attention should be paid to medium term oscillations of 8-30 years rather than the standard 2-8 years of the business cycle, while Beaudry et al. (2019) suggest that there is a cycle of 9/10 years in series such as hours per capita. We investigate what the evidence is for the latter and find that it explains little of the variance of the data. We then show that some of the fillters being used to locate either turning points or oscillations in the series are not appropriate for the nature of the series being analyzed, specifically whether they are I(1) or I(0). Finally, we assess if the concepts of medium term and 9/10 year cycles are useful for comparing models and data. This is done by examining models of endogenous versus exogenous technology as well as limit cycles due to accumulation and complementarity mechanisms.
C52|CO2 Emissions, Energy Consumption and Economic Growth|The paper investigates the role of consumption of both renewable and sustainable energy, as well as alternative and nuclear energy, in mitigating the effects of carbon dioxide (CO2) emissions, based on the Environmental Kuznets Curve (EKC). The papers introduces a novel variable to capture trade openness, which appears to be a crucial factor in inter-regional co-operation and development, in order to evaluate its effect on the environment, The empirical analysis is based on a sample of nine signatories to the Comprehensive and Progressive Agreement for Trans-Pacific Partnership (CPTPP) for the period 1971-2014, which is based on data availability. The empirical analysis is based on several time series econometric methods, such as the cointegration test, two long run estimators, namely the fully modified ordinary least squares (FMOLS) and dynamic ordinary least squares (DOLS) methods, as well as the Granger causality test. There are several noteworthy empirical findings: it is possible to confirm the U-shaped EKC hypothesis for six countries, namely Australia, Canada, Chile, New Zealand, Peru and Vietnam; there is no evidence of the EKC for Mexico; a reverse-shaped EKC is observed for Japan and Malaysia, there are long run relationships among the variables, the adoption of either renewable energy, or alternative energy and nuclear energy, mitigates CO2 emissions, trade openness leads to more beneficial than harmful impacts in the long run, the Granger causality tests show more bi-directional-relationships between the variables in the long run, and the Granger causality tests show more uni-directional-relationships between the variables in the short run.
C52|What They Did Not Tell You about Algebraic (Non-) Existence, Mathematical (IR-)Regularity, and (Non-) Asymptotic Properties of the Dynamic Conditional Correlation (DCC) Model|In order to hedge efficiently, persistently high negative covariances or, equivalently, correlations, between risky assets and the hedging instruments are intended to mitigate against financial risk and subsequent losses. If there is more than one hedging instrument, multivariate covariances and correlations have to be calculated. As optimal hedge ratios are unlikely to remain constant using high frequency data, it is essential to specify dynamic time-varying models of covariances and correlations. These values can either be determined analytically or numerically on the basis of highly advanced computer simulations. Analytical developments are occasionally promulgated for multivariate conditional volatility models. The primary purpose of this paper is to analyze purported analytical developments for the only multivariate dynamic conditional correlation model to have been developed to date, namely the widely used Dynamic Conditional Correlation (DCC) model. Dynamic models are not straightforward (or even possible) to translate in terms of the algebraic existence, underlying stochastic processes, specification, mathematical regularity conditions, and asymptotic properties of consistency and asymptotic normality, or the lack thereof. This paper presents a critical analysis, discussion, evaluation, and presentation of caveats relating to the DCC model, with an emphasis on the numerous dos and don’ts in implementing the DCC model, as well as a related model, in practice.
C52|New Misspecification Tests for Multinomial Logit Models|Misspecification tests for Multinomial Logit [MNL] models are known to have low power or large size distortion. We propose two new misspecification tests. Both use that preferences across binary pairs of alternatives can be described by independent binary logit models when MNL is true. The first test compares Composite Likelihood parameter estimates based on choice pairs with standard Maximum Likelihood estimates using a Hausman (1978) test. The second tests for overidentification in a GMM framework using more pairs than necessary. A Monte Carlo study shows that the GMM test is in general superior with respect to power and has correct size
C52|Estimates of quarterly GDP growth using MIDAS regressions|This paper provides new estimates of year-to-year quarterly real GDP growth in Suriname for 2013Q1 to 2018Q4. The methodology to arrive at these estimates consists of the following steps. Using the familiar Chow and Lin method, the available annual data are disaggregated into a first round of quarterly data. The quarterly data are then included in a MIDAS model, which links the quarterly observations with a new but well established monthly observed indicator of economic activity. The best-performing MIDAS model is then used to update the initial estimates of quarterly GDP growth to final estimates, which in turn can be used in macro-economic modelling and analysis.
C52|Linkages between Globalisation, Carbon dioxide emissions and Governance in Sub-Saharan Africa|This study investigates linkages between environmental degradation, globalisation and governance in 44 countries in Sub-Saharan Africa using data for the period 2000-2012. The Generalised Method of Moments is employed as empirical strategy. Environmental degradation is proxied by carbon dioxide emissions whereas globalisation is appreciated in terms of trade openness and net foreign direct investment inflows. Bundled and unbundled governance indicators are used, namely: political governance (consisting of political stability/no violence and “voice & accountability”), economic governance (encompassing government effectiveness and regulation quality), institutional governance (entailing corruption-control and the rule of law) and general governance (a composite measurement of political governance, economic governance and institutional governance). The following main finding is established. Trade openness modulates carbon dioxide emissions to have positive net effects on political stability, economic governance, the rule of law and general governance.
C52|The persistence of global terrorism|This study investigates persistence of global terrorism in a panel of 163 countries for the period 2010 to 2015. The empirical evidence is based on Generalised Method of Moments. The following findings are established. First, persistence in terrorism is a decreasing function of income levels because it consistently increases from high income (through upper middle income) to lower middle income countries. Second, compared to Christian-oriented countries, terrorism is more persistent in Islam-oriented nations. Third, landlocked countries also reflect a higher level of persistence relative to their coastal counterparts. Fourth, Latin American countries show higher degrees of persistence when compared with Middle East and North African (MENA) countries. Fifth, the main determinants of the underlying persistence are political instability and weapons import. The results are discussed to provide answers to four main questions which directly pertain to the reported findings. These questions centre on why comparative persistence in terrorism is based on income levels, religious orientation, landlockedness and regions.
C52|FDI in Selected Developing Countries: Evidence from Bundling and Unbundling Governance|The objective of this study is to assess governance drivers of FDI in a panel of BRICS and MINT countries for the period 2001-2011. We bundle and unbundle governance determinants using a battery of contemporary and non-contemporary estimation techniques. Our findings reveal the following: Firstly, for both contemporary and non-contemporary specifications, while the majority of our governance determinants of Gross FDI are significant, they are overwhelmingly insignificant for Net FDI. Secondly, the significance of the governance dynamics in increasing order of magnitude are general governance, political governance, economic governance, political stability, regulation quality and government effectiveness. Thirdly, for non-contemporary specifications, the significance of governance variables is as follows in ascending order of magnitude: economic governance, institutional governance, general governance, corruption-control, political governance and political stability. The importance of combining governance indicators is captured by the effects of political governance, economic governance and institutional governance. The results indicate that the simultaneous implementation of the various components of governance clarifies a country’s attractiveness for FDI location. Policy implications are discussed with particular emphasis on the timing of FDI and its targeting.
C52|Environmental Pollution, Economic Growth and Institutional Quality: Exploring the Nexus in Nigeria|The interaction between environmental pollution and economic growth determines the achievement of the green growth objective of developing economies. An economy turns around the inverted U-shaped Environmental Kuznets Curve (EKC) when pollution is effectively dampened by social, political and economic factors as such economy grows. Thus, this study examines the EKC considering the impact of institutional quality on six variables of environmental pollution [carbon dioxide (CO2), Nitrous Oxide (N2O), Suspended Particulate Maters (SPM), Rainfall, Temperature and Total Green House Emission (TGH)] using the case of Nigeria. The EKC model includes population density, education expenditure, foreign direct investment, and gross domestic investment as control variables, and it was analysed using the Auto Regressive Distribution Lag (ARDL) econometric technique, which has not been applied in the literature on Nigeria. The results, inter alia, indicate that there is EKC for CO2 and SPM. This implies that the green growth objective can be pursued in Nigeria with concerted efforts. Other environmental pollution indicators did not exert significant influence on economic growth. Therefore, it is recommended that Nigeriaâ€™s institutional quality be strengthened to limit environmental pollution in light of economic growth.
C52|Medidas De Riesgo Financiero Usando Cópulas: Teoría Y Aplicaciones|Este documento realiza una descripción de las medidas de dependencia consus principales ventajas y desventajas y presenta a la cópula como una estructura flexibleque permite caracterizar diferentes tipos de dependencia. Adicionalmente, introduce eluso de la cópula en la medici´on de riesgo financiero, tomando como ejemplo un portafoliocompuesto por tres activos representativos del mercado colombiano.Las pruebas de desempeño o de backtesting del valor en riesgo calculado por diferentesmetodologías en los años 2006 y 2007 muestran que las mejores son aquellas que modelanla dependencia en media y varianza, tales como modelos VAR-GARCH-C´opula(t) yVAR-GARCH-Co´pula(normal). Las técnicas con el peor desempeño son RiskmetricsR yla basada en el supuesto de normalidad.
C52|Bayesian combination for inflation forecasts: The effects of a prior based on central banks’ estimates|Typically, central banks use a variety of individual models (or a combination of models) when forecasting inflation rates. Most of these require excessive amounts of data, time, and computational power, all of which are scarce when monetary authorities meet to decide over policy interventions. In this paper we use a rolling Bayesian combination technique that considers inflation estimates by the staff of the Central Bank of Colombia during 2002–2011 as prior information. Our results show that: (1) the accuracy of individual models is improved by using a Bayesian shrinkage methodology, and (2) priors consisting of staff estimates outperform all other priors that comprise equal or zero vector weights. Consequently, our model provides readily available forecasts that exceed all individual models in terms of forecasting accuracy at every evaluated horizon.
C52|High-dimensional macroeconomic forecasting using message passing algorithms|This paper proposes two distinct contributions to econometric analysis of large information sets and structural instabilities. First, it treats a regression model with time-varying coeﬃcients, stochastic volatility and exogenous predictors, as an equivalent high-dimensional static regression problem with thousands of covariates. Inference in this speciﬁcation proceeds using Bayesian hierarchical priors that shrink the high-dimensional vector of coeﬃcients either towards zero or time-invariance. Second, it introduces the frameworks of factor graphs and message passing as a means of designing eﬃcient Bayesian estimation algorithms. In particular, a Generalized Approximate Message Passing (GAMP) algorithm is derived that has low algorithmic complexity and is trivially parallelizable. The result is a comprehensive methodology that can be used to estimate time-varying parameter regressions with arbitrarily large number of exogenous predictors. In a forecasting exercise for U.S. price inﬂation this methodology is shown to work very well.
C52|The Role of Credit Guarantee Schemes in the Development of Small and Medium-Sized Enterprises with an Emphasis on Knowledge-Based Enterprises|Small and medium-sized enterprises (SMEs) in their growth stage reach the point where, on the one hand, personal resources do not meet their needs, and, on the other, they do not have enough collateral to attract external finance. Access to finance can be facilitated by obtaining loans from financial institutions backed by governmental credit guarantees. Therefore, the development of a sound credit guarantee scheme will be an important step in filling the financing gap of SMEs. We investigate the situation of the credit guarantee scheme for SMEs in Iran by using the available data and interviews with activists from this field with the grounded theory method. We show the weaknesses of the Iranian credit guarantee scheme, and based on the analysis, present solutions and policy recommendations in accordance with the social and economic environment of the Islamic Republic of Iran. The most important problem is the lack of a credit database for comprehensive assessment of SMEs, especially knowledge-based enterprises. The lack of a robust database makes it impossible to carry out a comprehensive evaluation because these models require a large amount of data. The lack of accurate models makes it difficult to rate credit status and thus to issue credit guarantees. In addition, the current level of the capital of the credit guarantee funds in Iran is not sufficient given the large number of SMEs in the country.
C52|A Horse Race in High Dimensional Space|In this paper, we study the predictive power of dense and sparse estimators in a high dimensional space. We propose a new forecasting method, called Elastically Weighted Principal Components Analysis (EWPCA) that selects the variables, with respect to the target variable, taking into account the collinearity among the data using the Elastic Net soft thresholding. Then, we weight the selected predictors using the Elastic Net regression coefficient, and we finally apply the principal component analysis to the new “elastically” weighted data matrix. We compare this method to common benchmark and other methods to forecast macroeconomic variables in a data-rich environment, dived into dense representation, such as Dynamic Factor Models and Ridge regressions and sparse representations, such as LASSO regression. All these models are adapted to take into account the linear dependency of the macroeconomic time series. Moreover, to estimate the hyperparameters of these models, including the EWPCA, we propose a new procedure called “brute force”. This method allows us to treat all the hyperparameters of the model uniformly and to take the longitudinal feature of the time-series data into account. Our findings can be summarized as follows. First, the “brute force” method to estimate the hyperparameters is more stable and gives better forecasting performances, in terms of MSFE, than the traditional criteria used in the literature to tune the hyperparameters. This result holds for all samples sizes and forecasting horizons. Secondly, our two-step forecasting procedure enhances the forecasts’ interpretability. Lastly, the EWPCA leads to better forecasting performances, in terms of mean square forecast error (MSFE), than the other sparse and dense methods or naïve benchmark, at different forecasts horizons and sample sizes.
C52|A Principled Approach to Assessing Missing-Wage Induced Selection Bias|Multiple imputation (MI) techniques are applied to simulate missing wage rates of non-working wives under the missing-at-random (MAR) condition. The assumed selection effect of the labour force participation decision is framed as deviations of the imputed wage rates from MAR. By varying the deviations, we assess the severity of subsequent selection bias in standard human capital models through sensitivity analyses (SA). Our experiments show that the bias remains largely insignificant. While similar findings are possibly attainable through the Heckman procedure, SA under the MI approach provides a more structured and principled approach to assessing selection bias.
C52|Deep Haar Scattering Networks in Unidimensional Pattern Recognition Problems|The aim of this paper is to discuss the use of Haar scattering networks, which is a very simple architecture that naturally supports a large number of stacked layers, yet with very few parameters, in a relatively broad set of pattern recognition problems, including regression and classification tasks. This architecture, basically, consists of stacking convolutional filters, that can be thought as a generalization of Haar wavelets, followed by nonlinear operators which aim to extract symmetries and invariances that are later fed in a classification/regression algorithm. We show that good results can be obtained with the proposed method for both kind of tasks. We outperformed the best available algorithms in 4 out of 18 important data classification problems, and obtained a more robust performance than ARIMA and ETS time series methods in regression problems for data with invariances and symmetries, with desirable features, such as possibility to evaluate parameter stability and easy structural assessment.
C52|Robustness of the Norwegian wage formation system and free EU labour movement. Evidence from wage data for natives|Norway experienced a high immigration flow after the EEA directive in 2004 stating workers right to free movement within the European Union and EEA-countries. There is no clear consensus in the literature on how immigration affects native wages, but some studies using Norwegian micro data have estimated a negative effect of higher immigration for some type of workers. In this paper, to capture that the wage setting is highly coordinated in Norway, we model a system of native wages for three sectors; manufacturing, private service industries and public sector. We estimate that labour immigration has had a negative effect on the attainable wage growth for natives in all three sectors, but that the largest and most direct impact on wages has been in the private service industries. Immigration is found to be exogenous with respect to the parameters of our model of wage formation.
C52|The consumption Euler equation or the Keynesian consumption function?|We formulate a general cointegrated vector autoregressive (CVAR) model that nests both a class of consumption Euler equations and various Keynesian type consumption functions. Using likelihoodbased methods and Norwegian data, we find support for cointegration between consumption, income and wealth once a structural break around the financial crisis is allowed for. That consumption cointegrates with both income and wealth and not only with income points to the empirical irrelevance of an Euler equation. Moreover, we find that consumption equilibrium corrects to changes in income and wealth and not that income equilibrium corrects to changes in consumption, which would be the case if an Euler equation is true. We also find that most of the parameters stemming from the class of Euler equations are not corroborated by the data when considering conditional expectations of future consumption and income in CVAR models. Only habit formation seems important in explaining the Norwegian consumer behaviour. Our preferred model is a dynamic Keynesian type consumption function with a first year marginal propensity to consume out of income close to 25 per cent.
C52|The impact of public R&D support on firms' patenting|We examine the impact of both R&D tax credits and direct R&D subsidies on Norwegian firms' patenting. Whereas direct subsidies are aimed at projects with low private and high social return, tax credits do not discriminate between projects or technologies. We find that both direct subsidies and tax credits have significant positive effects on patenting. However, the magnitude of the effects depend critically on the firms' pre-treatment characteristics. In particular, the statistically significant estimates are all related to firms with no patent applications prior to obtaining support. Moreover, we estimate that direct subsidies have triggered at least three times as many granted patents per NOK million of support compared to tax credits. Our results suggest that R&D support should be directed to promote innovations at the extensive margin, i.e. to firms with a high potential of becoming innovative rather than to firms with a record of being innovative. Moreover, as targeted subsidies generate more innovations, society would benefit from distributing more of the subsidies to priority areas.
C52|Yield Curve Dynamics and Fiscal Policy Shocks|We use an affine term structure model with time-varying macro trends and a vector autoregression model to investigate the response of the US Treasury yield curve to changes in fiscal policy. By accounting for the timing of the fiscal policy in the shock identification we can separate the effect of news about future increases in government spending from the effect of innovations in changes of current government expenditures. Further, we use the Baker, Bloom, and Davis (2016) uncertainty index dataset to explain the flight to quality type of events. By controlling for the low frequency movement in yields and the decomposition of yield to risk neutral rates and term premia we show that the news channel is driven by a cautious response of agents to an increase in projected future government spending and leads to a drop in yields. This result contrasts with shock into contemporaneous spending which has no significant impact on bond yields.
C52|A Bayesian VAR Approach to Short-Term Inflation Forecasting|In this paper, we discuss the forecasting performance of Bayesian vector autoregression (BVAR) models for inflation under alternative specifications. In particular, we consider modelling in levels or in differences; choice of tightness; estimating BVARs of different model sizes and the accuracy of conditional and unconditional forecasts. Our empirical results point out that BVAR forecasts using variables in log-difference form outperform the ones using log-levels of the data. When we evaluate forecast performance in terms of model size, the lowest forecast errors belong to the models having relatively small number of variables, though we find only small difference in forecast accuracy among models of various sizes up to two quarter ahead. Finally, the conditioning seems to help to forecast inflation. Overall, pseudo evaluation findings suggest that small to medium size BVAR models having wisely selected variables in difference form and conditioning on the future paths of some variables appear to be a good choice to forecast inflation in Turkey.
C52|Do early-ending conditional cash transfer programs crowd out school enrollment?|This paper explores how a conditional cash transfer program influences students’ schooling decisions when program payments stop in the middle of the school career. To that end, I examine Mexico’s Progresa, which covered students only until the end of middle school (at age 15) in its early years. The experimental setup permits to study the program’s impact on the probability to continue with high school after middle school. Despite initial randomization, the program itself has likely rendered the respective samples of middle school graduates in the treatment and the control group incomparable. To account for this, I employ a newly developed semiparametric technique that uses a combination of machine learning methods in conjunction with doubly-robust estimation. I find that exposure to Progresa during middle school reduced the probability to transfer to high school by 10 to 14 percentage points. Possible explanations for this effect include parents’ loss aversion, motivation crowding, anchoring, and classroom peer effects.
C52|A dominance approach for comparing the performance of VaR forecasting models|We introduce three dominance criteria to compare the performance of alternative VaR forecasting models. The three criteria use the information provided by a battery of VaR validation tests based on the frequency and size of exceedances, offering the possibility of efficiently summarizing a large amount of statistical information. They do not require the use of any loss function defined on the difference between VaR forecasts and observed returns, and two of the criteria are not conditioned on any significance level for the VaR tests. We use them to explore the potential for 1-day ahead VaR forecasting of some recently proposed asymmetric probability distributions for return innovations, as well as to compare the APARCH and FGARCH volatility specifications with more standard alternatives. Using 19 assets of different nature, the three criteria lead to similar conclusions, suggesting that the unbounded Johnson SU, the skewed Student-t and the skewed Generalized-t distributions seem to produce the best VaR forecasts. The added flexibility of a free power parameter in the conditional volatility in the APARCH and FGARCH models leads to a better fit to return data, but it does not improve upon the VaR forecasts provided by GARCH and GJR-GARCH volatilities.
C52|Time-Series Momentum: A Monte-Carlo Approach|This paper develops a Monte-Carlo backtesting procedure for risk premia strategies and employs it to study Time-Series Momentum (TSM). Relying on time-series models, empirical residual distributions and copulas we overcome two key drawbacks of conventional backtesting procedures. We create 10,000 paths of different TSM strategies based on the S&P 500 and a cross-asset class futures portfolio. The simulations reveal a probability distribution which shows that strategies that outperform Buy-and-Hold in-sample using historical backtests may out-of-sample i) exhibit sizeable tail risks ii) underperform or outperform. Our results are robust to using different time-series models, time periods, asset classes, and risk measures.
C52|Macroeconomic simulation comparison with a multivariate extension of the Markov Information Criterion|Comparison of macroeconomic simulation models, particularly agent-based models (ABMs), with more traditional approaches such as VAR and DSGE models has long been identified as an important yet problematic issue in the literature. This is due to the fact that many such simulations have been developed following the great recession with a clear aim to inform policy, yet the methodological tools required for validating these models on empirical data are still in their infancy. The paper aims to address this issue by developing and testing a comparison framework for macroeconomic simulation models based on a multivariate extension of the Markov Information Criterion (MIC) originally developed in Barde (2017). The MIC is designed to measure the informational distance between a set of models and some empirical data by mapping the simulated data to the markov transition matrix of the underlying data generating process, and is proven to perform optimally (i.e. the measurement is unbiased in expectation) for all models reducible to a markov process. As a result, not only can the MIC provide an accurate measure of distance solely on the basis of simulated data, but it can do it for a very wide class of data generating processes. The paper first presents the strategies adopted to address the computational challenges that arise from extending the methodology to multivariate settings and validates the extension on VAR and DGSE models. The paper then carries out a comparison of the benchmark ABM of Caiani et al. (2016) and the DGSE framework of Smets and Wouters (2007), which to our knowledge, is the first direct comparison between a macroeconomic ABM and a DGSE model.
C52|How Large is the Demand for Money at the ZLB? Evidence from Japan|This paper estimates a money demand function using Japanese data from 1985 to 2017, which includes the period of near-zero interest rates over the last two decades. We compare a log-log specification and a semi-log specification by employing the methodology proposed by Kejriwal and Perron (2010) on cointegrating relationships with structural breaks. Our main finding is that there exists a cointegrating relationship with a single break between the money-income ratio and the interest rate in the case of the log-log form but not in the case of the semi-log form. More specifically, we show that the substantial increase in the money-income ratio during the period of near-zero interest rates is well captured by the log-log form but not by the semi-log form. We also show that the demand for money did not decline in 2006 when the Bank of Japan terminated quantitative easing and started to raise the policy rate, suggesting that there was an upward shift in the money demand schedule. Finally, we find that the welfare gain from moving from 2 percent inflation to price stability is 0.10 percent of nominal GDP, which is more than six times as large as the corresponding estimate for the United States.
C52|VAR-Based Granger-Causality Test in the Presence of Instabilities|In this article, we review Granger-causality tests robust to the presence of instabilities in a Vector Autoregressive framework. We also introduce the gcrobustvar command, which illustrates the procedure in Stata. In the presence of instabilities, the Granger-causality robust test is more powerful than the traditional Granger-causality test.
C52|Governance, CO2 emissions and Inclusive Human Development in Sub-Saharan Africa|This study investigates the relevance of government quality in moderating the incidence of environmental degradation on inclusive human development in 44 sub-Saharan African countries for the period 2000-2012. Environmental degradation is measured with CO2 emissions and the governance dynamics include: political stability, voice and accountability, government effectiveness, regulation quality, the rule of law and corruption-control. The empirical evidence is based on the Generalised Method of Moments. Regulation quality modulates CO2 emissions to exert a net negative effect on inclusive development. Institutional governance (consisting of corruption-control and the rule of law) modulates CO2 emissions to also exert a net negative effect on inclusive human development. Fortunately, the corresponding interactive effects are positive, which indicates that good governance needs to be enhanced to achieve positive net effects. A policy threshold of institutional governance at which institutional governance completely dampens the unfavourable effect of CO2 emissions on inclusive human development is established. Other policy implications are discussed.
C52|The uniform validity of impulse response inference in autoregressions|Existing proofs of the asymptotic validity of conventional methods of impulse response inference based on higher-order autoregressions are pointwise only. In this paper, we establish the uniform asymptotic validity of conventional asymptotic and bootstrap inference about individual impulse responses and vectors of impulse responses at fixed horizons. For inference about vectors of impulse responses based on Wald test statistics to be uniformly valid, lag-augmented autoregressions are required, whereas inference about individual impulse responses is uniformly valid under weak conditions even without lag augmentation. We introduce a new rank condition that ensures the uniform validity of inference on impulse responses and show that this condition holds under weak conditions. Simulations show that the highest finite-sample accuracy is achieved when bootstrapping the lag-augmented autoregression using the bias adjustments of Kilian (1999). The resulting confidence intervals remain accurate even at long horizons. We provide a formal asymptotic justification for this result.
C52|Old-fashioned parametric models are still the best. A comparison of Value-at-Risk approaches in several volatility states|Numerous advances in the modelling techniques of Value-at-Risk (VaR) have provided the financial institutions with a wide scope of market risk approaches. Yet it remains unknown which of the models should be used depending on the state of volatility. In this article we present the backtesting results for 1% and 2.5% VaR of six indexes from emerging and developed countries using several most known VaR models, among many: GARCH, EVT, CAViaR and FHS with multiple sets of parameters. The backtesting procedure has been based on the excess ratio, Kupiec and Christoffersen tests for multiple thresholds and cost functions. The added value of this article is that we have compared the models in four different scenarios, with different states of volatility in training and testing samples. The results indicate that the best of the models that is the least affected by changes in the volatility is GARCH(1,1) with standardized student's t-distribution. Non-parmetric techniques (e.g. CAViaR with GARCH setup (see Engle and Manganelli, 2001) or FHS with skewed normal distribution) have very prominent results in testing periods with low volatility, but are relatively worse in the turbulent periods. We have also discussed an automatic method to setting a threshold of extreme distribution for EVT models, as well as several ensembling methods for VaR, among which minimum of best models has been proven to have very good results - in particular a minimum of GARCH(1,1) with standardized student's t-distribution and either EVT or CAViaR models.
C52|Autonomy of profit rate distribution and its dynamics from firm size measures: A statistical equilibrium approach|This paper presents an empirical analysis of the distributional and dynamic properties of firm profit rates, measured by returns on assets, using panel data on 1095 long-lived Japanese (non-financial) listed firms over the 1971-2012 period. In particular, this paper tests the validity of statistical equilibrium approach of Alfarano et al. (2012), by investigating whether the two representative firm size measures of total assets and total sales are the significant determinants of key parameters ruling over the distributional outcome and stochastic motion of firm profit rates: a system-wide aver- age rate of profit, a system-wide dispersion measure of profit rates, and an idiosyncratic noise factor reecting individual firm characteristics. Employing information-theoretic model selection approach and standard panel data econometric techniques which control for both unobserved individual firm heterogeneity and time effects, this paper finds: (i) under the various levels of aggregation using the two size measures as firm classification instruments, the empirical density of profit rates is well described by the Laplace distribution; (ii) the key parameters characterizing the profit rate distribution and its dynamics are independent of the movements in firm size measures. These findings confirm the fundamental predictions from statistical equilibrium approach and the finding (ii) implies that firm competition is an autonomous system, immune to the size of individual firms.
C52|Predicting disaggregated tourist arrivals in Sierra Leone using ARIMA model|This study have uniquely mad use of Box-Jenkins ARIMA models to address the core of the threes objectives set out in view of the focus to add meaningful value to knowledge exploration. The outcome of the research have testify the achievements of this through successful nine months out-of-sample forecasts produced from the program codes, with indicating best model choices from the empirical estimation. In addition, the results also provide description of risks produced from the uncertainty Fan Chart, which clearly outlined possible downside and upside risks to tourist visitations in the country. In the conclusion, it was suggested that downside risks to the low level tourist arrival can be managed through collaboration between authorities concerned with the management of tourist arrivals in the country.
C52|The unprotecting effects of employment protection: the impact of the 2001 labor reform in Peru|According to the National Household Survey (ENAHO), approximately three out of four employment relationships within the formal sector of the Peruvian economy are based on temporary contracts. This percentage is larger than that of any OECD country and also considerably larger to that of any other country of the Latin American region. This study aims to elucidate the role that the 2001 labor reform played on these results and the effect this has had on variables associated to Peruvian workers’ well-being. To this end, we exploit the information on contract type and start date (identified by the employment duration), which are reported on the household surveys, to analyze the decision between using fixed-term contracts or indefinite-term contracts. The average impact obtained from a differences-in-differences estimation with matching, having workers with contract but with no health insurance as a control group, is a reduction of 41 percent in the probability of having contracts of indefinite duration in the short term (up to five years after the reform), whereas the long-term impact has been a drop by 70 percent. These results are consistent, and similarly large, as those found in a model of simple differences controlling for workers’ characteristics, firms and economic context. The results are robust to placebo tests and estimations by activity sectors and firm size. These results mean that, due to the reform, by 2015 over 900,000 jobs that could have been of indefinite-term were fixed-term contracts instead. Estimates based on Mincer equations suggest that this meant a loss of around 1.5 billion dollars in workers' labor income in 2015. Also, 36,000 workers would have affiliated to a union, had such reform not been implemented. These figures suggest than, instead of increasing workers’ protection, the reform implemented by the Constitutional Court left a large portion of them unprotected.
C52|VC - A method for estimating time-varying coefficients in linear models|This paper describes a moments estimator for a standard state-space model with coefficients generated by a random walk. This estimator does not require that disturbances are normally distributed, but if they are, the proposed estimator is asymptotically equivalent to the maximum likelihood estimator.
C52|Escape from model-land|"Both mathematical modelling and simulation methods in general have contributed greatly to understanding, insight and forecasting in many fields including macroeconomics. Never-theless, we must remain careful to distinguish model-land and model-land quantities from the real world. Decisions taken in the real world are more robust when informed by our best estimate of real-world quantities, than when ""optimal"" model-land quantities obtained from imperfect simulations are employed. The authors present a short guide to some of the temptations and pitfalls of model-land, some directions towards the exit, and two ways to escape."
C52|Beyond quantified ignorance: Rebuilding rationality without the bias bias|If we reassess the rationality question under the assumption that the uncertainty of the natural world is largely unquantifiable, where do we end up? In this article the author argues that we arrive at a statistical, normative, and cognitive theory of ecological rationality. The main casualty of this rebuilding process is optimality. Once we view optimality as a formal implication of quantified uncertainty rather than an ecologically meaningful objective, the rationality question shifts from being axiomatic/probabilistic in nature to being algorithmic/ predictive in nature. These distinct views on rationalitymirror fundamental and longstanding divisions in statistics.
C52|Determination of vector error correction models in high dimensions|We provide a shrinkage type methodology which allows for simultaneous model selection and estimation of vector error correction models (VECM) when the dimension is large and can increase with sample size. Model determination is treated as a joint selection problem of cointegrating rank and autoregressive lags under respective practically valid sparsity assumptions. We show consistency of the selection mechanism by the resulting Lasso-VECM estimator under very general assumptions on dimension, rank and error terms. Moreover, with computational complexity of a linear programming problem only, the procedure remains computationally tractable in high dimensions. We demonstrate the effectiveness of the proposed approach by a simulation study and an empirical application to recent CDS data after the financial crisis.
C52|Monetary Policy and Macroeconomic Stability Revisited|A large literature with canonical New Keynesian models has established that the Fed's policy change from a passive to an active response to inflation led to U.S. macroeconomic stability after the Great Inflation of the 1970s. We revisit this view by estimating a staggered price model with trend inflation using a Bayesian method that allows for equilibrium indeterminacy and adopts a sequential Monte Carlo algorithm. {{p}} The model empirically outperforms a canonical New Keynesian model and demonstrates an active response to inflation even in the Great Inflation era, during which the U.S. economy was likely in the indeterminacy region of the model's parameter space. A more active response to inflation alone does not suffice for explaining the shift to determinacy after the Great Inflation, unless it is accompanied by a decline in trend inflation or a change in policy responses to the output gap and output growth.
C52|Facts and Fiction in Oil Market Modeling|Baumeister and Hamilton (2019a) assert that every critique of their work on oil markets by Kilian and Zhou (2019a) is without merit. In addition, they make the case that key aspects of the economic and econometric analysis in the widely used oil market model of Kilian and Murphy (2014) and its precursors are incorrect. Their critiques are also directed at other researchers who have worked in this area and, more generally, extend to research using structural VAR models outside of energy economics. The purpose of this paper is to help the reader understand what the real issues are in this debate. The focus is not only on correcting important misunderstandings in the recent literature, but on the substantive and methodological insights generated by this exchange, which are of broader interest to applied researchers.
C52|The uniform validity of impulse response inference in autoregressions|Existing proofs of the asymptotic validity of conventional methods of impulse response inference based on higher-order autoregressions are pointwise only. In this paper, we establish the uniform asymptotic validity of conventional asymptotic and bootstrap inference about individual impulse responses and vectors of impulse responses at fixed horizons. For inference about vectors of impulse responses based on Wald test statistics to be uniformly valid, lag-augmented autoregressions are required, whereas inference about individual impulse responses is uniformly valid under weak conditions even without lag augmentation. We introduce a new rank condition that ensures the uniform validity of inference on impulse responses and show that this condition holds under weak conditions. Simulations show that the highest finite-sample accuracy is achieved when bootstrapping the lag-augmented autoregression using the bias adjustments of Kilian (1999). The resulting confidence intervals remain accurate even at long horizons. We provide a formal asymptotic justification for this result.
C52|Tests of Conditional Predictive Ability: Some Simulation Evidence|In this note we provide simulation evidence on the size and power of tests of predictive ability described in Giacomini and White (2006). Our goals are modest but non-trivial. First, we establish that there exist data generating processes that satisfy the null hypotheses of equal finite-sample (un)conditional predictive ability. We then consider various parameterizations of these DGPs as a means of evaluating the size and power properties of the proposed tests. While some of our results reinforce those in Giacomini and White (2006), others do not. We recommend against using the fixed scheme when conducting these tests and provide evidence that very large bandwidths are sometimes required when estimating long-run variances.
C52|Tests of Conditional Predictive Ability: A Comment|We investigate a test of equal predictive ability delineated in Giacomini and White (2006; Econometrica). In contrast to a claim made in the paper, we show that their test statistic need not be asymptotically Normal when a fixed window of observations is used to estimate model parameters. An example is provided in which, instead, the test statistic diverges with probability one under the null. Simulations reinforce our analytical results.
C52|A Generalized Factor Model with Local Factors|I extend the theory on factor models by incorporating âlocalâ factors into the model. Local factors affect a decreasing fraction of the observed variables. This implies a continuum of eigenvalues of the covariance matrix, as is commonly observed in applications. I derive conditions under which local factors will be estimated consistently using the common Principal Component Estimator. I further propose a novel class of estimators for the number of factors. Unlike estimators that have been proposed in the past, my estimators use information in the eigenvectors as well as in the eigenvalues. Monte Carlo evidence suggests significant finite sample gains over existing estimators. Empirically I find evidence of local factors in a large panel of US macroeconomic indicators.
C52|Dynamic specification tests for dynamic factor models|We derive computationally simple expressions for score tests of misspecification in parametric dynamic factor models using frequency domain techniques. We interpret those diagnostics as time domain moment tests which assess whether certain autocovariances of the smoothed latent variables match their theoretical values under the null of correct model specification. We also reinterpret reduced‐form residual tests as checking specific restrictions on structural parameters. Our Gaussian tests are robust to nonnormal, independent innovations. Monte Carlo exercises confirm the finite‐sample reliability and power of our proposals. Finally, we illustrate their empirical usefulness in an application that constructs a US coincident indicator.
C52|New Evidence on the Effects of Quantitative Easing|Have the macroeconomic effects of QE programs been overestimated empirically? Using a large set of model specifications that differ in the degree of time-variation in parameters, the answer is yes. Our forecasting exercise suggests that it is crucial to allow for time-variation in parameters, but not for stochastic volatility to improve the fit with data. Having a more reliable specification, we find that the portfolio balance and signaling channels had sizable contributions to the transmission of QE programs. Finally, our identified structural shocks show that QE1 had larger macroeconomic effects than QE2 and QE3, but much smaller than usually found in the literature.
C52|Estimating monetary policy rules in small open economies|This paper presents an approach for empirically estimating long-run monetary policy rules in small open economies. The approach utilizes the cointegrated VAR methodology and statistical tests on long- and short-run relations, and investigates policy responses. An application is presented for the case of Trinidad and Tobago. The analysis reveals an empirically supported long-run monetary policy rule for the nominal exchange rate, and provides empirical evidence that oil price shocks are transmitted through the TT economy in part via the effects on US prices. Dynamic specification of the nominal exchange rate reveals significant adjustment towards the target equilibrium level, and significant effects from foreign and domestic variables save for the exchange rate. Forecast analysis reveals the significance of oil-price forecasts, and forecast-errors, on monetary policy. The parsimonious model and its parameter estimates are empirically constant and generate reliable forecasts that provide important implications for using estimated policy rules.
C52|The asymmetric role of shadow economy in the energy-growth nexus in Bolivia|This paper estimates the energy demand function to examine the asymmetric relationship between the shadow economy and energy consumption in the case of Bolivia during the period of 1960–2015. The ambiguous empirical findings on shadow economy-energy demand nexus has inclined us to apply the nonlinear ARDL cointegration approach developed by Shin et al. (2014) and the Hatemi-J (2012) asymmetric causality test. The empirical evidence confirms the presence of an asymmetric relationship between the variables of interest. Positive and negative shocks to official GDP (true GDP) and the shadow economy have positive impacts on energy consumption. Energy consumption is positively and negatively affected by positive and negative shocks in financial development, respectively. A positive (negative) shock to capital decreases energy consumption. Another important finding concerns the complex causal direction between economic growth and energy consumption. This study provides new insights regarding to the use of official GDP (true GDP) and the shadow economy as economic tools to maintain energy demand for sustainable economic development.
C52|Interaction matrix selection in spatial autoregressive models with an application to growth theory|The interaction matrix, or spatial weight matrix, is the fundamental tool to model cross-sectional interdependence between observations in spatial autoregressive models. However, it is most of the time not derived from theory, as it should be ideally, but chosen on an ad hoc basis. In this paper, we propose a modified version of the J test to formally select the interaction matrix. Our methodology is based on the application of the robust against unknown heteroskedasticity GMM estimation method, developed by Lin and Lee (2010). We then implement the testing procedure developed by Hagemann (2012) to overcome the decision problem inherent to non-nested models tests. An application of the testing procedure is presented for the Schumpeterian growth model with worldwide interactions developed by Ertur and Koch (2011) using three different types of interaction matrices: genealogic distance, linguistic distance and bilateral trade flows. We find that the interaction matrix based on trade flows is the most adequate.
C52|Investment climate, outward orientation and manufacturing firm productivity: new empirical evidence| Drawing on the World Bank Enterprise Surveys, we revisit the link between firm-level investment climate and productive performance for a panel of enterprises surveyed twice in time in 70 developing countries and 11 manufacturing industries. We take advantage of the time dimension available for an increasing number of countries to tackle the endogeneity issue stressed in previous studies. We also use pertinent econometric techniques to address other biases inherent in the data (e.g.measurement errors, missing observations and multicollinearity). Our results reinforce previous findings by validating, with a larger than usual sample of countries and industries, the importance of a larger set of environment variables. We show that infrastructure quality, information & communication technologies, skills and experience of the labour force, cost of and access to financing, security and political stability, competition and government relation contribute to firms’ and countries’ performances gap. The empirical analysis also illustrates that firms which choose an outward orientation have higher productivity level. Nevertheless, outward oriented enterprises are more sensitive to investment climate limitations. These findings have important policy implications by showing which dimensions of the business environment, in which industry, could help manufacturing firms to be more competitive in the present context of increasing globalization.
C52|Bilateral Defaultable Financial Derivatives Pricing and Credit Valuation Adjustment|The one-side defaultable financial derivatives valuation problems have been studied extensively, but the valuation of bilateral derivatives with asymmetric credit qualities is still lacking convincing mechanism. This paper presents an analytical model for valuing derivatives subject to default by both counterparties. The default-free interest rates are modeled by the Market Models, while the default time is modeled by the reduced-form model as the first jump of a time-inhomogeneous Poisson process. All quantities modeled are market-observable. The closed-form solution gives us a better understanding of the impact of the credit asymmetry on swap value, credit value adjustment, swap rate and swap spread.
C52|The Relation between the Corporate Bond-Yield Spread and the Real Economy: Stable or TimeVarying?|In this paper we assess whether the relation between the corporate bond-yield spread and the real economy has been stable over time. Using quarterly US data from 1953Q1 to 2018Q2, we estimate Bayesian VAR models which allow for drifting parameters and/or stochastic volatility and conduct formal model selection in a Bayesian setting. Our results indicate that the relation between the variables has been stable; we do, however, find strong support for stochastic volatility. We conclude that the corporate bond-yield spread’s usefulness for predicting real economic activity has not changed to a relevant extent after the Great Reces-sion.
C52|Robustness of the Norwegian wage formation system and free EU labour movement. Evidence from wage data for natives|Norway experienced a high immigration flow after the EEA directive in 2004 stating workers right to free movement within the European Union and EEA-countries. There is no clear consensus in the literature on how immigration affects native wages, but some studies using Norwegian micro data have estimated a negative effect of higher immigration for some type of workers. In this paper, to capture that the wage setting is highly coordinated in Norway, we model a system of native wages for three sectors; manufacturing, private service industries and public sector. We estimate that labour immigration has had a negative effect on the attainable wage growth for natives in all three sectors, but that the largest and most direct impact on wages has been in the private service industries. Immigration is found to be exogenous with respect to the parameters of our model of wage formation.
C52|Price Dispersion In Internet Sales: Data From An Online Marketplace Contradict Lab Experiments|This paper considers three hypotheses about the strategic origin of price dispersion in homogeneous product online sales. The rst two are the E-equilibrium and the quantal-response equilibrium (QRE) in a pure Bertrand setting involving the boundedly rational behavior of sellers. The third introduces the share of loyal consumers into the model of competition. These hypotheses were supported by estimations on experimental lab data. We test the hypotheses on a set of real prices for 30 models of household appliances collected from the largest Russian online marketplace Market.Yandex.ru. In contrast to the previously reported experimental data, we found very limited support for any of these explanations. QRE showed the best performance on the data. For most of the products it accurately predicts central tendency, i.e. the mean and the median. However, the shape of the observed price distributions is not explained well by any of the models. These results pose new challenges for theoretical explanations of observed Internet prices.
C52|Generalised Anderson-Rubin statistic based inference in the presence of a singular moment variance matrix| The particular concern of this paper is the construction of a confidence region with pointwise asymptotically correct size for the true value of a parameter of interest based on the generalized Anderson-Rubin (GAR) statistic when the moment variance matrix is singular. The large sample behaviour of the GAR statistic is analysed using a Laurent series expansion around the points of moment variance singularity. Under a condition termed first order moment singularity the GAR statistic is shown to possess a limiting chi-square distribution on parameter sequences converging to the true parameter value. Violation, however, of this condition renders the GAR statistic unbounded asymptotically. The paper details an appropriate discretisation of the parameter space to implement a feasible GAR-based confidence region that contains the true parameter value with pointwise asymptotically correct size. Simulation evidence is provided that demonstrates the efficacy of the GAR-based approach to moment-based inference described in this paper.
C52|Time-Varying Risk Premia in Large International Equity Markets|We use an estimation methodology tailored for large unbalanced panels of individual stock returns to address key economic questions about the factor structure, pricing performance of factor models, and time-variations in factor risk premia in international equity markets. We estimate factor models with time-varying factor exposures and risk premia at the individual stock level using 62,320 stocks in 46 countries over the 1985-2018 period. We consider market, size, value, momentum, profitability, and investment factors aggregated at the country, regional, and world level. We find that adding an excess country market factor to world or regional factors is sufficient to capture the factor structure for both developed and emerging markets. We do not reject asset pricing restriction tests for multifactor models in 74% to 91% of countries. Value and momentum premia show more variability over time and across countries than profitability and investment premia. The excess country market premium is statistically significant in many developed and emerging markets but economically larger in emerging markets.
C52|Mind the gap: a multi-country BVAR benchmark for the Eurosystem projections|The Eurosystem staff forecasts are conditional on the financial markets, the global economy and fiscal policy outlook, and include expert judgement. We develop a multi-country BVAR for the four largest countries of the euro area and we show that it provides accurate conditional forecasts of policy relevant variables such as, for example, consumer prices and GDP. The forecasting accuracy and the ability to mimic the path of the Eurosystem projections suggest that the model is a valid benchmark to assess the consistency of the projections with the conditional assumptions. As such, the BVAR can be used to identify possible sources of judgement, based on the gaps between the Eurosystem projections and the historical regularities captured by the model. JEL Classification: C52, C53, E37
C52|Determinants of German outward FDI: variable selection using Bayesian statistical|"This paper provides new evidence on the drivers of German outward foreign direct investment (FDI) stocks for the period 1996-2012. In contrast to previous empirical studies, we adopt a Bayesian model averaging (BMA) approach for a robust selection of those variables. We find evidence that determinants that are associated with horizontal FDI appear to be dominant for explaining bilateral FDI with developed countries while for the group of developing countries covariates associated with vertical FDI motives play a larger role. Within Europe, while the majority of FDI is horizontally driven in “core"" countries, for peripheral ones the vertical motivation for FDI seems to prevail. Moreover, our results are compatible with more complex FDI models where vertical determinants and institutional variables are gaining prominence, in parallel with the development of global value chains (GVC). Our results can provide hints for policymakers’ strategies to attract German investment."
C52|Assessing Pension Expenditure Determinants – the Case of Portugal|Pension expenditure is a concern for the sustainability of public finances in the European Union. Therefore, assessing pension expenditure determinants is crucial. This study aims to disentangle the impact of demographic and economic variables, such as ageing, productivity, and unemployment, on pension expenditure. Using Portuguese time-series data, from 1975 to 2014, statistical evidence was found of co-integration between unemployed people aged between 15 and 64 years old, apparent productivity of labour, the old-age dependence index and pension expenditure as a share of gross domestic product. The use of a vector error correction model, with impulse-response functions and variance decomposition, showed that ageing has an almost insignificant impact in the long-run, when compared with unemployment and productivity.
C52|The dynamic relationship between stock market indexes and foreign exchange|his empirical study analyses the dynamic relationship between the FTSE100 Index and the EuroSTOXX50 Index and the USD/EUR and USD/GBP exchange rates, from January 2007 to April 2017. The Johansen co-integration tests suggest that these variables have a long-term relationship.The Granger causality test was conducted through the use of VECM equations, showing that the FTSE100 and the EuroSTOXX50 Index both have a causal feedback relationship. A unidirectional relationship was found between the FTSE100 Index stock prices and the USD/EURexchange rate.The presence of a unidirectional relationship between the USD/GBP exchange rate and FTSE100 and EuroSTOXX50 Index stock priceswas also detected.
C52|Marginal Jobs and Job Surplus: A Test of the Efficiency of Separations|"We present a sharp test for the efficiency of job separations. First, we document a dramatic increase in the separation rate - 11.2ppt (28%) over five years - in response to a quasi-experimental extension of UI benefit duration for older workers. Second, after the abolition of the policy, the ""job survivors"" in the formerly treated group exhibit exactly the same separation behavior as the control group. Juxta-posed, these facts reject the ""Coasean"" prediction of efficient separations, whereby the UI extensions should have extracted marginal (low-surplus) jobs and thereby rendered the remaining (high-surplus) jobs more resilient after its abolition. Third, we show that a formal model of predicted efficient separations implies a piece-wise linear function of the actual control group separations beyond the missing mass of marginal matches. A structural estimation reveals point estimates of the share of efficient separations below 4%, with confidence intervals rejecting shares above 13%. Fourth, to characterize the marginal jobs in the data, we extend complier analysis to difference-indifference settings such as ours. The UI-indiced separators stemmed from declining firms, blue-collar jobs, with a high share of sick older workers, and firms more likely to have works councils - while their wages were similar to program survivors. The evidence is consistent with a ""non-Coasean"" framework building on wage frictions preventing efficient bargaining, and with formal or informal institutional constraints on selective separations."
C52|Robust measures of skewness and kurtosis for macroeconomic and financial time series|The sample skewness and kurtosis of macroeconomic and financial time series are routinely scrutinized in the early stages of model-building and are often the central topic of studies in economics and finance. Notwithstanding the availability of several robust estimators, most scholars in economics rely on method-of-moments estimation that is known to be very sensitive to outliers. We carry out an extensive Monte Carlo analysis to compare the bias and root mean squared error of twelve different estimators of skewness and kurtosis. We consider nine statistical distributions that approximate the range of data generating processes of many macroeconomic and financial time series. Both in independently and identically distributed samples and in data generating processes featuring serial correlation L-moments and trimmed L-moments estimators are particularly resistant to outliers and deliver the lowest root mean squared error. The application to 128 macroeconomic and financial time series sourced from a large, monthly frequency, database (i.e. the FRED-MD of McCracken and Ng, 2016) confirms the findings of the simulation study.
C52|South African unemployment in the post-financial crisis era: What are the determinants?|High unemployment rates is one of the greatest economic challenges facing post-apartheid South African government over the past two decades and this problem has become more worrisome in the post-global financial crisis period. Our study examines the determinants of unemployment for the South African economy in the post-crisis period over a quarterly frequency period of 2009:Q1 to 2018:Q4. The determinants are examined for 4 classes of unemployment rates (total, male, female and youth) and we further partition possible unemployment determinants into fiscal, monetary and macroeconomic variables. The estimation results from the employed autoregressive distributive lag (ARDL) models find income tax, repo rates, economic growth, trade, investment, household debt and savings to be significant determinants of unemployment in the post-crisis South African economy and yet we note discrepancies of the significance of these determinants amongst different unemployment categories. Relevant policy implications are matched against our obtained empirical findings.
C52|Fiscal cyclicality in South African public expenditures: Do asymmetries explain inconsistencies?|The most recent sub-prime crisis, the Euro-debt crisis and the global recessionary period have resurrected a debate on the nature of fiscal cyclicality in the South African economy. Our study questions whether the cyclicality of public policy has evolved asymmetrically and holds differently over the recessionary and expansionary phases of the South African business cycle for a quarterly period of 2001:01 – 2018:04. To ensure that the business cycle is inherent to our estimation process we rely on the nonlinear autoregressive distributive lag (N-ARDL) model as empirical framework. Our empirical results point to nonlinear cyclicality in fiscal expenditures where government behaves procyclical in the upswing of the business cycle whilst behaving countercyclical during economic downswings. These findings are robust to alternative specifications, inclusion of control variables and estimations across different subsamples. Policy implications are also offered.
C52|Optimal Bias Correction of the Log-periodogram Estimator of the Fractional Parameter: A Jackknife Approach|We use the jackknife to bias correct the log-periodogram regression (LPR) estimator of the fractional parameter in a stationary fractionally integrated model. The weights for the jackknife estimator are chosen in such a way that bias reduction is achieved without the usual increase in asymptotic variance, with the estimator viewed as `optimal' in this sense. The theoretical results are valid under both the non-overlapping and moving-block sub-sampling schemes that can be used in the jackknife technique, and do not require the assumption of Gaussianity for the data generating process. A Monte Carlo study explores the Ã–nite sample performance of diÂ§erent versions of the optimal jackknife estimator under a variety of fractional data generating processes. The simulations reveal that when the weights are constructed using the true parameter values, a version of the optimal jackknife estimator almost always out-performs alternative bias-corrected estimators. A feasible version of the jackknife estimator, in which the weights are constructed using consistent estimators of the unknown parameters, whilst not dominant overall, is still the least biased estimator in some cases.
C52|Taming the Factor Zoo: A Test of New Factors|We propose a model-selection method to systematically evaluate the contribution to asset pricing of any new factor, above and beyond what a high-dimensional set of existing factors explains. Our methodology explicitly accounts for potential model-selection mistakes, unlike the standard approaches that assume perfect variable selection, which rarely occurs in practice and produces a bias due to the omitted variables. We apply our procedure to a set of factors recently discovered in the literature. While most of these new factors are found to be redundant relative to the existing factors, a few — such as profitability — have statistically significant explanatory power beyond the hundreds of factors proposed in the past. In addition, we show that our estimates and their significance are stable, whereas the model selected by simple LASSO is not.
C52|Macroeconomic Uncertainty Prices when Beliefs are Tenuous|"A representative investor does not know which member of a set of well-defined parametric ""structured models'' is best. The investor also suspects that all of the structured models are misspecified. These uncertainties about probability distributions of risks give rise to components of equilibrium prices that differ from the risk prices widely used in asset pricing theory. A quantitative example highlights a representative investor's uncertainties about the size and persistence of macroeconomic growth rates. Our model of preferences under ambiguity puts nonlinearities into marginal valuations that induce time variations in market prices of uncertainty. These arise because the representative investor especially fears high persistence of low growth rate states and low persistence of high growth rate states."
C52|The Promise and Pitfalls of Conflict Prediction: Evidence from Colombia and Indonesia|"Policymakers can take actions to prevent local conflict before it begins, if such violence can be accurately predicted. We examine the two countries with the richest available sub-national data: Colombia and Indonesia. We assemble two decades of fine-grained violence data by type, alongside hundreds of annual risk factors. We predict violence one year ahead with a range of machine learning techniques. Models reliably identify persistent, high-violence hot spots. Violence is not simply autoregressive, as detailed histories of disaggregated violence perform best. Rich socio-economic data also substitute well for these histories. Even with such unusually rich data, however, the models poorly predict new outbreaks or escalations of violence. ""Best case"" scenarios with panel data fall short of workable early-warning systems."
C52|On Testing Continuity and the Detection of Failures|Estimation of discontinuities is pervasive in applied economics: from the study of sheepskin effects to prospect theory and “bunching” of reported income on tax returns, models that predict discontinuities in outcomes are uniquely attractive for empirical testing. However, existing empirical methods often rely on assumptions about the number of discontinuities, the type, the location, or the underlying functional form of the model. We develop a nonparametric approach to the study of arbitrary discontinuities — point discontinuities as well as jump discontinuities in the nth derivative, where n = 0,1,... — that does not require such assumptions. Our approach exploits the development of false discovery rate control methods for lasso regression as proposed by G’Sell et al. (2015). This framework affords us the ability to construct valid tests for both the null of continuity as well as the significance of any particular discontinuity without the computation of nonstandard distributions. We illustrate the method with a series of Monte Carlo examples and by replicating prior work detecting and measuring discontinuities, in particular Lee (2008), Card et al. (2008), Reinhart and Rogoff (2010), and Backus et al. (2018b).
C52|Cross-Sectional Dispersion of Risk in Trading Time|We study the temporal behavior of the cross-sectional distribution of assets' market exposure, or betas, using a large panel of high-frequency returns. The asymptotic setup has the sampling frequency of the returns increasing to infinity, while the time span of the data remains fixed, and the cross-sectional dimension is fixed or increasing. We derive a Central Limit Theorem (CLT) for the cross-sectional beta dispersion at a point in time, enabling us to test whether this quantity varies across the trading day. We further derive a functional CLT for the dispersion statistics, allowing us to test if the beta dispersion, as a function of time-of-day, changes across days. We extend this further by developing inference techniques for the entire cross-sectional beta distribution at fixed points in time. We demonstrate, for constituents of the S&P 500 index, that the beta dispersion is elevated at the market open, gradually declines over the trading day, and is less than half the original value by the market close. The intraday beta dispersion pattern also changes over time and evolves differently on macroeconomic announcement days. Importantly, we find that the intraday variation in market betas is a source of priced risk.
C52|Modelling and forecasting the dollar-pound exchange rate in the presence of structural breaks|We employ a newly-developed partial cointegration system allowing for level shifts to examine whether economic fundamentals form the long-run determinants of the dollar-pound exchange rate in an era of structural change. The paper uncovers a class of local data generation mechanisms underlying long-run and short-run dynamic features of the exchange rate using a set of economic variables that explicitly reï¬‚ect the central banksâ€™ monetary policy stances and the inï¬‚uence of a forward exchange market. The impact of the Brexit referendum is evaluated by examining forecasts when the dollar-pound exchange rate fell substantially around the vote.
C52|Leveraging Loyalty Programs Using Competitor Based Targeting|Loyalty programs are widely used by firms but their effectiveness is subject to debate. These programs provide discounts and perks to loyal customers and are costly to administer, and with uncertain effectiveness at increasing spending or stealing business from rivals. We use a large new dataset on retail purchases before and after joining a loyalty program (LP) at the customer level to evaluate what determines LP effectiveness. We exploit detailed spatial data on customer and store locations, including locations of competing firms. A simple analysis shows that location relative to competitors is the strongest predictor of LP effectiveness, suggesting that LPs work primarily through business stealing and not through other demand expansion. We next estimate what variables best predict LP effectiveness using high-dimensional data on spatial relationships between customers, the focal firm’s stores, and competing stores as well as customers’ historical spending patterns. We use LASSO regularization to show that spatial relationships are more predictive of LP effects than are past sales data. Finally, we show how firms can use this type of predictive analytics model to leverage customer and competitor location data to substantially increase the performance of their LP through spatially driven targeting rules.
C52|Microeconometric Dynamic Panel Data Methods: Model Specification and Selection Issues|A motivated strategy is presented to find step by step an adequate model specification and a matching set of instrumental variables by applying the programming tools provided by the Stata package Xtabond2. The aim is to implement generalized method of moment techniques such that useful and reasonably accurate inferences are extracted from an observational panel data set on a single microeconometric structural presumably dynamic behavioral relationship. In the suggested specification search three comprehensive heavily interconnected goals are pursued, namely: (i) to include all the relevant appropriately transformed possibly lagged regressors, as well as any interactions between these if it is required to relax the otherwise very strict homogeneity restrictions on the dynamic impacts of the explanatories in standard linear panel data models; (ii) to correctly classify all regressors as either endogenous, predetermined or exogenous, as well as being either effect-stationary or effect-nonstationary, implying which internal variables could represent valid and relatively strong instruments; (iii) to enhance the accuracy of inference in finite samples by omitting irrelevant regressors and by profitably reducing the space spanned by the full set of available internal instruments. For the various tests which trigger the decisions to be made in the sequential selection process the relevant considerations are spelled out to interpret the magnitude of p-values. Also the complexities to establish and interpret the ultimately established dynamic impacts are explained. Finally the developed strategy is applied to a classic data set and is shown to yield new insights.
C52|Monotonic Estimation for the Survival Probability over a Risk-Rated Portfolio by Discrete-Time Hazard Rate Models|Monotonic estimation for the survival probability of a loan in a risk-rated portfolio is based on the observation arising, for example, from loan pricing that a loan with a lower credit risk rating is more likely to survive than a loan with a higher credit risk rating, given the same additional risk covariates. Two probit-type discrete-time hazard rate models that generate monotonic survival probabilities are proposed in this paper. The first model calculates the discrete-time hazard rate conditional on systematic risk factors. As for the Cox proportion hazard rate model, the model formulates the discrete-time hazard rate by including a baseline component. This baseline component can be estimated outside the model in the absence of model covariates using the long-run average discrete-time hazard rate. This results in a significant reduction in the number of parameters to be otherwise estimated inside the model. The second model is a general form model where loan level factors can be included. Parameter estimation algorithms are also proposed. The models and algorithms proposed in this paper can be used for loan pricing, stress testing, expected credit loss estimation, and modeling of the probability of default term structure.
C52|Monotonic Estimation for Probability Distribution and Multivariate Risk Scales by Constrained Minimum Generalized Cross-Entropy|Minimum cross-entropy estimation is an extension to the maximum likelihood estimation for multinomial probabilities. Given a probability distribution {r_i }_(i=1)^k, we show in this paper that the monotonic estimates {p_i }_(i=1)^k for the probability distribution by minimum cross-entropy are each given by the simple average of the given distribution values over some consecutive indexes. Results extend to the monotonic estimation for multivariate outcomes by generalized cross-entropy. These estimates are the exact solution for the corresponding constrained optimization and coincide with the monotonic estimates by least squares. A non-parametric algorithm for the exact solution is proposed. The algorithm is compared to the “pool adjacent violators” algorithm in least squares case for the isotonic regression problem. Applications to monotonic estimation of migration matrices and risk scales for multivariate outcomes are discussed.
C52|Economic Assessment of Climate Adaptation Options in Myanmar Rice-Based Farming System|Agriculture is highly sensitive to climate change and understandings how the adaptation options improve the farming household’s adaptive capacity are critical to the agricultural policies. The study was carried out for the economic assessment of climate adaption options in rice-based farming system of Myanmar. The propensity score matching approach was applied to explore the existing adaptation options and its contribution on the farm income. In addition, the binary probit model was used to analyse the factors influencing those adaptation decisions. The erratic rainfall, especially dry spell period and unexpected rain during the critical crop growth, was the critical challenge of rice-based farming in the study. The timely operation of farm machineries was one of the major adaptation options for the farmers, followed by other options such as use of more agrochemicals and changing rice varieties including early maturity, high yielding and stress tolerant varieties. The combination of those adaptations gave additional 0.86-0.89 ton/ha yield, 152-158 USD/ha total return and 108-124 USD/ha profit to the adapter farmers. The institutional factors such as irrigation access, access to credit, access to weekly weather information and participation to agricultural training were critically important to the adaptation decision. Moreover, the social capital factors like farming experience, farm size and farm income share were also major influencing variables.
C52|IFRS9 Expected Credit Loss Estimation: Advanced Models for Estimating Portfolio Loss and Weighting Scenario Losses|Estimation of portfolio expected credit loss is required for IFRS9 regulatory purposes. It starts with the estimation of scenario loss at loan level, and then aggregated and summed up by scenario probability weights to obtain portfolio expected loss. This estimated loss can vary significantly, depending on the levels of loss severity generated by the IFSR9 models, and the probability weights chosen. There is a need for a quantitative approach for determining the weights for scenario losses. In this paper, we propose a model to estimate the expected portfolio losses brought by recession risk, and a quantitative approach for determining the scenario weights. The model and approach are validated by an empirical example, where we stress portfolio expected loss by recession risk, and calculate the scenario weights accordingly.
C52|South African unemployment in the post-financial crisis era: What are the determinants?|High unemployment rates is one of the greatest economic challenges facing post-apartheid South African government over the past two decades and this problem has become more worrisome in the post-global financial crisis period. Our study examines the determinants of unemployment for the South African economy in the post-crisis period over a quarterly frequency period of 2009:Q1 to 2018:Q4. The determinants are examined for 4 classes of unemployment rates (total, male, female and youth) and we further partition possible unemployment determinants into fiscal, monetary and macroeconomic variables. The estimation results from the employed autoregressive distributive lag (ARDL) models find income tax, repo rates, economic growth, trade, investment, household debt and savings to be significant determinants of unemployment in the post-crisis South African economy and yet we note discrepancies of the significance of these determinants amongst different unemployment categories. Relevant policy implications are matched against our obtained empirical findings.
C52|Pricing Interest Rate Swap Subject to Bilateral Counterparty Risk|This paper presents an analytical model for valuing interest rate swaps, subject to bilateral counterparty credit risk. The counterparty defaults are modeled by the reduced-form model as the first jump of a time-inhomogeneous Poisson process. All quantities modeled are market-observable. The closed-form solution gives us a better understanding of the impact of the credit asymmetry on swap value, credit value adjustment, swap rate and swap spread.
C52|Forecasting with many predictors using message passing algorithms|Machine learning methods are becoming increasingly popular in economics, due to the increased availability of large datasets. In this paper I evaluate a recently proposed algorithm called Generalized Approximate Message Passing (GAMP) , which has been very popular in signal processing and compressive sensing. I show how this algorithm can be combined with Bayesian hierarchical shrinkage priors typically used in economic forecasting, resulting in computationally efficient schemes for estimating high-dimensional regression models. Using Monte Carlo simulations I establish that in certain scenarios GAMP can achieve estimation accuracy comparable to traditional Markov chain Monte Carlo methods, at a tiny fraction of the computing time. In a forecasting exercise involving a large set of orthogonal macroeconomic predictors, I show that Bayesian shrinkage estimators based on GAMP perform very well compared to a large set of alternatives.
C52|User-Specified General-to-Specific and Indicator Saturation Methods|General-to-Specific (GETS) modelling provides a comprehensive, systematic and cumulative approach to modelling that is ideally suited for conditional forecasting and counterfactual analysis, whereas Indicator Saturation (ISAT) is a powerful and flexible approach to the detection and estimation of structural breaks (e.g. changes in parameters), and to the detection of outliers. To these ends, multi-path backwards elimination, single and multiple hypothesis tests on the coefficients, diagnostics tests and goodness-of-fit measures are combined to produce a parsimonious final model. In many situations a specific model or estimator is needed, a specific set of diagnostics tests may be required, or a specific fit criterion is preferred. In these situations, if the combination of estimator/model, diagnostics tests and fit criterion is not offered by publicly available software, then the implementation of user-specified GETS and ISAT methods puts a large programming-burden on the user. Generic functions and procedures that facilitate the implementation of user-specified GETS and ISAT methods for specific problems can therefore be of great benefit. The R package gets, version 0.20 (September 2019), is the first software - both inside and outside the R universe - to provide a complete set of facilities for user-specified GETS and ISAT methods: User-specified model/estimator, user-specified diagnostics and user-specified goodness-of-fit criteria. The aim of this article is to illustrate how user-specified GETS and ISAT methods can be implemented.
C52|A Practical Guide to Harnessing the HAR Volatility Model|The standard heterogeneous autoregressive (HAR) model is perhaps the most popular benchmark model for forecasting return volatility. It is often estimated using raw realized variance (RV) and ordinary least squares (OLS). However, given the stylized facts of RV and wellknown properties of OLS, this combination should be far from ideal. One goal of this paper is to investigate how the predictive accuracy of the HAR model depends on the choice of estimator, transformation, and forecasting scheme made by the market practitioner. Another goal is to examine the effect of replacing its high-frequency data based volatility proxy (RV) with a proxy based on free and publicly available low-frequency data (logarithmic range). In an out-of-sample study, covering three major stock market indices over 16 years, it is found that simple remedies systematically outperform not only standard HAR but also state of the art HARQ forecasts, and that HAR models using logarithmic range can often produce forecasts of similar quality to those based on RV.
C52|Liquidity traps and large-scale financial crises|This paper estimates a nonlinear Threshold-VAR to investigate if a Keynesian liquidity trap due to a speculative motive was in place in the U.S. Great Depression and the recent Great Recession. We find clear evidence in favor of a breakdown of the liquidity effect after an unexpected increase in M2 in the 1921–1940 period. This evidence, which is consistent with the Keynesian view on a liquidity trap, is shown to be state contingent. In particular, it emerges only when a speculative regime identified by high realizations of the Dow Jones index is considered. A standard linear framework is shown to be ill-suited to test the hypothesis of a Keynesian liquidity trap. An investigation performed with the same data for the period 1991–2010 confirms the presence of a liquidity trap just in the speculative regime. This last result emerges significantly only when we consider the federal funds rate as the policy instrument and we model the Divisia M2 measure of liquidity.
C52|Choosing the Production Function Model for an Optimal Measurement of the Restructuring Efficiency of the Polish Metallurgical Sector in Years 2000–2015|Between 2000 and 2015, the Polish metallurgical sector was subject to serious restructuring. Presented research aimed at providing a framework for possibly most accurate measurement of efficiency of this process. The study employed: (I) Quantitative research for elaboration of production function models: power regression Cobb-Douglas function with its developments; (II) Qualitative research: Analytic Hierarchy Process for assessment of relevance of efficiency evaluation criteria in reference to various production function models in metallurgy sector: (i) sectoral added value (net production); (ii) production sold; and, (iii) steel production volume. Criteria relevance has been assessed by scientists and practitioners with specialization in metallurgy. As a result the sectoral added value function has been chosen as the one that optimally reflects sector’s restructuring efficiency. This, in turn, constitutes a qualitative confirmation of previous research result, which has been verified with a quantitative method. Practical outcome is a more precise modelling of efficiency of restructuring processes in the metallurgical sector, both for scientific and business needs. The main research limitations originate from the sector itself—in order to make our tool more universal, further research should be led in parallel branches of industry.
C52|Determinants of FDI in South Africa: Do macroeconomic variables matter?|In this study we examine the macroeconomic determinants of FDI for the South African economy using data collected between 1994 and 2016 using the ARDL model for cointegration. The specific macroeconomic determinants which are used in the study are per capita GDP, the inflation rate, government size, real interest rate variable, and terms of trade. With the exception of inflation the remaining macroeconomic determinants employed in the study are positively and significantly related with FDI. However, in the short-run all variables are positively and significantly correlated with FDI. Collectively, these results have important implications for policymakers.
C52|Forecasting tax revenues in an emerging economy: The case of Albania|Fiscal balance is one of the main concerns of fiscal policy. Although academic and political choices on budget deficit vary due to perspective differences, improving the quality of revenue and expenditure forecasting has become prominent. The seminal researches on this topic present that tax revenue forecasts suffer from high positive biases. As tax forecasts have chain implications on the expenditures side as well, this might lead to high unexpected deficits. According to the IMF 2016 country report on Albania, emerging market economies are suffering higher than advanced ones in tax revenue forecasting. The aim of this paper is to implement new forecasting models and to apply forecast combinations for Albania, where forecast errors are higher than average. The estimation results show that influence of internal and external factors on tax revenue forecasting create a significant improvement on tax revenue accuracy. The estimations and forecast combinations of this paper perform lower errors than official forecasts, which indicate that revision of tax forecasting methodology can increase the accuracy of predictions for emerging market economies.
C52|The behaviour of disaggregated transitory and potential output over the economic cycle|This paper examines the behaviour of disaggregated transitory and potential output over the economic cycle in South Africa. Aggregate output and output of the economic sectors and industries were decomposed into their transitory and potential components. These components were then examined for comovement. The results of the transitory component generally show a moderate to strong positive comovement between aggregate output and output of all the economic sectors and majority of the industries. The results of the potential component have generally show a weak positive comovement between aggregate output and output of majority of economic sectors and the economic industries. A generally weak comovement between aggregate output and output of general government services and community, social and personal services highlights a more laissez faire approach to economic management. Contrary to the investment literature, there does not seem to be a definite distinction between the companies industry categories, such as the defensive, cyclical and sensitive industries.
C52|Two Distinct Seasonally Fractionally Differenced Periodic Processes|This article is devoted to study the e¤ects of the S-periodical fractional di¤erencing filter (1-L^S)^Dt . To put this e¤ect in evidence, we have derived the periodic auto-covariance functions of two distinct univariate seasonally fractionally di¤erenced periodic models. A multivariate representation of periodically correlated process is exploited to provide the exact and approximated expression auto-covariance of each models. The distinction between the models is clearly obvious through the expression of periodic auto-covariance function. Besides producing di¤erent autocovariance functions, the two models di¤er in their implications. In the first model, the seasons of the multivariate series are separately fractionally integrated. In the second model, however, the seasons for the univariate series are fractionally co-integrated. On the simulated sample, for each models, with the same parameters, the empirical periodic autocovariance are calculated and graphically represented for illustrating the results and support the comparison between the two models.
C52|Comparison Between Static And Dynamic Forecast In Autoregressive Integrated Moving Average For Seasonally Adjusted Headline Consumer Price Index|This empirical study has provided interpretive outcome from a univariate forecast using Box-Jenkins ARIMA methodology. The HCPI_SA seasonally adjusted data for Sierra Leone shows a robust model outcome with three months ahead prediction based on the STATIC method result. Test results like Autocorrelation and also comparative values for MAPE and the Inverted Root values have indicated that the model is a good fit. Despite better choice of outcome from the STATIC result in comparison to DYNAMIC forecast, the conclusion a cautious means of advice when using results for policy outcomes and with comparative forecasts highly recommended a way forward in guiding policy makers' decision.
C52|Robust analysis of convergence in per capita GDP in BRICS economies|Whilst the issue of whether or not per capita GDP adheres to the convergence theory continues to draw increasing attention within the academic paradigm, with very little consensus having been reached in the literature thus far. Our study contributes to the literature by examining the stationarity of per capita GDP for BRICS countries using annual data collected between 1971 and 2015. Considering that our sample covers a period underlying a number of crisis and structural breaks within and amongst the BRICS countries, we rely on a robust nonlinear unit root testing procedure which captures a series of unobserved structural breaks. Our results confirm on Brazil and China being the only two BRICS economies who present the most convincing evidence of per capita GDP converging back to it’s natural equilibrium after an economic shock, whilst Russia and South Africa provide less convincing evidence of convergence dynamics in the time series and India having the weakest convergence properties.
C52|Inflation-Growth Nexus in Botswana: Can Lower Inflation Really Spur Growth in the Country?|Does inflation affect economic growth in Botswana over the short-run and long-run? In applying bounds procedure for modelling ARDL cointegation effects applied to empirical data collected between 1975 and 2016 we find that this hypothesis does not hold true for Botswana as inflation is found to be insignificantly related with economic growth over both the short and long-run. Our growth equation estimates point to exports (positive), government size (negative) and an Pula/Dollar exchange rate (negative) as being significantly correlated with steady-state GDP growth. Further empirical exercises show that an appreciated Pula/dollar exchange rate increases inflation whilst bearing no effect on economic growth. Conversely, a depreciated Pula/Dollar exchange simultaneously decreases inflation and economic growth for the Botswana economy. Policymakers should be this aware that attainment of lower inflation rates which occurs through a depreciated Pula/Dollar currency will only retard economic growth.
C52|Persistence of suicides in G20 countries: SPSM approach to three generations of unit root tests|Suicides represent an encompassing measure of psychological well-being, emotional stability as well as life satisfaction and have been recently identified by the World Health Organization (WHO) as a major global health concern. The G20 countries represent the powerhouse of global economic governance and hence possess the ability to influence the direction of global suicide rates. In applying the sequential panel selection method (SPSM) to three generations of unit root testing procedures, the study investigates whether G20 countries should be concerned with possible persistence within suicide rates. The results obtained from all three generation of tests provide rigid evidence of persistence within the suicides for most member states of the G20 countries hence supporting the current strategic agenda pushed by the WHO in reducing suicides to a target rate of 10 percent. In addition, we further propose that such strategies should emulate from within G20 countries and spread globally thereafter.
C52|Variational Bayes inference in high-dimensional time-varying parameter models|This paper proposes a mean field variational Bayes algorithm for efficient posterior and predictive inference in time-varying parameter models. Our approach involves: i) computationally trivial Kalman filter updates of regression coefficients, ii) a dynamic variables election prior that removes irrelevant variables in each time period, and iii) a fast approximate state-space estimator of the regression volatility parameter. In an exercise involving simulated data we evaluate the new algorithm numerically and establish its computational advantages. Using macroeconomic data for the US we find that regression models that combine time-varying parameters with the information in many predictors have the potential to improve forecasts over a number of alternatives.
C52|Synthesizing Cash for Clunkers: Stabilizing the Car Market, Hurting the Environment|We examine the impact of European car scrappage programs on new vehicle registrations and respective CO2 emissions. To construct proper counterfactuals, we develop MSCM-T, the multivariate synthetic control method using time series of economic predictors. Applying MSCM-T to a rich data set covering two outcomes of interest, ten economic predictors, and 23 countries, we first analyze Germany which implemented the largest program. We find that the German subsidy had an immensely positive effect of 1.3 million program-induced new car registrations. Disentangling this effect reveals that almost one million purchases were not pulled forward from future periods, worth more than three times the program's 5 billion budget. However, stabilizing the car market came at the cost of 2.4 million tons of additional CO2 emissions. For other European countries with comparable car retirement schemes, we show further positive results regarding vehicle registrations. Finally, we demonstrate that all non-scrapping countries could have considerably backed their vehicle markets by adopting scrappage subsidies.
C52|Endogenous monetary approach to optimal inflation-growth nexus in Swaziland|With the inflation-growth nexus being a hotly debated issue within the academic paradigm, the purpose of our study is to examine the relationship for Swaziland between 1975 and 2016 of which there currently exists very limited country-specific evidence. In the design of our study we theoretically depend on an endogenous monetary model of economic growth augmented with a credit technology which causes a nonlinear relationship between inflation and growth. Econometrically, we rely on the smooth transition regression (STR) which allows us to estimate an optimal inflation rate characterized by smooth transition between different inflation regimes. Our empirical results point to an inflation threshold estimate of 7.64% at which economic growth gains are maximized or similarly growth losses are minimized. In particular, we find that above the inflation threshold economic agents may be able to protect themselves from inflation through credit technology and a more urbanized population yet such high inflation adversely affects the influence of exports on economic growth. This noteworthy since a majority of government revenues is from trade activity via the country’s affiliation with the Southern African Customs Union (SACU). Nevertheless, the major contribution of this paper is that it becomes the first to use endogenous growth theory to estimate the inflation threshold for any African country which will hopefully pave a way for similar studies on other African countries.
C52|Drivers Of Growth In Fast Emerging Economies: A Dynamic Instrumental Quantile Approach To Real Output And Its Rates Of Growth In Brics And Mint Countries, 2001-2011|We analyze the evolution of fast emerging economies of the BRICS (Brazil, Russia, India, China & South Africa) and MINT (Mexico, Indonesia, Nigeria & Turkey) countries, by assessing growth determinants throughout the conditional distributions of the growth rate and real GDP output for the period 2001-2011. An instrumenal variable (IV) quantile regression approach is complemented with Two-Stage-Least Squares and IV Least Absolute Deviations. We find that the highest rates of growth of real GDP per head, among the nine countries of this study, corresponded to China, India, Nigeria, Indonesia and Turkey, but the highest increases in real GDP per capita corresponded, in descending order, to Turkey China, Brazil, South Africa and India. This study analyzes the impacts of several indicators on the increase of the rate of growth of real GDP and on the logarithm of the real GDP. We analyze several limitations of the methodology, related with the selection of the explained and the explanatory variables, the effect of missing variables, and the particular problems of some indicators. Our results show that Net Foreign Direct Investment, Natural Resources, and Political Stability have a positive and significant impact on the rate of growth of real GDP or on real GDP.
C52|Environmental degradation and inclusive human development in sub‐Saharan Africa|In the light of challenges to sustainable development in the post‐2015 development agenda, this paper assesses how increasing carbon dioxide (CO2) emissions affect inclusive human development in 44 countries in sub‐Saharan Africa for the period 2000–2012. The following findings are established from Fixed Effects and Tobit regressions: first, unconditional effects and conditional impacts are respectively positive and negative from CO2 emissions per capita, CO2 emissions from liquid fuel consumption, and CO2 intensity. This implies a Kuznets‐shaped curve because of consistent decreasing returns; and second, the corresponding net effects are consistently positive. The following findings are apparent from Generalized Method of Moments regressions: first, unconditional effects and conditional impacts are respectively negative and positive from CO2 emissions per capita, CO2 emissions from liquid fuel consumption, and CO2 intensity. This implies a U‐shaped curve because of consistent increasing returns; and second, the corresponding net effects are overwhelmingly negative. Based on the robust findings and choice of best estimator, the net effect of increasing CO2 emissions on inclusive human development is negative. Policy implications are discussed.
C52|Model simplification and variable selection: A Replication of the UK inflation model by Hendry (2001)|In this paper, we revisit the well-known UK inflation model by Hendry (Journal of Applied Econometrics 2001, 16:255-275. doi: 10.1002/jae.615). We replicate the results in a narrow sense using the gretl and PcGive programs. In a wide sense, we extend the study of model uncertainty using the Bayesian averaging of classical estimates (BACE) approach to compare model reduction strategies. Allowing for the investigation of other specifications, we confirm the same set of significant determinants but find that Hendrys' model is not the most probable.
C52|A Sequential Panel Selection Approach to Cointegration Analysis: An Application to Wagner’s Law for South African Provincial Data|The main aim of this study is to extend the recently introduced sequential panel selection method (SPSM) to a cointegration framework which is particularly used to investigate Wagner’s law for 9 South African provinces. We particularly apply the SPSM to the PMG and ARDL cointegration frameworks which we apply to annual data spanning from 2001 to 2016. The main findings show that when applying single country/region estimates we fail to find evidence of cointegration whereas within panel regressions, cointegration effects are present for theentire dataset. In further applying the SPSM we observed significant Wagner’s effects for panels inclusive of Gauteng, Eastern Cape and Kwazulu-Natal provinces and when these provinces are excluded from the panels, cointegration effects are unobserved.
C52|CO2 emission thresholds for inclusive human development in Sub-Saharan Africa|We provide policy-relevant critical masses beyond which, increasing CO2 emissions negatively affects inclusive human development. This study examines how increasing CO2 emissions affects inclusive human development in 44 Sub-Saharan African countries for the period 2000-2012. The empirical evidence is based on Fixed Effects and Tobit regressions. In order to increase the policy relevance of this study, the dataset is decomposed into fundamental characteristics of inclusive development and environmental degradation based on income levels (Low income versus (vs.) Middle income); legal origins (English Common law vs. French Civil law); religious domination (Christianity vs. Islam); openness to sea (Landlocked vs. Coastal); resource-wealth (Oil-rich vs. Oil-poor) and political stability (Stable vs. Unstable). All computed thresholds are within policy range. Hence, above these thresholds, CO2 emissions negatively affect inclusive human development.
C52|The role of inclusive development and military expenditure in modulating the effect of terrorism on governance|Purpose- The study investigates the role of inclusive human development and military expenditure in modulating the effect of terrorism on governance. Design/methodology/approach- It is based on 53 African countries for the period 1998-2012 and interactive Generalised Method of Moments is employed. Six governance indicators from the World Bank and two terrorism variables are used, namely: domestic and transnational terrorism dynamics. Findings- The following main findings are established. There is a negative net effect on governance (regulation quality and corruption-control) when inclusive human development is used to reduce terrorism. There is a positive net impact on governance (“voice and accountability” and rule of law) when military expenditure is used to reduce domestic terrorism. Originality/value- We have complemented the sparse literature on the use of policy variables to mitigate the effect of policy syndromes on macroeconomic outcomes.
C52|Pursuing the Phillips curve in an African monarchy: The Swazi case|The purpose of this study is to examine whether we can identify a Philips curve fit for the Kingdom of Swaziland as a low middle income Sub-Saharan Africa monarchy using data collected between 1991 and 2016. In our approach we rely on the recently introduced nonlinear autoregressive distributive lag (N-ARDL) model to a variety of Phillips curve specifications. For robustness sake, we further employ three filters (one-sided HP, two-sided HP and Corbae-Oularis filters) to extract the gap variables necessary for empirical analysis. Our findings point to a linear, short-run traditional Philips curve whereas we find strong support for concave shaped unemployment-gap and output –gap based Phillips curve specifications. Given the specific form of concavity discovered in the Phillips curves, the low inflation rate experienced over the last couple of decades can be attributed to a worsening labour and goods markets. Moreover, our evidence also cautions Swazi policymakers of ‘overheating’ of the economy during economic booms in which stabilization tools are required to implemented in such instances. Given the overall absence of empirical studies establishing the Philips curve for the Swazi economy our study makes a valid contribution to the literature.
C52|The predictive relationship between exchange rate expectations and base metal prices|In this paper we show that survey-based-expectations about the future evolution of the Chilean exchange rate have the ability to predict the returns of the six primary non-ferrous metals: aluminum, copper, lead, nickel, tin and zinc. Predictability is also found for returns of the London Metal Exchange Index. Previous studies have shown that the Chilean exchange rate has the ability to predict copper returns, a world commodity index and base metal prices. Nevertheless, our results indicate that expectations about the Chilean peso have stronger predictive ability relative to the Chilean currency. This is shown both in-sample and out-of-sample. By focusing on expectations of a commodity currency, and not on the currency itself, our paper provides indirect but new and strong evidence of the ability that commodity currencies have to forecast commodity prices. Our results are also consistent with the present-value-model for exchange rate determination.
C52|Exploring the Driving Forces of the Bitcoin Exchange Rate Dynamics: An EGARCH Approach|Bitcoin is a virtual currency scheme that is characterised by a decentralised network and cryptographic transfer veriﬁcation which has been attracting much public attention due to its technological innovation and its high exchange rate volatility. In this paper, Bitcoin’s exchange rate movement from 2011 to 2018 and its relationship with the global ﬁnancial markets are explored using an EGARCH framework. The results are as follows. First, fundamentals and Bitcoin-related events play a critical role in the exchange rate formation of Bitcoin. Second, the impact of regulation-related events on Bitcoin indicates that market sentiment is responding to market regulation statements. Third, news coverage is an essential factor in driving the volatility of Bitcoin. Fourth, Bitcoin may be a hedge in times of calm ﬁnancial markets and a safe haven against uncertain economic policy but is likely to expose to ﬂight-to-quality as global ﬁnancial uncertainty increases. Lastly, the positive effect of the central bank’s announcements on Bitcoin is marginal enough to rule out the involvement of global expansionary monetary policy in inﬂating Bitcoin’s exchange rate over the past years, as it may have been the case with traditional asset prices after the great recession.
C52|Renewable energy-economic growth nexus in South Africa: Linear, nonlinear or non-existent?|With escalating fears of climate change reaching irreversible levels, much emphasis has been recently placed on shifting to renewable sources of energy in supporting future economic livelihood. Focusing on South Africa, as Africa’s largest energy consumer and producer, our study investigates the short-run and long-run effects of renewable energy on economic growth using linear and nonlinear autoregressive distributive lag (ARDL) models. Working with data availability, our empirical analysis is carried out over the period of 1991 to 2016, and our results unanimously fail to confirm any linear or nonlinear cointegration effects of the consumption and production of renewable energy on South African economic growth. We view the absence of cointergation relations as an indication of inefficient usage of renewable energy in supporting sustainable growth in South Africa and hence advise policymakers to accelerate the establishment of necessary renewable infrastructure in supporting future energy requirements.
C52|On Identification Issues in Business Cycle Accounting Models|Since its introduction by Chari et al. (2018), Business Cycle Accounting (BCA) exercises have become widespread. Much attention has been devoted to the results of such exercises and to methodological departures from the baseline methodology. Little attention has been paid to identification issues within these classes of models, despite the methodology typically involving estimating relatively large scale dynamic stochastic general equilibrium models. In this paper we investigate whether such issues are of concern in the original methodology and in an extension proposed by Sustek (2011) called Monetary BCA. We resort to two types of identification tests in population. One concerns strict identification as theorized by Komuner and Ng (2011), while the other deals both with strict and weak identification as in Iskrev (2015). As to the former, when restricting the estimation to the parameters governing the latent variable's laws of motion, we find that both in the BCA and MBCA framework, all parameters fulfill the requirements for strict identification. If instead we estimate all structural parameters of the model jointly, both frameworks show strict identification failures in several parameters. These results hold for both tests. We show that restricting estimation of some deep parameters can obviate such failures. When we explore weak identification issues, we find that they affect both models. They arise from the fact that many of the estimated parameters do not have a distinct effect on the likelihood. Most importantly, we explore the extent to which these weak identification problems affect the main economic takeaways and find that the identification deficiencies are not relevant for the standard BCA model. Finally, we compute some statistics of interest to practitioners of the BCA methodology.
C52|Forecast ranked tailored equity portfolios|We use a dynamic model averaging (DMA) approach to construct forecasts of individual equity returns for a large cross-section of stocks contained in the SP500, FTSE100, DAX30, CAC40 and SPX30 headline indices, taking value, momentum, and quality factors as predictor variables. Fixing the set of ‘forgetting factors’ in the DMA prediction framework, we show that highly significant return forecasts relative to the historic average benchmark are obtained for 173 (281) individual equities at the 1% (5%) level, from a total of 895 stocks. These statistical forecast improvements also translate into considerable economic gains, producing out-of-sample R 2 values above 5% (10%) for 283 (166) of the 895 individual stocks. Equally weighted long only portfolios constructed from a ranking of the best 25% forecasts in each headline index can generate sizable returns in excess of a passive investment strategy in that index itself, even when transaction costs and risk taking are accounted for.
C52|Can we beat the Random Walk? The case of survey-based exchange rate forecasts in Chile|We examine the accuracy of survey-based expectations of the Chilean exchange rate relative to the US dollar. Our out-of-sample analysis reveals that survey-based forecasts outperform the Driftless Random Walk (DRW) in terms of Mean Squared Prediction Error at several forecasting horizons. This result holds true even when comparing the survey to a more competitive benchmark based on a refined information set. A similar result is found when precision is measured in terms of Directional Accuracy: survey-based forecasts outperform a “pure luck” benchmark at several forecasting horizons. Differing from the traditional “no predictability” result reported in the literature for many exchange rates, our findings suggest that the Chilean peso is indeed predictable.
C52|Model selection for modeling the demand for narrow money in transitional economies|The aim of this study was to verify the stability of monetary systems. Systems were measured by aggregate narrow money in selected emerging economies. The United Kingdom's economy was used as a benchmark. The Baumol-Tobin and Friedman monetary models were used as the theoretical basis for the for empirical error-correction models. A Bayesian averaging of classical estimates (BACE) approach was used to incorporate model uncertainty and select the best model. The results show that the monetary systems in 6 of the 11 economies were stable in the long run and that a set of factors changed in the short run. The robustness of the model selection based on the BACE procedure was strongly confirmed.
C52|Credit Risk and Fiscal Inflation|Is inflation ‘always and everywhere a monetary phenomenon’ or is it fundamentally a fiscal phenomenon? This article augments a standard monetary model to incorporate fiscal details and credit market frictions. These ingredients allow for both interpretations of the inflation process in a financially constrained environment. We find that adding financial frictions to the model generates important identifying restrictions on the observed pattern between inflation and measures of financial and fiscal stress, to the extent that it can overturn existing findings about which monetary-fiscal policy regime produced the pre-crisis U.S. data.
C52|A Frequency-Domain Approach to Dynamic Macroeconomic Models|This article is concerned with frequency-domain analysis of dynamic linear models under the hypothesis of rational expectations. We develop a unified framework for conveniently solving and estimating these models. Unlike existing strategies, our starting point is to obtain the model solution entirely in the frequency domain. This solution method is applicable to a wide class of models and permits straightforward construction of the spectral density for performing likelihood-based inference. To cope with potential model uncertainty, we also generalize the well-known spectral decomposition of the Gaussian likelihood function to a composite version implied by several competing models. Taken together, these techniques yield fresh insights into the model’s theoretical and empirical implications beyond what conventional time-domain approaches can offer. We illustrate the proposed framework using a prototypical new Keynesian model with fiscal details and two distinct monetary-fiscal policy regimes. The model is simple enough to deliver an analytical solution that makes the policy effects transparent under each regime, yet still able to shed light on the empirical interactions between U.S. monetary and fiscal policies along different frequencies.
C52|The Comparative Economics of ICT, Environmental Degradation and Inclusive Human Development in Sub-Saharan Africa|Abstract This study examines how information and communication technology (ICT) could be employed to dampen the potentially damaging effects of environmental degradation in order to promote inclusive human development in a panel of 44 Sub-Saharan African countries. ICT is captured with internet and mobile phone penetration rates whereas environmental degradation is measured in terms of CO2 emissions per capita and CO2 intensity. The empirical evidence is based on fixed effects and Tobit regressions using data from 2000 to 2012. In order to increase the policy relevance of this study, the dataset is decomposed into fundamental characteristics of inclusive development and environmental degradation based on income levels (low income vs. middle income); legal origins (English Common law vs. French Civil law); religious domination (Christianity vs. Islam); openness to sea (landlocked vs. coastal); resource-wealth (oil-rich vs. oil-poor) and political stability (stable vs. unstable). Baseline findings broadly show that improvement in both of measures of ICT would significantly diminish the possibly harmful effect of CO2 emissions on inclusive human development. When the analysis is extended with the above mentioned fundamental characteristics, we observe that the moderating influence of both our ICT variables on CO2 emissions is higher in the group of English Common law, middle income and oil-wealthy countries than in the French Civil law, low income countries and oil-poor countries respectively. Theoretical and practical policy implications are discussed.
C52|Determinants of foreign direct investment in fast-growing economies: evidence from the BRICS and MINT countries|Abstract The flow of foreign direct investment (FDI) into a country can benefit both the investing entity and host government. This study employed panel analysis to examine the factors that determine the direction of FDI to the fast-growing BRICS (Brazil, Russia, India, China, and South Africa) and MINT (Mexico, Indonesia, Nigeria, and Turkey) countries. First, we used a pooled time-series cross sectional analysis of data from 2001 to 2011 to estimate and model the determinants of FDI for three samples: BRICS only, MINT only, and BRICS and MINT combined. Then, a fixed effects approach was employed to provide the model for BRICS and MINT combined. The results demonstrate that market size, infrastructure availability, and trade openness play the most significant roles in attracting FDI to BRICS and MINT, while the roles of availability of natural resources and institutional quality are insignificant. To sustain and promote FDI inflow, the governments of BRICS and MINT must ensure that their countries remain attractive for investment by offering a level playing field for investors and political stability. BRICS and MINT governments also need to invest more in their human capital to ensure that their economies can absorb substantial skills and technology spillovers from FDI and promote sustainable long-term economic growth. This study is significant because it contributes to the literature on determinants of FDI by extending the scope of previous studies that often focused on BRICS only.
C52|Foreign Aid, Terrorism And Growth: Conditional Evidence From Quantile Regression|In this study, we investigate the role of development assistance in reducing a hypothetically negative impact of terrorism on economic growth, using a panel of 78 developing nations with data for the period 1984–2008. The empirical evidence is based on interactive quantile regressions. Domestic, transnational, unclear and total terrorism dynamics are employed while development assistance measurements comprised bilateral and multilateral aid variables. With regard to the investigated hypothesis, we consistently confirm that: (i) In quantiles where terrorism is found to increase (decrease) economic growth, its interaction with foreign aid decreases (increases) economic growth. (ii) Comparing thresholds of the modifying aid variables for which the hypothesis is either rejected or accepted reveals that higher levels of multilateral (bilateral) aid are needed to reverse the negative effect of total (unclear) terrorism on growth, than the quantity needed to reverse the positive impact of transnational (domestic and total) terrorism(s) on growth. (iii) There is scant evidence of positive net effects. Overall the findings broadly indicate that foreign aid is a necessary but not a sufficient policy tool for completely dampening the effects of terrorism on economic growth.
C52|Contemporary Drivers of Global Tourism: Evidence from Terrorism and Peace Factors|This study examines the effect of terrorism and peace on tourist destination arrivals using a panel of 163 countries with data for the period 2010 to 2015. The empirical evidence is based on Generalised Method of Moments and Negative Binomial (NB) regressions. Our best estimators are from NB regressions from which the following main findings are established. First, political instability, violent demonstrations and number of homicides negatively affect tourist arrivals while the number of incarcerations positively influences the outcome variable. Second the effects from military expenditure, “armed service personnel” and “security officers and polices” are not positively significant. Managerial implications are discussed.
C52|The Comparative African Economics of Governance in Fighting Terrorism|This study assesses the comparative economics of governance in fighting terrorism in 53 African countries for period 1996-2012. Four terrorism variables are used, namely: domestic, transnational, unclear and total terrorism dynamics. Nine bundled and unbundled governance variables are employed, notably: political stability/no violence, voice & accountability, political governance, government effectiveness, regulation quality, economic governance, corruption-control, the rule of law and institutional governance. The empirical evidence is based on Fixed Effects regressions. In the analytical procedure, we first bundle governance indicators by means of principal component analysis before engaging the empirical exercise with the full sample. In the final step, specifications are based on a decomposed full sample in order to articulate the fundamental characteristics for comparative purposes. The following broad findings are established. First, good governance is an appealing tool in fighting terrorism. Second, the relevance of the good governance dynamics is as follows in order of increasing relevance: economic governance, institutional governance and political governance. The findings are presented in increasing order of magnitude to emphasise fundamental features in which governance dynamics have the highest effect in mitigating terrorism.
C52|The Role of Monetary Policy Uncertainty in Predicting Equity Market Volatility of the United Kingdom: Evidence from over 150 Years of Data|Theory suggests a strong link between monetary policy rate uncertainty and equity return volatility, since asset pricing models assume the risk-free rate to be a key factor for equity prices. Given this, our paper uses historical monthly data for the United Kingdom over 1833:01 to 2018:07, to show that monetary policy uncertainty increases stock market volatility within sample. In addition, we show that the information on monetary policy uncertainty also adds value to forecasting out-of-sample equity market volatility.
C52|Calibration and the estimation of macroeconomic models|We propose two measures of the impact of calibration on the estimation of macroeconomic models.The first quantifies the amount of information introduced with respect to each estimated parameter as a result of fixing the value of one or more calibrated parameters.The second is a measure of the sensitivity of parameter estimates to perturbations in the calibration values.The purpose of the measures is to show researchers how much and in what way calibration affects their estimation results – by shifting the location and reducing the spread of the marginal posterior distributions of the estimated parameters.This type of analysis is often appropriate since macroeconomists do not always agree on whether and how to calibrate structural parameters in macroeconomic models. The methodology is illustrated using the models estimated in Smets and Wouters (2007) and Schmitt-Grohé and Uribe (2012).
C52|Are asset price data informative about news shocks? A DSGE perspective|Standard economic intuition suggests that asset prices are more sensitive to news than other economic aggregates.This has led many researchers to conclude that asset price data would be very useful for the estimation of business cycle models containing news shocks.This paper shows how to formally evaluate the information content of observed variables with respect to unobservedshocks in structural macroeconomic models.The proposed methodology is applied to two different real business cycle models with news shocks.The contribution of asset prices is found to be relatively small.The methodology is general and can be used to measure the informational importance of observables with respect to latent variables in DSGE models.Thus,it provides a framework for systematic treatment of such issues,which are usually discussed in an informal manner in the literature.
C52|Inference with difference-in-differences with a small number of groups: a review, simulation study and empirical application using SHARE data|Background - Difference-in-differences (DID) estimation has become increasingly popular as an approach to evaluate the effect of a group-level policy on individual-level outcomes. Several statistical methodologies have been proposed to correct for the within-group correlation of model errors resulting from the clustering of data. Little is known about how well these corrections perform with the often small number of groups observed in health research using longitudinal data. Methods - First, we review the most commonly used modelling solutions in DID estimation for panel data, including generalized estimating equations (GEE), permutation tests, clustered standard errors (CSE), wild cluster bootstrapping, and aggregation. Second, we compare the empirical coverage rates and power of these methods using a Monte Carlo simulation study in scenarios in which we vary the degree of error correlation, the group size balance, and the proportion of treated groups. Third, we provide an empirical example using the Survey of Health, Ageing and Retirement in Europe (SHARE). Results - When the number of groups is small, CSE are systematically biased downwards in scenarios when data are unbalanced or when there is a low proportion of treated groups. This can result in over-rejection of the null even when data are composed of up to 50 groups. Aggregation, permutation tests, bias-adjusted GEE and wild cluster bootstrap produce coverage rates close to the nominal rate for almost all scenarios, though GEE may suffer from low power. Conclusions - In DID estimation with a small number of groups, analysis using aggregation, permutation tests, wild cluster bootstrap, or bias-adjusted GEE is recommended.
C52|An Event Study of Chinese Tourists to Taiwan|The number of Chinese tourists visiting Taiwan has been closely related to the political relationship across the Taiwan Strait. The occurrence of political events and disasters or accidents have had, and will continue to have, a huge impact on the Taiwan tourism market. To date, there has been relatively little empirical research conducted on this issue. In this paper, tourists are characterized as being involved in one of three types of tourism: group tourism (group-type), individual tourism (individual-type), and medical cosmetology (medical-type). We use McAleer’s (2015) fundamental equation in tourism finance to examine the correlation that exists between the rate of change in the number of tourists and the rate of return on tourism. Second, we use the event study method to observe whether the numbers of tourists have changed abnormally before and after the occurrence of major events on both sides of the Strait. Three different types of conditional variance models, namely, GARCH (1,1), GJR (1,1) and EGARCH (1,1), are used to estimate the abnormal rate of change in the number of tourists. The empirical results concerning the major events affecting the changes in the numbers of tourists from China to Taiwan are economically significant, and confirm which types of tourists are most likely to be affected by such major events.
C52|Earnings Responses to Disability Benefit Cuts|Using Dutch administrative data, we assess the work and earnings capacity of disability insurance (DI) recipients by estimating employment and earnings responses to benefit cuts. Reassessment of DI entitlement under more stringent criteria removed 14.4 percent of recipients from the program and reduced benefits by 20 percent, on average. In response, employment increased by 6.7 points and earnings rose by 18 percent. Recipients were able to increase earnings by €0.64 for each €1 of DI income lost. Female and younger recipients, as well as those with more subjectively defined disabilities, were able to increase earnings most. The earnings response declined as claim duration lengthened, suggesting that earnings capacity deteriorates while on DI. The deterioration was steepest for male, younger and fully disabled recipients. Working while claiming partial disability benefits appears to slow the deterioration of earnings capacity.
C52|Classifying Firms with Text Mining|Statistics on the births, deaths and survival rates of firms are crucial pieces of information, as they enter as an input in the computation of GDP, the identification of each sectorâ€™s contribution to the economy, and the assessment of gross job creation and destruction rates. Official statistics on firm demography are made available only several months after data collection and storage, however. Furthermore, unprocessed and untimely administrative data can lead to a misrepresentation of the life-cycle stage of a firm. In this paper we implement an automated version of Eurostatâ€™s algorithm aimed at distinguishing true startup endeavors from the resurrection of pre-existing but apparently defunct firms. The potential gains from combining machine learning, natural language processing and econometric tools for pre- processing and analyzing granular data are exposed, and a machine learning method predicting reactivations of deceptively dead firms is proposed.
C52|Inference with difference-in-differences with a small number of groups: a review, simulation study and empirical application using SHARE data|Difference-in-differences (DID) estimation has become increasingly popular as an approach to evaluate the effect of a group-level policy on individual-level outcomes. Several statistical methodologies have been proposed to correct for the within-group correlation of model errors resulting from the clustering of data. Little is known about how well these corrections perform with the often small number of groups observed in health research using longitudinal data. First, we review the most commonly used modelling solutions in DID estimation for panel data, including generalized estimating equations (GEE), permutation tests, clustered standard errors (CSE), wild cluster bootstrapping, and aggregation. Second, we compare the empirical coverage rates and power of these methods using a Monte Carlo simulation study in scenarios in which we vary the degree of error correlation, the group size balance, and the proportion of treated groups. Third, we provide an empirical example using the Survey of Health, Ageing and Retirement in Europe (SHARE). When the number of groups is small, CSE are systematically biased downwards in scenarios when data are unbalanced or when there is a low proportion of treated groups. This can result in over-rejection of the null even when data are composed of up to 50 groups. Aggregation, permutation tests, bias-adjusted GEE and wild cluster bootstrap produce coverage rates close to the nominal rate for almost all scenarios, though GEE may suffer from low power. In DID estimation with a small number of groups, analysis using aggregation, permutation tests, wild cluster bootstrap, or bias-adjusted GEE is recommended.
C52|The fiction of full BEKK: Pricing fossil fuels and carbon emissions|The purpose of the paper is to (i) show that univariate GARCH is not a special case of multivariate GARCH, specifically the Full BEKK model, except under parametric restrictions on the off-diagonal elements of the random coefficient autoregressive coefficient matrix, that are not consistent with Full BEKK, and (ii) provide the regularity conditions that arise from the underlying random coefficient autoregressive process, for which the (quasi-) maximum likelihood estimates (QMLE) have valid asymptotic properties under the appropriate parametric restrictions. The paper provides a discussion of the stochastic processes that lead to the alternative specifications, regularity conditions, and asymptotic properties of the univariate and multivariate GARCH models. It is shown that the Full BEKK model, which in empirical practice is estimated almost exclusively compared with Diagonal BEKK (DBEKK), has no underlying stochastic process that leads to its specification, regularity conditions, or asymptotic properties, as compared with DBEKK. An empirical illustration shows the differences in the QMLE of the parameters of the conditional means and conditional variances for the univariate, DEBEKK and Full BEKK specifications.
C52|Stein-like Shrinkage Estimation of Panel Data Models with Common Correlated Effects|This paper examines the asymptotic properties of the Stein-type shrinkage combined (averaging) estimation of panel data models. We introduce a combined estimation when the fixed effects (FE) estimator is inconsistent due to endogeneity arising from the correlated common effects in the regression error and regressors. In this case the FE estimator and the CCEP estimator of Pesaran (2006) are combined. This can be viewed as the panel data model version of the shrinkage to combine the OLS and 2SLS estimators as the CCEP estimator is a 2SLS or control function estimator that controls for the endogeneity arising from the correlated common effects. The asymptotic theory, Monte Carlo simulation, and empirical applications are presented. According to our calculation of the asymptotic risk, the Stein-like shrinkage estimator is more efficient estimation than the CCEP estimator.
C52|A Combined Random Effect and Fixed Effect Forecast for Panel Data Models|When some of the regressors in a panel data model are correlated with the random individual effects, the random effect (RE) estimator becomes inconsistent while the fixed effect (FE) estimator is consistent. Depending on the various degree of such correlation, we can combine the RE estimator and FE estimator to form a combined estimator which can be better than each of the FE and RE estimators. In this paper, we are interested in whether the combined estimator may be used to form a combined forecast to improve upon the RE forecast (forecast made using the RE estimator) and the FE forecast (forecast using the FE estimator) in out-of-sample forecasting. Our simulation experiment shows that the combined forecast does dominate the FE forecast for all degrees of endogeneity in terms of mean squared forecast errors (MSFE), demonstrating that the theoretical results of the risk dominance for the in-sample estimation carry over to the out-of-sample forecasting. It also shows that the combined forecast can reduce MSFE relative to the RE forecast for moderate to large degrees of endogeneity and for large degrees of heterogeneity in individual effects.
C52|The Second-order Asymptotic Properties of Asymmetric Least Squares Estimation|"The higher-order asymptotic properties provide better approximation of the bias for a class of estimators. The first-order asymptotic properties of the asymmetric least squares (ALS) estimator have been investigated by Newey and Powell (1987). This paper develops the second-order asymptotic properties (bias and mean squared error) of the ALS estimator, extending the second-order asymptotic results for the symmetric least squares (LS) estimators of Rilstone, Srivastava and Ullah (1996). The LS gives the mean regression function while the ALS gives the ""expectile"" regression function, a generalization of the usual regression function. The second-order bias result enables an improved bias correction and thus an improved ALS estimation in finite sample. In particular, we show that the second-order bias is much larger as the asymmetry is stronger, and therefore the benefit of the second-order bias correction is greater when we are interested in extreme expectiles which are used as a risk measure in financial economics. The higher-order MSE result for the ALS estimation also enables us to better understand the sources of estimation uncertainty. The Monte Carlo simulation confirms the benefits of the second-order asymptotic theory and indicates that the second-order bias is larger at the extreme low and high expectiles."
C52|Combined Estimation of Semiparametric Panel Data Models|The combined estimation for the semiparametric panel data models is proposed. The properties of estimators for the semiparametric panel data models with random effects (RE) and fixed effects (FE) are examined. When the RE estimator suffers from endogeneity due to the individual effects correlated with the regressors, the semiparametric RE and FE estimators may be adaptively combined, with the combining weights depending on the degree of endogeneity. The asymptotic distributions of these three estimators (RE, FE, and combined estimators) for the semiparametric panel data models are derived using a local asymptotic framework. These three estimators are then compared in asymptotic risk. The semiparametric combined estimator has strictly smaller asymptotic risk than the semiparametric fixed effect estimator. The Monte Carlo study shows that the semiparametric combined estimator outperforms semiparametric FE and RE estimators except when the degrees of endogeneity and heterogeneity of the individual effects are very small. Also presented is an empirical application where the effect of public sector capital in the private economy production function is examined using the US state level panel data.
C52|The Demand for Money at the Zero Interest Rate Bound|This paper estimates a money demand function using US data from 1980 onward, including the period of near-zero interest rates following the global financial crisis. We conduct cointegration tests to show that the substantial increase in the money-income ratio during the period of near-zero interest rates is captured well by the money demand function in log-log form, but not by that in semi-log form. Our result is the opposite of the result obtained by Ireland (2009), who, using data up until 2006, found that the semi-log specification performs better. The difference in the result from Ireland (2009) mainly stems from the difference in the observation period employed: our observation period contains 24 quarters with interest rates below 1 percent, while Ireland's (2009) observation period contains only three quarters. We also compute the welfare cost of inflation based on the estimated money demand function to find that it is very small: the welfare cost of 2 percent inflation is only 0.04 percent of national income, which is of a similar magnitude as the estimate obtained by Ireland (2009) but much smaller than the estimate by Lucas (2000).
C52|Confidence intervals for bias and size distortion in IV and local projections — IV models|In this paper we propose methods to construct confidence intervals for the bias of the two-stage least squares estimator, and the size distortion of the associated Wald test in instrumental variables models. Importantly our framework covers the local projections — instrumental variable model as well. Unlike tests for weak instruments, whose distributions are non-standard and depend on nuisance parameters that cannot be estimated consistently, the confidence intervals for the strength of identification are straightforward and computationally easy to calculate, as they are obtained from inverting a chi-squared distribution. Furthermore, they provide more information to researchers on instrument strength than the binary decision offered by tests. Monte Carlo simulations show that the confidence intervals have good small sample coverage. We illustrate the usefulness of the proposed methods to measure the strength of identification in two empirical situations: the estimation of the intertemporal elasticity of substitution in a linearized Euler equation, and government spending multipliers.
C52|Unambiguous inference in sign-restricted VAR models|This paper demonstrates how sign restrictions can be used to infer the signs of certain historical shocks from reduced form VAR residuals. This is achieved without recourse to non-sign information. The method is illustrated by an application to the AD-AS model using UK data.
C52|A scoring rule for factor and autoregressive models under misspecification|Factor models (FM) are now widely used for forecasting with large set of time series. Another class of models, which can be easily estimated and used in a large dimensional setting, is multivariate autoregressive models (MAR), where independent autoregressive processes are assumed for the series in the panel. We compare the forecasting abilities of FM and MAR models when assuming both models are misspecified and the data generating process is a vector autoregressive model. We establish which conditions need to be satisfied for a FM to overperform MAR in terms of mean square forecasting error. The condition indicates in presence of misspecification that FM is not always overperforming MAR and that the FM predictive performance depends crucially on the parameter values of the data generating process. Building on the theoretical relationship between FM and MAR predictive performances, we provide a scoring rule which can be evaluated on the data to either select the model, or combine the models in forecasting exercises. Some numerical illustrations are provided both on simulated data and on wel-known large economic datasets. The empirical results show that the frequency of the true positive signals is larger when FM and MAR forecasting performances differ substantially and it decreases as the horizon increases.
C52|Modelling the spreading process of extreme risks via a simple agent-based model: Evidence from the China stock market|This paper focuses on investigating financial asset returns' extreme risks, which are defined as the negative log-returns over a certain threshold. A simple agent-based model is constructed to explain the behavior of the market traders when extreme risks occur. We consider both the volatility clustering and the heavy tail characteristics when constructing the model. Empirical study uses the China securities index 300 daily level data and applies the method of simulated moments to estimate the model parameters. The stationarity and ergodicity tests provide evidence that the proposed model is good for estimation and prediction. The goodness-of-fit measures show that our proposed model fits the empirical data well. Our estimated model performs well in out-of-sample Value-at-Risk prediction, which contributes to the risk management.
C52|Parametric models for biomarkers based on flexible size distributions|Recent advances in social science surveys include collection of biological samples. Although biomarkers offer a large potential for social science and economic research, they impose a number of statistical challenges, often being distributed asymmetrically with heavy tails. Using data from the UK Household Panel Survey, we illustrate the comparative performance of a set of flexible parametric distributions, which allow for a wide range of skewness and kurtosis: the four‐parameter generalized beta of the second kind (GB2), the three‐parameter generalized gamma, and their three‐, two‐, or one‐parameter nested and limiting cases. Commonly used blood‐based biomarkers for inflammation, diabetes, cholesterol, and stress‐related hormones are modelled. Although some of the three‐parameter distributions nested within the GB2 outperform the latter for most of the biomarkers considered, the GB2 can be used as a guide for choosing among competing parametric distributions for biomarkers. Going “beyond the mean” to estimate tail probabilities, we find that GB2 performs fairly well with some disparities at the very high levels of glycated hemoglobin and fibrinogen. Commonly used linear models are shown to perform worse than almost all the flexible distributions.
C52|On a quest for robustness: About model risk, randomness and discretion in credit risk stress tests|"In this paper we study the impact of model uncertainty, which occurs when linking a stress scenario to default probabilities, on reduced-form credit risk stress testing. This type of uncertainty is omnipresent in most macroeconomic stress testing applications due to short time series for banks' portfolio risk parameters and highly collinear macroeconomic covariates. We quantify the effect of model uncertainty on supervisory and bank stress tests in terms of predicted portfolio loss distributions and implied capital shortfalls by conducting a full-edged top-down credit risk stress test for over 1,500 German banks. Our results suggest that the impact of model uncertainty on predicted capital shortfalls can be huge, even among models with similar predictive power. This leaves both banks and supervisors with uncertainty when calculating stress impacts and implied capital requirements. To mitigate the impact of uncertainty, we suggest a modeling approach which filters the model space by combining the standard Bayesian model averaging (BMA) paradigm with a structural filter derived from the Merton/Vasicek credit risk model. Applying our stress testing framework, the dispersion decreases and the median stress effect is reduced from -5.0pp of CET1 ratio under the BMA model to -2.5pp under the structurally augmented BMA model, while the predicted capital shortfall is reduced by 70 %. The structural filter eliminates extreme outcomes on both sides of the stress forecast distribution, leading in our application to the German banking sector to a reduction in impact compared to the model without the ""stress testing plausibility"" filter."
C52|Theory and Practice of Testing for a Single Structural Break in Stata|The major objective of this paper is to demonstrate, theoretically and empirically, the test of a single structural break/change. Failure to address a structural break can lead to forecasting errors and the general unreliability of a model. Three approaches of testing for structural change are discussed using data from Johnston et al. (1997, p.130) on Stata 14 software. The first approach assesses whether there is a structural break in parameters (slope and intercept) while the second and third assess whether there is a break in slope and intercept respectively. The Residual Sum of Squares (RSS) for the restricted and unrestricted models are established to necessitate the use of an F-test in making inferences. According to the first approach, a structural break exists at 5% level of significance. This result is confirmed by the Chow test. The second and third approaches establish that the structural break is from the intercept and not the slope. These results are also affirmed by the Chow test. Furthermore, all these results, from the first to the third approach, are confirmed by an alternative approach which relies on the knowledge that . Therefore, the dependent variable is not affected by the policy change on the explanatory variable but it is mainly affected by the basic unobserved qualitative characteristics of the two sub-periods. For further analysis, it is recommended that a unit root test be conducted using the Zivot-Andrews test. This test has been established as the panacea for the interplay between unit root and structural changes.
C52|An investigation into the dependence structure of major cryptocurrencies|This paper attempts to examine the dependence structure of four major cryptocurrencies chosen by current market capitalisation. It is a well known fact that there is huge volatility in the prices of these cryptocurrencies. The Vine Copula model is used to get some insights about the dependence structure in these asset prices. This is done using daily closing price from August 2015 to May 2018. This information can be used to calculate risk based metrics such as expected shortfall of a portfolio of these currencies. This analysis becomes more important as complex ﬁnancial instruments (e.g. indices) based on these currencies are being introduced.
C52|How valid are synthetic panel estimates of poverty dynamics?|Abstract A growing literature uses repeated cross-section surveys to derive ‘synthetic panel’ data estimates of poverty dynamics statistics. It builds on the pioneering study by Dang et al. (‘DLLM’, Journal of Development Economics, 2014) providing bounds estimates and the innovative refinement proposed by Dang and Lanjouw (‘DL’, World Bank Policy Research Working Paper 6504, 2013) providing point estimates of the statistics of interest. We provide new evidence about the accuracy of synthetic panel estimates relative to benchmarks based on estimates derived from genuine household panel data, employing high quality data from Australia and Britain, while also examining the sensitivity of results to a number of analytical choices. For these two high-income countries we show that DL-method point estimates are distinctly less accurate than estimates derived in earlier validity studies, all of which focus on low- and middle-income countries. We also demonstrate that estimate validity depends on choices such as the age of the household head (defining the sample), the poverty line level, and the years analyzed. DLLM parametric bounds estimates virtually always include the true panel estimates, though the bounds can be wide.
C52|How valid are synthetic panel estimates of poverty dynamics?|Abstract A growing literature uses repeated cross-section surveys to derive ‘synthetic panel’ data estimates of poverty dynamics statistics. It builds on the pioneering study by Dang et al. (‘DLLM’, Journal of Development Economics, 2014) providing bounds estimates and the innovative refinement proposed by Dang and Lanjouw (‘DL’, World Bank Policy Research Working Paper 6504, 2013) providing point estimates of the statistics of interest. We provide new evidence about the accuracy of synthetic panel estimates relative to benchmarks based on estimates derived from genuine household panel data, employing high quality data from Australia and Britain, while also examining the sensitivity of results to a number of analytical choices. For these two high-income countries we show that DL-method point estimates are distinctly less accurate than estimates derived in earlier validity studies, all of which focus on low- and middle-income countries. We also demonstrate that estimate validity depends on choices such as the age of the household head (defining the sample), the poverty line level, and the years analyzed. DLLM parametric bounds estimates virtually always include the true panel estimates, though the bounds can be wide.
C52|Generalized Exogenous Processes in DSGE: A Bayesian Approach|The Reversible Jump Markov Chain Monte Carlo (RJMCMC) method can enhance Bayesian DSGE estimation by sampling from a posterior distribution spanning potentially nonnested models with parameter spaces of different dimensionality. We use the method to jointly sample from an ARMA process of unknown order along with the associated parameters. We apply the method to the technology process in a canonical neoclassical growth model using post war US GDP data and find that the posterior decisively rejects the standard AR(1) assumption in favor of higher order processes. While the posterior contains significant uncertainty regarding the exact order, it concentrates posterior density on hump-shaped impulse responses. A negative response of hours to a positive technology shock is within the posterior credible set when noninvertible MA representations are admitted.
C52|Nonlinear Relationship between Exchange Rate Volatility and Economic Growth|In this paper, we challenge the traditional assumption of a linear relationship between exchange rate volatility and economic growth in South Africa. By using data collected from 1970 to 2016 applied to a smooth transition regression (STR) model, we are able to prove that the exchange rate-economic growth correlation is indeed nonlinear within the sampled time period. In particular, we find that regime switching behaviour is facilitated by government size in which exchange rate volatility positively and significantly influences economic growth when growth in government spending is below 6 percent. Above this 6 percent threshold, volatility exerts an insignificant effect on economic growth. In light of the adoption of a free floating exchange rate regime by the Reserve Bank, our results emphasize the importance of the role which fiscal authorities play on the extent to which exchange rate movements affect economic growth.
C52|Replication and robustness analysis of 'energy and economic growth in the USA: a multivariate approach'|We replicate Stern (1993, Energy Economics), who argues and empirically demonstrates that it is necessary (i) to use quality-adjusted energy use and (ii) to include capital and labor as control variables in order to find Granger causality from energy use to GDP. Though we could not access the original dataset, we can verify the main original inferences using data that are as close as possible to the original. We analyze the robustness of the original findings to alternative definitions of variables, model specifications, and estimation approach for both the (almost) original time span (1949- 1990) and an extended time span (1949-2015). p-values tend to be substantially smaller if energy use is quality adjusted rather than measured by total joules and if capital is included. Including labor has mixed results. These findings tend to largely support Stern’s (1993) two main conclusions and emphasize the importance of accounting for changes in the energy mix in time series modeling of the energy-GDP relationship and controlling for other factors of production. We also discuss how the inclusion of the original author in designing the replication study using a pre-analysis plan can help to counterbalance the incentive of replicating authors to disconfirm major findings of the original article to increase the probability of getting published.
C52|Comparing hybrid time-varying parameter VARs|Empirical questions such as whether the Phillips curve or the Okun’s law is stable can often be framed as a model comparison—e.g., comparing a vector autoregression (VAR) in which the coefficients in one equation are constant versus one that has time-varying parameters. We develop Bayesian model comparison methods to compare a class of time-varying parameter VARs we call hybrid TVP-VARs—VARs with time-varying parameters in some equations but constant coefficients in others. Using US data, we find evidence that the VAR coefficients in some, but not all, equations are time varying. Our finding highlights the empirical relevance of these hybrid TVP-VARs.
C52|Stochastic volatility models with ARMA innovations: An application to G7 inflation forecasts|We introduce a new class of stochastic volatility models with autoregressive moving average (ARMA) innovations. The conditional mean process has a flexible form that can accommodate both a state space representation and a conventional dynamic regression. The ARMA component introduces serial dependence which renders standard Kalman filter techniques not directly applicable. To overcome this hurdle we develop an efficient posterior simulator that builds on recently developed precision based algorithms. We assess the usefulness of these new models in an inflation forecasting exercise across all G7 economies. We find that the new models generally provide competitive point and density forecasts compared to standard benchmarks, and are especially useful for Canada, France, Italy and the US.
C52|Nowcasting New Zealand GDP using machine learning algorithms|This paper analyses the real-time nowcasting performance of machine learning algorithms estimated on New Zealand data. Using a large set of real-time quarterly macroeconomic indicators, we train a range of popular machine learning algorithms and nowcast real GDP growth for each quarter over the 2009Q1-2018Q1 period. We compare the predictive accuracy of these nowcasts with that of other traditional univariate and multivariate statistical models. We find that the machine learning algorithms outperform the traditional statistical models. Moreover, combining the individual machine learning nowcasts further improves the performance than in the case of the individual nowcasts alone.<br><small>(This abstract was borrowed from another version of this item.)</small>
C52|Poorly Measured Confounders are More Useful on the Left than on the Right| Researchers frequently test identifying assumptions in regression-based research designs (which include instrumental variables or difference-in-differences models) by adding additional control variables on the right-hand side of the regression. If such additions do not affect the coefficient of interest (much), a study is presumed to be reliable. We caution that such invariance may result from the fact that the observed variables used in such robustness checks are often poor measures of the potential underlying confounders. In this case, a more powerful test of the identifying assumption is to put the variable on the left-hand side of the candidate regression. We provide derivations for the estimators and test statistics involved, as well as power calculations, which can help applied researchers interpret their findings. We illustrate these results in the context of estimating the returns to schooling.
C52|Bayesian estimation of DSGE models: Identification using a diagnostic indicator|Koop et al. (2013) suggest a simple diagnostic indicator for the Bayesian estimation of the parameters of a DSGE model. They show that, if a parameter is well identified, the precision of the posterior should improve as the (artificial) data size T increases, and the indicator checks the speed at which precision improves. As it does not require any additional programming, a researcher just needs to generate artificial data and estimate the model with increasing sample size, T. We apply this indicator to the benchmark Smets and Wouters (2007) DSGE model of the US economy, and suggest how to implement this indicator on DSGE models.
C52|Agnostic Structural Disturbances (ASDs): Detecting and Reducing Misspecification in Empirical Macroeconomic Models|Exogenous random structural disturbances are the main driving force behind fluctuations in most business cycle models and typically a wide variety is used. This paper documents that a minor misspecification regarding structural disturbances can lead to large distortions for parameter estimates and implied model properties, such as impulse response functions with a wrong shape and even an incorrect sign. We propose a novel concept, namely an agnostic structural disturbance (ASD), that can be used to both detect and correct for misspecification of the structural disturbances. In contrast to regular disturbances and wedges, ASDs do not impose additional restrictions on policy functions. When applied to the Smets-Wouters (SW) model, we find that its risk-premium disturbance and its investment-specific productivity disturbance are rejected in favor of our ASDs. While agnostic in nature, studying the estimated associated coefficients and the impulse response functions of these ASDs allows us to interpret them economically as a risk-premium/preference and an investment-specific productivity type disturbance as in SW, but our results indicate that they enter the model quite differently than the original SW disturbances. Our procedure also selects an additional wage mark-up disturbance that is associated with increased capital efficiency.
C52|The Missing Link: Monetary Policy and The Labor Share|The New-Keynesian transmission mechanism of monetary policy has clear implications for the behavior of the labor share. In the basic version of the model, the labor share is negatively related to the price markup and hence is pro-cyclical conditional on monetary policy shocks. However, little empirical evidence is available on the effect of monetary policy on the labor share and its components. We present a comprehensive cross country empirical analysis and find that the data are at odds with the theory. Cyclically, a monetary policy tightening increased the labor share and decreased real wages and labor productivity during the Great Moderation period in the US, the Euro Area, the UK, Australia and Canada. We then examine models allowing for a wide range of nominal and real rigidities that are important to separate the dynamics of the markup and the labor share. We show that models that do a good job at reproducing the responses of real variables to a monetary policy shock are unable to reproduce the responses of the labor share observed in the data.
C52|Sample selection biases and the historical growth pattern of children|Bodenhorn et al. (2017) have recently sparked considerable controversy by arguing that the fall in adult stature observed in military samples in the United States and Britain during industrialisation was a figment of sample selection bias. While subsequent papers have questioned the extent of the bias (Komlos and A’Hearn 2016; Zimran 2017), there is renewed concern about selection bias in historical anthropometric datasets. This paper extends Bodenhorn et al.’s discussion of selection bias on unobservables to sources of children’s growth, specifically focussing on biases that could distort the age pattern of growth. Understanding how the growth pattern of children has changed is important since these changes underpinned the secular increase in adult stature and are related to child stunting observed in developing countries today. However, there is potential for selection on unobservables in historical datasets containing children’s and adolescents’ height, so scholars must be aware of these biases before analysing these sources. This paper highlights, among others, three common sources of bias: 1) positive selection of children into secondary school in the late nineteenth and early twentieth centuries; 2) distorted height by age profiles created by age thresholds for enlistment in the military; and 3) changing institutional ecology which determines to which institutions children are sent. Accounting for these biases weakens the evidence of a strong pubertal growth spurt in the nineteenth century and raises doubts on some long run analyses of changes in children’s growth, especially for Japan.
C52|An Event Study of Chinese Tourists to Taiwan|The number of Chinese tourists visiting Taiwan has been closely related to the political relationship across the Taiwan Strait. The occurrence of political events and disasters or accidents have had, and will continue to have, a huge impact on the Taiwan tourism market. To date, there has been relatively little empirical research conducted on this issue. In this paper, tourists are characterized as being involved in one of three types of tourism: group tourism (group-type), individual tourism (individual-type), and medical cosmetology (medical-type). We use McAleer’s (2015) fundamental equation in tourism finance to examine the correlation that exists between the rate of change in the number of tourists and the rate of return on tourism. Second, we use the event study method to observe whether the numbers of tourists have changed abnormally before and after the occurrence of major events on both sides of the Strait. Three different types of conditional variance models, namely, GARCH (1,1), GJR (1,1) and EGARCH (1,1), are used to estimate the abnormal rate of change in the number of tourists. The empirical results concerning the major events affecting the changes in the numbers of tourists from China to Taiwan are economically significant, and confirm which types of tourists are most likely to be affected by such major events.
C52|Variational Bayes inference in high-dimensional time-varying parameter models|This paper proposes a mean field variational Bayes algorithm for efficient posterior and predictive inference in time-varying parameter models. Our approach involves: i) computationally trivial Kalman filter updates of regression coefficients, ii) a dynamic variable selection prior that removes irrelevant variables in each time period, and iii) a fast approximate state-space estimator of the regression volatility parameter. In an exercise involving simulated data we evaluate the new algorithm numerically and establish its computational advantages. Using macroeconomic data for the US we find that regression models that combine time-varying parameters with the information in many predictors have the potential to improve forecasts over a number of alternatives.
C52|Effects of Macroeconomic Indicators on the Financial Markets Interrelations|Analyses of financial market interrelationships are important for effective portfolio diversification. The interdependencies between markets are stronger during turbulent times on financial markets than during periods of calm. This fact was especially evident during the global crisis. So, the predictability of stock return interrelationships is a topic discussed most-frequently in empirical studies. In this paper, the role of macroeconomics indicators in the dynamic of interrelationships between financial markets will be considered. Effects of the unemployment rate, CPI, long-term interest rate, and industrial production on the comovement between markets from the G6 group will be verified. For this purpose, the Markov-switching copula model with time-varying matrix transition probability (TVPMS) will be adapted. It has been found that the unemployment rate and long-term interest rate are important factors for interrelationships between the Polish market and the developed market from Germany, France or Italy. The long-term interest rate appears to be important for interrelationships between the Poland and British market and between some developed markets.
C52|Environmental Degradation and Inclusive Human Development in subâ€ Saharan Africa|In the light of challenges to sustainable development in the post-2015 development agenda, this study assesses how increasing carbon dioxide (CO2) emissions affect inclusive human development in 44 countries in sub-Saharan Africa for the period 2000-2012. The following findings are established from Fixed Effects and Tobit regressions. First, unconditional effects and conditional impacts are respectively positive and negative from CO2 emissions per capita, CO2 emissions from liquid fuel consumption and CO2 intensity. This implies a Kuznets shaped curve because of consistent decreasing returns. Second, the corresponding net effects are consistently positive. The following findings are apparent from Generalised Method of Moments (GMM) regressions. First, unconditional effects and conditional impacts are respectively negative and positive from CO2 emissions per capita, CO2 emissions from liquid fuel consumption and CO2 intensity. This implies a U-shaped curve because of consistent increasing returns. Second, the corresponding net effects are overwhelmingly negative. Based on the robust findings and choice of best estimator, the net effect of increasing CO2 emissions on inclusive human development is negative. Policy implications are discussed.
C52|CO2 emission thresholds for inclusive human development in Sub-Saharan Africa|We provide policy-relevant critical masses beyond which, increasing CO2 emissions negatively affects inclusive human development. This study examines how increasing CO2 emissions affects inclusive human development in 44 Sub-Saharan African countries for the period 2000-2012. The empirical evidence is based on Fixed Effects and Tobit regressions. In order to increase the policy relevance of this study, the dataset is decomposed into fundamental characteristics of inclusive development and environmental degradation based on income levels (Low income versus (vs.) Middle income); legal origins (English Common law vs. French Civil law); religious domination (Christianity vs. Islam); openness to sea (Landlocked vs. Coastal); resource-wealth (Oil-rich vs. Oil-poor) and political stability (Stable vs. Unstable). All computed thresholds are within policy range. Hence, above these thresholds, CO2 emissions negatively affect inclusive human development.
