C45|Colombian liberalization and integration to world trade markets: Much ado about nothing|The objective of this paper is to study the evolution of Colombian liberalization and integration to world trade from 1995 to 2016. We achieve our objective by measuring Colombia’s importance in the world trade network. We employ several types of network centrality metrics to measure importance (i.e. degree, strength, hub, authority), and examine their dynamics against a set of regional peers that serve as benchmark countries. Consistent with previous literature, more than two decades of dedicated trade policies and institutional changes resulted in increased exports and imports. However, when compared to regional peers such as Chile, Brazil, Mexico, and Peru, and China and the United States as trade leading countries, Colombia’s centrality in the world trade network did not improve accordingly. Absolute changes in the evolution of trade did not materialize in an enhanced integration to world markets. Colombia’s ranking in the world trade network did not improve materially, whereas that of some of her regional peers did manifestly (i.e. Peru and Chile). Results highlight the perils of analyzing a country’s trade dynamics in isolation, and emphasizes the usefulness of examining the world trade network. From the economic policy and institutional perspectives, results underscore the challenges ahead to better integrate to world markets and to achieve long-term economic growth from trade. **** RESUMEN: El objetivo del documento es estudiar la evolución de la apertura e integración de Colombia al comercio mundial entre 1995 y 2016 y evaluar su importancia en la red de comercio mundial. El documento emplea varios tipos de métricas de centralidad de red (es decir, grado, valor de los flujos de comercio, centro, autoridad), y examina su dinámica y las compara con las de pares regionales que sirven como países de referencia. De acuerdo con la literatura colombiana, más de dos décadas de políticas comerciales y cambios institucionales resultaron en un aumento de las exportaciones e importaciones. Sin embargo, en comparación con Chile, Brasil, México y Perú, y China y Estados Unidos como países líderes en el comercio mundial, la centralidad de Colombia en la red mundial no mejoró. Los cambios absolutos en los flujos de exportaciones e importaciones no se materializaron en una mayor integración a los mercados mundiales. La posición de Colombia en la red de comercio mundial no mejoró sustancialmente, mientras que la de algunos de sus pares regionales sí lo hizo (es decir, Perú y Chile). Desde una perspectiva de política económica e institucional, los resultados resaltan los desafíos futuros de Colombia para integrarse mejor en los mercados mundiales y lograr un mayor crecimiento económico de largo plazo derivado del comercio internacional.
C45|Shapley regressions: a framework for statistical inference on machine learning models|Machine learning models often excel in the accuracy of their predictions but are opaque due to their non-linear and non-parametric structure. This makes statistical inference challenging and disqualifies them from many applications where model interpretability is crucial. This paper proposes the Shapley regression framework as an approach for statistical inference on non-linear or non-parametric models. Inference is performed based on the Shapley value decomposition of a model, a pay-off concept from cooperative game theory. I show that universal approximators from machine learning are estimation consistent and introduce hypothesis tests for individual variable contributions, model bias and parametric functional forms. The inference properties of state-of-the-art machine learning models — like artificial neural networks, support vector machines and random forests — are investigated using numerical simulations and real-world data. The proposed framework is unique in the sense that it is identical to the conventional case of statistical inference on a linear model if the model is linear in parameters. This makes it a well-motivated extension to more general models and strengthens the case for the use of machine learning to inform decisions.
C45|Predicting systemic financial crises with recurrent neural networks|We consider predicting systemic financial crises one to five years ahead using recurrent neural networks. The prediction performance is evaluated with the Jorda-Schularick-Taylor dataset, which includes the crisis dates and relevant macroeconomic series of 17 countries over the period 1870-2016. Previous literature has found simple neural network architectures to be useful in predicting systemic financial crises. We show that such predictions can be greatly improved by making use of recurrent neural network architectures, especially suited for dealing with time series input. The results remain robust after extensive sensitivity analysis.
C45|Predicting Consumer Default: A Deep Learning Approach|We develop a model to predict consumer default based on deep learning. We show that the model consistently outperforms standard credit scoring models, even though it uses the same data. Our model is interpretable and is able to provide a score to a larger class of borrowers relative to standard credit scoring models while accurately tracking variations in systemic risk. We argue that these properties can provide valuable insights for the design of policies targeted at reducing consumer default and alleviating its burden on borrowers and lenders, as well as macroprudential regulation.
C45|Financial Frictions and the Wealth Distribution|This paper investigates how, in a heterogeneous agents model with financial frictions, idiosyncratic individual shocks interact with exogenous aggregate shocks to generate time-varying levels of leverage and endogenous aggregate risk. To do so, we show how such a model can be efficiently computed, despite its substantial nonlinearities, using tools from machine learning. We also illustrate how the model can be structurally estimated with a likelihood function, using tools from inference with diffusions. We document, first, the strong nonlinearities created by financial frictions. Second, we report the existence of multiple stochastic steady states with properties that differ from the deterministic steady state along important dimensions. Third, we illustrate how the generalized impulse response functions of the model are highly state-dependent. In particular, we find that the recovery after a negative aggregate shock is more sluggish when the economy is more leveraged. Fourth, we prove that wealth heterogeneity matters in this economy because of the asymmetric responses of household consumption decisions to aggregate shocks.
C45|Nutritional and Schooling Impact of a Cash Transfer Program in Ethiopia: A Retrospective Analysis of Childhood Experience|The rate of malnutrition among under-five children in Ethiopia is among the highest in Sub-Saharan Africa and in the world. Since 2005, the Government of Ethiopia has been implementing a nation-wide social protection program, with the aim to improve nutrition and food security, decrease poverty and enhance human capital accumulation. This paper investigates the direct impact of this program on long-term anthropometric measures of nutritional status and the indirect effects on enrollment delay and educational attainment. Our research design uses unique administrative data on the regional coverage of the program and combines differences in program intensity across regions with differences across cohorts induced by the timing of the program. Findings show that early childhood exposure to the program leads to better nutritional status and hence higher human capital accumulation. Results are robust to different measures of program intensity, estimation samples, empirical models and some placebo tests.
C45|Liquidity stress detection in the European banking sector|Liquidity stress constitutes an ongoing threat to financial stability in the banking sector. A bank that manages its liquidity inadequately might find itself unable to meet its payment obligations. These liquidity issues, in turn, can negatively impact the liquidity position of many other banks due to contagion effects. For this reason, central banks carefully monitor the payment activities of banks in financial market infrastructures and try to detect early-warning signs of liquidity stress. In this paper, we investigate whether this monitoring task can be performed by supervised machine learning. We construct probabilistic classifiers that estimate the probability that a bank faces liquidity stress. The classifiers are trained on a dataset consisting of various payment features of European banks and which spans several known stress events. Our experimental results show that the classifiers detect the periods in which the banks faced liquidity stress reasonably well.
C45|Alignment of Multinational Firms along Global Value Chains: A Network-based Perspective|The multiple location choices of firms respond to a complex design of simultaneous and related goals. This paper reveals how the geographic coverage of French multinational firms is increasingly determined by Global Value Chains (GVCs) since the mid-1990s. I study the network structure of French multinationals using firm-level data, and of the Global Value Chains using the bilateral value added (VA) content of exports (from international Input-Output tables). The comparison of the two networks reveals an alignment of firms' plants along global value chains, supported by several econometric estimations including a multiple regression Quadratic Assignment Procedure and a panel OLS defined at the country-pair level. Multinationals have modified their foreign affiliates' locations to settle in countries engaged in sequential production, and this result holds when controlling for gravity-like location's determinants and raw international trade flows. Specifically, I report how MNEs are moving up the value chain, as their new locations turned to be more driven by an upstream alignment on GVCs (toward the VA source) rather than a downstream one (toward VA destination). Eventually, the GVCs' attraction increased after the 2008 crisis, even though GVCs were experiencing a slowdown. Therefore, firm's vulnerability to the dismantling of GVCs similarly increased.
C45|Metrics for Measuring the Performance of Machine Learning Prediction Models: An Application to the Housing Market| There has been a recent surge of interest in machine learning (ML) prediction methods. While cross-validation (CV) is typically used for model selection, metrics are still required to evaluate predictive performance at each stage of the CV exercise. The existing literature is interdisciplinary, making it hard to compare the available metrics. We collect the most commonly used metrics, classify them by type, and then evaluate them with respect to two novel symmetry conditions. While none of these metrics satisfy both conditions, we propose a number of new metrics that do. Our findings are illustrated with an application in which five ML methods are used to predict apartment prices. We show that the most popular metrics in the automated valuation literature generate misleading results. A different picture emerges when the full set of metrics is considered, and especially when we focus on four key metrics with the best symmetry properties.
C45|Forecasting economic decisions under risk: The predictive importance of choice-process data|We investigate various statistical methods for forecasting risky choices and identify important decision predictors. Subjects (n=44) are presented a series of 50/50 gambles that each involves a potential gain and a potential loss, and subjects can choose to either accept or reject a displayed lottery. From this data, we use information on 8800 individual lottery gambles and specify four predictor-sets that include different combinations of input categories: lottery design, socioeconomic characteristics, past gambling behavior, eye-movements, and various psychophysiological measures that are recorded during the first three seconds of lottery-information processing. The results of our forecasting experiment show that choice-process data can effectively be used to forecast risky gambling decisions; however, we find large differences among models’ forecasting capabilities with respect to subjects, predictor-sets, and lottery payoff structures.
C45|Predicting Consumer Default: A Deep Learning Approach|We develop a model to predict consumer default based on deep learning. We show that the model consistently outperforms standard credit scoring models, even though it uses the same data. Our model is interpretable and is able to provide a score to a larger class of borrowers relative to standard credit scoring models while accurately tracking variations in systemic risk. We argue that these properties can provide valuable insights for the design of policies targeted at reducing consumer default and alleviating its burden on borrowers and lenders, as well as macroprudential regulation.
C45|Financial Frictions and the Wealth Distribution|This paper investigates how, in a heterogeneous agents model with ?nancial frictions, idiosyncratic individual shocks interact with exogenous aggregate shocks to generate time-varying levels of leverage and endogenous aggregate risk. To do so, we show how such a model can be e?ciently computed, despite its substantial nonlinearities, using tools from machine learning. We also illustrate how the model can be structurally estimated with a likelihood function, using tools from inference with di?usions. We document, ?rst, the strong nonlinearities created by ?nancial frictions. Second, we report the existence of multiple stochastic steady states with properties that di?er from the deterministic steady state along important dimensions. Third, we illustrate how the generalized impulse re-sponse functions of the model are highly state-dependent. In particular, we ?nd that the recovery after a negative aggregate shock is more sluggish when the economy is more lever-aged. Fourth, we prove that wealth heterogeneity matters in this economy because of the asymmetric responses of household consumption decisions to aggregate shocks.
C45|Linking Aid to the Sustainable Development Goals – a machine learning approach|Official Development Assistance amounted USD 146.6 billions in 2017 but do we know how much of this aid contributed to the Sustainable Development Goals (SDGs)? And to what SDG in particular? This paper present a new methodology using machine learning designed to link project-based flows to the Sustainable Development Goals. It provide first estimates of DAC and non-DAC donors’ aid contribution for the goal and show that similar analysis can be done at the recipient level and for other type of textual database such as private sector reports; opening wide array for policy analysis.The methodology presented in this working paper uses semantic analysis of the text description of each project present in the Creditor Reporting System (CRS).
C45|Financial Frictions and the Wealth Distribution|This paper investigates how, in a heterogeneous agents model with financial frictions, idiosyncratic individual shocks interact with exogenous aggregate shocks to generate time-varying levels of leverage and endogenous aggregate risk. To do so, we show how such a model can be efficiently computed, despite its substantial nonlinearities, using tools from machine learning. We also illustrate how the model can be structurally estimated with a likelihood function, using tools from inference with diffusions. We document, first, the strong nonlinearities created by financial frictions. Second, we report the existence of multiple stochastic steady states with properties that differ from the deterministic steady state along important dimensions. Third, we illustrate how the generalized impulse response functions of the model are highly state-dependent. In particular, we find that the recovery after a negative aggregate shock is more sluggish when the economy is more leveraged. Fourth, we prove that wealth heterogeneity matters in this economy because of the asymmetric responses of household consumption decisions to aggregate shocks.
C45|Econometric Ways to Estimate the Age and Price of Abalone|Abalone is a rich nutritious food resource in the many parts of the world. The economic value of abalone is positively correlated with its age. However, determining the age of abalone is a cumbersome as well as expensive process which increases the cost and limits its popularity. This article proposes very simple ways to determine the age of abalones using econometric methods to reduce the costs of producers as well as consumers.
C45|Leveraging Loyalty Programs Using Competitor Based Targeting|Loyalty programs are widely used by firms but their effectiveness is subject to debate. These programs provide discounts and perks to loyal customers and are costly to administer, and with uncertain effectiveness at increasing spending or stealing business from rivals. We use a large new dataset on retail purchases before and after joining a loyalty program (LP) at the customer level to evaluate what determines LP effectiveness. We exploit detailed spatial data on customer and store locations, including locations of competing firms. A simple analysis shows that location relative to competitors is the strongest predictor of LP effectiveness, suggesting that LPs work primarily through business stealing and not through other demand expansion. We next estimate what variables best predict LP effectiveness using high-dimensional data on spatial relationships between customers, the focal firm’s stores, and competing stores as well as customers’ historical spending patterns. We use LASSO regularization to show that spatial relationships are more predictive of LP effects than are past sales data. Finally, we show how firms can use this type of predictive analytics model to leverage customer and competitor location data to substantially increase the performance of their LP through spatially driven targeting rules.
C45|Bank Net Interest Margin Forecasting and Capital Adequacy Stress Testing by Machine Learning Techniques|The 2007-09 financial crisis revealed that the investors in the financial market were more concerned about the future as opposed to the current capital adequacy for banks. Stress testing promises to complement the regulatory capital adequacy regimes, which assess a bank's current capital adequacy, with the ability to assess its future capital adequacy based on the projected asset-losses and incomes from the forecasting models from regulators and banks. The effectiveness of stress-test rests on its ability to inform the financial market, which depends on whether or not the market has confidence in the model-projected asset-losses and incomes for banks. Post-crisis studies found that the stress-test results are uninformative and receive insignificant market reactions; others question its validity on the grounds of the poor forecast accuracy using linear regression models which forecast the banking-industry incomes measured by Aggregate Net Interest Margin. Instead, our study focuses on NIM forecasting at an individual bank's level and employs both linear regression and non-linear Machine Learning techniques. First, we present both the linear and non-linear Machine Learning regression techniques used in our study. Then, based on out-of-sample tests and literature-recommended forecasting techniques, we compare the NIM forecast accuracy by 162 models based on 11 different regression techniques, finding that some Machine Learning techniques as well as some linear ones can achieve significantly higher accuracy than the random-walk benchmark, which invalidates the grounds used by the literature to challenge the validity of stress-test. Last, our results from forecast accuracy comparisons are either consistent with or complement those from existing forecasting literature. We believe that the paper is the first systematic study on forecasting bank-specific NIM by Machine Learning Techniques; also, it is a first systematic study on forecast accuracy comparison including both linear and non-linear Machine Learning techniques using financial data for a critical real-world problem; it is a multi-step forecasting example involving iterative forecasting, rolling-origins, recalibration with forecast accuracy measure being scale-independent; robust regression proved to be beneficial for forecasting in presence of outliers. It concludes with policy suggestions and future research directions.
C45|Cholesky-ANN models for predicting multivariate realized volatility|Accurately forecasting multivariate volatility plays a crucial role for the financial industry. The Cholesky-Artificial Neural Networks specification here presented provides a twofold advantage for this topic. On the one hand, the use of the Cholesky decomposition ensures positive definite forecasts. On the other hand, the implementation of artificial neural networks allows to specify nonlinear relations without any particular distributional assumption. Out-of-sample comparisons reveal that Artificial neural networks are not able to strongly outperform the competing models. However, long-memory detecting networks, like Nonlinear Autoregressive model process with eXogenous input and long shortterm memory, show improved forecast accuracy respect to existing econometric models.
C45|Realized Volatility Forecasting with Neural Networks|In the last few decades, a broad strand of literature in finance has implemented artificial neural networks as forecasting method. The major advantage of this approach is the possibility to approximate any linear and nonlinear behaviors without knowing the structure of the data generating process. This makes it suitable for forecasting time series which exhibit long memory and nonlinear dependencies, like conditional volatility. In this paper, I compare the predictive performance of feed-forward and recurrent neural networks (RNN), particularly focusing on the recently developed Long short-term memory (LSTM) network and NARX network, with traditional econometric approaches. The results show that recurrent neural networks are able to outperform all the traditional econometric methods. Additionally, capturing long-range dependence through Long short-term memory and NARX models seems to improve the forecasting accuracy also in a highly volatile framework.
C45|Macroeconomic Indicator Forecasting with Deep Neural Networks|Economic policymaking relies upon accurate forecasts of economic conditions. Current methods for unconditional forecasting are dominated by inherently linear models {{p}} that exhibit model dependence and have high data demands. {{p}} We explore deep neural networks as an {{p}} opportunity to improve upon forecast accuracy with limited data and while remaining agnostic as to {{p}} functional form. We focus on predicting civilian unemployment using models based on four different neural network architectures. Each of these models outperforms bench- mark models at short time horizons. One model, based on an Encoder Decoder architecture outperforms benchmark models at every forecast horizon (up to four quarters).
C45|The Informational Content of the Term Spread in Forecasting the US Inflation Rate: A Nonlinear Approach| The difficulty in modelling inflation and the significance in discovering the underlying data‐generating process of inflation is expressed in an extensive literature regarding inflation forecasting. In this paper we evaluate nonlinear machine learning and econometric methodologies in forecasting US inflation based on autoregressive and structural models of the term structure. We employ two nonlinear methodologies: the econometric least absolute shrinkage and selection operator (LASSO) and the machine‐learning support vector regression (SVR) method. The SVR has never been used before in inflation forecasting considering the term spread as a regressor. In doing so, we use a long monthly dataset spanning the period 1871:1–2015:3 that covers the entire history of inflation in the US economy. For comparison purposes we also use ordinary least squares regression models as a benchmark. In order to evaluate the contribution of the term spread in inflation forecasting in different time periods, we measure the out‐of‐sample forecasting performance of all models using rolling window regressions. Considering various forecasting horizons, the empirical evidence suggests that the structural models do not outperform the autoregressive ones, regardless of the model's method. Thus we conclude that the term spread models are not more accurate than autoregressive models in inflation forecasting. Copyright © 2016 John Wiley & Sons, Ltd.
C45|Usage of artificial neural networks in data classification|Data classification is broadly defined as the process of organizing data by respective categories so that it can be used and protected more efficiently. Data classification is performed for different purposes, one of the most common is for preserving data privacy. Data classification often includes a number of attributes, determining the type of data, confidentiality, and integrity. Neural networks help solve different problems. They are very good at data classification problems, they can classify any data with arbitrary precision.
C45|Deep Haar Scattering Networks in Unidimensional Pattern Recognition Problems|The aim of this paper is to discuss the use of Haar scattering networks, which is a very simple architecture that naturally supports a large number of stacked layers, yet with very few parameters, in a relatively broad set of pattern recognition problems, including regression and classification tasks. This architecture, basically, consists of stacking convolutional filters, that can be thought as a generalization of Haar wavelets, followed by nonlinear operators which aim to extract symmetries and invariances that are later fed in a classification/regression algorithm. We show that good results can be obtained with the proposed method for both kind of tasks. We outperformed the best available algorithms in 4 out of 18 important data classification problems, and obtained a more robust performance than ARIMA and ETS time series methods in regression problems for data with invariances and symmetries, with desirable features, such as possibility to evaluate parameter stability and easy structural assessment.
C45|Machine Learning vs Traditional Forecasting Methods: An Application to South African GDP|This study employs traditional autoregressive and vector autoregressive forecasting models, as well as machine learning methods of forecasting, in order to compare the performance of each of these techniques. Each technique is used to forecast the percentage change of quarterly South African Gross Domestic Product, quarter-on-quarter. It is found that machine learning methods outperform traditional methods according to the chosen criteria of minimising root mean squared error and maximising correlation with the actual trend of the data. Overall, the outcomes suggest that machine learning methods are a viable option for policy-makers to use, in order to aid their decision-making process regarding trends in macroeconomic data. As this study is limited by data availability, it is recommended that policy-makers consider further exploration of these techniques.
C45|Robustness of Support Vector Machines in Algorithmic Trading on Cryptocurrency Market|This study investigates the profitability of a algorithmic trading strategy based on training SVM model to identify cryptocurrencies with high or low predicted returns. A tail set is defined to be a group of coins whose volatility-adjusted returns are in the highest or lowest quantile. Each cryptocurrency is represented by a set of six technical features. SVM is trained on historical tail sets and tested on the current data. The classifier is chosen to be a nonlinear support vector machine. Portfolio is formed by ranking coins using SVM output. The highest ranked coins are used for long positions to be included in the portfolio for one reallocation period. The following metrics were used to estimate the portfolio profitability: %ARC (the annualized rate of change), %ASD (the annualized standard deviation of daily returns), MDD (the maximum drawdown coefficient), IR1, IR2 (the information ratio coefficients). The performance of the SVM portfolio is compared to the performance of the four benchmark strategies based on the values of the information ratio coefficient IR1 which quantifies the risk-weighted gain. The question on how sensitive the portfolio performance is to the parameters set in the SVM model is also addressed in this study.
C45|Does the inclusion of exposure to volatility into diversified portfolio improve the investment results? Portfolio construction from the perspective of a Polish investor|The main goal of this research is to analyse the investment benefits from an incorporation of the volatility exposure to the diversified portfolio from the perspective of a Polish investor. Volatility, treated as a new asset class, may improve the performance of the portfolio due to its negative correlation with most types of assets. This topic has been widely investigated for the United States and Europe whereas Polish market appears to be not heavily researched and this study may fill this gap. The research covers the period from October 2010 to July 2018 and is performed on the daily close prices. To construct the portfolios, the analysis uses the mean-variance framework and the naïve diversification approach. The comparison of risk-adjusted returns between investments with and without volatility exposure enables to answer the research question about an improvement of the results by the addition of a non-standard asset to the diversified portfolios. The VXX is considered as the proxy for volatility as it is the most popular ETN which follows the volatility index derivatives with the given maturity. To test the robustness of the results, the portfolios are constructed with a broad range of different parameters and assumptions imposed on the optimization procedure.
C45|Hybrid Investment Strategy Based on Momentum and Macroeconomic Approach|The purpose of this research is to test the potential returns and robustness of an automated investment strategy. The strategy is based on momentum and macroeconomic approach, that consists of the technical core – momentum, and the additional macro screening, which is used to determine whether investment signals generate relevant investment opportunities or just technical noise. In order to check whether the macroeconomic factor is the value added to the momentum strategy, the hybrid approach is tested and compared with the simple momentum and the macroeconomic strategy alone and then assessed on a risk-adjusted return basis. The main aim of this paper is to answer the question, whether an investor can gain surplus risk-adjusted returns from merging short-term momentum strategy with the long-term macroeconomic approach. Strategies are based on the data for the selected companies from the S&P500 index in the period ranging from 02/01/1990 to 31/12/2018.
C45|Electricity price forecasting|Electricity price forecasting (EPF) is a branch of energy forecasting on the interface between econometrics/statistics and engineering, which focuses on predicting the spot and forward prices in wholesale electricity markets. Its beginnings can be traced back to the early 1990s, when power sector deregulation led to the introduction of competitive markets in the UK and Scandinavia. The changes quickly spread throughout Europe and North America, and nowadays - in many countries worldwide - electricity is traded under market rules using spot and derivative contracts. Over the last 25 years, a variety of methods and ideas have been tried for EPF, with varying degrees of success. In this chapter we first briefly discuss the forecasting horizons and the types of forecasts, then review the forecasting tools and the evaluation techniques used in the EPF literature.
C45|Estimation of a Scale-Free Network Formation Model|Growing evidence suggests that many social and economic networks are scale free in that their degree distribution has a power-law tail. A common explanation for this phenomenon is a random network formation process with preferential attachment. For a general version of such a process, we develop the pseudo maximum likelihood and generalized method of moments estimators. We prove consistency of these estimators by establishing the law of large numbers for growing networks. Simulations suggest that these estimators are asymptotically normally distributed and outperform the commonly used non-linear least squares and Hill (1975) estimators in finite samples. We apply our estimation methodology to a co-authorship network.
C45|Size matters: Estimation sample length and electricity price forecasting accuracy|Electricity price forecasting models are typically estimated via rolling windows, i.e. by using only the most recent observations. Nonetheless, the current literature does not provide much guidance on how to select the size of such windows. This paper shows that determining the appropriate window prior to estimation dramatically improves forecasting performances. In addition, it proposes a simple two-step approach to choose the best performing models and window sizes. The value of this methodology is illustrated by analyzing hourly datasets from two large power markets with a selection of ten different forecasting models. Incidentally, our empirical application reveals that simple models, such as the linear regression, can perform surprisingly well if estimated on extremely short samples.
C45|Machine learning in algorithmic trading strategy optimization - implementation and efficiency|The main aim of this paper was to formulate and analyze the machine learning methods, fitted to the strategy parameters optimization specificity. The most important problems are the sensitivity of a strategy performance to little parameter changes and numerous local extrema distributed over the solution space in an irregular way. The methods were designed for the purpose of significant shortening of the computation time, without a substantial loss of a strategy quality. The efficiency of methods was compared for three different pairs of assets in case of moving averages crossover system. The methods operated on the in sample data, containing 20 years of daily prices between 1998 and 2017. The problem was presented for three sets of two assets portfolios. In the first case, a strategy was trading on the SPX and DAX index futures, in the second on the AAPL and MSFT stocks and finally, in the third case on the HGF and CBF commodities futures. The major hypothesis verified in this thesis is that machine learning methods select strategies with evaluation criterion near to the highest one, but in significantly lower execution time than the Exhaustive Search.
C45|A note on averaging day-ahead electricity price forecasts across calibration windows|We propose a novel concept in energy forecasting and show that averaging day-ahead electricity price forecasts of a predictive model across 28- to 728-day calibration windows yields better results than selecting only one 'optimal' window length. Even more significant accuracy gains can be achieved by averaging over a few, carefully selected windows.
C45|Probabilistic electricity price forecasting with NARX networks: Combine point or probabilistic forecasts?|A recent electricity price forecasting (EPF) study has shown that the Seasonal Component Artificial Neural Network (SCANN) modeling framework, which consists of decomposing a series of spot prices into a trend-seasonal and a stochastic component, modeling them independently and then combining their forecasts, can yield more accurate point predictions than an approach in which the same non-linear autoregressive NARX-type neural network is calibrated to the prices themselves. Here, considering two novel extensions of the SCANN concept to probabilistic forecasting, we find that (i) efficiently calibrated NARX networks can outperform their autoregressive counterparts, even without combining forecasts from many runs, and that (ii) in terms of accuracy it is better to construct probabilistic forecasts directly from point predictions, however, if speed is a critical issue, running quantile regression on combined point forecasts (i.e., committee machines) may be an option worth considering. Moreover, we confirm an earlier observation that averaging probabilities outperforms averaging quantiles when combining predictive distributions in EPF.
C45|Ranking the socioeconomic and environmental framework of European Union farms: A network analysis|The context of European Union farms is markedly diverse. This is verified when, for example, several dimensions of the agricultural sector are analyzed, considering variables related with economic, social, or environmental dynamics. Indeed, frequently the farms in EU countries present relevant differences inside economic, social, or environmental dimensions. To better understand the interrelationships inside the economic, social, and environmental framework of European Union farms, this work analyzes the networks among these fields by considering rankings in decreasing order (with the statistical information of the representative farms of each European country) for variables like farming output, current subsidies, total inputs, gross investments, subsidies on investments, fertilizer and crop protection consumption, farming labor, and wages paid. Data was gathered from the Farm Accountancy Data Network, across the European Union member-states and over the period 2012-2014. This statistical information was analyzed through network analysis, exploring edgelists among the several variables considered and the network bridges between European countries. The results show that, in fact, there are significant differences in the scores found for European countries inside economic, social, and environmental dimensions, which was highlighted by the network analysis. However, the European Union farms are, in general, networked in the rankings for the variables considered.
C45|Rethinking Policy Evaluation – Do Simple Neural Nets Bear Comparison with Synthetic Control Method?|With the advent of big data in economics machine learning algorithms become more and more appealing to economists. Despite some attempts of establishing artificial neural networks in in the early 1990s, only little is known about their ability of estimating causal effects in policy evaluation. We employ a simple forecasting neural network to analyze the effect of the construction of the Oresund bridge on the local economy. The outcome is compared to the causal effect estimated by the proven Synthetic Control Method. Our results suggest that – especially in so-called prediction policy problems – neural nets may outperform traditional approaches.
C45|Artificial neural network regression models: Predicting GDP growth|Artificial neural networks have become increasingly popular for statistical model fitting over the last years, mainly due to increasing computational power. In this paper, an introduction to the use of artificial neural network (ANN) regression models is given. The problem of predicting the GDP growth rate of 15 industrialized economies in the time period 1996-2016 serves as an example. It is shown that the ANN model is able to yield much more accurate predictions of GDP growth rates than a corresponding linear model. In particular, ANN models capture time trends very flexibly. This is relevant for forecasting, as demonstrated by out-of-sample predictions for 2017.
C45|Early Detection of Students at Risk – Predicting Student Dropouts Using Administrative Student Data and Machine Learning Methods|To successfully reduce student attrition, it is imperative to understand what the underlying determinants of attrition are and which students are at risk of dropping out. We develop an early detection system (EDS) using administrative student data from a state and a private university to predict student success as a basis for a targeted intervention. The EDS uses regression analysis, neural networks, decision trees, and the AdaBoost algorithm to identify student characteristics which distinguish potential dropouts from graduates. Prediction accuracy at the end of the first semester is 79% for the state university and 85% for the private university of applied sciences. After the fourth semester, the accuracy improves to 90% for the state university and 95% for the private university of applied sciences.
C45|“A regional perspective on the accuracy of machine learning forecasts of tourism demand based on data characteristics”|In this work we assess the role of data characteristics in the accuracy of machine learning (ML) tourism forecasts from a spatial perspective. First, we apply a seasonal-trend decomposition procedure based on non-parametric regression to isolate the different components of the time series of international tourism demand to all Spanish regions. This approach allows us to compute a set of measures to describe the features of the data. Second, we analyse the performance of several ML models in a recursive multiple-step-ahead forecasting experiment. In a third step, we rank all seventeen regions according to their characteristics and the obtained forecasting performance, and use the rankings as the input for a multivariate analysis to evaluate the interactions between time series features and the accuracy of the predictions. By means of dimensionality reduction techniques we summarise all the information into two components and project all Spanish regions into perceptual maps. We find that entropy and dispersion show a negative relation with accuracy, while the effect of other data characteristics on forecast accuracy is heavily dependent on the forecast horizon.
C45|Nowcasting economic activity with electronic payments data: A predictive modeling approach|Economic activity nowcasting (i.e. making current-period estimates) is convenient because most traditional measures of economic activity come with substantial lags. We aim at nowcasting ISE, a short-term economic activity indicator in Colombia. Inputs are ISE’s lags and a dataset of payments made with electronic transfers and cheques among individuals, firms, and the central government. Under a predictive modeling approach, we employ a nonlinear autoregressive exogenous neural network model. Results suggest that our choice of inputs and predictive method enable us to nowcast economic activity with fair accuracy. Also, we validate that electronic payments data significantly reduces the nowcast error of a benchmark non-linear autoregressive neural network model. Nowcasting economic activity from electronic payment instruments data not only contributes to agents’ decision making and economic modeling, but also supports new research paths on how to use retail payments data for appending current models.
C45|The Nobel Prize in Economics: individual or collective merits?|We analyse the research production of Nobel laureates in Economics, employing the JCR Impact Factor (IF) of their publications. We associate this production indicator with the level of collaboration established with other authors, using Complex Networks techniques applied to the co-authorship networks. We study both individual and collaborative behaviours, and how the professional output, in terms of publications, is related to the Nobel Prize. The study encompasses a total of 2,150 papers published between 1935 and the end of 2015 by the laureates in Economics awarded between 1969 and 2016. Our results indicate that direct collaborations among laureates are, in general, rare, but when we add all the co-authors of the laureates, the network becomes more dense, and appears as a giant component containing 70% of the nodes, which means that more than two thirds of the laureates can be connected through only two steps. We have been able to measure that, in general, a higher level of collaboration leads to a larger production. Finally, when looking at the evolution of the research output of the laureates, we find that, for most of those awarded up to the mid-1990s, the production is more stable, with a gradual decrease after the awarding of the Prize, and those awarded later experience a sharp growth in the IF before the Prize, a decrease during the years immediately following, and a new increase afterwards, returning to high levels of impact.
C45|Compilation of Experimental Price Indices Using Big Data and Machine Learning:A Comparative Analysis and Validity Verification of Quality Adjustments|This paper compiles experimental price indices for 20 home electrical appliances and digital consumer electronic products using big data obtained from Kakaku.com, the largest price comparison website in Japan, and a machine-learning algorithm which pairs legacy and successor products with high precision. In so doing, authors examine the validity of quality adjustment methods by performing comparative analyses on the difference these methods have on price indices. Findings from the analyses are as follows: Indices applied with the Webscraped Prices Comparison Method--the quality adjustment method newly developed and introduced by the Bank of Japan--are more cost-effective than those applied with the Hedonic Regression Method which is known to possess high accuracy in index creation. Indices applied with the Matched-Model Method, which is frequently applied to price indices using big data is unable to precisely reflect price increases intended to ensure the profitability often seen in home electronics at time of product turnover. This indicates the significant downward bias in price indices. These findings once again highlight the importance of selecting the appropriate quality adjustment method when compiling price indices.
C45|Early Detection of Students at Risk - Predicting Student Dropouts Using Administrative Student Data and Machine Learning Methods|High rates of student attrition in tertiary education are a major concern for universities and public policy, as dropout is not only costly for the students but also wastes public funds. To successfully reduce student attrition, it is imperative to understand which students are at risk of dropping out and what are the underlying determinants of dropout. We develop an early detection system (EDS) that uses machine learning and classic regression techniques to predict student success in tertiary education as a basis for a targeted intervention. The method developed in this paper is highly standardized and can be easily implemented in every German institution of higher education, as it uses student performance and demographic data collected, stored, and maintained by legal mandate at all German universities and therefore self-adjusts to the university where it is employed. The EDS uses regression analysis and machine learning methods, such as neural networks, decision trees and the AdaBoost algorithm to identify student characteristics which distinguish potential dropouts from graduates. The EDS we present is tested and applied on a medium-sized state university with 23,000 students and a medium-sized private university of applied sciences with 6,700 students. Our results indicate a prediction accuracy at the end of the 1st semester of 79% for the state university and 85% for the private university of applied sciences. Furthermore, accuracy of the EDS increases with each completed semester as new performance data becomes available. After the fourth semester, the accuracy improves to 90% for the state university and 95% for the private university of applied sciences.
C45|Cascading Logistic Regression Onto Gradient Boosted Decision Trees to Predict Stock Market Changes Using Technical Analysis|In the data mining and machine learning fields, forecasting the direction of price change can be generally formulated as a supervised classfii cation. This paper attempts to predict the direction of daily changes of the Nasdaq Composite Index (NCI) and of the Standard & Poor's 500 Composite Stock Price Index (S&P 500) covering the period from January 3, 2012 to December 23, 2016, and of the Shanghai Stock Exchange Composite Index (SSEC) from January 4, 2010 to December 31, 2014. Due to the complexity of stock index data, we carefully combine raw price data and eleven technical indicators with a cascaded learning technique to improve the performance of the classifi cation. The proposed learning architecture LR2GBDT is obtained by cascading the logistic regression (LR) model onto the gradient boosted decision trees (GBDT) model. Given the same test conditions, the experimental results show that the LR2GBDT model performs better than the baseline LR and GBDT models for these stock indices, according to the performance metrics Hit ratio, Precision, Recall and F-measure. Furthermore, we use these models to develop simple trading strategies and assess their performance in terms of their Average Annual Return, Maximum Drawdown, Sharpe Ratio and Average Annualized Return/Maximum Drawdown. When transaction costs and buy-sell thresholds are taken into account, the best trading strategy derived from LR2GBDT model still reaches the highest Sharpe Ratio and clearly beats the buy-and-hold strategy. The performances are found to be both statistically and economically signi ficant.
C45|Estimation and Updating Methods for Hedonic Valuation|Purpose – We use a large and rich data set consisting of over 123,000 single-family houses sold in Switzerland between 2005 and 2017 to investigate the accuracy and volatility of different methods for estimating and updating hedonic valuation models. Design/methodology/approach – We apply six estimation methods (linear least squares, robust regression, mixed effects regression, random forests, gradient boosting, and neural networks) and two updating methods (moving and extending windows). Findings – The gradient boosting method yields the greatest accuracy while the robust method provides the least volatile predictions. There is a clear trade-off across methods depending on whether the goal is to improve accuracy or avoid volatility. The choice between moving and extending windows has only a modest effect on the results. Originality/value – This paper compares a range of linear and machine learning techniques in the context of moving or extending window scenarios that are used in practice but which have not been considered in prior research. The techniques include robust regression, which has not previously been used in this context. The data updating allows for analysis of the volatility in addition to the accuracy of predictions. The results should prove useful in improving hedonic models used by property tax assessors, mortgage underwriters, valuation firms, and regulatory authorities.
C45|Economic transition and the rise of alternative institutions : Political connections in Putin's Russia|The economic transition from socialism in Russia has not resulted in the emergence of impersonal, rule‐based institutions. Instead, the natural demand for institutions that protect property rights has led to the emergence of alternative, inefficient institutions such as that of cronyism – the practice of appointing personal acquaintances of the political leader to key positions. A political leader not constrained by institutions appoints cronies, as competent subordinates are more prone to switching allegiance to a potential challenger. As competence makes a bigger difference in a rule‐based environment, such a leader has no interest in any institutional development. In a simple empirical exercise, using a dataset that covers the richest Russians, we find a positive and significant effect of direct connections to the personal circle of President Putin on the wealth of businessmen. The magnitude of the effect varies at different levels of rents available for redistribution and ‘network centrality of a businessman’: it is higher during the years of high oil prices, but is attenuated by the prominence of the businessman in the network.
C45|How Do Households Allocate Risk?|Individuals often have to decide to which degree of risk they want to expose others, or how much risk to accept if their choice has an externality on third parties. One typical application is a household. We run an experiment in the German Socio-Economic Panel with two members from 494 households. Participants have a good estimate of each other’s risk preferences, even if not explicitly informed. They do not simply match this preference when deciding on behalf of the other household member, but shy away from exposing others to risk. We model the situation, and we find four distinct types of individuals, and two distinct types of households.
C45|Machine learning with screens for detecting bid-rigging cartels|We combine machine learning techniques with statistical screens computed from the distribution of bids in tenders within the Swiss construction sector to predict collusion through bid-rigging cartels. We assess the out of sample performance of this approach and find it to correctly classify more than 84% of the total of bidding processes as collusive or non-collusive. We also discuss tradeoffs in reducing false positive vs. false negative predictions and find that false negative predictions increase much faster in reducing false positive predictions. Finally, we discuss policy implications of our method for competition agencies aiming at detecting bid-rigging cartels.
C45|Learning from man or machine: Spatial aggregation and house price prediction|House prices vary with location. At the same time the border between two neighboring housing markets tends to be fuzzy. When we seek to explain or predict house prices we need to correct for spatial price variation. A much used way is to include neighborhood dummy variables. In general, it is not clear how to choose a spatial subdivision in the vast space of all possible spatial aggregations. We take a biologically inspired approach, where diﬀerent spatial aggregations mutate and recombine according to their explanatory power in a standard hedonic housing market model. We ﬁnd that the genetic algorithm consistently ﬁnds aggregations that outperform conventional aggregation both in and out of sample. A comparison of best aggregations of diﬀerent runs of the genetic algorithm shows that even though they converge to a similar high explanatory power, they tend to be genetically and economically diﬀerent. Diﬀerences tend to be largely conﬁned to areas with few housing market transactions.
C45|Radial Basis Functions Neural Networks for Nonlinear Time Series Analysis and Time-Varying Effects of Supply Shocks|I propose a flexible nonlinear method for studying the time series properties of macroeconomic variables. In particular, I focus on a class of Artificial Neural Networks (ANN) called the Radial Basis Functions (RBF). To assess the validity of the RBF approach in the macroeconomic time series analysis, I conduct a Monte Carlo experiment using the data generated from a nonlinear New Keynesian (NK) model. I find that the RBF estimator can uncover the structure of the nonlinear NK model from the simulated data whose length is as small as 300 periods. Finally, I apply the RBF estimator to the quarterly US data and show that the response of the macroeconomic variables to a positive supply shock exhibits a substantial time variation. In particular, the positive supply shocks are found to have significantly weaker expansionary effects during the zero lower bound periods as well as periods between 2003 and 2004. The finding is consistent with a basic NK model, which predicts that the higher real interest rate due to the monetary policy inaction weakens the effects of supply shocks.
C45|Forecasting Inflation Expectations from the CESifo World Economic Survey: An Empirical Application in Inflation Targeting|The purpose of this paper is twofold. First, we evaluate the responses to the questions on inflation expectations in the World Economic Survey for sixteen inflation targeting countries. Second, we compare inflation expectation forecasts across countries by using a two-step approach that selects the most accurate linear or non-linear forecasting method for each country. Then, using Self Organizing Maps, we cluster the inflation expectations, setting June 2014 as a benchmark. At this time there was a sharp decline in oil prices and by analyzing inflation expectations in the context of this price change, we can discriminate between countries that anticipated the oil shock smoothly and those that had to significantly adjust their expectations. Our main findings from the in-sample comparison of the WES surveys suggest that expert forecasts of inflation expectations are systematically distorted in 83 percent of the countries in the sample. On the other hand, our out of sample forecast analysis indicates that Non-linear Artificial Neural Networks combined with Bayesian regularization outperform ARIMA linear models for longer forecasting horizons. This holds true for countries with both soft and brisk changes of expectations. However, when forecasting one step ahead, the performance between the two methods is similar.
C45|How Do Households Allocate Risk?|Individuals often have to decide to which degree of risk they want to expose others, or how much risk to accept if their choice has an externality on third parties. One typical application is a household. We run an experiment in the German Socio-Economic Panel with two members from 494 households. Participants have a good estimate of each other’s risk preferences, even if not explicitly informed. They do not simply match this preference when deciding on behalf of the other household member, but shy away from exposing others to risk. We model the situation, and we ﬁnd four distinct types of individuals, and two distinct types of households.
C45|Social Media Networks, Fake News, and Polarization|We study how the structure of social media networks and the presence of fake news might affect the degree of misinformation and polarization in a society. For that, we analyze a dynamic model of opinion exchange in which individuals have imperfect information about the true state of the world and are partially bounded rational. Key to the analysis is the presence of internet bots: agents in the network that do not follow other agents and are seeded with a constant flow of biased information. We characterize how the flow of opinions evolves over time and evaluate the determinants of long-run disagreement among individuals in the network. To that end, we create a large set of heterogeneous random graphs and simulate a long information exchange process to quantify how the bots’ ability to spread fake news and the number and degree of centrality of agents susceptible to them affect misinformation and polarization in the long-run.
C45|Empirical Asset Pricing via Machine Learning|We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premia. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best performing methods (trees and neural networks) and trace their predictive gains to allowance of nonlinear predictor interactions that are missed by other methods. All methods agree on the same set of dominant predictive signals which includes variations on momentum, liquidity, and volatility. Improved risk premium measurement through machine learning simplifies the investigation into economic mechanisms of asset pricing and highlights the value of machine learning in financial innovation.
C45|Arbitrage Opportunities in CDS Term Structure: Theory and Implications for OTC Derivatives|Absence-of-Arbitrage (AoA) is the basic assumption underpinning derivatives pricing theory. As part of the OTC derivatives market, the CDS market not only provides a vehicle for participants to hedge and speculate on the default risks of corporate and sovereign entities, it also reveals important market-implied default-risk information concerning the counterparties with which financial institutions trade, and for which these financial institutions have to calculate various valuation adjustments (collectively referred to as XVA) as part of their pricing and risk management of OTC derivatives, to account for counterparty default risks. In this study, we derive No-arbitrage conditions for CDS term structures, first in a positive interest rate environment and then in an arbitrary one. Using an extensive CDS dataset which covers the 2007-09 financial crisis, we present a catalogue of 2,416 pairs of anomalous CDS contracts which violate the above conditions. Finally, we show in an example that such anomalies in the CDS term structure can lead to persistent arbitrage profits and to nonsensical default probabilities. The paper is a first systematic study on CDS-term-structure arbitrage providing model-free AoA conditions supported by ample empirical evidence.
C45|The Fama 3 and Fama 5 factor models under a machine learning framework|We examine four empirical models which are popular in money and stock markets world. These models are Fama – French 3 & 5 factors model, the Capital Asset Pricing Model (CAPM) and the Arbitrage Pricing Theory (APT) model. These tools are intensively used by investors and market professionals as an important part of the investment decision process and for the evaluation of the applied investment strategies. The last years, several surveys and studies have done, and various methodologies were implemented to evaluate the effectiveness of these four models. The methodological approach of the current thesis focuses on the Support Vector Regression (SVR). This method is running in comparison with the Ordinary Least Squares linear regression.
C45|A Thick ANN Model for Forecasting Inflation|Inflation forecasting is an essential activity at central banks to formulate forward looking monetary policy stance. Like in other fields, machine learning is finding its way to forecasting; inflation forecasting is not any exception. In machine learning, most popular tool for forecasting is artificial neural network (ANN). Researchers have used different performance measures (including RMSE) to optimize set of characteristics - architecture, training algorithm and activation function - of an ANN model. However, any chosen ‘optimal’ set may not remain reliable on realization of new data. We suggest use of ‘mode’ or most appearing set from a simulation based distribution of optimum ‘set of characteristics of ANN model’; selected from a large number of different sets. Here again, we may have a different trained network in case we re-run this ‘modal’ optimal set since initial weights in training process are assigned randomly. To overcome this issue, we suggest use of ‘thickness’ to produce stable and reliable forecasts using modal optimal set. Using January 1958 to December 2017 year on year (YoY) inflation data of Pakistan, we found that our YoY inflation forecasts (based on aforementioned multistage forecasting scheme) outperform those from a number of inflation forecasting models of Pakistan economy.
C45|Comparative Study of Three Time Series Methods in Forecasting Dengue Hemorrhagic Fever Incidence in Thailand|Accurate incidence forecasting of infectious disease such as dengue hemorrhagic fever is critical for early prevention and detection of outbreaks. This research presents a comparative study of three different forecasting methods based on the monthly incidence of dengue hemorrhagic fever. Holt and Winters method, Box-Jenkins method and Artificial Neural Networks were compared. The data were taken from the Bureau of Epidemiology, Department of Disease Control, Ministry of Public Health starting from January, 2003 to December, 2016. The data were divided into 2 sets. The first set from January, 2003 to December, 2015 were used for constructing and selection the forecasting models. The second set from January, 2016 to December, 2016 were used for computing the accuracy of the forecasting model. The forecasting models were chosen by considering the smallest root mean square error (RMSE) and mean absolute percentage error (MAPE) were used to measure the accuracy of the model. The results showed that Artificial Neural Networks obtained the smallest RMSE in the modeling process and the MAPE in the forecasting process was 14.05%
C45|Artificial Neural Network Based Chaotic Generator Design for The Prediction of Financial Time Series|series. The ANN architecture is usually designed and optimized based on trial and error using a given training data set. It is generally required to obtain big data for ANN training in order to achieve good training performance. Financial time series are subject to highly complex conditions of external inputs and their dynamic features can change fast and unpredictably. The aim of this research is to design an adaptive ANN architecture, which can be trained in real time with short time series for near future prediction. ANN based chaotic system generator is designed for the simulation and analysis of the dynamic features in financial time series.
C45|CDS Rate Construction Methods by Machine Learning Techniques|Regulators require financial institutions to estimate counterparty default risks from liquid CDS quotes for the valuation and risk management of OTC derivatives. However, the vast majority of counterparties do not have liquid CDS quotes and need proxy CDS rates. Existing methods cannot account for counterparty-specific default risks; we propose to construct proxy CDS rates by associating to illiquid counterparty liquid CDS Proxy based on Machine Learning Techniques. After testing 156 classifiers from 8 most popular classifier families, we found that some classifiers achieve highly satisfactory accuracy rates. Furthermore, we have rank-ordered the performances and investigated performance variations amongst and within the 8 classifier families. This paper is, to the best of our knowledge, the first systematic study of CDS Proxy construction by Machine Learning techniques, and the first systematic classifier comparison study based entirely on financial market data. Its findings both confirm and contrast existing classifier performance literature. Given the typically highly correlated nature of financial data, we investigated the impact of correlation on classifier performance. The techniques used in this paper should be of interest for financial institutions seeking a CDS Proxy method, and can serve for proxy construction for other financial variables. Some directions for future research are indicated.
C45|Measuring Business Cycles Intra-Synchronization in US: A Regime-switching Interdependence Framework|This paper proposes a Markov-switching framework to endogenously identify periods where economies are more likely to (i) synchronously enter recessionary and expansionary phases, and (ii) follow independent business cycles. The reliability of the framework is validated with simulated data in Monte Carlo experiments. The framework is applied to assess the timevarying intra-country synchronization in US. The main results report substantial changes over time in the cyclical affiliation patterns of US states, and show that the more similar the economic structures of states, the higher the correlation between their business cycles. A synchronization-based network analysis discloses a change in the propagation pattern of aggregate contractionary shocks across states, suggesting that the US has become more internally synchronized since the early 1990s.<br><small>(This abstract was borrowed from another version of this item.)</small>
C45|Clustering and forecasting inflation expectations using the World Economic Survey: the case of the 2014 oil price shock on inflation targeting countries|This paper examines inflation expectations of the World Economic Survey for ten inflation targeting countries. First, by a Self Organizing Maps methodology, we cluster the trajectory of agents inflation expectations using the beginning of the oil price shock occurred in June of 2014 as a benchmark in order to discriminate between those countries that anticipated the shock smoothly and those with brisk changes in expectations. Then, the expectations are modeled by artificial neural networks forecasting models. Second, for each country we investigate the information content of the quantitative survey forecast by comparing it to the average annual inflation based on national consumer price indices. The results indicate the presence of heterogeneity among countries to anticipate inflation under the oil shock and, also different patterns of accuracy to predict average annual inflation were found depending on the observed inflation trend. Classification JEL: C02, C222, C45, C63, E27
C45|Machine learning at central banks|We introduce machine learning in the context of central banking and policy analyses. Our aim is to give an overview broad enough to allow the reader to place machine learning within the wider range of statistical modelling and computational analyses, and provide an idea of its scope and limitations. We review the underlying technical sources and the nascent literature applying machine learning to economic and policy problems. We present popular modelling approaches, such as artificial neural networks, tree-based models, support vector machines, recommender systems and different clustering techniques. Important concepts like the bias-variance trade-off, optimal model complexity, regularisation and cross-validation are discussed to enrich the econometrics toolbox in their own right. We present three case studies relevant to central bank policy, financial regulation and economic modelling more widely. First, we model the detection of alerts on the balance sheets of financial institutions in the context of banking supervision. Second, we perform a projection exercise for UK CPI inflation on a medium-term horizon of two years. Here, we introduce a simple training-testing framework for time series analyses. Third, we investigate the funding patterns of technology start-ups with the aim to detect potentially disruptive innovators in financial technology. Machine learning models generally outperform traditional modelling approaches in prediction tasks, while open research questions remain with regard to their causal inference properties.
C45|Risk Adjustment Revisited using Machine Learning Techniques|Risk adjustment is vital in health policy design. Risk adjustment defines the annual capitation payments to health insurers and is a key determinant of insolvency risk for health insurers. In this study we compare the current risk adjustment formula used by Colombia's Ministry of Health and Social Protection against alternative specifications that adjust for additional factors. We show that the current risk adjustment formula, which conditions on demographic factors and their interactions, can only predict 30% of total health expenditures in the upper quintile of the expenditure distribution. We also show the government's formula can improve significantly by conditioning ex ante on measures indicators of 29 long-term diseases. We contribute to the risk adjustment literature by estimating machine learning based models and showing non-parametric methodologies (e.g., boosted trees models) outperform linear regressions even when fitted in a smaller set of regressors.
C45|Evaluación de pronósticos de modelos lineales y no lineales de la tasa de cambio de Colombia|El presente trabajo de grado explora las predicciones de los modelos Markov Switching y ARIMA para la tasa de cambio de Colombia, junto con la combinación de pronósticos de los dichos modelos. El modelo Markov Switching pertenece a una clase de modelos de cambio de regímenes, cuyo supuesto principal es que una variable no observada que sigue un proceso de Markov, gobierna el régimen de la serie. La estimación del modelo MS se lleva a cabo por el algoritmo «Expected Maximization». Los pronósticos de los modelos individuales y sus combinaciones se evalúan por fuera de muestra en la serie diaria de la tasa representativa de mercado (TRM) de Colombia. Se realizan pronósticos para un horizonte de 25 días y pronósticos recursivos para el mismo horizonte y los resultados se comparan con las predicciones de un paseo aleatorio. El modelo MS obtuvo la mejor puntuación en la raíz del error cuadrático medio y en el error absoluto medio cuando se efectúa el pronóstico a horizonte 25. Al hacer el pronóstico recursivo, el modelo ARIMA obtuvo los mejores resultados. En el apéndice se desarrolla la derivación matemática para la estimación del modelo MS y las pruebas de especi?cación. También se muestra el código de programación desarrollado en el software estadístico R, para la estimación del modelo Markov Switching por el algoritmo EM y el código con el que se efectuó la combinación de pronóstico.
C45|Prevalence of Diseases and Health Care Utilization ofthe Self-Employed Artists and TheirEmpirical Determinants: Evidence From a Slovenian Survey|Empirical studies on precarious work are still at their beginnings, even more so when the health of precarious workers is under concern. Commonly, precarious workers are assumed to have the inferior health to the employees and even to the population in general, although some recent studiesfound counter evidence to this claim. In particular, studies on the labor market of artists have so far almost completely neglected the question of the health of the artists,and this study tries to fill in this large and important void. In the study, we employ a survey of Slovenian self-employed artists, undertaken in 2015, to study the determinants of the prevalence of diseases and health utilization of self-employed artists in Slovenia using econometric modelling and network analysis. We study and find the determinants, influencing the prevalence of each type of the most common disease among the self-employed artists, determine the most common groupings/multiple diseases among this population, and, finally, study the determinants of health care utilization of self-employed artists and model the heterogeneity in the observed sample. Aninteresting result lies in determining two differentgroups according to their health care utilization and providingtheir interpretation which fits into the existing literature on artist labor markets.
C45|Network Structure of French Multinational Firms|This paper develops a network approach of French multinationals' host countries network. It reveal and describe the expansion pattern of multinational firms through their affiliates, identifying core destinations, centralization degree of the network, and favorite countries linkages. The main results are in line with previous findings on foreign investment decision, location choices and gradual pattern. Further findings appear when looking at the network structure over time, showing an increasing geographical dispersion of affiliates. In addition, building directed networks allows us to observe upstream and downstream stages of internationalization. I later examine separately top productive firms network from least productive ones, and show the sensitivity of network structure to firm heterogeneity.
C45|Obeying vs. resisting unfair laws. A structural analysis of the internalization of collective preferences on redistribution using classification trees and random forests|In this paper, we study whether individual normative preferences are affected by the knowledge of collective normative preferences. In a questionnaire-experimental framework, we study whether respondents obey, resist or are indifferent to a very unfair but legal distribution of an inheritance between a minimum wage-earner and a millionaire. In addition to regressions, we use classification trees and random forests to provide a full picture of how asymmetric combinations of self-interest and ideological factors may lead to identical individual redistributive preferences and law internalization attitudes. We find that sensitivity to procedural fairness and responsibility cut opinions are good predictors of individual redistributive preferences. We also find that law internalization is associated with the support of core normative values, but not with the support of fairness as procedures. This echoes Cooterâ€™s hypothesis of â€˜meta preferencesâ€™ triggering an expressive vs. backlash effects of laws. Lastly, we find that, among the law-sensitive, the social â€˜losersâ€™ tend to submit to the unfair but legal collective preference while the social â€˜winnersâ€™ tend to either be indifferent of voice their disagreement.
C45|Financial distress prediction: The case of French small and medium-sized firms|Financial distress prediction is a central issue in empirical finance that has drawn a lot of research interests in the literature. This paper aims to predict the financial distress of French small and medium firms using Logit model, Artificial Neural Networks, Support Vector Machine techniques, Partial Least Squares, and a hybrid model integrating Support Vector Machine with Partial Least Squares. Empirical results indicate that for one year prior to financial distress, Support Vector Machine is the best classifier with an overall accuracy of 88.57%. Meanwhile, in the case of two years prior to financial distress, the hybrid model outperforms Support Vector Machine, Logit model, Partial Least Squares, andArtificial Neural Networks with an overall accuracy of 94.28%. Distressed firms are found to be smaller, more leveraged and with lower repayment capacity. Moreover, they have lower liquidity, profitability, and solvency ratios. Besides the academic research contribution, our findings can be useful for managers, investors, and creditors. With respect to managers, our findings provide them with early warnings signals of performance deterioration in order to take corrective actions and reduce the financial distress risk. For investors, understanding the main factors leading to financial distress allows them to avoid investing in risky firms. Creditors should correctly evaluate the firm financial situation and be vigilant to signs of impending financial distress to avoid capital loss and costs related to counterpart risk.
C45|Organizational and Financial Modeling of Transnational Industrial Clusters Sustainable Development: Experience, Risks, Management Innovation|The article is devoted to a research of current trends and priorities of organizational and financial modeling of sustainable development. The experience of transnational industrial clusters formation and development is the object of the research. In this article authors conduct a research of the opportunities for sustainable development strategies modeling in the context of new phenomena and precedents, demonstrated by global economic system and revealing in convergence of risk events and divergence of their assessment: new phenomena, properties and characteristics of economic systems, important for the management systems modernization with the purpose to resolve problems of their theoretical and methodological support, had been analyzed; and as a result the method of system and diagnostic analysis, oriented on proactive modeling of management systems competitive performance, was proposed. The authors came to conclusion about the need to include system and diagnostic analysis in the methodological basis of social and economic systems management theory, including all forms of business entities in the industry. The stated improvement of management methodology will form instrumental and methodical apparatus of a new economic growth strategy modeling. Thus, the restructuring of the Russian economic system should be carried out considering the objective need to bring its organizational and financial model to the form of hyper-network super-system, which will enable preventive modeling and testing of the process of all sectors and clusters integration, maintaining a positive synergetic effect. As a result the authors concluded that the development of transnational industrial clusters should be conducted as a part of the adaptive stability provision concept, which is achieved by applying system and functional analysis as a methodological basis for organizational and financial modeling.
C45|How Centralized is U.S. Metropolitan Employment?|"Centralized employment remains a benchmark stylization of metropolitan land use.To address its empirical relevance, we delineate ""central employment zones"" (CEZs)- central business districts together with nearby concentrated employment|for 183 metropolitan areas in 2000. To do so, we first subjectively classify which census tracts in a training sample of metros belong to their metro's CEZ and then use a learning algorithm to construct a function that predicts our judgment. {{p}} Applying this prediction function to the full cross section of metros estimates the probability we would judge each census tract as belonging to its metro's CEZ. Using a high probability threshold for tract inclusion conservatively delineates a predicted CEZ for each metro. On average, the conservatively predicted CEZs account for only 12 percent of metropolitan employment in 2000. But the distribution of shares is positively skewed, with the conservatively predicted CEZs accounting for at least 20 percent of employment in 29 metros. Employment centralization is considerably higher for agglomerative occupations|those that arguably bene t most from face-to-face contact. The conservatively predicted CEZs account for at least 33 percent of agglomerative employment in 24 metros and at least 50 percent of legal employment in 79 metros."
C45|Tail event driven networks of SIFIs|The interdependence, dynamics and riskiness of financial institutions are the key features frequently tackled in financial econometrics. We propose a Tail Event driven Network Quantile Regression (TENQR) model which addresses these three aspects. More precisely, our framework captures the risk propagation and dynamics in terms of a panel quantile autoregression involving network effects that are quantified through a time-varying adjacency matrix. To reflect the risk content in stress situations the construction of the adjacency matrix is suggested to include tail events. More precisely we employ the conditional expected shortfall as risk profile. Based on the similarity of the risk profiles we create a positive and a negative network factor, which capture the effects of risk contagion and risk diversification respectively. The developed joint spacings variance ratio test supports the suggested methodology. The TENQR technique is evaluated using the SIFIs (systemically important financial institutions) identified by the Financial Stability Board (FSB). The risk decomposition of the resulting network identifies the systemic importance of SIFIs and thus provides measures for the required level of additional loss absorbency. It is discovered that the positive network effect, as a function of the tail probability level, becomes more profound in stress situations and varies in its impact to SIFIs located in different geographic regions.
C45|Bagged artificial neural networks in forecasting inflation: An extensive comparison with current modelling frameworks|Accurate inflation forecasts lie at the heart of effective monetary policy. By utilizing a thick modelling approach, this paper investigates the out-of-sample quality of the short-term Polish headline inflation forecasts generated by a combination of thousands of bagged single hidden-layer feed-forward artificial neural networks in the period of systematically falling and persistently low inflation. Results indicate that the forecasts from this model outperform a battery of popular approaches, especially at longer horizons. During the excessive disinflation it has more accurately accounted for the slowly evolving local mean of inflation and remained only mildly biased. Moreover, combining several linear and nonlinear approaches with diverse underlying model assumptions delivers further statistically significant gains in the predictive accuracy and statistically outperforms a panel of examined benchmarks at multiple horizons. The robustness analysis shows that resigning from data preprocessing and bootstrap aggregating severely compromises the forecasting ability of the model.
C45|Robust modelling of the impacts of climate change on the habitat suitability of forest tree species|In Europe, forests play a strategic multifunctional role, serving economic, social and environmental purposes. However, forests are among the most complex systems and their interaction with the ongoing climate change – and the multifaceted chain of potential cascading consequences for European biodiversity, environment, society and economy – is not yet well understood. The JRC PESETA project series proposes a consistent multi-sectoral assessment of the impacts of climate change in Europe. Within the PESETA II project, a robust methodology is introduced for modelling the habitat suitability of forest tree species (2071-2100 time horizon). Abies alba (the silver fir) is selected as a case study: a main European tree species often distributed in bioclimatically complex areas, spanning over various forest types and with multiple populations adapted to different conditions. The modular modelling architecture is based on relative distance similarity (RDS) estimates which link field observations with bioclimatic patterns, projecting their change under climate scenarios into the expected potential change of suitable habitat for tree species. Robust management of uncertainty is also examined. Both technical and interpretation core aspects are presented in an integrated overview. The semantics of the array of quantities under focus and the uneven sources of uncertainty at the continental scale are discussed (following the semantic array programming paradigm), with an effort to offer some minimal guidance on terminology, meaning and methodological limitations not only of the proposed approach, but also of the broad available literature – whose heterogeneity and partial ambiguity might potentially reverberate at the science-policy interface. ► How to cite: ◄ de Rigo, D., Caudullo, G., San-Miguel-Ayanz, J, Barredo, J.I., 2017. Robust modelling of the impacts of climate change on the habitat suitability of forest tree species. Publication Office of the European Union, Luxembourg. 58 pp. ISBN:978-92-79-66704-6 , https://doi.org/10.2760/296501
C45|CDS Rate Construction Methods by Machine Learning Techniques|Regulators require financial institutions to estimate counterparty default risks from liquid CDS quotes for the valuation and risk management of OTC derivatives. However, the vast majority of counterparties do not have liquid CDS quotes and need proxy CDS rates. Existing methods cannot account for counterparty-specific default risks; we propose to construct proxy CDS rates by associating to illiquid counterparty liquid CDS Proxy based on Machine Learning Techniques. After testing 156 classifiers from 8 most popular classifier families, we found that some classifiers achieve highly satisfactory accuracy rates. Furthermore, we have rank-ordered the performances and investigated performance variations amongst and within the 8 classifier families. This paper is, to the best of our knowledge, the first systematic study of CDS Proxy construction by Machine Learning techniques, and the first systematic classifier comparison study based entirely on financial market data. Its findings both confirm and contrast existing classifier performance literature. Given the typically highly correlated nature of financial data, we investigated the impact of correlation on classifier performance. The techniques used in this paper should be of interest for financial institutions seeking a CDS Proxy method, and can serve for proxy construction for other financial variables. Some directions for future research are indicated.
C45|Measuring Tanker Market Future Risk with the use of FORESIM|Future market risk has always been a critical question in decision support processes. FORESIM is a simulation technique that models shipping markets (developed recently). In this paper we present the application of this technique in order to obtain useful information regarding future values of the tanker market risk. This is the first attempt to express future tanker market risk in relation to current market fundamentals. We follow a system’s analysis seeking for internal and external parameters affecting risk. Therefore we apply dynamic features in risk measurement taking into account all Tanker market characteristics and potential excitations from non-systemic parameters as well as their contribution to freight level formulation and fluctuation. In this way we are able to measure the behavior of future market risk as long as twelve months ahead with very encouraging results. The output information is therefore useful in all aspects of risk analysis and decision making in shipping markets.
C45|Whose Balance Sheet is this? Neural Networks for Banks’ Pattern Recognition|The balance sheet is a snapshot that portraits the financial position of a firm at a specific point of time. Under the reasonable assumption that the financial position of a firm is unique and representative, we use a basic artificial neural network pattern recognition method on Colombian banks’ 2000-2014 monthly 25-account balance sheet data to test whether it is possible to classify them with fair accuracy. Results demonstrate that the chosen method is able to classify out-of-sample banks by learning the main features of their balance sheets, and with great accuracy. Results confirm that balance sheets are unique and representative for each bank, and that an artificial neural network is capable of recognizing a bank by its financial accounts. Further developments fostered by our findings may contribute to enhancing financial authorities’ supervision and oversight duties, especially in designing early-warning systems.
C45|Importance of the long-term seasonal component in day-ahead electricity price forecasting revisited: Neural network models|In day-ahead electricity price forecasting the daily and weekly seasonalities are always taken into account, but the long-term seasonal component was believed to add unnecessary complexity and in most studies ignored. The recent introduction of the Seasonal Component AutoRegressive (SCAR) modeling framework has changed this viewpoint. However, the latter is based on linear models estimated using Ordinary Least Squares. Here we show that considering non-linear neural network-type models with the same inputs as the corresponding SCAR model can lead to a yet better performance. While individual Seasonal Component Artificial Neural Network (SCANN) models are generally worse than the corresponding SCAR-type structures, we provide empirical evidence that committee machines of SCANN networks can significantly outperform the latter.
C45|Forecasting Macroeconomic Variables Using Neural Network Models and Three Automated Model Selection Techniques| When forecasting with neural network models one faces several problems, all of which influence the accuracy of the forecasts. First, neural networks are often hard to estimate due to their highly nonlinear structure. To alleviate the problem, White (2006) presented a solution (QuickNet) that converts the specification and nonlinear estimation problem into a linear model selection and estimation problem. We shall compare its performance to that of two other procedures building on the linearization idea: the Marginal Bridge Estimator and Autometrics. Second, one must decide whether forecasting should be carried out recursively or directly. This choice is investigated in this work. The economic time series used in this study are the consumer price indices for the G7 and the Scandinavian countries. In addition, a number of simulations are carried out and results reported in the article.
C45|Measuring the impact of Mision Alimentacion in Merida by means of fuzzy logic|This paper presents a model for measuring the impact of Mision Alimentacion in the metropolitan area of Merida, Venezuela, through fuzzy logic is determined, taking into account a wide range of qualitative and quantitative indicators that explain the effect on society of this public politics. For this fuzzy subsets techniques were used: expertizajes and uncertain ratios, which can measure and evaluate all areas comprehensively explain the net effect of the mission for supply and protection of food security for Venezuelans.
C45|Forecasting with Neural Networks Models|This paper deals with so-called feedforward neural network model which we consider from a statistical and econometric viewpoint. It was shown how this model can be estimated by maximum likelihood. Finally, we apply the ANN methodology to model demand for electricity in South Africa. The comparison of forecasts based on a linear and ANN model respectively shows the usefulness of the latter.
C45|Predicting customer churn in banking industry using neural networks|The aim of this article is to present a case study of usage of one of the data mining methods, neural network, in knowledge discovery from databases in the banking industry. Data mining is automated process of analysing, organization or grouping a large set of data from different perspectives and summarizing it into useful information using special algorithms. Data mining can help to resolve banking problems by finding some regularity, causality and correlation to business information which are not visible at first sight because they are hidden in large amounts of data. In this paper, we used one of the data mining methods, neural network, within the software package Alyuda NeuroInteligence to predict customer churn in bank. The focus on customer churn is to determinate the customers who are at risk of leaving and analysing whether those customers are worth retaining. Neural network is statistical learning model inspired by biological neural and it is used to estimate or approximate functions that can depend on a large number of inputs which are generally unknown. Although the method itself is complicated, there are tools that enable the use of neural networks without much prior knowledge of how they operate. The results show that clients who use more bank services (products) are more loyal, so bank should focus on those clients who use less than three products, and offer them products according to their needs. Similar results are obtained for different network topologies.
C45|Tests of the Constancy of Conditional Correlations of Unknown Functional Form in Multivariate GARCH Models|We introduce two tests for the constancy of conditional correlations of unknown functional form in multivariate GARCH models. The first test is based on artificial neural networks and the second on a Taylor expansion of each unknown conditional correlation. They can be seen as general misspecification tests for a large set of multivariate GARCH-type models. We investigate their size and their power through Monte Carlo experiments. Moreover, we study the robustness of these tests to nonnormality by simulating some models, such as the GARCH - t and Beta - t - EGARCH. We give some illustrative empirical examples based on financial data.
C45|EU ETS Facets in the Net: How Account Types Influence the Structure of the System|In this work, we investigate which countries have been more central during Phases I and II of the European Emission Trading Scheme (EU ETS) with respect to the different types of accounts operating in the system. We borrow a set of centrality measures from Network Theory's tools to describe how the structure of the system has evolved over time and to identify which countries have been in the core or in the periphery of the network. In doing this, we investigate by means of extensive partitions on the different types of accounts and transactions characterizing the EU ETS whether the role of intermediaries (approximated by Person Holding Accounts - PHAs) has affected the overall structure of the system. Preliminary findings over the period 2005-2012 suggest that PHAs have played a prominent role in the transaction of permits, heavily influencing the configuration of the system. This motivates further research on the impact of non-regulated entities in the EU ETS design.
C45|A Network-Science Support System for Food Chain Safety: A Case from Hungarian Cattle Production| In a risk analysis framework, food chain safety measures should be objective and scientifically based. Network science-as a decision support tool-may have an important role in bringing safety to the food supply. The aim of the present work is to develop a network-based assessment methodology for Hungarian cattle holdings. The criteria of which is (1) suitable for risk-based planning in order to put resources into the most critical elements of the cattle production network; (2) should be capable of simulating different epidemiological situations in order to increase preparedness for real epidemics.
C45|A fuzzy model for the evaluation of suppliers of material resources to machine-building enterprises|The subject of our research is the process of selecting a supplier of material resources to machine-building enterprises. The aim of this work is to identify factors that improve the process of supplier selection of material resources to large enterprises. Testing this model is made on the example of the “KRANEKS” machinebuilding enterprise. The methodological basis of the scientific research is work in the fields of systems analysis, management accounting, and econometrics. As a result of the research, a methodology and estimation algorithm of suppliers was developed, as well as a hierarchical model of fuzzy inference to supplier evaluation. These developments are mainly aimed at supporting decision making in choosing the best provider for a machine-building enterprise. These developments mainly focused on decision support in choosing the best provider for an engineering enterprise whose list of material resources runs into the thousands. They are aimed at improving the efficiency of administrative decisions in the supplier selection process of machine-building enterprises and improving the work process of their purchasing departments.
C45|Credit Scoring Models for a Tunisian Microfinance Institution: Comparison between Artificial Neural Network and Logistic Regression|This paper compares, for a microfinance institution, the performance of two individual classification models: Logistic Regression (Logit) and Multi-Layer Perceptron Neural Network (MLP), to evaluate the credit risk problem and discriminate good creditors from bad ones. Credit scoring systems are currently in common use by numerous financial institutions worldwide. However, credit scoring using a non-parametric statistical technique with the microfinance industry, is a relatively recent application. In Tunisia, no model which employs a non-parametric statistical technique has yet, as far as we know, been published. This lack is surprising since the implementation of credit scoring should contribute towards the efficiency of microfinance institutions, thereby improving their competitiveness. This paper builds a non-parametric credit scoring model based on the Multi-Layer perceptron approach (MLP) and benchmarks its performance against Logistic Regression (LR) techniques. Based on a sample of 300 borrowers from a Tunisian microfinance institution, the results reveal that Logistic Regression outperforms neural network models.
C45|Neural Nets for Indirect Inference|For simulable models, neural networks are used to approximate the limited information posterior mean, which conditions on a vector of statistics, rather than on the full sample. Because the model is simulable, training and testing samples may be generated with sizes large enough to train well a net that is large enough, in terms of number of hidden layers and neurons, to learn the limited information posterior mean with good accuracy. Targeting the limited information posterior mean using neural nets is simpler, faster, and more successful than is targeting the full information posterior mean, which conditions on the observed sample. The output of the trained net can be used directly as an estimator of the model’s parameters, or as an input to subsequent classical or Bayesian indirect inference estimation. Examples of indirect inference based on the out- put of the net include a small dynamic stochastic general equilibrium model, estimated using both classical indirect inference methods and approximate Bayesian computing (ABC) methods, and a continuous time jump-diffusion model for stock index returns, estimated using ABC.
C45|Co-authorship and Academic Productivity in Economics: Interaction Maps from the Complex Networks Approach|We explore the relationship between collaborations in writing papers and the academic productivity of economists and, particularly, we describe the magnitude and intensity of co-authorship among economists. To that end, we employ interaction maps from Complex Systems methods to study the global properties of specific networks. We use 8,253 JCR papers from ISI-WOK, published by 5,188 economists from Spanish institutions, and their co-authors, up to 8,202 researchers, from 2002 to 2014, to identify and determine the collaborative structure of economics research in Spain, with its primary communities and figures of influence. Our results indicate that centrality and productivity are correlated, particularly with respect to a local estimator of centrality (page rank), and we provide certain recommendations, such as promoting interactions among highly productive authors who have few co-authors with other researchers in their environment, or recommending that authors who may be well-positioned but minimally productive strive to improve their productivity.
C45|A self-organizing map analysis of survey-based agents? expectations before impending shocks for model selection: The case of the 2008 financial crisis|This paper examines the role of clustering techniques to assist in the selection of the most indicated method to model survey-based expectations. First, relying on a Self-Organizing Map (SOM) analysis and using the financial crisis of 2008 as a benchmark, we distinguish between countries that show a progressive anticipation of the crisis, and countries where sudden changes in expectations occur. We then generate predictions of survey indicators, which are usually used as explanatory variables in econometric models. We compare the forecasting performance of a multi-layer perceptron (MLP) Artificial Neural Network (ANN) model to that of three different time series models. By combining both types of analysis, we find that ANN models outperform time series models in countries in which the evolution of expectations shows brisk changes before impending shocks. Conversely, in countries where expectations follow a smooth transition towards recession, autoregressive integrated moving-average (ARIMA) models outperform neural networks.
C45|The Nature of Volatility Spillovers across the International Capital Markets|This paper studies the nature of volatility spillovers across countries from the perspective of network theory and by relying on data of US-listed ETFs. I use a Lasso-related technique to estimate the International Volatility Network (IVN) where the nodes correspond to large-cap international stock markets while the links account for significant volatility lead-lags. Also included in the analysis is the International TradeNetwork (ITN), whose links measure bilateral export-import flows thus, capturing fundamental interconnections between countries. I find that the IVN and the ITN resemble each other closely pointing out that volatility does not disseminate randomly but tends to spread across fundamentally related economies. I also note that the lagged volatility reactions embedded in the IVN are consistent with the notion of gradual diffusion of information across investors who are subject to limited attention and home bias. This hypothesis is formally tested by using as a direct proxy of investors’ attention the aggregate search frequency in Google. The empirical results support this intuition indicating that higher volatility surprises in key foreign markets predict higher domestic attention upon those markets in subsequent days. Once domestic attention is captured by such external shocks, it is contemporaneously transformed into higher domestic volatility.
C45|Being Central and Productive? Evidence from Slovenian Visual Artists in the 19th and 20th Century|Slovenian art history has received very little (if any) attention from the viewpoint of network theory although there were several examples of artists co-working or working in groups, collectives or even loosely organized clusters (groups from the impressionist Sava in 1904 to postmodern Irwin in 1984). This may be interpreted as a way to acquire better positions in the national and international art circles and on the art market. In our article we use web-based dataset of Slovenska biografija (operated by the Slovenian Academy of Sciences and Arts), which contains data on numerous notable persons throughout Slovenian history to analyze the centrality of individual artistic figures and movements throughout Slovenian art history. We also study the influence of network centrality on cultural production controlling for endogeneity following the instrumental variable approach, proposed in the literature while using a new instrumental variable to solve the problem. Finally, we present results which show that women visual artists used their network positions more intensively than men and provide some first explanations for this observed relationship. In conclusion, we provide some reflections on the importance of these findings for further research work in the area.
C45|Using Nature-Inspired Metaheuristics to Train Predictive Machines|Nature-inspired metaheuristics for optimization have proven successful, due to their fine balance between exploration and exploitation of a search space. This balance can be further refined by hybridization. In this paper, we conduct experiments with some of the most promising nature-inspired metaheuristics, for assessing their performance when using them to replace backpropagation as a learning method for neural networks. The selected metaheuristics are: Cuckoo Search (CS), Gravitational Search Algorithm (GSA), Particle Swarm Optimization (PSO), the PSO-GSA hybridization, Many Optimizing Liaisons (MOL) and certain combinations of metaheuristics with local search methods. Both the neural network based classifiers and function approximators are evolved in this way. Classifiers have been evolved against a training dataset having bankruptcy prediction as a target, whereas function approximators have been evolved as NNARX models, where the target is to predict foreign exchange rates.
C45|Machine Learning Techniques For Stock Market Prediction.Acase Study Of Omv Petrom|The research reported in the paper focuses on the stock market prediction problem, the main aim being the development of a methodology to forecast the OMV Petrom stock closing price. The methodology is based on some novel variable selection methods and an analysis of neural network and support vector machines based prediction models. Also, a hybrid approach which combines the use of the variables derived from technical and fundamental analysis of stock market indicators in order to improve prediction results of the proposed approaches is reported in this paper. Two novel variable selection methods are used to optimize the performance of prediction models. In order to identify the most informative time series to predict a stock price, both methods are essentially based on the general forecasting error minimization when a certain stock price is expressed exclusively in terms of other indicators. After the variable selection is over, the forecasting is performed in terms of the historical values of the given stock price and selected variables respectively. The performance of the proposed methodology is evaluated by a long series of tests, the results being very encouraging as compared to similar developments.
C45|Credit risk spillover between financials and sovereigns in the euro area during 2007-2015|This paper presents time-varying contagion indices of credit risk spillover and feedback between 64 financials and sovereigns in the euro area, where spillover is identified based on bilateral Granger causality regressions. Over-identification of contagion between financials JEL Classification: C45, E44, E65, G01, G13, G28, H81
C45|Multi-layered interbank model for assessing systemic risk|In this paper, we develop an agent-based multi-layered interbank network model based on a sample of large EU banks. The model allows for taking a more holistic approach to interbank contagion than is standard in the literature. A key finding of the paper is that there are non-negligible non-linearities in the propagation of shocks to individual banks when taking into account that banks are related to each other in various market segments. In a nutshell, the contagion effects when considering the shock propagation simultaneously across multiple layers of interbank networks can be substantially larger than the sum of the contagion-induced losses when considering the network layers individually. In addition, a bank 'systemic importance' measure based on the multi-layered network model is developed and is shown to outperform standard network centrality indicators.
C45|Crisis severity and the international trade network|In this paper we analyse the role of the international trade network for the strength of the global recession across countries. The novelty of our paper is the use of value-added trade data to capture the importance of trade network structure. We estimate with BMA techniques how far network indicators measuring interlinkages in terms of value added trade has explanatory power both for the length and the depth of the recent crisis once we control for pre-crisis macroeconomic fundamentals. Our main findings are that the macroeconomic control variables with the strongest explanatory power for the length and the depth of the crisis are the growth rates of credit and of the real effective exchange rate in the pre-crisis period and, though to a lesser extent, GDP and inflation growth over the same period and pre-crisis foreign exchange reserves. Government debt, the GVC participation index and net foreign assets have very little explanatory power in the BMA estimations. The models JEL Classification: F14, C45, C52, C67
C45|Forecasting the term structure of crude oil futures prices with neural networks|The paper contributes to the limited literature modelling the term structure of crude oil markets. We explain the term structure of crude oil prices using the dynamic Nelson–Siegel model and propose to forecast oil prices using a generalized regression framework based on neural networks. The newly proposed framework is empirically tested on 24years of crude oil futures prices covering several important recessions and crisis periods. We find 1-month-, 3-month-, 6-month- and 12-month-ahead forecasts obtained from a focused time-delay neural network to be significantly more accurate than forecasts from other benchmark models. The proposed forecasting strategy produces the lowest errors across all times to maturity.
C45|On business cycle fluctuations in USA macroeconomic time series|This study employs eighteen USA macroeconomic time series variables to investigate possible existence of asymmetries in business cycle fluctuations in the series. Detection of asymmetric fluctuations in economic activity is important for policymakers since effective monetary policy relies on asymmetric business cycle fluctuations in all the series. The asymmetric deviations from the long-term growth trend in each of the series are modeled using regime switching models and artificial neural networks. The results based on nonlinear switching time series models reveal strong evidence of business cycle asymmetries in most of the series. The results based on in-sample approximations from artificial neural networks show statistically significant evidence of asymmetries in all the series. Similar results are obtained when jackknife out-of-sample approximations from artificial neural networks are used. Thus, the study results show statistically significant evidence of asymmetries in all the series which indicates that business cycle fluctuations in the series are asymmetric, thus alike. Therefore, the impact of monetary policy shocks on the output and the other macroeconomic variables can be anticipated using nonlinear models only. The results on asymmetric business cycle fluctuations in real GDP are in line with recent studies but in sharp contrast with Balke and Fomby (1994).
C45|Dynamics of global business cycle interdependence|In this paper, we provide a comprehensive analysis of the time-varying interdependence among the economic cycles of the major world economies during the post-Great Moderation period. We document a significant increase in the global business cycle interdependence occurred in the early 2000s. Such increase is mainly attributed to the emerging market economies, since their business cycles became more synchronized with the rest of the world around that time. Moreover, we find that the increase in global interdependence is highly related to decreasing differences in sectoral composition among countries.
C45|A self-organizing map analysis of survey-based agents׳ expectations before impending shocks for model selection: The case of the 2008 financial crisis|This paper examines the role of clustering techniques to assist in the selection of the most indicated method to model survey-based expectations. First, relying on a Self-Organizing Map (SOM) analysis and using the financial crisis of 2008 as a benchmark, we distinguish between countries that show a progressive anticipation of the crisis, and countries where sudden changes in expectations occur. We then generate predictions of survey indicators, which are usually used as explanatory variables in econometric models. We compare the forecasting performance of a multi-layer perceptron (MLP) Artificial Neural Network (ANN) model to that of three different time series models. By combining both types of analysis, we find that ANN models outperform time series models in countries in which the evolution of expectations shows brisk changes before impending shocks. Conversely, in countries where expectations follow a smooth transition towards recession, autoregressive integrated moving-average (ARIMA) models outperform neural networks.
C45|EU ETS Facets in the Net: How Account Types Influence the Structure of the System| In this work, we investigate which countries have been more central during Phases I and II of the European Emission Trading Scheme (EU ETS) with respect to the different types of accounts operating in the system. We borrow a set of centrality measures from Network Theory's tools to describe how the structure of the system has evolved over time and to identify which countries have been in the core or in the periphery of the network. In doing this, we investigate by means of extensive partitions on the different types of accounts and transactions characterizing the EU ETS whether the role of intermediaries (approximated by Person Holding Accounts - PHAs) has affected the overall structure of the system. Preliminary findings over the period 2005-2012 suggest that PHAs have played a prominent role in the transaction of permits, heavily influencing the configuration of the system. This motivates further research on the impact of non-regulated entities in the EU ETS design.
C45|Spatial Dependence and Data-Driven Networks of International Banks|This paper computes data-driven correlation networks based on the stock returns of international banks and conducts a comprehensive analysis of their topological properties. We first apply spatial-dependence methods to filter the effects of strong common factors and a thresholding procedure to select the significant bilateral correlations. The analysis of topological characteristics of the resulting correlation networks shows many common features that have been documented in the recent literature but were obtained with private information on banks' exposures, including rich and hierarchical structures, based on but not limited to geographical proximity, small world features, regional homophily, and a core-periphery structure.
C45|Forecasting the Market Risk Premium with Artificial Neural Networks|This paper aims to forecast the Market Risk premium (MRP) in the US stock market by applying machine learning techniques, namely the Multilayer Perceptron Network (MLP), the Elman Network (EN) and the Higher Order Neural Network (HONN). Furthermore, Univariate ARMA and Exponential Smoothing models are also tested. The Market Risk Premium is defined as the historical differential between the return of the benchmark stock index over a short-term interest rate. Data are taken in daily frequency from January 2007 through December 2014. All these models outperform a Naive benchmark model. The Elman network outperforms all the other models during the insample period, whereas the MLP network provides superior results in the out-of-sample period. The contribution of this paper to the existing literature is twofold. First, it is the first study that attempts to forecast the Market Risk Premium in a daily basis using Artificial Neural Networks (ANNs). Second, it is not based on a theoretical model but is mainly data driven. The chosen calculation approach fits quite well with the characteristics of ANNs. The forecasting model is tested with data from the US stock market. The proposed model-based forecasting method aims to capture patterns in the data that improve the forecasting accuracy of the Market Risk Premium in the tested market and indicates potential key metrics for investment and trading purposes for short time horizons.
C45|Comparing the market risk premia forecasts in JSE and NYSE equity markets|This paper examines the evidence regarding predictability in the market risk premium using artificial neural networks (ANNs), namely the Elman Network (EN) and the Higher Order Neural network (HONN), univariate ARMA and exponential smoothing techniques, such as Single Exponential Smoothing (SES) and Exponentially Weighted Moving Average (EWMA). The contribution of this paper is the inclusion of the South African market risk premium to the forecasting exercise and its direct comparison with US forecasting results. The market risk premium is defined as the expected rate of return on the market portfolio in excess of the shortterm interest rate for each market. All data are taken from January 2007 till December 2014 on a daily basis. Elman networks provide superior results among the tested models in both insample and out-of sample periods as well as among the tested markets. In general, neural networks beat the naive benchmark model and achieve to perform better than the rest of their linear tested counterparts. The forecasting models successfully capture patterns in the data that improve the forecasting accuracy of the tested models. Therefore, they can be applied to trading and investment purposes.
C45|A Comparison between Neural Networks and GARCH Models in Exchange Rate Forecasting|Modeling and forecasting of dynamics nominal exchange rate has long been a focus of financial and economic research. Artificial Intelligence (IA) modeling has recently attracted much attention as a new technique in economic and financial forecasting. This paper proposes an alternative approach based on artificial neural network (ANN) to predict the daily exchange rates. Our empirical study is based on a series of daily data in Tunisia. In order to evaluate this approach, we compare it with a generalized autoregressive conditional heteroskedasticity (GARCH) model in terms of their performance. Results indicate that the proposed nonlinear autoregressive (NAR) model is an accurate and a quick prediction method. This finding helps businesses and policymakers to plan more appropriately.
C45|A Comparison between Neural Networks and GARCH Models in Exchange Rate Forecasting|Modeling and forecasting of dynamics nominal exchange rate has long been a focus of financial and economic research. Artificial Intelligence (IA) modeling has recently attracted much attention as a new technique in economic and financial forecasting. This paper proposes an alternative approach based on artificial neural network (ANN) to predict the daily exchange rates. Our empirical study is based on a series of daily data in Tunisia. In order to evaluate this approach, we compare it with a generalized autoregressive conditional heteroskedasticity (GARCH) model in terms of their performance. Results indicate that the proposed nonlinear autoregressive (NAR) model is an accurate and a quick prediction method. This finding helps businesses and policymakers to plan more appropriately.
C45|Spatial Dependence and Data-Driven Networks of International Banks|This paper computes data-driven correlation networks based on the stock returns of international banks and conducts a comprehensive analysis of their topological properties. We first apply spatial-dependence methods to filter the effects of strong common factors and a thresholding procedure to select the significant bilateral correlations. The analysis of topological characteristics of the resulting correlation networks shows many common features that have been documented in the recent literature but were obtained with private information on banks’ exposures. Our analysis validates these market-based adjacency matrices as inputs for the spatio-temporal analysis of shocks in the banking system.
C45|A Toolkit for Stability Assessment of Tree-Based Learners|Recursive partitioning techniques are established and frequently applied for exploring unknown structures in complex and possibly high-dimensional data sets. The methods can be used to detect interactions and nonlinear structures in a data-driven way by recursively splitting the predictor space to form homogeneous groups of observations. However, while the resulting trees are easy to interpret, they are also known to be potentially unstable. Altering the data slightly can change either the variables and/or the cutpoints selected for splitting. Moreover, the methods do not provide measures of confidence for the selected splits and therefore users cannot assess the uncertainty of a given fitted tree. We present a toolkit of descriptive measures and graphical illustrations based on resampling, that can be used to assess the stability of the variable and cutpoint selection in recursive partitioning. The summary measures and graphics available in the toolkit are illustrated using a real world data set and implemented in the R package stablelearner.
C45|Using Recursive Partitioning to Account for Parameter Heterogeneity in Multinomial Processing Tree Models|In multinomial processing tree (MPT) models, individual differences between the participants in a study lead to heterogeneity of the model parameters. While subject covariates may explain these differences, it is often unknown in advance how the parameters depend on the available covariates, that is, which variables play a role at all, interact, or have a nonlinear influence, etc. Therefore, a new approach for capturing parameter heterogeneity in MPT models is proposed based on the machine learning method MOB for model-based recursive partitioning. This recursively partitions the covariate space, leading to an MPT tree with subgroups that are directly interpretable in terms of effects and interactions of the covariates. The pros and cons of MPT trees as a means of analyzing the effects of covariates in MPT model parameters are discussed based on a simulation experiment as well as on two empirical applications from memory research. Software that implements MPT trees is provided via the mpttree function in the psychotree package in R.
C45|Co-authorship and Academic Productivity in Economics: Interaction Maps from the Complex Networks Approach|We explore the relationship between collaborations in writing papers and the academic productivity of economists and, particularly, we describe the magnitude and intensity of co-authorship among economists. To that end, we employ interaction maps from Complex Systems methods to study the global properties of specific networks. We use 8,253 JCR papers from ISI-WOK, published by 5,188 economists from Spanish institutions, and their co-authors, up to 8,202 researchers, from 2002 to 2014, to identify and determine the collaborative structure of economics research in Spain, with its primary communities and figures of influence. Our results indicate that centrality and productivity are correlated, particularly with respect to a local estimator of centrality (page rank), and we provide certain recommendations, such as promoting interactions among highly productive authors who have few co-authors with other researchers in their environment, or recommending that authors who may be well-positioned but minimally productive strive to improve their productivity.
C45|Weighted merge context for clustering and quantizing spatial data with self-organizing neural networks|Abstract This publication presents a generalization of merge context, named weighted merge context (WMC), which is particularly useful for clustering and quantizing spatial data with self-organizing neural networks. In contrast to merge context, WMC does not depend on a predefined (sequential) ordering of the data; distance is evaluated by recursively taking neighboring observations into account. For this purpose, WMC utilizes a weight matrix that describes the neighborhood relationships between observations. This property distinguishes WMC from existing approaches like contextual neural gas (NG) or the GeoSOM, which force spatially close observations to be represented by similar prototypes, but neglected the similarity of the observations’ neighborhoods. For practical studies, WMC is combined with the NG algorithm to obtain weighted merging NG (WMNG). The properties of WMNG and its usefulness for clustering and quantizing spatial data are investigated on two different case studies which utilize an simulated binary grid and a real-world continuous data set.
C45|Weighted merge context for clustering and quantizing spatial data with self-organizing neural networks|This publication presents a generalization of merge context, named weighted merge context (WMC), which is particularly useful for clustering and quantizing spatial data with self-organizing neural networks. In contrast to merge context, WMC does not depend on a predefined (sequential) ordering of the data; distance is evaluated by recursively taking neighboring observations into account. For this purpose, WMC utilizes a weight matrix that describes the neighborhood relationships between observations. This property distinguishes WMC from existing approaches like contextual neural gas (NG) or the GeoSOM, which force spatially close observations to be represented by similar prototypes, but neglected the similarity of the observations’ neighborhoods. For practical studies, WMC is combined with the NG algorithm to obtain weighted merging NG (WMNG). The properties of WMNG and its usefulness for clustering and quantizing spatial data are investigated on two different case studies which utilize an simulated binary grid and a real-world continuous data set. Copyright Springer-Verlag Berlin Heidelberg 2016
C45|Detección de fraude financiero mediante redes neuronales de clasificación en un caso real español /Detecting Financial Fraud using Neural Network Classification Models in a Real Spanish Case|Este análisis supone una primera aproximación a la implementación de modelos de redes neuronales al trabajo pericial para la detección de operaciones de fraude. Los datos analizados provienen de un caso real de blanqueo de capitales en el que se está colaborando con la Policía Nacional Española. En ellos se cuenta con información de operaciones contables individuales entre las que se cuenta con una proporción de operaciones bien identificadas como fraudulentas con la que es posible entrenar un modelo de clasificación. En este trabajo, tras describir breve¬mente la metodología utilizada y la estrategia de ajuste se obtiene un modelo con una capacidad predictiva reseñable, incluso con datos de entrenamiento fuertemente desequilibrados. Además, al aplicar técnicas de balanceado de los datos de entrenamiento (SMOTE) se obtiene un resultado que indicaría la viabilidad de este tipo de modelos como herramienta en la planificación y priorización de las tareas de investigación policial, ya que uno de los principales problemas de los investigadores expertos en estos delitos financieros es la incapacidad para traducir la gran cantidad de información que se deriva de las empresas implicadas en patrones de compra de los individuos claramente fraudulentos. This paper explores the possibilities offered by statistical tools based on artificial neural networks for pattern recognition in expert work for money-laundering detection. The data is provided by the Spanish Police Department and comes from a case in which is actually working at. Account information is provided, where some accounting entries are identified as fraud. Hence it is possible to use this information to train a classification model. In this analysis, after briefly describing methodology used and fitting strategy, it is presented a model with a promising predictive capacity, even with strongly unbalanced training data set. After applying balancing technique to the training data (SMOTE) the result is remarkably improved which would indicate the viability of those models as tool for police experts planification, providing a way to reduce the use of expensive research resources.
C45|The International Practice of Statistical Property Valuation Methods and the Possibilities of Introducing Automated Valuation Models in Hungary|In the wake of regulatory, information technology and methodological changes, statistical property valuation has gained traction in Hungary. This paper looks at the available methods of appraisal based on the literature. We provide an overview of the advantages and drawbacks of the currently known methods. Based on these, automated valuation models (AVMs) can be readily introduced alongside the estimated median value based methods used so far. For real estate industryspecific reasons, the introduction of parametric hedonic estimates supplemented with spatial correlations can be expected for the time being. The better performance of statistical models would need improved quality of duties office data.
C45|The diversity of agents and evolution of overlapping patents on electric vehicles|This article studies the inventive activity related to electric vehicles. Particularly, examining the relationship between the overlapping patents and a diverse population of agents: Companies, non-practicing entities, individual inventors and alliances. Based on information from the United States Patent and Trademark Office (USPTO), period from 1976 to 2012, it is presented: 1) as from network analysis, the increasing complexity of inventive electric vehicles; 2) the characteristics and evolution of inventive activity and, 3) its relationship with the existence of overlapping patents. It is shown that the evolution of overlapping patents (from low to high complexity) is associated with greater complexity of technological knowledge as well as greater diversity of agents.
C45|Broadband Mergers and Dynamic Bargaining: An Application to Netflix| I measure how mergers in the market for broadband internet service affect short-run welfare. Mergers between internet service providers (ISPs) with non-overlapping markets may decrease welfare by increasing ISP bargaining leverage against content providers. However, study of this welfare channel has been stymied by a lack of data on interconnection fees between content and internet service providers. I estimate an industry model of demand, plan choice, pricing and interconnection bargaining using data on plan prices, consumer choice sets and bargaining delays between major U.S ISPs and the leading purveyor of streaming video content, Netflix. Intuitively, if delaying agreement over interconnection degrades quality of service to subscribers, then the opportunity cost of lost subscriptions identifies the fee. To map disagreement times and ISP competition into interconnection fees, I develop a multilateral dynamic bargaining model with asymmetric information. ISPs make take-it-or-leave it offers to learn about Netflix's benefit from interconnection, while simultaneously competing for subscribers who value Netflix quality of service. I structurally estimate the model and recover fixed interconnection fees ranging from 44 to 69 million USD. I find that a proposed merger between TimeWarner and Comcast that was challenged by the Federal Communications Commission would have slightly raised interconnection fees and bargaining length, reducing consumer welfare by 1.9 percent.
C45|The Effect of Word of Mouth on Sales: New Answers from the Comprehensive Consumer Journey Data| Online Word-of-Mouth has great impact on product sales. Although aggregate data suggests that customers read review text rather than relying only on summary statistics, little is known about consumersâ€™ review reading behavior and its impact on conversion at the granular level. To fill this research gap, we analyze a comprehensive dataset that tracks individual-level search, review reading, as well as purchase behaviors and achieve two objectives. First, we describe consumersâ€™ review reading behaviors. In contrast to what has been found with aggregate data, individual level consumer journey data shows that around 70% of the time, consumers do not read reviews in their online journeys; they are less likely to read reviews for products that are inexpensive and have many reviews. Second, we quantify the causal impact of quantity and content information of reviews read on sales. The identification relies on the variation in the reviews seen by consumers due to newly added reviews. To extract content information, we apply Deep Learning natural language processing models and identify six dimensions of content in the reviews. We find that aesthetics and price content in the reviews significantly affect conversion. Counterfactual simulation suggests that re-ordering review content can have the same effect as a 1.6% price cut for boosting conversion.
C45|Wavelet Analisis of Unemployment Rate in Visegrad Countries|Visegrad countries, Poland, Slovakia, Czech Republic and Hungary have common history and have faced the same challenges created by globalisation process for the last three decades. They have successfully transformed form central planned to market economies. They have implemented fundamental reforms of their whole institutional systems and finally joined the European Union in the year 2004. During this process the most significant changes, which were directly influenced by opening of these economies in the reality of globalisation, have been seen on the labour markets. From the policy point of view the labour markets are always considered as crucial for social and macroeconomic stability of economies. This forces the economists to constant empirical research in this field. In this context the aim of the article is to conduct comparative analysis of the unemployment phenomena in the four countries. For this purpose wavelet analysis was applied. In the research a discrete wavelet transformation was used, which has been recently effectively used for analysis of macroeconomic indicators. The empirical research was conducted for the years 1998-2016 and it was based on the Eurostat data. In the research the following hypothesis was verified: the phenomenon of unemployment in the case of Poland, Slovakia and Hungary is formed in a quite similar way, whereas in Czech Republic the situation on the labour markets is mainly determined by factors of different nature.
C45|Methodological Aspects of Qualitative-Quantitative Analysis of Decision-Making Processes|The paper aims at recognizing the possibilities and perspectives of application of qualitative-quantitative research methodology in the field of economics, with a special focus on production engineering management processes. The main goal of the research is to define the methods that would extend the research apparatus of economists and managers by tools that allow the inclusion of qualitative determinants into quantitative analysis. Such approach is justified by qualitative character of many determinants of economic occurrences. At the same time quantitative approach seems to be predominant in production engineering management, although methods of transposition of qualitative decision criteria can be found in literature. Nevertheless, international economics and management could profit from a mixed methodology, incorporating both types of determinants into joint decision-making models. The research methodology consists of literature review and own analysis of applicability of mixed qualitative-quantitative methods for managerial decision-making. The expected outcome of the research is to find which methods should be applied to include qualitative-quantitative analysis into multicriteria decision-making models in the fields of economics, with a special regard to production engineering management.
C45|Nightlights as a Development Indicator: The Estimation of Gross Provincial Product (GPP) in Turkey|For a while in Turkey, researchers dealing with spatial economics are unable to make detailed comparative and descriptive analysis on sub-national base due to lack of data. In particular, GDP, which is a basic indicator of economic activities, has not been published in Turkey at sub-national level since 2001. In this study, we use a different data source, night-time satellite imagery, to obtain sub-national GDP and GDP per capita series for the period between 2001 and 2013 at the level of provinces which is the basic administrative division of the Country. We also re-construct the series for the period between 1992 and 2001. For the estimation of sub-national GDP, we use Neural Network Algorithm.
C45|Chaos in G7 stock markets using over one century of data: A note|In our study, we tested for chaos in the historical daily and monthly datasets spanning over one century of stock returns for G7 countries. Applying the 0–1 test proposed by Gottwald and Melbourne (2005) and the recent test developed by BenSaïda and Litimi (2013), which is powerful in detecting chaotic dynamics, we found that (a) it is better to denoise the data before testing for chaos and (b), in general, chaos is observed for all countries, using both tests, when we denoised the data.
C45|Improving Efficiency of the Oil and Gas Sector and Other Extractive Industries by Applying Methods of Artificial Intelligence<BR>[Применение Методов Искусственного Интеллекта Для Повышения Эффективности В Нефтегазовой И Других Сырьевых Отраслях]|A considerable decline in commodity prices in recent years, primarily oil prices, is the result of a new equilibrium in the market which, in turn, is a direct consequence of technological innovations. In such circumstances, those producers which can adapt to lower prices by reducing costs and increasing efficiency will gain a strong competitive advantage. Until recently, the main driving force of innovative development of the energy sector had been the “shale revolution”. The situation is changing rapidly — the oil and gas industry is in need of new technological solutions that would allow it to weather the storm of lower prices. Currently, one of the areas where innovation is fastest is artificial intelligence. The article provides a brief overview of the most widespread method within artificial intelligence — artificial neural networks and describes their main applications within the oil and gas sector. In their work the authors distinguish highlight three main applications — interpretation of geological data, hydrocarbon production (smart fields) and price forecasting. The use of artificial intelligence can increase efficiency of both geological exploration and production — it allows to achieve more at a lower cost. Under the new market conditions formed in the energy and mining sectors it is crucially important to utilise all available mechanisms to increase efficiency. Following the drop in commodity prices, it has become of vital for companies to acquire more accurate forecasting methods which would allow to analyze market developments and improve strategic planning.
C45|"“Butterfly Effect"" vs Chaos in Energy Futures Markets"|"In this paper we test for the sensitive dependence on initial conditions (the so called \butter y e ect"") of energy futures time series (heating oil, natural gas), and thus the determinism of those series. Unlike previous studies, we test for the time series for sensitive dependence on initial conditions, introducing a coecient that describes the determinism rate of the series and that represents its reliability level (in percentage). The introduction of this reliability level is motivated by the fact that time series generated from stochastic systems also might show sensitive dependence on initial conditions. The reliability level obtained for the NYMEX energy futures considered here is always approximately 50% and this means that the stochastic component and the deterministic one turn up approximately in the same proportions. Such a tangible presence of a stochastic component does not warrant strong evidence of chaotic behaviour."
C45|A Fuzzy-Neural Performance Evaluation Approach of Selecting Outsource International Logistic Company|Owing to lack of confidence, the usage of domestic logistics services in the Asian region, e.g. Taiwanese companies, is comparatively lower than the use of international logistics companies. This paper develops an integrated fuzzy neural network performance evaluation model which is able to consider five key factors to evaluate their performance in the internationalization competence, namely, flexibility in organization structure, competitiveness in the global environment, versatility in service contents, sophistication in information technology application, and compliance in administrative regulations. The model successfully provides a transparent and systematic evaluation tool for industries to select appropriate logistic companies for international logistics services.
C45|Evaluating the combined forecasts of the dynamic factor model and the artificial neural network model using linear and nonlinear combining methods|Abstract The paper evaluates the advantages of combined forecasts from the dynamic factor model (DFM) and the artificial neural networks (ANN). The analysis was based on three financial variables namely the Johannesburg Stock Exchange Return Index, Government Bond Return Index and the Rand/Dollar Exchange Rate in South Africa. The forecasts were based on the out-of-sample period from January 2006 to December 2011. Compared to benchmark autoregressive (AR) models, both the DFM and ANN offer more accurate forecasts with reduced root-mean-square error (RMSE) of around 2–12 % for all variables and over all forecasting horizons. The ANN as a nonlinear combining method outperforms all linear combining methods for all variables and over all forecasting horizons. The results suggest that the ANN combining method can be used as an alternative to linear combining methods to achieve greater forecasting accuracy. The ANN combining method produces out-of-sample forecasts that are substantially more accurate with a sizeable reduction in RMSE of both the AR benchmark model and the best individual forecasting model. We attribute the superiority of the ANN combining method to its ability to capture any existing nonlinear relationship between the individual forecasts and the actual forecasting values.
C45|Modelling cross-dependencies between Spain’s regional tourism markets with an extension of the Gaussian process regression model|Abstract This study presents an extension of the Gaussian process regression model for multiple-input multiple-output forecasting. This approach allows modelling the cross-dependencies between a given set of input variables and generating a vectorial prediction. Making use of the existing correlations in international tourism demand to all seventeen regions of Spain, the performance of the proposed model is assessed in a multiple-step-ahead forecasting comparison. The results of the experiment in a multivariate setting show that the Gaussian process regression model significantly improves the forecasting accuracy of a multi-layer perceptron neural network used as a benchmark. The results reveal that incorporating the connections between different markets in the modelling process may prove very useful to refine predictions at a regional level.
C45|Testing the Constancy of Conditional Correlations in Multivariate GARCH-type Models (Extended Version with Appendix)|We introduce two multivariate constant conditional correlation tests that require little knowledge of the functional relationship determining the conditional correlations. The first test is based on artificial neural networks and the second one is based on a Taylor expansion of each unknown conditional correlation. These new tests can be seen as general misspecification tests of a large set of multivariate GARCH-type models. We investigate the size and the power of these tests through Monte Carlo experiments. Moreover, we study their robustness to non-normality by simulating some models such as the GARCH−t and Beta−t−EGARCH models. We give some illustrative empirical examples based on financial data.
C45|Credit Risk Prediction: A Comparative Study between Discriminant Analysis and the Neural Network Approach|Banks are concerned with the assessment of the risk of financial distress before giving out a loan. Many researchers proposed the use of models based on the Neural Networks in order to help the banker better make a decision. The objective of this paper is to explore a new practical way based on the Neural Networks that would help the banker to predict the non payment risk the companies asking for a loan. This work is motivated by the insufficiency of traditional prevision models. The sample consists of 86 Tunisian companies and 15 financial ratios were calculated, over the period from 2005 to 2007. The results were compared with those of discriminant analysis. They show that the neural networks technique is more accurate in term of predictability.
C45|Bank Credit Risk Analysis with K-Nearest-Neighbor Classifier: Case of Tunisian Banks|Credit risk is defined as the risk that borrowers will fail to pay its loan obligations. In recent years, a large number of banks have developed sophisticated systems and models to help bankers in quantifying, aggregating and managing risk. The outputs of these models also play increasingly important roles in banks’ risk management and performance measurement processes. In this study we try to tackle the question of default prediction of short term loans for a Tunisian commercial bank. We use a database of 924 credit records of Tunisian firms granted by a Tunisian commercial bank from 2003 to 2006. The K-Nearest Neighbor classifier algorithm was conducted and the results indicate that the best information set is relating to accrual and cash-flow and the good classification rate is in order of 88.63 % (for k=3). A curve ROC is plotted to assess the performance of the model. The result shows that the AUC (Area Under Curve) criterion is in order of 87.4% (for the first model), 95% (third model) and 95.6% for the best model with cash flow information.
C45|“Multiple-input multiple-output vs. single-input single-output neural network forecasting”|This study attempts to improve the forecasting accuracy of tourism demand by using the existing common trends in tourist arrivals form all visitor markets to a specific destination in a multiple-input multiple-output (MIMO) structure. While most tourism forecasting research focuses on univariate methods, we compare the performance of three different Artificial Neural Networks in a multivariate setting that takes into account the correlations in the evolution of inbound international tourism demand to Catalonia (Spain). We find that the MIMO approach does not outperform the forecasting accuracy of the networks when applied country by country, but it significantly improves the forecasting performance for total tourist arrivals. When comparing the forecast accuracy of the different models, we find that radial basis function networks outperform multilayer-perceptron and Elman networks.
C45|“Effects of removing the trend and the seasonal component on the forecasting performance of artificial neural network techniques”|This study aims to analyze the effects of data pre-processing on the performance of forecasting based on neural network models. We use three different Artificial Neural Networks techniques to forecast tourist demand: a multi-layer perceptron, a radial basis function and an Elman neural network. The structure of the networks is based on a multiple-input multiple-output setting (i.e. all countries are forecasted simultaneously). We use official statistical data of inbound international tourism demand to Catalonia (Spain) and compare the forecasting accuracy of four processing methods for the input vector of the networks: levels, growth rates, seasonally adjusted levels and seasonally adjusted growth rates. When comparing the forecasting accuracy of the different inputs for each visitor market and for different forecasting horizons, we obtain significantly better forecasts with levels than with growth rates. We also find that seasonally adjusted series significantly improve the forecasting performance of the networks, which hints at the significance of deseasonalizing the time series when using neural networks with forecasting purposes. These results reveal that, when using seasonal data, neural networks performance can be significantly improved by working directly with seasonally adjusted levels.
C45|“Regional Forecasting with Support Vector Regressions: The Case of Spain”|This study attempts to assess the forecasting accuracy of Support Vector Regression (SVR) with regard to other Artificial Intelligence techniques based on statistical learning. We use two different neural networks and three SVR models that differ by the type of kernel used. We focus on international tourism demand to all seventeen regions of Spain. The SVR with a Gaussian kernel shows the best forecasting performance. The best predictions are obtained for longer forecast horizons, which suggest the suitability of machine learning techniques for medium and long term forecasting.
C45|“Self-organizing map analysis of agents' expectations. Different patterns of anticipation of the 2008 financial crisis”|By means of Self-Organizing Maps we cluster fourteen European countries according to the most suitable way to model their agents’ expectations. Using the financial crisis of 2008 as a benchmark, we distinguish between those countries that show a progressive anticipation of the crisis and those where sudden changes in expectations occur. By mapping the trajectory of economic experts’ expectations prior to the recession we find that when there are brisk changes in expectations before impending shocks, Artificial Neural Networks are more suitable than time series models for modelling expectations. Conversely, in countries where expectations show a smooth transition towards recession, ARIMA models show the best forecasting performance. This result demonstrates the usefulness of clustering techniques for selecting the most appropriate method to model and forecast expectations according to their behaviour.
C45|A Comparative Analysis Of Serbia And The Eu Member States In The Context Of The Networked Readiness Index Values|Nowadays it is generally accepted that information and communication technologies (ICT) are important drivers and ‘enabling’ technologies that have a broad impact on many sectors of the economy and social life. Therefore, measuring the level of ICT development, their economic and social impact, and the country’s readiness to use them are of great importance. In this paper we present the conceptual framework of the Networked Readiness Index (NRI) proposed by the World Economic Forum, and analyse the relative position of Serbia and its ‘distance’ from the EU member states in the domain of NRI indicator variables. For this purpose we have applied the Kohonen Self-Organizing Map (the SOM algorithm), which provides the visual image, as a virtual map, of observed countries and their groupings. The resulting SOM map indicates that in the complex NRI space, Serbia is located in a group of EU states that includes Romania, Croatia, Bulgaria, Cyprus, Greece, Italy, Poland, the Czech Republic, and the Slovak Republic. In comparison to other countries, this group shows the poorest performance in the NRI landscape. In addition, our empirical analysis points to the areas in which policy intervention can boost the impact of ICT on Serbian economic development and growth.
C45|Capital and contagion in financial networks|We implement a novel method to detect systemically important financial institutions in a network. The method consists in a simple model of distress and losses redistribution derived from the interaction of banks' balance-sheets through bilateral exposures. The algorithm goes beyond the traditional default-cascade mechanism, according to which contagion propagates only through banks that actually default. We argue that even in the absence of other defaults, distressed-but-non-defaulting institutions transmit the contagion through channels other than solvency: weakness in their balance sheet reduces the value of their liabilities, thereby negatively affecting their interbank lenders even before a credit event occurs. In this paper, we apply the methodology to a unique dataset covering bilateral exposures among all Italian banks in the period 2008-2012. We find that the systemic impact of individual banks has decreased over time since 2008. The result can be traced back to decreasing volumes in the interbank market and to an intense recapitalization process. We show that the marginal effect of a bank's capital on its contribution to systemic risk in the network is considerably larger when interconnectedness is high (good times): this finding supports the regulatory work on counter-cyclical (macroprudential) capital buffers.<br><small>(This abstract was borrowed from another version of this item.)</small>
C45|Data Mining as Support to Knowledge Management in Marketing|Background: Previous research has shown success of data mining methods in marketing. However, their integration in a knowledge management system is still not investigated enough.
C45|Formation of Migrant Networks| In this paper, we provide the first direct evidence on the internal structure of the migrant social network. By using a purposely designed survey on Sri Lankan immigrants living in Milan, we show that the pattern of within-group link formation is heterogeneous across immigrants, and differentiated according to the network function (i.e., accommodation, credit, job-finding). We find that migrants tend to interact with co-nationals who come from nearby localities at origin, while the time of arrival has a U-shaped effect. Once the link is formed, material support is provided mainly to relatives, while early migrant fellows are helpful for job-finding.
C45|Text mining for central banks|Although often applied in other social sciences, text mining has been less frequently used in economics and in policy circles, particularly inside central banks. This Handbook is a brief introduction to the field, discussing how text mining is useful for addressing research topics of interest to central banks, and providing a step-by-step primer on how to mine text, including an overview of unsupervised and supervised techniques.
C45|A Support Vector Machine Approach For Developing Telemedicine Solutions: Medical Diagnosis|Support vector machine represents an important tool for artificial neural networks techniques including classification and prediction. It offers a solution for a wide range of different issues in which cases the traditional optimization algorithms and methods cannot be applied directly due to different constraints, including memory restrictions, hidden relationships between variables, very high volume of computations that needs to be handled. One of these issues relates to medical diagnosis, a subset of the medical field. In this paper, the SVM learning algorithm is tested on a diabetes dataset and the results obtained for training with different kernel functions are presented and analyzed in order to determine a good approach from a telemedicine perspective.
C45|Network-based Measures as Leading Indicators of Market Instability: The case of the Spanish Stock|This paper studies the undirected partial-correlation stock network for the Spanish market that considers the constituents of IBEX-35 as nodes and their partial correlations of returns as links. I propose a novel methodology that combines a recently developed variable selection method, Graphical Lasso, with Monte Carlo simulations as fundamental ingredients for the estimation recipe. Three major results come from this study. First, in topological terms, the network shows features that are not consistent with random arrangements and it also presents a high level of stability over time. International comparison between major European stock markets extends that conclusion beyond the Spanish context. Second, the systemic importance of the banking sector, relative to the other sectors in the economy, is quantitatively uncovered by means of its network centrality. Particularly interesting is the case of the two major banks that occupy the places of the most systemic players. Finally, the empirical evidence indicates that some network-based measures are leading indicators of distress for the Spanish stock market.
C45|Concrete strength control charts pattern recognition based on Linear Vector Quantization neural networks|The objective in this study is to detect the errors that occur or may occur in the future during the process in which the company’s quality objectives are fulfilled and to show the applicability of the Artificial Neural Networks (ANN) which is one of the Artificial Intelligence (AI) techniques. Thus, it will be able to contribute to the main purposes which make quality control necessary such as to raise the level of quality, reduce operating costs, time savings, raising employees’ motivation and reducing customer complaints. For this purpose, average compressive strength, one of the most important quality indicators, of a company that produces ready-mixed concrete has been used. Linear Vector Quantization (LVQ) type ANN model has been established by using the quality characteristics observation values related to control charts and the parameters related to control charts, and when these two models are compared, it has been found out that the model whose quality characteristics have been constructed using the observation values result in more successful results than that constructed with the model's control charts.
C45|Classification models via Tabu search: An application to early stage venture classification|We model the decision making process used by Experts at the Canadian Innovation Centre to classify early stage venture proposals based on potential commercial success. The decision is based on thirty-seven attributes that take values in {-1,0,1}{-1,0,1}. We adopt a conjunctive decision framework due to Åstebro and Elhedhli (2005) that selects a subset of attributes and determines two threshold values: one for the maximum allowed negatives (n) and one for minimum required positives (p). A proposal is classified as a success if the number of positives is greater than or equal to p and the number of negatives is less than or equal to n over the selected attributes. Based on a data set of 561 observations, the selection of attributes and the determination of the threshold values is modeled as a large-scale mixed integer program. Two solution approaches are explored: Benders decomposition and Tabu search. The first, was very slow to converge, while the second provided high quality solutions quickly. Tabu search provides excellent classification accuracy for predicting commercial successes as well as replicating Experts' forecasts, opening the venue for the use of Tabu search in scoring and classification problems.
C45|Heuristic learning in intraday trading under uncertainty|Until recently economists focused on structural models that were constrained by a lack of high-frequency data and theoretical deficiencies. Little academic research has been invested in actually trying to build successful real-time trading models for the high-frequency foreign exchange market, which is characterized by inherent complexity and heterogeneity. The present work opens new directions for inference on market efficiency in an attempt to account for the use of technical analysis by practitioners over many years now. This paper presents a heuristic model that efficiently emulates the dynamic learning of intraday traders. The proposed setup incorporates agent beliefs, preferences and expectations while it integrates the calibration of technical rules by means of adaptive training. The study focuses on EUR/USD which is the most liquid and widely traded currency pair. The data consist of a very large tick-by-tick sample of bid and ask prices covering many trading periods to enhance robustness in the results. The efficiency of a technical trading strategy based on the proposed model is investigated in terms of directional predictability. The heuristic learning system is compared against many non-linear models, a random walk and a buy & hold strategy. Based on statistical testing it is shown that, with the inclusion of transaction costs, the profitability of the new model is consistently superior. These findings provide evidence of technical predictability under incomplete information and can be justified by invoking the existence of heterogeneity caused by many factors affecting market microstructure. Overall, the results suggest that the proposed model can be used to improve upon traditional technical analysis approaches.
C45|Forecasting short-term electricity consumption using a semantics-based genetic programming framework: The South Italy case|Accurate and robust short-term load forecasting plays a significant role in electric power operations. This paper proposes a variant of genetic programming, improved by incorporating semantic awareness in algorithm, to address a short term load forecasting problem. The objective is to automatically generate models that could effectively and reliably predict energy consumption. The presented results, obtained considering a particularly interesting case of the South Italy area, show that the proposed approach outperforms state of the art methods. Hence, the proposed approach reveals appropriate for the problem of forecasting electricity consumption. This study, besides providing an important contribution to the energy load forecasting, confirms the suitability of genetic programming improved with semantic methods in addressing complex real-life applications.
C45|Forecasting the COMEX copper spot price by means of neural networks and ARIMA models|This paper examines the forecasting performance of ARIMA and two different kinds of artificial neural networks models (multilayer perceptron and Elman) using published data of copper spot prices from the New York Commodity Exchange, (COMEX). The empirical results obtained showed a better performance of both neural networks models over the ARIMA. The findings of this research are in line with some previous studies, which confirmed the superiority of neural networks over ARIMA models in relative research areas.
C45|The predictive accuracy of Sukuk ratings; Multinomial Logistic and Neural Network inferences|The development of Sukuk market as the alternative to the existing conventional bond market has risen the issue of rating the Sukuk issuance. These credit ratings fulfill a key function of information transmission in capital market. Moreover, Basel Committee for Banking Supervision has now instituted capital charges for credit risk based on credit ratings. Basel II framework allowed the bank to establish capital adequacy requirements based on ratings provided by external credit rating agencies or determine rating of its investment internally for more advance approach. For these reasons, ratings are considered important by issuers, investors, and regulators alike. This study provides an empirical foundation for the investors to estimate the ratings assigned using the approach from several rating agencies and past researches on bond ratings. It tries to compare the accuracy of two logistic models; Multinomial Logistic Regression and Neural Network to create a model of rating probability from several financial variables.
C45|Islamic versus conventional banks in the GCC countries: A comparative study using classification techniques|This paper contributes to the empirical literature on Islamic finance by investigating the feature of Islamic and conventional banks in Gulf Cooperation Council (GCC) countries over the period 2003–2010. We use parametric and non-parametric classification models (Linear discriminant analysis, Logistic regression, Tree of classification and Neural network) to examine whether financial ratios can be used to distinguish between Islamic and conventional banks. Univariate results show that Islamic banks are, on average, more profitable, more liquid, better capitalized, and have lower credit risk than conventional banks. We also find that Islamic banks are, on average, less involved in off-balance sheet activities and have more operating leverage than their conventional peers. Results from classification models show that the two types of banks may be differentiated in terms of credit and insolvency risk, operating leverage and off-balance sheet activities, but not in terms of profitability and liquidity. More interestingly, we find that the recent global financial crisis has a negative impact on the profitability for both Islamic and conventional banks, but time shifted. Finally, results show that Logit regression obtained slightly higher classification accuracies than other models.
C45|Nonlinear Time Series and Neural-Network Models of Exchange Rates between the US Dollar and Major Currencies|This paper features an analysis of major currency exchange rate movements in relation to the US dollar, as constituted in US dollar terms. Euro, British pound, Chinese yuan, and Japanese yen are modelled using a variety of non-linear models, including smooth transition regression models, logistic smooth transition regressions models, threshold autoregressive models, nonlinear autoregressive models, and additive nonlinear autoregressive models, plus Neural Network models. The models are evaluated on the basis of error metrics for twenty day out-of-sample forecasts using the mean average percentage errors (MAPE). The results suggest that there is no dominating class of time series models, and the different currency pairs relationships with the US dollar are captured best by neural net regression models, over the ten year sample of daily exchange rate returns data, from August 2005 to August 2015.
C45|Systemic Sovereign Risk and Asset Prices: Evidence from the CDS Market, Stressed European Economies and Nonlinear Causality Tests|The use of non-parametric methodologies, the introduction of non-financial variables, and the development of models geared towards the homogeneous characteristics of corporate sub-populations have recently experienced a surge of interest in the bankruptcy literature. However, no research on default prediction has yet focused on micro-entities (MEs), despite such firms’ importance in the global economy. This paper builds the first bankruptcy model especially designed for MEs by using a wide set of accounts from 1999 to 2008 and applying artificial neural networks (ANNs). Our findings show that ANNs outperform the traditional logistic regression (LR) models. In addition, we also report that, thanks to the introduction of non-financial predictors related to age, the delay in filing accounts, legal action by creditors to recover unpaid debts, and the ownership features of the company, the improvement with respect to the use of solely financial information is 3.6%, which is even higher than the improvement that involves the use of the best ANN (2.6%).
C45|How Do Different Time Spans Affect The Prediction Accuracy Of Business Failure?|The prediction of business failure has been widely studied by many authors. Most of the studies focused on improve the results by applying new methodologies or by using more suitable financial information. This study aims to analyze the impact of the input data timeframe on the prediction accuracy of business failure. Using an artificial neural network, the self-organizing maps (SOM), we compare the results obtained by using 9, 6 and 3 years of input data. We concluded that the 3-year case provides a better global results despite of the 6-year case presents the lowest error type I.
C45|Testing the Constancy of Conditional Correlations in Multivariate GARCH-type Models (Extended Version with Appendix)|We introduce two multivariate constant conditional correlation tests that require little knowledge of the functional relationship determining the conditional correlations. The first test is based on artificial neural networks and the second one is based on a Taylor expansion of each unknown conditional correlation. These new tests can be seen as general misspecification tests of a large set of multivariate GARCH-type models. We investigate the size and the power of these tests through Monte Carlo experiments. Moreover, we study their robustness to non-normality by simulating some models such as the GARCH?t and Beta?t?EGARCH models. We give some illustrative empirical examples based on financial data.
C45|A Power Market Forward Curve with Hydrology Dependence An Approach based on Artificial Neural Networks|This paper develops an hourly forward curve for power markets where the intra-day and intra-week shapes (profiles) depend on the level of the hydrological balance. The shaping model is based on a feed-forward Artificial Neural Network (ANN), which is trained on a historical data set of hourly electricity spot prices from the Nord Pool market and weekly measurements of the Nordic hydrological balance. The yearly seasonal cycle is estimated with historical electricity forward prices from the Nasdaq OMX Commodities exchange. We calibrate the shaping model to prevailing electricity forward prices and proceed to demonstrate its most important properties. By using comparative static analysis we particulary focus on the hydro dependence of the shapes. We conclude the paper with a real world valuation task. By combining our proposed forward curve with a simple Ornstein-Uhlenbeck process we price a strip of hourly call options on the electricity spot price under different hydrological scenarios.
C45|Approximating Innovation Potential With Neurofuzzy Robust Model / Aproximación Al Potencial Innovador Con Un Modelo Robusto De Neuro-Fuzzy|In a remarkably short time, economic globalisation has changed the world’s economic order, bringing new challenges and opportunities to SMEs. These processes pushed the need to measure innovation capability, which has become a crucial issue for today’s economic and political decision makers. Companies cannot compete in this new environment unless they become more innovative and respond more effectively to consumers’ needs and preferences – as mentioned in the EU’s innovation strategy. Decision makers cannot make accurate and efficient decisions without knowing the capability for innovation of companies in a sector or a region. This need is forcing economists to develop an integrated, unified and complete method of measuring, approximating and even forecasting the innovation performance not only on a macro but also a micro level. In this recent article a critical analysis of the literature on innovation potential approximation and prediction is given, showing their weaknesses and a possible alternative that eliminates the limitations and disadvantages of classical measuring and predictive methods. / En un plazo increíblemente corto, la globalización económica ha cambiado el orden de la economía, creando nuevos retos y oportunidades a las pequeñas y medianas empresas. Por ello se esta dando la necesidad de crear maneras de medir capacidad de innovación que resulta fundamental para quien debe tomar decisiones politico-economicas. Las compañías no pueden competir en este nuevo entorno a no ser que sean mas innovadoras y respondan de manera más eficiente a las necesidades y preferencias del consumidor-como de hecho se ha mencionado en la Estrategia de Innovación de la UE. Las decisiones no pueden ser tomadas de manera eficiente y adecuada sin el conocimiento de la capacidad de innovación de compañías de un determinada región y/o sector. Esta necesidad está forzando a los economistas a desarrollar un método completo integrado y unificador de medir, aproximar e incluso predecir el rendimiento innovativo tanto a micro como a macro niveles. En este reciente articulo se ha hecho un análisis critico de la literatura que trata sobre aproximaciones y/o predicciones del potencial innovador, mostrando sus defectos y posibles alternativas que eliminarían las limitaciones y desventajas de las mediciones clásicas y métodos predictivos.
C45|Analyzing the impact of global financial crisis on the interconnectedness of Asian stock markets using network science|As importance of Asian Stock Markets (ASM) has increased after the globalization, it is become significant to know how this network of ASM behaves on the onset of financial crises. For this study, the Global Financial Crisis is considered whose origin was in the developed country, US, unlike the Asian crisis of 1997. To evaluate the impact of financial crisis on the ASM, network theory is used as a tool here. Network modeling of stock markets is useful as it can help to avert the spillover of crises by preventing the stock markets which are highly connected in the network. In this empirical work, weekly indices data from 2000-2013 for fifteen stock markets is used, which is further partitioned into three periods: pre, during and post crisis. This study shows how 13 important stock markets in Asia namely, India, Bangladesh, Philippines, China, Japan, Indonesia, Malaysia, Singapore, Hong Kong, Pakistan, South Korea and Thailand are connected to each other and how India, Japan, Hong Kong and Korea stock market appeared as the systemically important stock markets from them. Introduction of the US stock market into this network gives insight how the US stock market might had connected to systemically important markets which resulted into spread of crisis in the Asian region. Furthermore, using Kruskal algorithm spread of contagion is explained like how it first hit the Hong Kong stock market and from there it proceeds to the other systemic important stock markets like a virus. Addition to that, we quantified the network behavior in the form of metrics such as adjacency matrix, clustering coefficient, degree of nodes and Minimum Spanning Tree (MST), and on the basis of these some of the important questions like which stock markets are highly connected in Asia which if affected can induce the crises in the other stock markets of region are answered. This study can be used for the portfolio optimization as well as for policy making for which network analysis should be conducted on a regular basis.
C45|Detecting Treatment-Subgroup Interactions in Clustered Data with Generalized Linear Mixed-Effects Model Trees|Identification of subgroups of patients for which treatment A is more effective than treatment B, and vice versa, is of key importance to the development of personalized medicine. Several tree-based algorithms have been developed for the detection of such treatment-subgroup interactions. In many instances, however, datasets may have a clustered structure, where observations are clustered within, for example, research centers, studies or persons. In the current paper we propose a new algorithm, generalized linear mixed-effects model (GLMM) trees, that allows for detection of treatment-subgroup interactions, as well as estimation of cluster-specific random effects. The algorithm uses model-based recursive partitioning (MOB) to detect treatment-subgroup interactions, and a GLMM for the estimation of random-effects parameters. In a simulation study, we evaluate the performance of GLMM tree and compare it with that of MOB without random-effects estimation. GLMM tree was found to have a much lower Type I error rate than MOB trees without random effects (4% and 33%, respectively). Furthermore, in datasets with treatment-subgroup interactions, GLMM tree recovered the true treatment subgroups much more often than MOB without random effects (in 90% and 61% of the datasets, respectively). Also, GLMM tree predicted treatment outcome differences more accurately than MOB without random effects (average predictive accuracy of .94 and .88, respectively). We illustrate the application of GLMM tree on a patient-level dataset of a meta-analysis on the effects of psycho- and pharmacotherapy for depression. We conclude that GLMM tree is a promising algorithm for the detection of treatment-subgroup interactions in clustered datasets.
C45|“Multiple-input multiple-output vs. single-input single-output neural network forecasting”|This study attempts to improve the forecasting accuracy of tourism demand by using the existing common trends in tourist arrivals form all visitor markets to a specific destination in a multiple-input multiple-output (MIMO) structure. While most tourism forecasting research focuses on univariate methods, we compare the performance of three different Artificial Neural Networks in a multivariate setting that takes into account the correlations in the evolution of inbound international tourism demand to Catalonia (Spain). We find that the MIMO approach does not outperform the forecasting accuracy of the networks when applied country by country, but it significantly improves the forecasting performance for total tourist arrivals. When comparing the forecast accuracy of the different models, we find that radial basis function networks outperform multilayer-perceptron and Elman networks.
C45|“Effects of removing the trend and the seasonal component on the forecasting performance of artificial neural network techniques”|This study aims to analyze the effects of data pre-processing on the performance of forecasting based on neural network models. We use three different Artificial Neural Networks techniques to forecast tourist demand: a multi-layer perceptron, a radial basis function and an Elman neural network. The structure of the networks is based on a multiple-input multiple-output setting (i.e. all countries are forecasted simultaneously). We use official statistical data of inbound international tourism demand to Catalonia (Spain) and compare the forecasting accuracy of four processing methods for the input vector of the networks: levels, growth rates, seasonally adjusted levels and seasonally adjusted growth rates. When comparing the forecasting accuracy of the different inputs for each visitor market and for different forecasting horizons, we obtain significantly better forecasts with levels than with growth rates. We also find that seasonally adjusted series significantly improve the forecasting performance of the networks, which hints at the significance of deseasonalizing the time series when using neural networks with forecasting purposes. These results reveal that, when using seasonal data, neural networks performance can be significantly improved by working directly with seasonally adjusted levels.
C45|“Regional Forecasting with Support Vector Regressions: The Case of Spain”|This study attempts to assess the forecasting accuracy of Support Vector Regression (SVR) with regard to other Artificial Intelligence techniques based on statistical learning. We use two different neural networks and three SVR models that differ by the type of kernel used. We focus on international tourism demand to all seventeen regions of Spain. The SVR with a Gaussian kernel shows the best forecasting performance. The best predictions are obtained for longer forecast horizons, which suggest the suitability of machine learning techniques for medium and long term forecasting.
C45|“Self-organizing map analysis of agents’ expectations. Different patterns of anticipation of the 2008 financial crisis”|By means of Self-Organizing Maps we cluster fourteen European countries according to the most suitable way to model their agents’ expectations. Using the financial crisis of 2008 as a benchmark, we distinguish between those countries that show a progressive anticipation of the crisis and those where sudden changes in expectations occur. By mapping the trajectory of economic experts’ expectations prior to the recession we find that when there are brisk changes in expectations before impending shocks, Artificial Neural Networks are more suitable than time series models for modelling expectations. Conversely, in countries where expectations show a smooth transition towards recession, ARIMA models show the best forecasting performance. This result demonstrates the usefulness of clustering techniques for selecting the most appropriate method to model and forecast expectations according to their behaviour.
C45|Geospatial and machine learning techniques for wicked social science problems: analysis of crash severity on a regional highway corridor|The contention of this paper is that many social science research problems are too “wicked” to be suitably studied using conventional statistical and regression-based methods of data analysis. This paper argues that an integrated geospatial approach based on methods of machine learning is well suited to this purpose. Recognizing the intrinsic wickedness of traffic safety issues, such approach is used to unravel the complexity of traffic crash severity on highway corridors as an example of such problems. The support vector machine (SVM) and coactive neuro-fuzzy inference system (CANFIS) algorithms are tested as inferential engines to predict crash severity and uncover spatial and non-spatial factors that systematically relate to crash severity, while a sensitivity analysis is conducted to determine the relative influence of crash severity factors. Different specifications of the two methods are implemented, trained, and evaluated against crash events recorded over a 4-year period on a regional highway corridor in Northern Iran. Overall, the SVM model outperforms CANFIS by a notable margin. The combined use of spatial analysis and artificial intelligence is effective at identifying leading factors of crash severity, while explicitly accounting for spatial dependence and spatial heterogeneity effects. Thanks to the demonstrated effectiveness of a sensitivity analysis, this approach produces comprehensive results that are consistent with existing traffic safety theories and supports the prioritization of effective safety measures that are geographically targeted and behaviorally sound on regional highway corridors. Copyright Springer-Verlag Berlin Heidelberg 2015
C45|Is There an Arms Race Between Pakistan and India? An Application of GMM|This study employs the Richardson model to investigate the presence of an arms race between Pakistan and India during the period 1972–2010. Using the generalized method of moments approach, we find that the grievance term for the Pakistan model is positive while that for India is negative. Both countries’ defense spending in the previous period is negatively related to the change in their own defense spending due to the economic or administrative incidence of an arms race. Moreover, the defense or reaction coefficients in the specified model determine the presence of an arms race between the two countries. The signs of these coefficients are positive in accord with the classical Richardson model, suggesting that an arms race does indeed exist between Pakistan and India.
C45|Topology of the foreign currency/forint swap market|In our study, we examine the network structure of the currency swap market, the volume of which amounts to several times the Hungarian GDP. With this paper, we aim to establish a complete picture of the market by complementing the results for the overnight market. Additionally, we can now analyse a longer time series than in our 2013 study. We look at the properties of the graphs in segments representing various maturities. We find that the properties of the graph derived from the overall market and the dynamics of those properties are identical to those of the short-term market, while trends differ for various tenors. The longer the maturities, the less the graphs satisfy the small-world property. The longest markets are increasingly closer to random graphs. Although the effect of shocks to large actors is smaller in such graphs, this change also suggests that counterparties trusted each other less as transactions became longer. This is also reinforced by the fact that following the onset of the crisis, the number of connected vertices gradually decreased in the networks of longer markets. In other words, weakening trust is also manifested in the decreasing number of counterparties. This is confirmed by the development of average degree and average path length, and by affinity functions.
C45|Analyzing Multiday Route Choice Behavior using GPS Data|Understanding variability in daily behavior is one of the most important missions in travel behavior modeling. In traditional method, in order to find the differences, respondents were asked to list the used multiday paths. The quality of results is sensitive to the accuracy of respondentsâ€™ memories. However, few empirical studies of revealed route characteristics, chosen by the travelers day-to-day, have been reported in the literature. In this study, accurate Global Position Systems (GPS) and Geographic Information System (GIS) data were employed to reveal multiday routes people used, to study multiday route choice behavior for the same origin-destination (OD) trips. Travelers are classified into three kinds based on their route types. A two-stage route choice process is proposed. After analyzing the characteristics of different types of travelers, a neural network was adopted to classify travelers and model route choice behavior. An empirical study using GPS data collected in Minneapolis-St. Paul metropolitan area was carried out in the following part. It finds that most travelers follow the same route during commute trips on successive days. The results indicate that neural network framework can classify travelers and model route choice well.
C45|Homo Economicus and Homo Sapiens|The assumption that individuals are behaving rationally can, at times, usefully constrain predictions of individual and collective behavior. However, success in predicting human and group behavior will often require relaxing this assumption of rationality, instead employing evolutionary, neural, and cognitive constraints. One particularly important form of neural and cognitive constraint is that interacting individuals each possess a network of concepts, and communities are accordingly social networks of neural networks. The structured nature of human conceptual systems suggests that communicating is better modeled as a process of aligning conceptual systems rather than simply transmitting atomic beliefs. Communicating individuals can establish norms, conceptual structures, and rule systems that did not preexist prior to the communication process. For this reason, the dichotomy between rule-based and centralized groups versus self-organized and decentralized groups is false â€“ one of the major activities that self-organized and decentralized groups engage in is the establishment of rules, laws, norms, leaders, and institutional hierarchies that will then govern their subsequent interactions.
C45|Security Assessment And Optimization Of Energy Supply (Neural Networks Approach)|The question of energy supply continuity is essential from the perspective of the functioning of society and the economy today. The study describes modern methods of forecasting emergency situations using Artificial Intelligence (AI) tools, especially neural networks. It examines the structure of a properly functioning model in the areas of input data selection, network topology and learning algorithms, analyzes the functioning of an energy market built on the basis of a reserve market, and discusses the possibilities of economic optimization of such a model, including the question of safety.
C45|Measuring the Core Inflation in Turkey with the SM-AR Model|This paper employs a new econometric technique to estimate the core inflation in Turkey measured as the shifting means in levels between 1955 and 2014. Using monthly series, we determine the number of shifts using the BIC, the hv-block cross-validation, the Lin-Teräsvirta parameter constancy test, and the neural networks test for neglected non-linearity. We find that there are at least three shifts in the inflation series. The findings help detect the exact dates of the shifts between different inflation regimes and the duration of each shift, which should be important information in evaluating the success of past economic policies in fighting inflation.
C45|Estimation and prediction of an Index of Financial Safety of Tunisia|This paper analyses the strength of the financial system of Tunisia through the construction of an Index of Financial Safety (IFS). Over the period 2000Q1 – 2014Q3, the IFS is built using a wide range of financial and macroeconomic indicators. The empirical results show that it can capture the disturbances in Tunisian financial system with sufficient accuracy. The nonlinear autoregressive with exogenous input (NARX) model with Levenberg-Marquardt algorithm of training was selected to forecast changes in IFS, and provides significant results.
C45|Forecasting the US CPI: Does Nonlinearity Matter?|The objective of this paper is to predict, both in-sample and out-of-sample, the consumer price index (CPI) of the United States (US) economy based on monthly data covering the period of 1980:1-2013:12, using a variety of linear (random walk (RW), autoregressive (AR) and seasonally-adjusted autoregressive moving average (SARIMA)) and nonlinear (artificial neural network (ANN) and genetic programming (GP)) univariate models. Our results show that, while the SARIMA model is superior relative to other linear and nonlinear models, as it tends to produce smaller forecast errors; statistically, these forecasting gains are not significant relative to higher-order AR and nonlinear models, though simple benchmarks like the RW and AR(1) models are statistically outperformed. Overall, we show that in terms of forecasting the US CPI, accounting for nonlinearity does not necessarily provide us with any statistical gains.
C45|Quantitative Easing and the U.S. Stock Market: A Decision Tree Analysis|"""The Financial Crisis of 2007-09 caused the U.S. economy to experience a relatively long recession from December 2007 to June 2009. Both the U.S. government and the Federal Reserve undertook expansive fiscal and monetary policies to minimize both the severity and length of the recession. Most notably, the Federal Reserve initiated three rounds of unconventional monetary policies known as Quantitative Easing. These policies were intended to reduce long-term interest rates when the short term federal funds rates had reached the zero lower bound and could not become negative. It was argued that the lowering of longer-term interest rates would help the stock market and thus the wealth of consumers. This paper investigates this hypothesis and concludes that quantitative easing has contributed to the observed increases in the stock market’s significant recovery since its crash due to the financial crisis."""
C45|Out-Of-Sample Forecasting Performance Of A Robust Neural Exchange Rate Model Of Ron/Usd|This paper aims to explore the forecasting accuracy of RON/USD exchange rate structural models with monetary fundamentals. I used robust regression approach for constructing robust neural models less sensitive to contamination with outliers and I studied its predictability on 1 to 6-month horizon against nonrobust linear and nonlinear regressions and, especially, random walk. The results show that robust model with low breakdown point improve the forecast accuracy of RW and AR models on 1- and 4-month horizon and performs better than RW at all time horizons.
C45|On the trail of core–periphery patterns in innovation networks: measurements and new empirical findings from the German laser industry|It has been frequently argued that a firm’s location in the core of an industry’s innovation network improves its ability to access information and absorb technological knowledge. The literature has still widely neglected the role of peripheral network positions for innovation processes. In addition to this, little is known about the determinants affecting a peripheral actors’ ability to reach the core. To shed some light on these issues, we have employed a unique longitudinal dataset encompassing the entire population of German laser source manufacturers (LSMs) and laser-related public research organizations (PROs) over a period of more than two decades. The aim of our paper is threefold. First, we analyze the emergence of core–periphery (CP) patterns in the German laser industry. Then, we explore the paths on which LSMs and PROs move from isolated positions toward the core. Finally, we employ non-parametric event history techniques to analyze the extent to which organizational and geographical determinates affect the propensity and timing of network core entries. Our results indicate the emergence and solidification of CP patterns at the overall network level. We also found that the paths on which organizations traverse through the network are characterized by high levels of heterogeneity and volatility. The transition from peripheral to core positions is impacted by organizational characteristics, while an organization’s geographical location does not play a significant role. Copyright Springer-Verlag Berlin Heidelberg 2015
C45|Recurrent support vector regression for a non-linear ARMA model with applications to forecasting financial returns|Motivated by recurrent neural networks, this paper proposes a recurrent support vector regression (SVR) procedure to forecast nonlinear ARMA model based simulated data and real data of financial returns. The forecasting ability of the recurrent SVR based ARMA model is compared with five competing models (random walk, threshold ARMA model, MLE based ARMA model, recurrent artificial neural network based ARMA model and feed-forward SVR based ARMA model) by using two forecasting accuracy evaluation metrics (NSME and sign) and robust Diebold–Mariano test. The results reveal that for one-step-ahead forecasting, the recurrent SVR model is consistently better than the benchmark models in forecasting both the magnitude and turning points, and statistically improves the forecasting performance as opposed to the usual feed-forward SVR. Copyright Springer-Verlag Berlin Heidelberg 2015
C45|Is forecasting inflation easier under inflation targeting?|This paper investigates whether monetary-policy regime changes affect the success of forecasting inflation. The forecasting performances of some linear and nonlinear univariate models are analyzed for 14 different countries that have adopted inflation-targeting (IT) monetary regimes at some point in their economic history. The results show that forecasting performance is generally superior under an IT monetary regime compared to nonIT (NIT) periods. In more than half of the countries covered in this study, superior forecasting accuracy can be achieved in IT periods regardless of the model used. In contrast, among most of the remaining countries, the results remain ambiguous, and the evidence on the superiority of NIT is limited to very few countries. Copyright Springer-Verlag Berlin Heidelberg 2015
C45|Predicting Banking Crises with Artificial Neural Networks: The Role of Nonlinearity and Heterogeneity|Studies of the early warning systems (EWSs) for banking crises usually rely on linear classifiers, estimated with international datasets. I construct an EWS based on an artificial neural network (ANN) model, and I also account for regional heterogeneity in order to improve the generalization ability of EWS models. All of the banking crises in my test set are then predictable at a 24‐month horizon, using information from earlier crises. For some countries, estimation with a regional dataset significantly improves the predictions. The ANN outperforms the usual logit regression, assessed by the area under the receiver operating characteristics curve.
C45|Isolation and Innovation – Two Contradictory Concepts? Explorative Findings from the German Laser Industry|We apply a network perspective and study the emergence of core-periphery (CP) structures in innovation networks to shed some light on the relationship between isolation and innovation. It has been frequently argued that a firm's location in a densely interconnected network area improves its ability to access information and absorb technological knowledge. This, in turn, enables a firm to generate new products and services at a higher rate compared to less integrated competitors. However, the importance of peripheral positions for innovation processes is still a widely neglected issue in literature. Isolation may provide unique conditions that induce innovations which otherwise may never have been invented. Such innovations have the potential to lay the ground for a firm's pathway towards the network core, where the industry's established technological knowledge is assumed to be located. The aim of our paper is twofold. Firstly, we propose a new CP indicator and apply it to analyze the emergence of CP patterns in the German laser industry. We employ publicly funded Research and Development (R&D) cooperation project data over a period of more than two decades. Secondly, we explore the paths on which firms move from isolated positions towards the core (and vice versa). Our exploratory results open up a number of new research questions at the intersection between geography, economics and network research.
C45|A patent search strategy based on machine learning for the emerging field of service robotics|Emerging technologies are in the core focus of supra-national innovation policies. These strongly rely on credible data bases for being effective and efficient. However, since emerging technologies are not yet part of any official industry, patent or trademark classification systems, delineating boundaries to measure their early development stage is a nontrivial task. This paper is aimed to present a methodology to automatically classify patents as concerning service robots. We introduce a synergy of a traditional technology identification process, namely keyword extraction and verification by an expert community, with a machine learning algorithm. The result is a novel possibility to allocate patents which (1) reduces expert bias regarding vested interests on lexical query methods, (2) avoids problems with citational approaches, and (3) facilitates evolutionary changes. Based upon a small core set of worldwide service robotics patent applications we derive apt n-gram frequency vectors and train a support vector machine (SVM), relying only on titles, abstracts and IPC categorization of each document. Altering the utilized Kernel functions and respective parameters we reach a recall level of 83% and precision level of 85%.
C45|Emergence of networks and market institutions in a large virtual economy|A complete set of transactions, more than 40 million within a 1.8 year span, allows us to track the evolution of the trader network and the goods network in an on-line trading community. The computer platform was designed to make barter exchange as attractive as possible; money was not part of the design and all players were created equal. Yet, within weeks, several specific goods began to emerge as media of exchange, and not long after that various sorts of specialized traders began to appear. We track their progress using network-theoretic metrics such as node strength, assortativity, betweenness and closeness. By the end of our sample, virtually all trade was money-mediated and market makers played a major role.
C45|Determinación del riesgo de fracaso financiero mediante la utilización de modelos paramétricos, de inteligencia artificial, y de información de auditoría|En este artículo aportamos evidencia empírica de predicción del fallo financiero en empresas no financieras. Hemos desarrollado diversos modelos para la evaluación del riesgo de fallo financiero en PYME. Contrastada la capacidad predictiva de modelos paramétricos (análisis discriminante multivariante, LOGIT) comparando con la información aportada por la auditoria. Los modelos están fundamentados en variables financieras relevantes y ratios, de lógica financiera y en situaciones de estrés. Examinamos una muestra aleatoria de empresas, comprobando la capacidad predictiva en distintos momentos del tiempo, verificando si los modelos muestran señales anticipadas de futuros eventos de fallo financiero, simulando el impacto de los costes de los errores de estimación en función del modelo previsional. Los resultados sugieren que nuestros modelos son efectivos en el corto y medio plazo, ofreciendo mayor capacidad predictiva que las auditorías externas.
C45|A Point Of View On The Logic Modelling Of The Financial Network|The identification and solving of the different problems that confront us presently, particularly due to the process of globalization, requires a more complex approach of the financial domain. We hereby undertake to bring clarifications and proposals for a more profound approach of the analytical aspects of the network-type models. We thus identify the elements of a financial network which bestow upon it its character if specificity, such as knots, instruments, operations, interconnections, interactions, determinants and flows. We also identify some defining characteristics of the financial network, such as its credibility, representativeness, complexity, efficacy, extensiveness, intensiveness, connectivity, integrability and establishment. Finally, we describe a mechanism of transformation of the financial flows within a network knot, using the concept of interface. We mention that, to a significant extent, the present paper was expounded at the International Conference Financial and Monetary Economics FME 2013 for Financial and Monetary Research, 25 October 2013.
C45|Particularities Of Transfer Channel In The Financial Network Modeling|As is known network model is based on the defining element: the transfer channel. The most research in this area focused on what is transferred (flows of material, financial, energy, information) between two nodes in the network and less on “technical support” transfer – transfer channel. It seeks to propose an analytical model of channel transfer taking into account the types of transfer flows and nodes that are connected. Our proposals we believe can contribute to a better identification of financial networks and for solving practical problems in the modelling type can’t provide a solution (network bottlenecks, transfer between different networks, a network relationship with the environment and so on).
C45|The Computational Intelligence Techniques For Predictions - Artificial Neural Networks|The computational intelligence techniques are used in problems which can not be solved by traditional techniques when there is insufficient data to develop a model problem or when they have errors.Computational intelligence, as he called Bezdek (Bezdek, 1992) aims at modeling of biological intelligence. Artificial Neural Networks( ANNs) have been applied to an increasing number of real world problems of considerable complexity. Their most important advantage is solving problems that are too complex for conventional technologies - problems that do not have an algorithmic solution or for which an algorithmic solution is too complex to be found. In general, because of their abstraction from the biological brain, ANNs are well suited for problems that people are good at solving, but for which computers are not. The ability to accurately predict the future is fundamental to many decision activities in many functional areas of business.In this paper emphasize the advantages and disadvantages of using ANNs for predictions.
C45|Is there a trade-off between the predictive power and the interpretability of bankruptcy models? The case of the first Hungarian bankruptcy prediction model|In our work, we compare the predictive power of different bankruptcy prediction models built on financial indicators calculable from businesses’ accounting data on the database of the first Hungarian bankruptcy model. For modelling, we use data-mining methods often applied in bankruptcy prediction: neural networks (NN), support vector machines (SVM) and the rough set theory (RST) capable of rule-based classification. The point of departure for our comparative analysis is the practical finding that black-box-type data-mining methods typically show better classification performance than models whose results are easy to interpret, i.e. there seems to be a kind of trade-off between the interpretability and predictive power of bankruptcy models. Empirical results lead us to conclude that the RST approach can be a competitive alternative to black-box-type SVM and NN models. In our research, we did not find any major trade-off between the interpretability and predictive performance of bankruptcy models on the database of the first Hungarian bankruptcy model.
C45|“A multivariate neural network approach to tourism demand forecasting”|This study compares the performance of different Artificial Neural Networks models for tourist demand forecasting in a multiple-output framework. We test the forecasting accuracy of three different types of architectures: a multi-layer perceptron network, a radial basis function network and an Elman neural network. We use official statistical data of inbound international tourism demand to Catalonia (Spain) from 2001 to 2012. By means of cointegration analysis we find that growth rates of tourist arrivals from all different countries share a common stochastic trend, which leads us to apply a multivariate out-of-sample forecasting comparison. When comparing the forecasting accuracy of the different techniques for each visitor market and for different forecasting horizons, we find that radial basis function models outperform multi-layer perceptron and Elman networks. We repeat the experiment assuming different topologies regarding the number of lags used for concatenation so as to evaluate the effect of the memory on the forecasting results, and we find no significant differences when additional lags are incorporated. These results reveal the suitability of hybrid models such as radial basis functions that combine supervised and unsupervised learning for economic forecasting with seasonal data.
C45|A New Approach to Infer Changes in the Synchronization of Business Cycle Phases|This paper proposes a Markov-switching framework useful to endogenously identify regimes where economies enter recessionary and expansionary phases synchronously, and regimes where economies are unsynchronized following independent business cycle phases. The reliability of the framework to track synchronization changes is corroborated with Monte Carlo experiments. An application to the case of U.S. states reports substantial changes over time in the cyclical affiliation patterns of states. Moreover, a network analysis discloses a change in the propagation pattern of aggregate contractionary shocks across states, suggesting that regional economies in U.S. have become more interdependent since the early 90s.
C45|A Comparison of Machine Learning Methods in a High-Dimensional Classification Problem|Background: Large-dimensional data modelling often relies on variable reduction methods in the pre-processing and in the post-processing stage. However, such a reduction usually provides less information and yields a lower accuracy of the model. Objectives: The aim of this paper is to assess the high-dimensional classification problem of recognizing entrepreneurial intentions of students by machine learning methods. Methods/Approach: Four methods were tested: artificial neural networks, CART classification trees, support vector machines, and k-nearest neighbour on the same dataset in order to compare their efficiency in the sense of classification accuracy. The performance of each method was compared on ten subsamples in a 10-fold cross-validation procedure in order to assess computing sensitivity and specificity of each model. Results: The artificial neural network model based on multilayer perceptron yielded a higher classification rate than the models produced by other methods. The pairwise t-test showed a statistical significance between the artificial neural network and the k-nearest neighbour model, while the difference among other methods was not statistically significant. Conclusions: Tested machine learning methods are able to learn fast and achieve high classification accuracy. However, further advancement can be assured by testing a few additional methodological refinements in machine learning methods.
C45|Co-movements in commodity prices: a note based on network analysis| This article analyses co-movements in a wide group of commodity prices during the time period 1992–2010. Our methodological approach is based on the correlation matrix and the networks inside. Through this approach we are able to summarize global interaction and interdependence, capturing the existing heterogeneity in the degrees of synchronization between commodity prices. Our results produce two main findings: (a) we do not observe a persistent increase in the degree of co-movement of the commodity prices in our time sample, however from mid-2008 to the end of 2009 co-movements almost doubled when compared with the average correlation; (b) we observe three groups of commodities which have exhibited similar price dynamics (metals, oil and grains, and oilseeds) and which have increased their degree of co-movement during the sampled period.
C45|Une seule fonction de demande ?. Une enquête sur la stabilité des préférences par mélanges discrets de réseaux de neurones|We introduce a new test of preference stability across consumers, based on a discrete mixture model of feedforward neural networks, designed to detect clusters of consumers following distinct consumption functions. The procedure is tested on simulated data and applied to two sets of Canadian household consumption microdata (1969-1986 and 2004-2008). On both these datasets we find that two clusters best explain the data, thus rejecting preference stability. Finally we address elasticity estimation, and find on simulated data that for most practical uses simple demand systems suffice. Classification JEL : C45, D12
C45|Estimación bayesiana del valor en riesgo: una aplicación para el mercado de valores colombiano|Esta investigación tiene como propósito implementar la metodología de regresión cuantil bayesiana en el cálculo del valor en riesgo (VaR, en inglés) en el mercado de valores colombiano. Para este objetivo se valoran algunos requerimientos regulatorios sobre riesgo de mercado definidos por la Superintendencia Financiera de Colombia sobre metodologías, medidas de desempeño y factores de riesgo para el cálculo del VaR, y se compara con el modelo APARCH y de regresión cuantil tradicional; se halla que la regresión cuantil tiene una mejor capacidad para adaptarse a los patrones exhibidos por un portafolio de acciones colombianas dadas varias medidas de desempeño. ***** The purpose of this research is to implement the Bayesian quantile regression methodology in the estimation of the Value at Risk (VaR), in the Colombian stock market. For this objective, some regulatory requirements on market risk are compared using the APARCH model, and traditional quantile regressions. Colombia’s Financial Superintendence defines these requirements based on where they address methodologies, performance measures, and risk factors relevant to the calculation of the VaR. We found out that the latter technique has a greater capacity to adapt to the patterns exhibited by a portfolio of Colombian stock given several performance measures.
C45|Evaluando las intervenciones cambiarias en Colombia: 2004-2012|Esta investigación tiene como propósito evaluar la efectividad de las intervenciones en el mercado cam- biario colombiano, utilizando el modelo teórico canal de coordinación bajo la metodología red neuronal de regresión cuantil. Con este objetivo se estima el efecto de los inversionistas no informados, informados y el emisor en diferentes cuantiles de la distribución del retorno de la tasa de cambio en el largo plazo. Se encuentra que la autoridad cambiaria tiene una mayor influencia en los cuantiles inferiores de la distribución, como son los de 5 y 25%, que recogen efectos asociados con la revaluación del peso.
C45|Forecasting tourism demand to Catalonia: Neural networks vs. time series models|The increasing interest aroused by more advanced forecasting techniques, together with the requirement for more accurate forecasts of tourism demand at the destination level due to the constant growth of world tourism, has lead us to evaluate the forecasting performance of neural modelling relative to that of time series methods at a regional level. Seasonality and volatility are important features of tourism data, which makes it a particularly favourable context in which to compare the forecasting performance of linear models to that of nonlinear alternative approaches. Pre-processed official statistical data of overnight stays and tourist arrivals from all the different countries of origin to Catalonia from 2001 to 2009 is used in the study. When comparing the forecasting accuracy of the different techniques for different time horizons, autoregressive integrated moving average models outperform self-exciting threshold autoregressions and artificial neural network models, especially for shorter horizons. These results suggest that the there is a trade-off between the degree of pre-processing and the accuracy of the forecasts obtained with neural networks, which are more suitable in the presence of nonlinearity in the data. In spite of the significant differences between countries, which can be explained by different patterns of consumer behaviour, we also find that forecasts of tourist arrivals are more accurate than forecasts of overnight stays.
C45|Forecasting energy markets using support vector machines|In this paper we investigate the efficiency of a support vector machine (SVM)-based forecasting model for the next-day directional change of electricity prices. We first adjust the best autoregressive SVM model and then we enhance it with various related variables. The system is tested on the daily Phelix index of the German and Austrian control area of the European Energy Exchange (ΕΕΧ) wholesale electricity market. The forecast accuracy we achieved is 76.12% over a 200day period.
C45|A compressed sensing based AI learning paradigm for crude oil price forecasting|Due to the complexity of crude oil price series, traditional statistics-based forecasting approach cannot produce a good prediction performance. In order to improve the prediction performance, a novel compressed sensing based learning paradigm is proposed through integrating compressed sensing based denoising (CSD) and certain artificial intelligence (AI), i.e., CSD-AI. In the proposed learning paradigm, CSD is first performed as a preprocessor for the original data of international crude oil price to eliminate the noise, and then a certain powerful AI tool is employed to conduct prediction for the cleaned data. In particular, the process of CSD aims to reduce the level of noise which pollutes the data, and to further enhance the prediction performance of the AI model. For verification purpose, international crude oil price series of West Texas Intermediate (WTI) are taken as sample data. Empirical results demonstrate that the proposed CSD-AI learning paradigm significantly outperforms all other benchmark models including single models without CSD process and hybrid models with other denoising techniques, in terms of level and directional accuracies. Furthermore, in the case of different data samples with different time ranges, the proposed model performs the best, indicating that the proposed CSD-AI learning paradigm is an effective and robust approach in crude oil price prediction.
C45|Stochastic and genetic neural network combinations in trading and hybrid time-varying leverage effects|The motivation of this paper is 3-fold. Firstly, we apply a Multi-Layer Perceptron (MLP), a Recurrent Neural Network (RNN) and a Psi-Sigma Network (PSN) architecture in a forecasting and trading exercise on the EUR/USD, EUR/GBP and EUR/CHF exchange rates and explore the utility of Kalman Filter, Genetic Programming (GP) and Support Vector Regression (SVR) algorithms as forecasting combination techniques. Secondly, we introduce a hybrid leverage factor based on volatility forecasts and market shocks and study if its application improves the trading performance of our models. Thirdly, we introduce a specialized loss function for Neural Networks (NNs) in financial applications. In terms of our results, the PSN from the individual forecasts and the SVR from our forecast combination techniques outperform their benchmarks in statistical accuracy and trading efficiency. We also note that our trading strategy is successful, as it increased the trading performance of most of our models, while our NNs loss function seems promising.
C45|Forecasting performances of three automated modelling techniques during the economic crisis 2007–2009|In this work we consider the forecasting of macroeconomic variables during an economic crisis. The focus is on a specific class of models, the so-called single hidden-layer feed-forward autoregressive neural network models. What makes these models interesting in the present context is the fact that they form a class of universal approximators and may be expected to work well during exceptional periods such as major economic crises. Neural network models are often difficult to estimate, and we follow the idea of White (2006) of transforming the specification and nonlinear estimation problem into a linear model selection and estimation problem. To this end, we employ three automatic modelling devices. One of them is White’s QuickNet, but we also consider Autometrics, which is well known to time series econometricians, and the Marginal Bridge Estimator, which is better known to statisticians. The performances of these three model selectors are compared by looking at the accuracy of the forecasts of the estimated neural network models. We apply the neural network model and the three modelling techniques to monthly industrial production and unemployment series from the G7 countries and the four Scandinavian ones, and focus on forecasting during the economic crisis 2007–2009. The forecast accuracy is measured using the root mean square forecast error. Hypothesis testing is also used to compare the performances of the different techniques.
C45|Disa Aciklik ve Demokratik Yapinin Kamu Kesimi Buyuklugu Uzerindeki Etkisi: Rodrik Hipotezine Gecis Ekonomilerinden Kanit|Bu calismada, 24 gecis ekonomisi icin disa acikliktan kaynaklanan risk ile kamu kesimi buyuklugu arasinda pozitif yonlu bir iliskinin oldugunu one suren Rodrik hipotezinin gecerliligi 1990-2011 donemi icin sinanmistir. Ampirik analizin ilk kisminda, demokratik yapisi guclu gecis ulkelerinin Rodrik hipotezini saglamaya kismen daha yatkin oldugu bulgusu elde edilmistir. Ikinci kisimda ise, karma rejime sahip Bosna Hersek, Ukrayna, Gurcistan ve Moldova’da Rodrik hipotezinin gecerli oldugu tespit edilmistir. Bu ulkelerde kamu kesimi buyuklugu, ticari acikliktaki degismelerden bagimsiz olarak dis risk ile pozitif iliskilidir.
C45|Bulanik TOPSIS ve Bulanik VIKOR Yontemleriyle Alisveris Merkezi Kurulus Yeri Secimi ve Bir Uygulama|Isletmelerin artan rekabet kosullari nedeniyle pazarda faaliyetlerini surdurebilmesi icin yer secimiyle ilgili dogru kararlarvermesi cok onemlidir. Yanlis yer secimi isletmeler icin buyuk zararlara, hatta iflasa sebep olabilmektedir. Kendileri icin yer seciminin cok onemli oldugu sektorlerin basinda Alisveris Merkezleri (AVM) gelmektedir. Alisveris Merkezi (AVM) olarak secilecek yer; musteri potansiyelinin yuksek oldugu, dusuk maliyet ve yuksek kâri saglayabilecek bir yer olmalidir. Bu calismada, Erzincan ilinde yeni bir AVM acilmasina karar verilmesi durumunda, olasi kurulus yerinin belirlenmesine calisilmistir. Bunun icin birden fazla karar vericiyle, bircok alternatif arasindan dogru olan alternatifi secmek icin Cok Kriterli Karar Verme (CKKV) tekniklerinden yararlanilmistir. Calismada, Cok Kriterli Karar Verme (CKKV) tekniklerinden Bulanik TOPSIS ve Bulanik VIKOR teknikleriyle, Erzincan icin AVM kurulus yeri secimi icin potansiyel bolgelerin degerlendirilmesi ve bunun sonucunda en uygun yer secimi belirlenmeye calisilmistir.
C45|Suleyman Demirel Universitesi, Iktisadi ve Idari Bilimler Fakultesi, Isletme Bolumu|Bu arastirmanin amaci, bagimsiz denetim surecinin planlama asamasinda analitik inceleme teknigi olarak yapay sinir agi yonteminin kullaniminin gosterilmesidir. Bu baglamda calismada, denetim sureci, analitik inceleme proseduru ve tekniklerinden bahsedilmekte ve yapay sinir aglarinin denetciye ve denetim surecine sagladigi kolayliklar ortaya konulmaktadir. Borsa Istanbul’da imalat sektorunde islem goren bir isletmenin Mart 2004–Aralik 2012 donemindeki denetimden gecmis 8 yillik finansal tablolari kullanilmaktadir. Calismanin sonucunda yapay sinir aglari yonteminin iyi bir tahmin araci oldugu gorulmektedir
C45|On Estimation of Gravity Equation: A Cluster Analysis|This article questions the slope homogeneity in a gravity equation and proposes a partially heterogeneous framework for its estimation using panel data. We suggest to employ K-mean clustering to group countries according to the gravity equation variables. Further, the gravity model is estimated on these created homogeneous groups. We apply this procedure on analysis of German trade data and confirm the slope heterogeneity in the model. When we estimate the model on each cluster separately, the estimated coefficients and their standard errors vary sufficiently. Moreover, we show that the pooled estimation technique severely under- or overestimate the effect of given variables.
C45|Learning the Ramsey outcome in a Kydland & Prescott economy|We study if adaptive learning by a Central Bank (CB) in the Kydland and Prescott environment can steer the economy to the Pareto-optimal outcome. Our CB evaluates its potential strategies regarding the announced and the actual inflation rate through expectations of the performance of these strategies, formed thanks to its mental model of the economy. This model is forward looking and adaptive at the same time. As a starting point, we follow Arifovic et al. (2010), and initially assume that there are two types of agents: Believers who set their inflation forecast equal to the announced inflation, and Non-believers who form static optimal forecast coupled with a forecast error correction mechanism. Our results show that the economy can reach near Ramsey outcomes most of the time. In the absence of Believers, the economies almost always converge to the Ramsey outcome.
