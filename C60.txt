C60|Cross-sectional noise reduction and more efficient estimation of Integrated Variance|In this paper we propose a straightforward approach to obtain a more efficient estimate of the integrated variance of an asset through a cross-sectional combination with a futures contract written on it. Our method constructs a variance-preserving series with reduced noise size as a linear combination of the underlying asset and the futures and base measurement of the integrated variance on this new series. We first illustrate how a theoretically but infeasible optimal series can be obtained and then suggest a feasible procedure to attain noise reduction. In a simulation study we verify how prevalent estimators of integrated variance applied to such noise-reduced series outperform estimators applied directly to the asset price. Finally, we apply the method to an empirical data set and, through the stabilized signature plot, we show how the noise reduced series provides consistent integrated variance estimates using naive realized measures at very high frequencies.
C60|Distributed Optimal Control Models in Environmental Economics: A Review|We review the most recent advances in distributed optimal control applied to environmental economics, covering in particular problems where the state dynamics are governed by partial differential equations (PDEs). This is a quite fresh application area of distributed optimal control, which has already suggested several new mathematical research lines due to the specificities of the environmental economics problems involved. We enhance the latter through a survey of the variety of themes and associated mathematical structures beared by this literature. We also provide a quick tour of the existing tools in the theory of distributed optimal control that have been applied so far in environmental economics.
C60|The inflation-distribution nexus: a theoretical and empirical approach|There are two unconnected strands of the inflation-distribution literature, one that studies the impact of inflation on income distribution and the other the impact of distribution on inflation. This paper is an attempt to fill a gap in this literature, by taking into account the simultaneous determination between inflation and income distribution. We set forth a Post-Keynesian model in which inflation and income distribution are jointly determined in a dynamical system of difference equations. The theoretical framework advanced in the paper allows us to show that conflicting claims on income, expectation formation and the realisation of increasing returns to scale ascribed to demand-pull and distributive factors also play a key role in the determination of the inflation and income distribution dynamics. Then, we conducted an empirical investigation of the relationship between inflation and distribution. Both empirical exercises were done using GMM estimator. This econometric technique is robust to reverse causality as it uses lagged observations in difference and level of endogenous variables as instruments and hence is the preferred method of estimation. Our findings corroborate our theoretical model by showing that, in average, increases in the wage share tend to exert a downward pressure in future inflation. Our estimates also show that the wage share is highly dependent of its past values, thus suggesting that income distribution may be only sensitive to autonomous (political) factors.
C60|Should Faustmann forecast climate change?|Climate change is predicted to substantially alter forest growth. Optimally, forest owners should take these future changes into account when making rotation decisions today. However, the fundamental uncertainty surrounding climate change makes predicting these shifts hard. Hence, this paper asks whether forecasting them is necessary for optimal rotation decisions. While climate-change uncertainty makes it theoretically impossible to calculate expected profit losses of not forecasting, we suggest a method utilizing Monte-Carlo simulations to obtain a credible upper bound on these losses. We show that an owner following a rule of thumb - ignoring future changes and only observing changes as they come - will closely approximate optimal management. If changes are observed without too much delay, profit losses and errors in harvesting are negligible. This means that the very complex analytical problem of optimal rotation with changing growth dynamics can be simplified to a sequence of stationary problems. It also implies the argument that boundedly-rational agents may behave “as if” being fully rational has traction in forestry.
C60|Mutually Consistent Revealed Preference Demand Predictions|Revealed preference restrictions are increasingly used to predict demand behaviour at new budgets of interest and as shape restrictions in nonparametric estimation exercises. However, the restrictions imposed are not sufficient for rationality when predictions are made at multiple budgets. I highlight the nonconvexities in the set of predictions that arise when making multiple predictions. I develop a mixed integer programming characterisation of the problem that can be used to impose rationality on multiple predictions. The approach is applied to the UK Family Expenditure Survey to recover rational demand predictions with substantially reduced computational resources compared to known alternatives.
C60|Estimating the impacts of financing support policies towards photovoltaic market in Indonesia: A social-energy-economy-environment (SE3) model simulation|This study estimates the impacts of four solar energy policy interventions on the photovoltaic (PV) market potential, government expenditure, economic growth, and the environment. An agent-based model is developed to capture the specific economic and institutional features of developing economies, citing Indonesia as a specific case study. We undertake a novel approach to energy modelling by combining energy system analysis, input-output analysis, life-cycle analysis, and socio-economic analysis to obtain a comprehensive and integrated impact assessment. Our results, after sensitivity analysis, call for abolishing the existing PV grant policy in the Indonesian rural electrification programs. The government, instead, should encourage the PV industry to improve production efficiency and to provide after-sales service. A 100-watt peak (Wp) PV under this policy is affordable for 33.2 percent of rural households without electricity access in 2010. Rural PV market size potentially increases to 82.4 percent with rural financing institutions lending 70 percent of capital cost for five years at 12 percent annual interest rate. Additional 30 percent capital subsidy and 5 percent interest subsidy slightly increase the rural PV market potential to 89.6 percent of PV adopters. However, the subsidies are crucial for creating PV demands by urban households but the most effective policy for promoting PV to urban households is the net metering scheme. Several policy proposals are discussed in response to these findings.
C60|How Does Asymmetric Information Create Market Incompleteness?|The aim of this work is to show that incompleteness is due in general not only to a lack of assets, but also to a lack of information. In this paper we present a simple inuence model where the inuencial agent has access to additional information. This leads to the construction of two models, a complete model and an incomplete model where the only dierence is a dierence of information. This leads to a simple model of incomplete market where the number of assets is not the cause of incompleteness: incomplete information is the explanation. Keywords Information · asymmetric information · option pricing · martin-gales · insider trading · complete market · incomplete market AMS Classication (2000): 60H10, 60G44, 60H07, 60J75, 91G20, 91B70, 93E11. JEL Classication: C60,G11,G14.
C60|Distributed Optimal Control Models in Environmental Economics: A Review|We review the most recent advances in distributed optimal control applied to environmental economics, covering in particular problems where the state dynamics are governed by partial differential equations (PDEs). This is a quite fresh application area of distributed optimal control, which has already suggested several new mathematical research lines due to the specificities of the environmental economics problems involved. We enhance the latter through a survey of the variety of themes and associated mathematical structures beared by this literature. We also provide a quick tour of the existing tools in the theory of distributed optimal control that have been applied so far in environmental economics
C60|A Statistical Field Approach to Capital Accumulation|This paper presents a model of capital accumulation for a large number of heterogenous producer-consumers in an exchange space in which interactions depend on agents' positions. Each agent is described by his production, consumption, stock of capital, as well as the position he occupies in this abstract space. Each agent produces one differentiated good whose price is fixed by market clearing conditions. Production functions are Cobb-Douglas, and capital stocks follow the standard capital accumulation dynamic equation. Agents consume all goods but have a preference for goods produced by their closest neighbors. Agents in the exchange space are subject both to attractive and repulsive forces. Exchanges drive agents closer, but beyond a certain level of proximity, agents will tend to crowd out more distant agents. The present model uses a formalism based on statistical field theory developed earlier by the authors. This approach allows the analytical treatment of economic models with an arbitrary number of agents, while preserving the system's interactions and complexity at the individual level. Our results show that the dynamics of capital accumulation and agents' position in the exchange space are correlated. Interactions in the exchange space induce several phases of the system. A first phase appears when attractive forces are limited. In this phase, an initial central position in the exchange space favors capital accumulation in average and leads to a higher level of capital, while agents far from the center will experience a slower accumulation process. A high level of initial capital drives agents towards a central position, i.e. improve the terms of their exchanges: they experience a higher demand and higher prices for their product. As usual, high capital productivity favors capital accumulation, while higher rates of capital depreciation reduce capital stock. In a second phase, attractive forces are predominant. The previous results remain, but an additional threshold effect appears. Even though no restriction was imposed initially on the system, two types of agents emerge, depending on their initial stock of capital. One type of agents will remain above the capital threshold and occupy and benefit from a central position. The other type will remain below the threshold, will not be able to break it and will remain at the periphery of the exchange space. In this phase, capital distribution is less homogenous than in the first phase.
C60|Samuelson's Approach to Revealed Preference Theory: Some Recent Advances|Since Paul Samuelson introduced the theory of revealed preference, it has become one of the most important concepts in economics. This chapter surveys some recent contributions in the revealed preference literature. We depart from Afriat's theorem, which provides the conditions for a data set to be consistent with the utility maximization hypothesis. We provide and motivate a new condition, which we call the Varian inequalities. The advantage of the Varian inequalities is that they can be formulated as a set of mixed integer linear inequalities, which are linear in the quantity and price data. We show how the Varian inequalities can be used to derive revealed preference tests for weak separability, and show how it can be used to formulate tests of the collective household model. Finally, we discuss measurement errors in the observed data and measures of goodness-of-fit, power and predictive success.
C60|Autonomous vessels: State of the art and potential opportunities in logistics|The growth in technology on autonomous transportation systems is currently motivating a number of research initiatives. This paper first presents a survey of the literature on autonomous marine vessels in general. By identifying the main research interests in this field, we define nine thematic categories. The collected articles are then classified according to these categories. We show that research on autonomous vessels has increased dramatically in the past decade. However, most of the published articles have focused on navigation control and safety issues. Studies regarding other topics, such as transport and logistics, are very limited. While our main interest is the literature on autonomous vessels, we contrast its development with respect to the literature on autonomous cars so as to have a better understanding about the future potentials in the research on autonomous vessels. The comparison shows that there are great opportunities for research about transportation and logistics with autonomous vessels. Finally, several potential research areas regarding logistics with autonomous vessels are proposed. As the technology behind remote-controlled or autonomous ships is maturing rapidly, we believe that it is already time for researchers in the field to start looking into future water-borne transport and logistics using autonomous vessels.
C60|Evaluating welfare and economic effects of raised fertility|In the context of the second demographic transition, many countries consider rising fertility through pro-family polices as a potentially viable solution to the fiscal pressure stemming from longevity. However, an increased number of births implies private and immediate costs, whereas the gains are not likely to surface until later and appear via internalizing the public benefits of younger and larger population. Hence, quantification of the net effects remains a challenge. We propose using an overlapping generations model with a rich family structure to quantify the effects of increased birth rates. We analyze the overall macroeconomic and welfare effects as well as the distribution of these effects across cohorts and study the sensitivity of the final effects to the assumed target value and path of increased fertility. We find that fiscal effects are positive but, even in the case of relatively large fertility increase, they are small. The sign and the size of both welfare and fiscal effects depend substantially on the patterns of increased fertility: if increased fertility occurs via lower childlessness, the fiscal effects are smaller and welfare effects are more likely to be negative than in the case of the intensive margin adjustments.
C60|When to Ease Off the Brakes--and Hopefully Prevent Recessions|"Increases in the federal funds rate aimed at stabilizing the economy have inevitably been followed by recessions. Recently, peaks in the federal funds rate have occurred 6-16 months before the start of recessions; reductions in interest rates apparently occurred too late to prevent those recessions. Potential leading indicators include measures of labor productivity, labor utilization, and demand, all of which influence stock market conditions, the return to capital, and changes in the federal funds rate, among many others. We investigate the dynamics of the spread between the 10-year Treasury rate and the federal funds rate in order to better understand ""when to ease off the (federal funds) brakes."""""
C60|New Approach to Estimating Gravity Models with Heteroscedasticity and Zero Trade Values|This paper proposes new estimation techniques for gravity models with zero trade values and heteroscedasticity. We revisit the standard PPML estimator and we propose an improved version. We also propose various Heckman estimators with different distributions of the residuals, nonlinear forms of both selection and measure equations, and various process of the variance. We add to the existent literature alternative estimation methods taking into account the non-linearity of both the variance and the selection equation. Moreover, because of the unavailability of pre-set package in the econometrics software (Stata, Eviews, Matlab, etc.) to perform the estimation of the above-mentioned Heckman versions, we had to code it in Matlab using a combination of fminsearch and fminunc functions. Using numerical gradient matrix G, we report standard errors based on the BHHH technique. The proposed new Heckman version could be used in other applications. Our results suggest that previous empirical studies might be overestimating the contribution of the GDP of both import and export countries in determining the bilateral trade.
C60|An attitude of complexity: thirteen essays on the nature and construction of reality under the challenge of Zeno's Paradox|This book is about the construction of reality. The central aim of this study is to understand how gravity works and how it may be focused and manipulated. While I do not have an answer to this question, the discoveries along the way have been worth collecting into a single volume for future reference.
C60|Saving and dissaving under Ramsey - Rawls criterion|This article studies an inter-temporal optimization problem using a criterion which is a combination between Ramsey and Rawls criteria. A detailed description of the saving behaviour through time is provided. The optimization problem under $\alpha-$\emph{maximin} criterion is also considered with optimal solution characterized.
C60|Investigating on Hydrodynamic Behavior of Slotted Breakwater Walls Under Sea Waves|Breakwater walls are buildings that are built to prevent the collapse of the soil or other granular materials and the safety of the sea. One of the destructive phenomena in these structures is the impact of sea wave forces on the overturning phenomenon and instability of the coastal wall, which has damaged the structures existing on these sites. The pattern of interaction between water and seas is complex in coastal structures. In this research, the influence of the different wall heights and soil type changes on wall stability and water pressure distribution in the coastal wall have been investigated. Also, studies will be done on the investigation and optimization of the wall and Finally, by comparing the results obtained with classical methods, the strengths and weaknesses of the classical methods have been analyzed and the effectiveness of these methods (classical) has been evaluated. These walls are made in two types of weighted and flexible (mainly metal) types, in which flexible performance is considered in this research. The behavior of metal shields in front of the water will be examined using the ANSYS software. Several methods for calculating wave forces on perforated coastal walls are also reviewed. In this study, the behavior of the elastic wall is assumed. Coastal walls have been investigated in different hardships and the distribution of pressure and anchor due to hydrodynamic pressure of water on the wall have been investigated. The walls are different in terms of material and amount of rigidity.
C60|Just-noticeable difference as a behavioural foundation of the critical cost-efficiency index|Critical cost-efficiency index (or CCEI), proposed in Afriat (1973) and Varian (1990), is one of the most commonly used measures of departures from rationality. We show that this index is equivalent to a particular notion of the just-noticeable difference, that is, a measure of dissimilarity between alternatives that is sufficient for the agent to tell them apart. Therefore, we show that CCEI evaluates the consumer's cognitive inability to discriminate among options.
C60|The Impact of Jumps on American Option Pricing: The S&P 100 Options Case|This paper analyzes the importance of asset and volatility jumps in American option pricing models. Using the Heston (1993) stochastic volatility model with asset and volatility jumps and the Hull and White (1987) short rate model, American options are numerically evaluated by the Method of Lines. The calibration of these models to S&P 100 American options data reveals that jumps, especially asset jumps, play an important role in improving the models’ ability to fit market data. Further, asset and volatility jumps tend to lift the free boundary, an effect that augments during volatile market conditions, while the additional volatility jumps marginally drift down the free boundary. As markets turn more volatile and exhibit jumps, American option holders become more prudent with their exercise decisions, especially as maturity of the options approaches.
C60|Optimal time allocation in active retirement|We set up a lifecycle model of a retired scholar who chooses optimally the time devoted to different activities including physical activity, continued work and social engagement. While time spent in physical activity increases life expectancy, continued scientific publications increases the knowledge stock. We show the optimal trade off between these activities in retirement and its sensitivity with respect to alternative settings of the preference parameters.
C60|Mathematics vs. Statistics in tackling Environmental Economics uncertainty|In this paper the appropriate background in Mathematics and Statistics is considered in developing methods to investigate Risk Analysis problems associated with Environmental Economics uncertainty. New senses of uncertainty are introduced and a number of sources of uncertainty are discussed and presented. The causes of uncertainty are recognized helping to understand how they affect the adopted policies and how important their management is in any decision-making process. We show Mathematical Models formulate the problem and Statistical models offer possible solutions, restricting the underlying uncertainty, given the model and the error assumptions are correct. As uncertainty is always present we suggest ways on how to handle it.
C60|A mixed integer linear programming model to regulate the electricity sector|Abstract This paper presents a mixed-integer linear programming model for the optimal long-term electricity planning of the Greek wholesale generation system. In order to capture more accurately the technical characteristics of the problem, we have divided the Greek territory into a number of individual interacted networks (geographical zones). In the next stage we solve the system of equations and provide simulation results for the daily/hourly energy prices based on the different scenarios adopted. The empirical findings reveal an inverted-M shaped curve for electricity demand in Greece, while the system marginal price curve also follows a non-linear pattern. Lastly, given the simulations results, we provide the necessary policy implications for government officials, regulators and the rest of the marketers.
C60|On the extension of a preorder under translation invariance|This paper proves the existence, for any preorder on a real vector space satisfying translation invariance, of a complete preorder extending the preorder and satisfying translation invariance. As application, the existence of a translation-invariant complete preorder on infinite utility streams satisfying strong Pareto and fixed-step anonymity, is established.
C60|A second welfare theorem in a non-convex economy: The case of antichain-convexity|We introduce the notion of an antichain-convex set to extend Debreu (1954)’s version of the second welfare theorem to economies where either the aggregate production set or preference relations are not convex. We show that – possibly after some redistribution of individuals’ wealth – the Pareto optima of some economies which are marked by certain types of non-convexities can be spontaneously obtained as valuation quasiequilibria and equilibria: both equilibrium notions are to be understood in Debreu (1954)’s sense. From a purely structural point of view, the mathematical contribution of this work is the study of the conditions that guarantee the convexity of the Minkowski sum of finitely many possibly non-convex sets. Such a study allows us to obtain a version of the Minkowski\Hahn–Banach separation theorem which dispenses with the convexity of the sets to be separated and which can be naturally applied in standard proofs of the second welfare theorem; in addition – and equally importantly – the study allows to get a deeper understanding of the conditions on the single production sets of an economy that guarantee the convexity of their aggregate.
C60|Zur mathematischen Struktur der Wertformen von Karl Marx in 'Das Kapital'<BR>[About the mathematical structure of the form of value of Karl Marx in 'Das Kapital']|In the first section of Das Kapital by Karl Marx different forms of values are analysed. From a mathematical point of view one can find therein structures, which correspond to elements of the mathematical theory of categories. These are especially the limit of cones and the definition of subobjects as morphisms. Using the limit cone, the concept of money contains the categorial product of commodities. The concept of the value of a commodity contains the categorial definition of a subobject.
C60|Expectations, Price Fluctuations and Lorenz Attractor|This paper describes expectations and Buy-Sell transactions of selected Stokes between economic agents and Exchange on economic space as ground for modeling trading volume and price fluctuations. We study simple model of mutual relations between transactions and expectations and derive economic equations that describe disturbances of price, trading volume and expectations. We obtain simple harmonic oscillations for price fluctuations. We show that our model economic equations can take form of Lorenz attractor. Our approximation of transactions and expectations and economic equations on disturbances of price, trading volume and expectations allows apply dynamical systems methods for modeling chaotic behavior of economic and financial systems.
C60|A Path Integral Approach to Business Cycle Models with Large Number of Agents|This paper presents an analytical treatment of economic systems with an arbitrary number of agents that keeps track of the systems' interactions and agents' complexity. This formalism does not seek to aggregate agents. It rather replaces the standard optimization approach by a probabilistic description of both the entire system and agents' behaviors. This is done in two distinct steps. A first step considers an interacting system involving an arbitrary number of agents, where each agent's utility function is subject to unpredictable shocks. In such a setting, individual optimization problems need not be resolved. Each agent is described by a time-dependent probability distribution centered around his utility optimum. The entire system of agents is thus defined by a composite probability depending on time, agents' interactions and forward-looking behaviors. This dynamic system is described by a path integral formalism in an abstract space - the space of the agents' actions - and is very similar to a statistical physics or quantum mechanics system. We show that this description, applied to the space of agents' actions, reduces to the usual optimization results in simple cases. Compared to a standard optimization, such a description markedly eases the treatment of systems with small number of agents. It becomes however useless for a large number of agents. In a second step therefore, we show that for a large number of agents, the previous description is equivalent to a more compact description in terms of field theory. This yields an analytical though approximate treatment of the system. This field theory does not model the aggregation of a microeconomic system in the usual sense. It rather describes an environment of a large number of interacting agents. From this description, various phases or equilibria may be retrieved, along with individual agents' behaviors and their interactions with the environment. For illustrative purposes, this paper studies a Business Cycle model with a large number of agents.
C60|Integrated Strategic Heating and Cooling Planning on Regional Level for the case of Brasov|In this work a method for integrated strategic heating and cooling planning on regional level is presented and applied for the case study city of Brasov. The overall methodology comprises the calculation of the cost-optimal combination of heat savings with either district heating or individual supply technologies for different building groups located in different areas according to the availability of a current district heating network. This optimal combination is calculated for different scenarios and framework conditions, and different indicators like total system costs, total CO2 emissions, share of renewables etc. are calculated and compared to analyse the economic efficiency as well as the CO2 reduction potentials of various options to save heat and supply heat in the buildings. The results of the assessment show that at least a certain amount of heat savings is cheaper than all assessed heat supply options for all building groups but that renewable supply options are not the most economical alternatives per se in the assessed case study under stated conditions. The presented integrated planning process reveals that a long term planning is essential to reach decarbonisation goals and that current framework conditions should be adapted to generate more favourable conditions for renewable heating systems.
C60|Project appraisal and the Intrinsic Rate of Return|Building upon Magni (2011)’s approach, we propose a new rate of return measuring a project’s economic profitability. It is called the intrinsic rate of return (IROR). It is defined as the ratio of project return to project’s intrinsic value. The IROR approach decomposes the NPV into project scale and economic efficiency. In particular, NPV is found as the product of the project’s total invested capital and the excess rate of return, obtained as the difference between the IROR and the minimum attractive rate of return (MARR). This approach provides correct project ranking and is capable of managing time-varying costs of capital. In case of levered projects, shareholder value creation is captured by the equity IROR, which we call Intrinsic Return On Equity (IROE) (net income divided by total equity capital invested). If the project is unlevered, the IROE and the IROR lead to the same decision; if the project is levered, and the nominal value of debt is not equal to the market value of debt, the IROE should be preferred to project IROR.
C60|A lattice test for additive separability| We derive necessary and sufficient conditions for a finite data set of price and demand observations to be consistent with an additively separable preference. We do so without imposing concavity on any of the subutility functions or convexity of the budget set a priori, thereby generalizing earlier results. Our simple and intuitive lattice test easily accommodates departures from rationality, or errors, which subsequently facilitates a rich empirical analysis. We apply our econometric techniques to the food consumption of a panel of British households. The primary empirical finding is that additive separability has considerable success in explaining the data.
C60|Investigating the Efficiencies of World Cup Teams via DEA Approach|It is crucial to analyze football teams since football has increasingly become a significant industry within the economy. Data Envelopment Analysis (DEA) has been applied to many branches and especially to football. This paper investigates the technical efficiency levels of national teams participating in World Cup 2014 to shed light on the sport performance. Input oriented CCR/BCC model and super efficiency analyses have been used to investigate the football efficiencies. In this study, passes completed, attempts on target and possession are used as input while the only output is goal scored. The results have illustrated that World Cup winner Germany is found as efficient on both CCR and BCC model. On the other hand, only four teams are technically efficient within 32 teams while nine teams have demonstrated pure efficiency. Colombia is the most efficient team for both CCR and BCC super efficiency model. In this regard, we may conclude that efficiency would be a viable instrument in analyzing the football teams.
C60|Developing a shared environmental responsibility principle for distributing cost of restoring marine habitats destroyed by industrial harbors|For decades, industrial harbor expansion has been destroying coastal marine ecosystems. Many estuaries are sites for industrial harbors and critical fish nursery habitat. Considering fish population decreases and the global biodiversity crisis, restoring these habitats is justified and supported by international institutions. However, restoration programs can be prohibitively costly, particularly when considering the Polluter Pays Principle. While harbors destroy nurseries, at the same time they generate benefits for society and contribute to the public interest. This raises questions of who is responsible for environmental degradation and who can afford environmental restoration costs? One way to allocate restoration costs is in proportion of those who have benefitted from harbor activities. This paper addresses these questions by calculating burden-sharing scenarios with inputoutput matrix equations. These scenarios are based on a shared producer and consumer responsibility approach to distribute restoration costs among stakeholders that use, either directly or indirectly, harbor services. The scenarios are applied to the Seine estuary, France, and calculated as a function of sectorial value-added as well as direct and indirect economic linkages between economic sectors and harbor activities. Economic linkages with final consumers (e.g. households) are also included. The shared environmental responsibility calculation developed in this paper shares restoration costs for previously damaged marine habitats between a wide-range of economic agents, thereby preventing industrial harbors from bearing expensive restoration costs alone, and making restoration more likely.
C60|Momentum and Reversal in Financial Markets with Persistent Heterogeneity|This paper investigates whether short-term momentum and long-term reversal may emerge from the wealth reallocation process taking place in speculative markets. We assume that there are two classes of investors who trade long-lived assets by holding constantly rebalanced portfolios based on their beliefs. Provided beliefs, and thus portfolios, are sufficiently diversified, all investors survive in the long-run and, due to waves of mispricing, the resulting equilibrium returns exhibit long-term reversal. If, moreover, asset dividends are positively correlated, investors’ profitable trades become positively correlated too, thus generating short-term momentum in equilibrium returns. We use the model to replicate the performance of the Winners and Losers portfolios highlighted by the empirical literature and to provide insights on how to improve upon them. Finally, we show that dividend positive autocorrelation is positively related to momentum and negatively related to reversal while diversity of beliefs is positively related to both momentum and reversal.
C60|Toward a new microfounded macroeconomics in the wake of the crisis|The Great Recession that followed the financial crisis of 2007 is not only the largest economic crisis after the Great Depression of the 1930s, it also signals a crisis of economics as a discipline. This is not only the consequence of the inadequacy of mainstream macroeconomics, and specifically the Dynamic Stochastic General Equilibrium (DSGE) workhorse model, to forecast such a huge event, or at least to detect the worrying tendencies towards it. Even more relevant is the choice to explicitly avoid the modeling of large crises (that for someone is a motivation for not attacking pre-crisis DSGE models focused on the analysis of small deviations from the steady-state), so denying the intrinsic nature of capitalism, a system that necessarily proceeds through cycles and (extended) crises. The replies of the DSGE approach to critics have led to extensions regarding for instance the role of financial frictions, heterogeneous agents, and bounded rationality (though typically in the form of quasi-rational expectations). The alternative paradigm of agent-based (AB) macroeconomics can take into account all these elements at once within an evolutionary modeling framework based on heterogeneity and interaction, so capable to endogenously reproduce complex dynamics, from small fluctuations to large crises, due to innovation and industrial dynamics, rising inequality and financial instability, and so on. The integration between AB macroeconomics and the (post-Keynesian) stock–flow consistent approach represents a promising way for the future development of this research field.
C60|Do Software and Videogames firms share location patterns across cities? Evidence from Barcelona, Lyon and Hamburg| The aim of this paper is to analyse common location patterns of Software and Videogames (SVE) industry in Barcelona, Lyon and Hamburg. This is a key industry in developed countries that mainly located at core of bigger metropolitan areas, looking for agglomeration economies, skilled labour and a wide range of spillover effects existent there. Cities used in our empirical application share some common features in terms of size, manufacturing tradition and, specially, economic strategies, as they have managed to promote high-tech neighbourhoods through ambitious urban renewal policies. When analysing location patterns of firms from these industries, although our results highlight predominant role of urban cores of three cities, also indicate important specificities in terms of core-periphery. distribution of SVEâ€™s firms. JEL Codes: R12, C60, L86, N90. Keywords: Software Industry, microgeographic analysis, spatial location patterns, Barcelona, Hamburg, Lyon
C60|Pricing American Options with Jumps in Asset and Volatility|Jump risk plays an important role in current financial markets, yet it is a risk that cannot be easily measured and hedged. We numerically evaluate American call options under stochastic volatility, stochastic interest rates and jumps in both the asset price and volatility. By employing the Method of Lines (Meyer (2015)), the option price, the early exercise boundary and the Greeks are computed as part of the solution, which makes the numerical implementation time efficient. We conduct a numerical study to gauge the impact of jumps and stochastic interest rates on American call option prices and on their free boundaries. Jumps tend to increase the values of OTM and ATM options while decreasing the value of ITM options. The option delta is affected in a similar way. The impact of jumps on the free boundary is substantial and depends on the time to maturity. Near expiry, including asset jumps lowers the free boundary and the option holder is more likely to exercise the option, whilst including asset-volatility jumps elevates the free boundary and the option holder is less likely to exercise the option. This relation reverses at the beginning of the options life. The volatility, interest rates and their volatilities have a positive impact on the free boundaries and the option holder is less likely to exercise as these parameters increase.
C60|Momentum and Reversal in Financial Markets with Persistent Heterogeneity|This paper investigates whether short-term momentum and long-term reversal may emerge from the wealth reallocation process taking place in speculative markets. We assume that there are two classes of investors who trade long-lived assets by holding constantly rebalanced portfolios based on their beliefs. Provided beliefs, and thus portfolios, are sufficiently diversified, all investors survive in the long-run and, due to waves of mispricing, the resulting equilibrium returns exhibit long-term reversal. If, moreover, asset dividends are positively correlated, investors' profitable trades become positively correlated too, thus generating short-term momentum in equilibrium returns. We use the model to replicate the performance of the Winners and Losers portfolios highlighted by the empirical literature and to provide insights on how to improve upon them. Finally, we show that dividend positive autocorrelation is positively related to momentum and negatively related to reversal while diversity of beliefs is positively related to both momentum and reversal.
C60|Pricing in Day-Ahead Electricity Markets with Near-Optimal Unit Commitment|This paper revisits some peculiar pricing properties of near-optimal unit commitment solutions. Previous work has found that prices can behave erratically even as unit commitment solutions approach the optimal solution, resulting in potentially large wealth transfers due to suboptimality of the solution. Our analysis considers how recently proposed pricing models affect this behavior. Results demonstrate a previously unknown property of one of these pricing models, called approximate Convex Hull Pricing (aCHP), that eliminates erratic price behavior, and therefore limits wealth transfers with respect to the optimal unit commitment solution. The absence of wealth transfers may imply fewer strategic bidding incentives, which could enhance market efficiency.
C60|R&D-driven medical progess, health care costs, and the future of human longevity|In this paper we set up an overlapping generations model of gerontological founded human aging that takes the interaction between R&D-driven medical progress and access to health care into account. We use the model to explore potential futures of human health and longevity. For the baseline policy scenario of health care access, the calibrated model predicts substantial future increases in health and life expectancy, associated with rising shares of health expenditure in GDP. Freezing the expenditure share at the 2020 level by rationing access to health care severely reduces potential gains in health, longevity and welfare. These losses are greatest in the long run due to reduced incentives for medical R&D. For example, rationing is predicted to reduce potential gains of life-expectancy at age 65 by about 4 years in the year 2050. Generally, and perhaps surprisingly, young individuals (i.e. those who save the most health care contributions through rationing) are predicted to suffer the greatest losses in terms of life expectancy and welfare.
C60|Foundations for Intertemporal Choice|We consider discounted-utility models with a reference stream of outcomes. We provide a common framework for the main empirically supported discount functions in terms of three underlying functions: The delay, speedup and generating functions. Each of the delay and speedup functions can be uniquely elicited from behavior and, hence, can be fitted to the data. These two functions determine whether the discount function is subadditive, additive or superadditive; and whether the discount function exhibits declining, constant or increasing impatience. The third function, the generating function, links the speedup function to the discount function. Our framework nests several important attribute-based models that are typically considered to be in a separate class. We also show that apparent intransitivities of time preferences can be accounted for by framing effects.
C60|Decarbonization of Power Markets under Stability and Fairness: Do They Influence Efficiency?|Market integration is seen as a complementary measure to decarbonize energy markets. In the context of power markets, this translates into regions that coordinate to maximize welfare in the power market with respect to a climate target. Yet, the maximization of overall welfare through cooperation leads to redistribution and can result in the reduction of a region's welfare compared to the case without cooperation. This paper assesses why cooperation in the European power market is not stable from the perspective of single regions and identifies cost allocations that increase fairness. In a first step, the EU-REGEN model is applied to find the future equilibrium outcome of the European power market under a cooperative, subadditive cost-sharing game. Secondly, resulting cost allocations are analyzed by means of cooperative game theory concepts. Results show that the value of cooperation is a € 69 billion reduction in discounted system cost and rational behavior of regions can maintain at most 16 % of this reduction. The evaluation of alternative cost allocations reveals the trade-off between accounting for robustness against cost changes and individual rationality. Moreover, the cost-efficient decarbonization path of the European power sector under the grand coalition is characterized by the interplay between wind power, gas power, and biomass with geologic storage of CO2. Last, with singleton coalitions only, the market outcome shifts to a higher contribution from nuclear power.
C60|Inefficient Bubbles and Efficient Drawdowns in Financial Markets|At odds with the common “rational expectations” framework for bubbles, economists like Hyman Minsky, Charles Kindleberger and Robert Shiller have documented that irrational behavior, ambiguous information or certain limits to arbitrage are essential drivers for bubble phenomena and financial crisis. Following this understanding that asset price bubbles are generated by market failures, we present a framework for explosive semimartingales that is based on the antagonistic combination of (i) an excessive, unstable pre-crash process and (ii) a drawdown starting at some random time. This unifying framework allows one to accommodate and compare many discrete and continuous time bubble models in the literature that feature such market inefficiencies. Moreover, it significantly extends the range of feasible asset price processes during times of financial speculation and frenzy and provides a strong theoretical background for future model design. Our framework also allows us to elucidate the status of rational expectation bubbles, which are by design afflicted with an inherent error in both discrete and continuous time models that can be traced down to a problematic definition of the fundamental value. While the discrete time case has been extensively discussed in the literature and is most criticized for a structure that is based on a payoff at infinity, we show that a new version of this error also permeates the continuous, finite time “strict local martingale”-approach to bubbles. In summary, our framework will simplify and foster interdisciplinary exchange at the intersection of economics and mathematical finance and encourage further research.
C60|Cascading Logistic Regression Onto Gradient Boosted Decision Trees to Predict Stock Market Changes Using Technical Analysis|In the data mining and machine learning fields, forecasting the direction of price change can be generally formulated as a supervised classfii cation. This paper attempts to predict the direction of daily changes of the Nasdaq Composite Index (NCI) and of the Standard & Poor's 500 Composite Stock Price Index (S&P 500) covering the period from January 3, 2012 to December 23, 2016, and of the Shanghai Stock Exchange Composite Index (SSEC) from January 4, 2010 to December 31, 2014. Due to the complexity of stock index data, we carefully combine raw price data and eleven technical indicators with a cascaded learning technique to improve the performance of the classifi cation. The proposed learning architecture LR2GBDT is obtained by cascading the logistic regression (LR) model onto the gradient boosted decision trees (GBDT) model. Given the same test conditions, the experimental results show that the LR2GBDT model performs better than the baseline LR and GBDT models for these stock indices, according to the performance metrics Hit ratio, Precision, Recall and F-measure. Furthermore, we use these models to develop simple trading strategies and assess their performance in terms of their Average Annual Return, Maximum Drawdown, Sharpe Ratio and Average Annualized Return/Maximum Drawdown. When transaction costs and buy-sell thresholds are taken into account, the best trading strategy derived from LR2GBDT model still reaches the highest Sharpe Ratio and clearly beats the buy-and-hold strategy. The performances are found to be both statistically and economically signi ficant.
C60|Wage inequality, skills and mastering new technologies|In this paper I provide a new explanation for the increasing inequality between skilled and unskilled. This work introduces a problem solving based model in which agents invest in technological innovations to solve problems and sell their solutions in the market. Each agent has feasible set they can solve which depends on their skills and the set of technology they have access to. However, unlike the skilled biased technological change explanation, I do not assume that new technologies are necessarily complementary with skills. Instead, skills will play a role in how fast agents are able to adopt new technologies. High skilled individuals will adopt new technologies a lower cost relative to the unskilled. Under this frame, the model sets two different mechanisms that feed the wage gap between skilled and unskilled: productivity and access to the latest technological innovations.
C60|Approximate super-resolution and truncated moment problems in all dimensions|We study the problem of reconstructing a discrete measure on a compact set K subset Rn from a finite set of moments (possibly known only approximately) via convex optimization. We give new uniqueness results, new quantitative estimates for approximate recovery and a new sum-of-squares based hierarchy for approximate super-resolution on compact semi-algebraic sets.
C60|Equitable Voting Rules|A celebrated result in social choice is May's Theorem (May, 1952), providing the foundation for majority rule. May's crucial assumption of symmetry, often thought of as a procedural equity requirement, is violated by many choice procedures that grant voters identical roles. We show that a modification of May's symmetry assumption allows for a far richer set of rules that still treat voters equally, but have minimal winning coalitions comprising a vanishing fraction of the population. We conclude that procedural fairness can coexist with the empowerment of a small minority of individuals. Methodologically, we introduce techniques from discrete mathematics and illustrate their usefulness for the analysis of social choice questions.
C60|A Two-Period Unionized Mixed Oligopoly Model: Public-Private Wage Differentials and â€œEurosclerosisâ€ Reconsidered|In the present paper we develop a two-period unionized mixed duopoly model, furnished with second period- demand shocks, where decentralized firm-specific wage bargains are struck in each period before product market competition is in place.
C60|Pricing in Day-Ahead Electricity Markets with Near-Optimal Unit Commitment|No abstract is available for this item.
C60|Reliably Computing Nonlinear Dynamic Stochastic Model Solutions: An Algorithm with Error Formulas|This paper provides a new technique for representing discrete time nonlinear dynamic stochastic time invariant maps. Using this new series representation, the paper augments the usual solution strategy with an additional set of constraints thereby enhancing algorithm reliability. The paper also provides general formulas for evaluating the accuracy of proposed solutions. The technique can readily accommodate models with occasionally binding constraints and regime switching. The algorithm uses Smolyak polynomial function approximation in a way which makes it possible to exploit a high degree of parallelism.
C60|Robust comparative statics for non-monotone shocks in large aggregative games|A policy change that involves a redistribution of income or wealth is typically controversial, affecting some people positively but others negatively. In this paper we extend the “robust comparative statics” result for large aggregative games established by Acemoglu and Jensen (2010) to possibly controversial policy changes. In particular, we show that both the smallest and the largest equilibrium values of an aggregate variable increase in response to a policy change to which individuals' reactions may be mixed but the overall aggregate response is positive. We provide sufficient conditions for such a policy change in terms of distributional changes in parameters.
C60|The value of foresight in the drybulk freight market|"We analyze the value of foresight in the drybulk freight market when repositioning a vessel through space and time. In order to do that, we apply an optimization model on a network with dynamic regional freight rate differences and stochastic travel times. We evaluate the value of the geographical switching option for three cases: the upper bound based on having perfect foresight, the lower bound based on a ""coin flip"", and the case of perfect foresight but only for a limited horizon. By combining a neural network with optimization, we can assess the impact of varying foresight horizon on economic performance. In a simple but realistic two-region case, we show empirically that the upper bound for large vessels can be as high as 25% cumulative outperformance, and that a significant portion of this theoretical value can be captured with limited foresight of several weeks. Our research sheds light on the important issue of spatial efficiency in global ocean freight markets and provides a benchmark for the value of investing in predictive analysis."
C60|Can an Emission Trading Scheme really reduce CO2 emissions in the short term? Evidence from a maritime fleet composition and deployment model|Global warming has become one of the most popular topics on this planet in the past decades, since it is the challenge that needs the efforts from the whole mankind. Maritime transportation, which carries more than 90% of the global trade, plays a critical role in the contribution of green house gases (GHGs) emission. Unfortunately, the GHGs emitted by the global fleet still falls outside the emission reduction scheme established by the Kyoto Protocol. Alternative solutions are therefore strongly desired. Several market-based measures are proposed and submitted to IMO for discussion and evaluation. In this paper, we choose to focus on one of these measures, namely Maritime Emissions Trading Scheme (METS). An optimization model integrating the classical fleet composition and deployment problem with the application of ETS (global or regional) is proposed. This model is used as a tool to study the actual impact of METS on fleet operation and corresponding CO2 emission. The results of the computational study suggest that in the short term the implementation of METS may not guarantee further emission reduction in certain scenarios. However, in other scenarios with low bunker price, high allowance cost or global METS coverage, a more significant CO2 decrease in the short term can be expected.
C60|Inequality in an OLG economy with heterogeneous cohorts and pension systems|Abstract We analyze the consumption and wealth inequality in an OLG model with mandatory pension systems. Our framework features within-cohort heterogeneity of endowments and heterogeneity of preferences. We allow for population aging and gradual decline in TFP growth. We show four main results. First, increasing longevity translates to substantial increases in aggregate consumption inequality and wealth inequality. Second, a pension system reform from a defined benefit to a defined contribution works to reinforce consumption inequality and reduce wealth inequality. Third, minimum pension benefits are able to partially counteract an increase in inequality introduced by the defined contribution system, at a fiscal cost. Fourth the minimum pension benefit guarantee mostly addresses the sources of inequality which stem from differentiated endowments rather than those which stem from heterogeneous preferences.
C60|Inequality in an OLG economy with heterogeneous cohorts and pension systems|Abstract We analyze the consumption and wealth inequality in an OLG model with mandatory pension systems. Our framework features within-cohort heterogeneity of endowments and heterogeneity of preferences. We allow for population aging and gradual decline in TFP growth. We show four main results. First, increasing longevity translates to substantial increases in aggregate consumption inequality and wealth inequality. Second, a pension system reform from a defined benefit to a defined contribution works to reinforce consumption inequality and reduce wealth inequality. Third, minimum pension benefits are able to partially counteract an increase in inequality introduced by the defined contribution system, at a fiscal cost. Fourth the minimum pension benefit guarantee mostly addresses the sources of inequality which stem from differentiated endowments rather than those which stem from heterogeneous preferences.
C60|A Lattice Test for Additive Separability|We derive necessary and sufficient conditions for a finite data set of price and demand observations to be consistent with an additively separable preference. We do so without imposing concavity on any of the subutility functions or convexity of the budget set a priori, thereby generalizing earlier results. Our simple and intuitive lattice test easily accommodates departures from rationality, or errors, which subsequently facilitates a rich empirical analysis. We apply our econometric techniques to the food consumption of a panel of British households. The primary empirical finding is that additive separability has considerable success in explaining the data.
C60|Two Stage 2 × 2 Games With Strategic Substitutes and Strategic Heterogeneity|Feng and Sabarwal (2018) show that there is additional scope to study strategic complements in extensive form games, by investigating in detail the case of two stage, 2×2 games. We show the same for two stage, 2 × 2 games with strategic substitutes and with strategic heterogeneity. We characterize strategic substitutes and strategic heterogeneity in such games, and show that the set of each class of games has infinite Lebesgue measure. Our conditions are easy to apply and yield uncountably many examples of such games, indicating greater possibilities for the manifestation and study of these types of interactions. In contrast to the case for strategic complements, we show that generically, the set of subgame perfect Nash equilibria in both classes of games is totally unordered (no two equilibria are comparable). Consequently, with multiple equilibria, some nice features of strategic complements that depend on the complete lattice structure of the equilibrium set may not transfer to the case of strategic substitutes or strategic heterogeneity.
C60|Anomaly detection in streaming nonstationary temporal data|This article proposes a framework that provides early detection of anomalous series within a large collection of non-stationary streaming time series data. We define an anomaly as an observation that is very unlikely given the recent distribution of a given system. The proposed framework first forecasts a boundary for the system's typical behavior using extreme value theory. Then a sliding window is used to test for anomalous series within a newly arrived collection of series. The model uses time series features as inputs, and a density-based comparison to detect any significant changes in the distribution of the features. Using various synthetic and real world datasets, we demonstrate the wide applicability and usefulness of our proposed framework. We show that the proposed algorithm can work well in the presence of noisy non-stationarity data within multiple classes of time series. This framework is implemented in the open source R package oddstream. R code and data are available in the supplementary materials.
C60|Systematic Systemic Stress Tests|For a given set of banks, which economic and financial scenarios will lead to big losses? How big can losses in such scenarios possibly get? These are the two central questions of macro stress tests. We believe that most current macro stress testing models have deficits in answering these questions. They select stress scenarios in a way which might leave aside many dangerous scenarios and thus create an illusion of safety; and which might consider highly implausible scenarios and thus trigger a false alarm. With respect to loss evaluation most stress tests do not include tools to analyse systemic risk arising from the interactions of banks with each other and with the markets. We make a conceptual proposal how these shortcomings may be addressed and how stress tests could be made both systematic and systemic. We demonstrate the application of our concepts using publicly available data on European banks and capital markets, in particular the EBA 2016 stress test results.
C60|Stability and Universal Implementability of the Price Mechanism|An axiomatic characterization of price or market mechanisms is one of the most important problems in general equilibrium theory. Based on the general equilibrium framework and the characterization of the price mechanism, this paper provides a new perspective or a uni ed viewpoint on some axioms in social choice theory and a setting for the informational efficiency problem of the allocation mechanisms. Our arguments focus on a contemporary reconsideration and generalization of the category theoretic method in Sonnenschein (1974) and the replica stability arguments in social choice theory like Thom- son (1988) and Nagahisa (1994). Sonnenschein's axiomatic characterization of the price mechanism is extended to an economy-dependent welfare form of a universal implementability theorem. The frame- work provides new methods and general settings in treating mechanisms with messages or information, and many social choice axioms. Our result also has an important economic interpretation that the price mechanism can be characterized as a universal rule that is stable in assuring sufficiently high utility levels for each member of a small economy relative to its large expansions.
C60|Stochastic programs with binary distributions: Structural properties of scenario trees and algorithms|Binary random variables often refer to such as customers that are present or not, roads that are open or not, machines that are operable or not. At the same time, stochastic programs often apply to situations where penalties are accumulated when demand is not met, travel times are too long, or profits too low. Typical for these situations is that the penalties imply a partition of the scenarios into two sets: Those that can result in penalties for some decisions, and those that never lead to penalties. We demonstrate how this observation can be used to efficiently calculate out-of-sample values, find good scenario trees and generally simplify calculations. Most of our observations apply to general integer random variables, and not just the 0/1 case.
C60|Scrubber: a potentially overestimated compliance method for the Emission Control Areas - The importance of involving a ship's sailing pattern in the evaluation|Different methods for sulphur emission reductions, available to satisfy the latest Emission Control Areas (ECA) regulations, may lead to different sailing patterns (route and speed choices of a vessel) and thus have significant impact on a shipping company's operating costs. However, the current literature does not include sailing pattern optimization caused by ECA, and its corresponding cost effects, in the evaluation and selection process for sulphur abatement technology. This leads to an inaccurate estimation of the value of certain technologies and hence an incorrect investment decision. In this paper, we integrate the optimization of a ship's sailing pattern into the lifespan cost assessment of the emission control technology, so that such expensive and irreversible decisions can be made more accurately. The results shows that a considerable overestimation of the value of scrubbers, and thus a substantial loss, can occur if the sailing pattern of a ship is not considered in the decision-making process. Furthermore, we also illustrate that it is more important to involve a ship's sailing pattern when the port call density inside ECA is low.
C60|Market Power Under Nodal and Zonal Congestion Management Techniques|Contrary to the common thought that nodal pricing provides more opportunities for a strategic player to exert market power than the zonal model, we show that in the latter one because of the need for re-dispatch or counter-trading, another extra place is created letting more gaming possibilities. Therefore, if proper market power mitigation approaches are not utilized in both day-ahead and re-dispatch markets, then zonal pricing may be more susceptible to market power, especially in zonal model which is based on available transfer capacity (ATC), strategic player's profit and social welfare can be very volatile. In general, the more network constraints are incorporated in day-ahead market (100% in nodal and almost zero in ATC), the more social welfare is attainable. Hence, nodal model is acquitted from the more market power denunciation.
C60|Sectoral scope and colocalisation of Spanish manufacturing industries|Abstract In this paper, we use distance-based methods, specifically a slight variation of Ripley’s K function and a bivariate generalisation of this function, to explore the detailed location pattern of the Spanish manufacturing industry, the scope of localisation and the tendency towards colocalisation between horizontally and vertically linked industries. To do so, we use micro-geographic data, considering a narrowly defined industry classification. Our results show heterogeneous location patterns, but with a significant tendency towards localisation. The sectoral scope is very sensitive to the degree of homogeneity of the activities in each sector. The more homogeneous the activities in a specific sector are, the more similarities we find in the spatial location patterns among its industries. Finally, although the patterns of colocalisation detected are sensitive to the counterfactuals used, between 20 and 48% of the pairs of industries with strong input–output linkages considered in this study show a significant tendency to colocalisation, and among them 74% are vertically linked industries.
C60|Multi-objective local environmental simulator (MOLES 1.0): Model specification, algorithm design and policy applications|This paper describes MOLES 1.0, an integrated land-use and transport model developed with Object-Oriented Programming principles in order to combine selected characteristics from Spatial Computable General Equilibrium and microsimulation models.
C60|Local Independence, Monotonicity, Incentive Compatibility and Axiomatic Characterization of Price-Money Message Mechanism|To characterize money in a static economic model, it is known to be important to consider the agentcommodity double-infinity settings, i.e., the overlapping-generations framework. There does not seem to exist abundant literature, however, treating the axiomatic characterization problems for such monetary Walras allocations under the social choice and/or mechanism design settings. We show that the monetary Walras allocation for the economy with double infinities is characterized by weak Paretooptimality, individual rationality and local independence or the monotonicity, or the incentive compatibility conditions of social choice correspondence among the allocation mechanisms with messages under the category theoretic approach in Sonnenschein (1974). We utilize Sonnenschein fs market extension axiom for swamped economies that is closely related to the replica stability axiom of Thomson (1988). We can see how these conditions characterize the price-money message mechanism universally among a wide class of mechanisms, and efficiently in the sense that it has the minimal message spaces (pricemoney dictionary theorems). Moreover, by using the category theoretic framework, we can obtain the up-to-isomorphism uniqueness for such a dictionary object (isomorphism theorems).
C60|Stability and Universal Implementability of the Price Mechanism|This paper provides a uni ed viewpoint on some axioms in social choice theory and a setting for the allocation mechanism with messages in the informational efficiency problem. In particular, our arguments are concerned with the category theoretical axiomatic method in Sonnenschein (1974) and the replica stability axiom in the social choice arguments like Thomson (1988) and Nagahisa (1994). The uni ed view enables us to obtain an extension of Sonnenschein's axiomatic characterization of the price mechanism as an agent-characteristics form dictionary property to a utility form economy- dependent universal implementability theorem.
C60|A theory of sequential group reciprocity|Abstract Games that appear to be independent, involving none of the same players, may be related by emotions of reciprocity between the members of the same groups. In the real world, individuals are members of groups and want to reward or punish those groups whose members have been kind or unkind to members of their own. In this paper, we extend Dufwenberg and Kirchsteiger’s model of sequential reciprocity (Games Econ Behav 47(2):268–298, 2004) to groups of individuals and define a new “sequential group reciprocity equilibrium” for which we prove its existence. We study the case of two games with two players in each game, where each player belongs to the same group as a player in the other game. We show that when the payoffs of one game are much higher than the payoffs of the other, the outcome of the game with higher payoffs determines the outcome of the other game. We also find that when the payoffs are very asymmetric, the outcome where the sum of the payoffs is maximized is a sequential group reciprocity equilibrium.
C60|Improving Algebraic Thinking Skill, Beliefs And Attitude For Mathematics Throught Learning Cycle Based On Beliefs|In the recent years, problem-solving become a central topic that discussed by educators or researchers in mathematics education. it’s not only as the ability or as a method of teaching. but also, it is a little in reviewing about the components of the support to succeed in problem-solving, such as student's belief and attitude towards mathematics, algebraic thinking skills, resources and teaching materials. In this paper, examines the algebraic thinking skills as a foundation for problem-solving, and learning cycle as a breath of continuous learning. In this paper, learning cycle to be used is a modified type of 5E based on beliefs.
C60|A Path Integral Approach to Interacting Economic Systems with Multiple Heterogeneous Agents|This paper presents an analytical treatment of economic systems with an arbitrary number of agents that keeps track of the systems' interactions and agent's complexity. The formalism does not seek to aggregate agents: it rather replaces the standard optimization approach by a probabilistic description of the agents' behaviors and of the whole system. This is done in two distinct steps. A first step considers an interacting system involving an arbitrary number of agents, where each agent's utility function is subject to unpredictable shocks. In such a setting, individual optimization problems need not be resolved. Each agent is described by a time-dependent probability distribution centered around its utility optimum. The whole system of agents is thus defined by a composite probability depending on time, agents' interactions, relations of strategic dominations, agents' information sets and expectations. This setting allows for heterogeneous agents with different utility functions, strategic domination relations, heterogeneity of information, etc. This dynamic system is described by a path integral formalism in an abstract space – the space of the agents' actions –and is very similar to a statistical physics or quantum mechanics system. We show that this description, applied to the space of agents' actions, reduces to the usual optimization results in simple cases. Compared to the standard optimization, such a description markedly eases the treatment of a system with a small number of agents. It becomes however useless for a large number of agents. In a second step therefore, we show that, for a large number of agents, the previous description is equivalent to a more compact description in terms of field theory. This yields an analytical, although approximate, treatment of the system. This field theory does not model an aggregation of microeconomic systems in the usual sense, but rather describes an environment of a large number of interacting agents. From this description, various phases or equilibria may be retrieved, as well as the individual agents' behaviors, along with their interaction with the environment. This environment does not necessarily have a unique or stable equilibrium and allows to reconstruct aggregate quantities without reducing the system to mere relations between aggregates. For illustrative purposes, this paper studies several economic models with a large number of agents, some presenting various phases. These are models of consumer/producer agents facing binding constraints , business cycle models, and psycho-economic models of interacting and possibly strategic agents.
C60|Regional environmental efficiency in waste generation|This paper employs Data Envelopment Analysis (DEA) to consider waste generation at a regional level in the European Union (EU). By doing so both good and bad outputs are taken into account and different frameworks are designed. Five parameters (waste generation, employment rate, capital formation, GDP and population density) are used for 172 EU regions and for the years 2009, 2011 and 2013. In doing so four frameworks have been designed with different inputs and outputs each time. The results show the more efficient EU regions according to each framework, but it should be noted that results from different frameworks should not be compared to each other. Overall results suggest that the highest performers are regions in Belgium, Italy, Portugal and the UK. Finally the efficiency results from DEA were reviewed against the treatment options employed in the relevant regions. Our findings show that although a country might be efficient according to DEA and by taking many factors into consideration, it is not necessary that regions within a country use sustainable waste treatment options as it is essential to account for trade and shipment of waste between regions and countries as well.
C60|What is the impact of the policy framework on the future of district heating in Eastern European countries? The case of Brasov|District heating in general is seen as an important opportunity to decarbonise the heating sector especially in urban areas and therefore important to reach European and global climate goals. In this case study we analyse possible future scenarios for the city of Brasov, Romania. Like in many other cities in Eastern Europe a district heating system exists in the city, however, facing severe challenges like old and inefficient infrastructure and loss of consumers due to unreliability of supply over the last decades. This work assesses the impact of different policies on the feasibility of renewable and efficient heating under various conditions and suggests favourable policy frameworks to ensure an economically and ecologically viable future heating system for the city.
C60|Revealed preferences over risk and uncertainty| Consider a finite data set where each observation consists of a bundle of contingent consumption chosen by an agent from a constraint set of such bundles. We develop a general procedure for testing the consistency of this data set with a broad class of models of choice under risk and under uncertainty. Unlike previous work, we do not require that the agent has a convex preference, so we allow for risk loving and elation seeking behavior. Our procedure can also be extended to calculate the magnitude of violations from a particular model of choice, using an index first suggested by Afriat (1972, 1973). We then apply this index to evaluate different models (including expected utility and disappointment aversion) in the data collected by Choi et al. (2007). We show that among those subjects exhibiting choice behavior consistent with the maximization of some increasing utility function, more than half are consistent with models of expected utility and disappointment aversion.
C60|Economic Analysis of Price Premiums in the Presence of Non-convexities - Evidence from German Electricity Markets|Analyzing price data from sequential German electricity markets, namely the day-ahead and intraday auction, a puzzling but apparently systematic pattern of price premiums can be identified. The price premiums are highly correlated with the underlying demand profile. As there is evidence that widespread models for electricity forward premiums are not applicable to the market dynamics under analysis, a theoretical model is developed within this article which reveals that non-convexities in only a subset of sequential markets with differing product granularity may cause systematic price premiums at equilibrium. These price premiums may be bidirectional and reflect a value for additional short-term power supply system flexibility.
C60|Revealed preferences over risk and uncertainty|Consider a finite data set where each observation consists of a bunde of contingent consumption chosen by an agent from a constraint set of such bundles. We develop a general procedure for testing the consistency of this data set with a broad class of models of choice under risk and under uncertainty. Unlike previous work, we do not require that the agent has a convex preference, so we allow for risk loving and elation seeking behavior. Our procedure can also be extended to calculate the magnitude of violations from a particular model of choice, using an index first suggested by Afriat (1972, 1973). We then apply this index to evaluate different models (including expected utility and disappointment aversion) in the data collected by Choi et al. (2007). We show that more than half of all subjects exhibiting choice behavior consistent with utility maximization are also consistent with models of expected utility and disappointment aversion.
C60|Maximal elements of quasi upper semicontinuous preorders on compact spaces|Abstract We introduce the concept of quasi upper semicontinuity of a not necessarily total preorder on a topological space and we prove that there exists a maximal element for a preorder on a compact topological space provided that it is quasi upper semicontinuous. In this way, we generalize many classical and well known results in the literature. We compare the concept of quasi upper semicontinuity with the other semicontinuity concepts to arrive at the conclusion that our definition can be viewed as the most appropriate and natural when dealing with maximal elements of preorders on compact spaces.
C60|On a Class of Smooth Preferences|We construct a complete space of smooth strictly convex preference relations defined over physical commodities and monetary transfers. This construction extends the classic one by assuming that preferences are monotone in transfers, but not necessarily in all commodities. This provides a natural framework to perform genericity analyses in situations involving inventory costs or decisions under risk.
C60|The transformation function, technical efficiency, and the CCR ratio|Charnes, Cooper, and Rhodes define the ratio of the virtual output to the virtual input as a measure of the technical efficiency of a multiple output multiple input firm. The aggregation weights used in construction the virtual output and the virtual input may be arbitrarily chosen so long as the weights are non-negative and using these weights no firm's input-output bundle shows efficiency exceeding 100%. In production economics, the ratio of aggregate output to aggregate input is a measure of total factor productivity and a direct link of the Charnes, Cooper, and Rhodes ratio to technical efficiency is not obvious. Usually the ratio measure is rationalized as efficiency by showing its equivalence to the Farrell efficiency measure. This paper offers a direct derivation of the ratio measure of efficiency from a transformation function. We also show how Banker, Charnes, and Cooper measure under variable returns to scale can be derived from the transformation function.
C60|Optimal Social Insurance and Health Inequality|This paper integrates into public economics a biologically founded, stochastic process of individual ageing. The novel approach enables us to investigate the interaction between health and retirement policy in order to quantitatively characterize the optimal joint design of the social insurance system today and in response to future medical progress, and its implications for health inequality. Calibrating our model to Germany, we find that currently the public health and pension system is approximately optimal. Future progress in medical technology calls for a potentially drastic increase in health spending that typically shall be accompanied with a lower pension savings rate and a higher retirement age. Medical progress and higher health spending is predicted to lead to more health inequality.
C60|Die ökonomische Lehrbuchwissenschaft: Zum interdisziplinären Selbstverständnis der Volkswirtschaftslehre|Die moderne Volkswirtschaftslehre wirkt auf andere Wissenschaften, auf die Gesellschaft und die Politik ein. Im Kontext dazu wird in dem Paper das disziplinäre Selbstverständnis von Ökonominnen und Ökonomen herausgearbeitet, wie es den wichtigsten Lehrbüchern des Faches inhärent ist. Anhand von Thomas S. Kuhns Konzept der Lehrbuchwissenschaft wird die zentrale Bedeutung des Lehrbuches für die moderne Volkswirtschaftslehre aufgezeigt und dann im Rückgriff auf einschlägige Lehrbuchliteratur des Faches untersucht, welche wissenschaftshistorischen, methodologischen und didaktischen Grundpositionen darin festgehalten sind und im Rahmen akademischer ökonomischer Bildung vermittelt werden.
C60|A Simple Framework for Climate-Change Policy under Model Uncertainty|"We propose a novel framework for the economic assessment of climate-change policy. Our main point of departure from existing work is the adoption of a ""satisficing"", as opposed to optimizing, modeling approach. Along these lines, we place primary emphasis on the extent to which different policies meet a set of goals at a specific future date instead of their performance vis-à-vis some intertemporal objective function. Consistent to the nature of climate-change policy making, our model takes explicit account of model uncertainty. To this end, the value function we propose is an analogue of the well-known success-probability criterion adapted to settings characterized by model uncertainty. We apply this decision criterion to probability distributions constructed by Drouet et al. (2015) linking carbon budgets to future consumption. The main result that emerges is the superiority of ""medium"" carbon budgets in line with a 3°C target (i.e., 2000-3000 GtCO2) in preventing large future consumption losses with high probability. Insights from computational geometry facilitate computations considerably, and allow for the efficient application of the model in high-dimensional settings."
C60|An interdisciplinary model for macroeconomics|Macroeconomic modelling has been under intense scrutiny since the Great Financial Crisis, when serious shortcomings were exposed in the methodology used to understand the economy as a whole. Criticism has been levelled at the assumptions employed in the dominant models, particularly that economic agents are homogeneous and optimizing and that the economy is equilibrating. This paper seeks to explore an interdisciplinary approach to macroeconomic modelling, with techniques drawn from other (natural and social) sciences. Specifically, it discusses agent-based modelling, which is used across a wide range of disciplines, as an example of such a technique. Agent-based models are complementary to existing approaches and are suited to answering macroeconomic questions where complexity, heterogeneity, networks, and heuristics play an important role.
C60|Impactos estruturais do programa Bolsa família na dinâmica recente da economia brasileira|The aim of this study is to introduce a methodology to project impacts of cash transfers programs, which we exemplify through the evaluation of the BolsaFamília Program in a recent period of the Brazilian economy (2009-2015). An original dynamic recursive computable general equilibrium model, modified to consider issues related to income distribution and their impact on households consumption levels as well as on sectoral output was applied. The results suggest that the program also generates income gains for classes which do not receive cash transfers from Government by its indirect effects on labor and capital income, but has effects on labor income inequality decrease and on productive structure. We conclude that cash transfers policieshave important impacts over the process of development of the country, even though its effect on growth is small.
C60|Assimetrias na tributação da renda do trabalho e do capital : impactos de modificações na estrutura de tributação da renda de pessoa física no Brasil|This paper discusses the asymmetry between labor and capital (profits and distributed dividends) income taxation in Brazil and simulates changes in the personal income tax structure in a neutral approach. An original dynamic recursive computable general equilibrium modelis applied. The results indicate that a more progressive taxation of personal income would lead to a drop in household income inequality in the Brazilian economy. Consumption, Investment and production oriented to domestic market would be encouraged. However, the impacts on inequality and on the economy are small given the small representativeness of the personal income tax in the Brazilian tax base. We conclude that for effective changes towards a more progressive tax structure in Brazil it is needed to study a higher taxation on capital income associated to a decreaseof consumption taxes
C60|Uniform Integrability of a Single Jump Local Martingale with State-Dependent Characteristics|We investigate a deterministic criterion to determine whether a diffusive local martingale with a single jump and state-dependent characteristics is a uniformly integrable martingale. We allow the diffusion coefficient, the jump hazard rate and the relative jump size to depend on the state and prove that the process is a uniformly integrable martingale if and only if the relative jump size is bounded away from one and the hazard rate is large enough compared to the diffusion component. The result helps to classify seemingly explosive behaviour in diffusive local martingales compensated by the existence of a jump. Moreover, processes of this type can be used to model financial bubbles in stock prices as deviation from the fundamental value. We present a simple framework to illustrate this application.
C60|Dynamic Mean-Variance Optimisation Problems with Deterministic Information|We solve the problems of mean-variance hedging (MVH) and mean-variance portfolio selection (MVPS) under restricted information. We work in a setting where the underlying price process S is a semimartingale, but not adapted to the filtration G which models the information available for constructing trading strategies. We choose as G = Fdet the zero-information filtration and assume that S is a time-dependent affine transformation of a square-integrable martingale. This class of processes includes in particular arithmetic and exponential Lévy models with suitable integrability. We give explicit solutions to the MVH and MVPS problems in this setting, and we show for the Lévy case how they can be expressed in terms of the Lévy triplet.
C60|Kalman on dynamics and contro, Linear System Theory, Optimal Control, and Filter|Rudolf Emil Kalman (“R.E.K.”) passed away on July, 2nd, 2016. Among contemporary economists Kalman is mainly remembered for his filter, an algorithm that allows recursive estimation of unobserved time varying variables in a system. However, he has also a key part on the whole of recursive macroeconomic theory as is notably expressed by Lars Ljungqvist’s and Thomas Sargent’s book [Ljunqvist and Sargent, 2012]. Our paper is a contribution to show the links between Kalman’s works on filtering, linear quadratic optimal control, and system theory. We also provide a model on cooperative advertising to show that Kalman’s works on dynamics and control can be useful in macroeconomics as in microeconomics, a domain where his contributions seem to be unfortunately less used.
C60|Nonstationary Z-Score measures|In this work we develop advanced techniques for measuring bank insolvency risk. More specifically, we contribute to the existing body of research on the Z-Score. We develop bias reduction strategies for state-of-the-art Z-Score measures in the literature. We introduce novel estimators whose aim is to effectively capture nonstationary returns; for these estimators, as well as for existing ones in the literature, we discuss analytical confidence regions. We exploit moment-based error measures to assess the effectiveness of these estimators. We carry out an extensive empirical study that contrasts state-of-the-art estimators to our novel ones on over ten thousand banks. Finally, we contrast results obtained by using Z-Score estimators against business news on the banking sector obtained from Factiva. Our work has important implications for researchers and practitioners. First, accounting for nonstationarity in returns yields a more accurate quantification of the degree of solvency. Second, our measure allows researchers to factor in the degree of uncertainty in the estimation due to the availability of data.
C60|Effects of common factors on stock correlation networks and portfolio diversification|This study empirically investigates the effects of common factors on the connectivity of the network among stocks and on the distribution of the investment weights for stocks. The network is defined as a stock correlation network from the minimal spanning tree (MST), and portfolio is defined as an efficient portfolio from the Markowitz mean-variance (MV) optimization function (MVOF). For these research goals, we devise a method using the comparative correlation matrix (C-CM), which does not have the property of a single common factor included in the sample correlation matrix (S-CM). The results reveal that common factors clearly affect the changes of connectivity among stocks in the networks, and that their influence is much greater on stocks with many links to other stocks in the network. Further, common factors significantly affect the determination of the investment weight's distribution for stocks from the MVOF. In particular, among the common factors, a market factor plays a dominant role in both structuring the network among stocks and in constructing the well-diversified portfolio. In addition, the devised method of the C-CM without the property of the market factor in the S-CM plays a crucial role in constructing a more diversified portfolio with better out-of-sample performance in the future period. These results are robust in both the Korean and the U.S. stocks markets.
C60|Intensity-based framework for surrender modeling in life insurance|In this paper, we propose an intensity-based framework for surrender modeling. We model the surrender decision under the assumption of stochastic intensity and use, for comparative purposes, the affine models of Vasicek and Cox–Ingersoll–Ross for deriving closed-form solutions of the policyholder’s probability of surrendering the policy. The introduction of a closed-form solution is an innovative aspect of the model we propose. We evaluate the impact of dynamic policyholders’ behavior modeling the dependence between interest rates and surrendering (affine dependence) with the assumption that mortality rates are independent of interest rates and surrendering. Finally, using experience-based decrement tables for both surrendering and mortality, we explain the calibration procedure for deriving our model’s parameters and report numerical results in terms of best estimate of liabilities for life insurance under Solvency II.
C60|Ordinal aggregation results via Karlin's variation diminishing property|When is the weighted sum of quasi-concave functions quasi-concave? We answer this, extending an analogous preservation of the single-crossing property in QS: Quah and Strulovici (2012). Our approach develops a general preservation of n-crossing properties, applying the variation diminishing property in Karlin (1956). The QS premise is equivalent to Karlin's total positivity of order two, while our premise uses total positivity of order three: The weighted sum of quasi-concave functions is quasi-concave if each has an increasing portion more risk averse than any decreasing portion.
C60|The tradeoff of the commons under stochastic use|We develop a model of scarce renewable resources to study the problem of the commons. Our model formulation differs from the existing literature in that it assumes the use of the commons to be stochastic in nature. One example is microwave spectrum for mobile and wireless communications. We investigate three mechanisms of resource allocation: free usage, the exclusive franchise, and a regulated monopoly. We show that the welfare tradeoff among these three mechanisms depends on the characteristics of the commons and their usage patterns. In particular, we find that property rights are not always the best solution. We then make four extensions that apply to spectrum allocations.
C60|A typology of distance-based measures of spatial concentration|Over the last decade, distance-based methods have been introduced and then improved in the field of spatial economics to gauge the geographic concentration of activities. There is a growing literature on this theme including new tools, discussions on their specific properties and various applications. However, there is currently no typology of distance-based methods. This paper fills that gap. The proposed classification helps understand all the properties of distance-based methods and proves that they are variations on the same framework.
C60|Portfolio optimization under lower partial moments in emerging electricity markets: Evidence from Turkey|Optimization of the electricity markets under modern portfolio theory has a crucial role for financial decision makers. Power suppliers in deregulated electricity markets need to optimize their generation capacities and bidding strategies so as to effectively participate in bilateral contract and spot markets. Market players have to deal with continuously changing electricity prices in competitive electricity market environment during their daily routine system operations. Electricity not like the others is a unique product/service and cannot be stored economically, however it should be generated and consumed simultaneously. In addition to all, power suppliers face with fuel price, water regime, delivery, and network risks. In view of the scene described above, prudent decision making methodologies are of critical importance to maximize profit while minimizing managing risks.
C60|Uncertainty quantification and global sensitivity analysis for economic models|We present a global sensitivity analysis that quantifies the impact of parameter uncertainty on model outcomes. Specifically, we propose variance‐decomposition‐based Sobol' indices to establish an importance ranking of parameters and univariate effects to determine the direction of their impact. We employ the state‐of‐the‐art approach of constructing a polynomial chaos expansion of the model, from which Sobol' indices and univariate effects are then obtained analytically, using only a limited number of model evaluations. We apply this analysis to several quantities of interest of a standard real‐business‐cycle model and compare it to traditional local sensitivity analysis approaches. The results show that local sensitivity analysis can be very misleading, whereas the proposed method accurately and efficiently ranks all parameters according to importance, identifying interactions and nonlinearities.
C60|A Simple Framework for Climate-Change Policy under Model Uncertainty|" We propose a novel framework for the economic assessment of climate-change policy. Our main point of departure from existing work is the adoption of a ""satisficing"", as opposed to optimizing, modeling approach. Along these lines, we place primary emphasis on the extent to which different policies meet a set of goals at a specific future date instead of their performance vis-à-vis some intertemporal objective function. Consistent to the nature of climate-change policy making, our model takes explicit account of model uncertainty. To this end, the value function we propose is an analogue of the well-known success-probability criterion adapted to settings characterized by model uncertainty. We apply this decision criterion to probability distributions constructed by Drouet et al. (2015) linking carbon budgets to future consumption. The main result that emerges is the superiority of ""medium"" carbon budgets in line with a 3°C target (i.e., 2000-3000 GtCO2) in preventing large future consumption losses with high probability. Insights from computational geometry facilitate computations considerably, and allow for the efficient application of the model in high-dimensional settings."
C60|Une nouvelle approche expérimentale pour tester les modèles quantiques de l’erreur de conjonction|In classical probability theory, the probability of the conjunction of two events is smaller than the probability of only one of these events. Yet, agents do not always empirically judge in this way: this is the traditional conjunction fallacy. One of the currently promising accounts of this paradox relies on so-called quantum-like models, which have been developed from mathematical tools used in quantum theory. But are these models empirically adequate? Which versions of these models can be used? In particular, can the simplest versions, the non-degenerate ones, be sufficient? We propose here an original experimental protocol to test the quantum-like models for the conjunction fallacy in the lab. The results we obtain suggest that the non-degenerate models are not empirically adequate, and that future research on quantum-like models should consider degenerate ones. Classification JEL : C60, C91, D03.
C60|A Path Integral Approach to Interacting Economic Systems with Multiple Heterogeneous Agents|This paper presents an analytical treatment of economic systems with an arbitrary number of agents that keeps track of the systems’ interactions and complexity. The formalism does not seek to aggregate agents: it rather replaces the standard optimization approach by a probabilistic description of the agent’s behavior. This is done in two distinct steps. A first step considers an interaction system involving an arbitrary number of agents, where each agent's utility function is subject to unpredictable shocks. In such a setting, individual optimization problems need not be resolved. Each agent is described by a time-dependent probability distribution centered around its utility optimum. The whole system of agents is thus defined by a composite probability depending on time, agents' interactions, relations of strategic dominations, agents' information sets and expectations. This setting allows for heterogeneous agents with different utility functions, strategic domination relations, heterogeneity of information, etc. This dynamic system is described by a path integral formalism in an abstract space -- the space of the agents' actions -- and is very similar to a statistical physics or quantum mechanics system. We show that this description, applied to the space of agents' actions, reduces to the usual optimization results in simple cases. Compared to the standard optimization, such a description markedly eases the treatment of a system with a small number of agents. It becomes however useless for a large number of agents. In a second step therefore, we show that, for a large number of agents, the previous description is equivalent to a more compact description in terms of field theory. This yields an analytical, although approximate, treatment of the system. This field theory does not model an aggregation of microeconomic systems in the usual sense, but rather describes an environment of a large number of interacting agents. From this description, various phases or equilibria may be retrieved, as well as the individual agents’ behaviors, along with their interaction with the environment. This environment does not necessarily have a unique or stable equilibrium and allows to reconstruct aggregate quantities without reducing the system to mere relations between aggregates. For illustrative purposes, this paper studies several economic models with a large number of agents, some presenting various phases. These are models of consumer/producer agents facing binding constraints, business cycle models, and psycho-economic models of interacting and possibly strategic agents.
C60|Measuring Economic Growth Using Data Envelopment Analysis|Exploring and explaining development gaps between countries is an important theoretical and empirical task. This paper presents empirical studies related to economic growth and its determinants across countries, based on the use of data envelopment analysis method. It emphasizes the importance of this nonparametric approach to macroeconomic efficiency analysis and provides a broader and more comprehensive perspective to the researchers on this issue.
C60|Exploring the Community Structure of Complex Networks|Regarding complex networks, one of the most relevant problems is to understand and to explore community structure. In particular it is important to define the network organization and the functions associated to the different network partitions. In this context, the idea is to consider some new approaches based on interval data in order to represent the different relevant network components as communities. The method is also useful to represent the network community structure, especially the network hierarchical structure. The application of the methodologies is based on the Italian interlocking directorship network.
C60|Land Requirement, Feedstock Haul Distance, and Expected Cost Consequences of Restricting Switchgrass Production to Marginal Land| Energy crop production has been proposed for land of poor quality to avoid competition with food production and negative indirect land use consequences. The objective of this study was to determine the land area requirements, biomass transportation distance, and expected cost consequences of restricting switchgrass biomass production, for use as biofuel feedstock, to marginal land relative to unrestricted land use. The USA soils capability classification system was used to differentiate between high quality land (Class I) and land of marginal quality (Class IV). Switchgrass biomass yield distributions were simulated for each of four land capability classes for counties in the Eastern Oklahoma case study region. For a 70 million gallons per year cellulosic ethanol biorefinery, restricting land use to capability Class IV (defined as marginal) increases the quantity of land required to support the biorefinery by 47%; increases biomass trucking distance by 218%; increases cost to delivery feedstock by 13%; and increases the expected cost to produce a gallon of ethanol by $0.19. In the absence of government restrictions, for-profit companies are not likely to limit energy crop production to land of marginal quality.
C60|Restricting Switchgrass Biomass Feedstock Production to Marginal Land to Limit Competition with Food Production| Production of switchgrass as a dedicated energy crop in the U.S. was proposed as a way to produce valuable products on millions of acres that had been bid from traditional crop production by a variety of federal programs. The objective of the present study is to determine the expected economic consequences in terms of cost to deliver biomass feedstock, from restricting switchgrass production to marginal land for a case study region, when (a) land use is restricted to class IV; (b) land use is restricted to classes III and IV; and (c) use of land capability classes I, II, III, and IV is permitted. A mathematical programming model was constructed and solved to determine the optimal quantity, location, and quality of the land leased. For the case study region, restricting land use to only capability class IV increases the land requirement by 44% and increases the cost to deliver feedstock by 32% compared to when switchgrass production is permitted on land classes I-IV.
C60|Measuring Firms’ Input Congestion with Consideration of Environmental Factors: The Case of European Railway Transport|In practice, input congestion effects appear in railway transport due to the difficulties of disposing of unnecessary input factors. This study measures the output-oriented technical efficiency and input congestion with consideration of categorical variables for railway transport by using the DEA extension approach. The empirical results from 24 European railway companies show that in 12 railways, the presence of weak congestion can be proved. Based on the results of identifying the source(s) of input congestion and further determining its amount, one can obtain more insights into railways’ operation and thus propose more effective strategies for improvement.
C60|Economically Consistent Valuations and Put-Call Parity|We propose an approach to the valuation of contingent claims in general, symmetric semimartingale models of financial markets. We start from two simple, economically motivated axioms, namely absence of arbitrage (in the sense of NUPBR) and absence of relative arbitrage among all buy-and-hold strategies (called static efficiency). We then call a valuation process for a contingent claim economically consistent if the financial market enlarged by that process still satisfies this combination of properties. It turns out that this approach lies in the middle between the extremes of valuing by risk-neutral expectation or by absence of arbitrage alone. We show that this always yields put-call parity, although put and call values themselves can be nonunique, even for complete markets. We provide general formulas for put and call values in complete markets and show that these are symmetric and that both contain in general three terms. We also show that our approach contains all the put-call parity respecting valuation formulas in the classic theory as special cases, and we explain precisely when and how the different terms in the put and call valuation formulas disappear or simplify.
C60|Intrinsic Risk Measures|Monetary risk measures are usually interpreted as the smallest amount of external capital that must be added to a financial position to make it acceptable. We propose a new concept: intrinsic risk measures and argue that this approach provides a direct path from unacceptable positions towards the acceptance set. Intrinsic risk measures use only internal resources and return the smallest percentage of the currently held financial position which has to be sold and reinvested into an eligible asset such that the resulting position becomes acceptable. While avoiding the problem of infinite values, intrinsic risk measures allow a free choice of the eligible asset and they preserve desired properties such as monotonicity and quasi-convexity. A dual representation on convex acceptance sets is derived and the link of intrinsic risk measures to their monetary counterparts on cones is detailed.
C60|Technology modelling and technology innovetion. How a technology model may be useful in studying the innovation process|This work concerns an extension of a mathematical model of technology developed at the Santa Fe Institute in the late nineties. It is based on analogies existing between technological and biological evolution and not on economic principles. This extension has the purpose to make the model useful in the studies of the innovation process. The model considers technology activity, independently of possible economic purposes, and having its own properties, structure, processes as well as an evolution independently by economic factors but more similar to biologic evolution. Considered purpose of technology is reaching of a technical result and not necessarily an economic result. The model considers technology as a structured set of technological operations that may be represented by a graph or matrix. That opens a description of a technology in term of technological spaces and landscapes, as well as in term of spaces of technologies, in which it is possible to represent search of optimal and evolutive paths of technologies, changes in their efficiency and measure of their radical degree linked to their technological competitiveness. The model is presented in a descriptive way and its mathematical development is presented in annex. The main applications of the model concern the use of the defined radical degree of a technology linked to its technological competitiveness. In this way it is explained the existence of Red Queen Regimes, characterized by continuous technical but not economical developments, among firms producing the same product. Such regimes are disrupted only by the entering of a technology with a high radical degree. Changes in operational structure of technologies may suggest the existence of three types of technology innovations, the first concerning learning by doing and consisting in minor changes giving incremental innovations, the second and the third, both able to obtain radical innovations through R&D activity, but the second exploiting scientific results and the third based only on a combinatory process of pre-existing technologies. This last way of innovation may explain the innovative potential, existing for example in Italian industrial districts, without resorting to any scientific research.Length: 38 pages
C60|Multiple Attribute Group Decision Making Methods Based on Intuitionistic Fuzzy Generalized Hamacher Aggregation Operator|With respect to multiple attribute group decision making (MAGDM) problems in which attribute values take the form of the intuitionistic fuzzy values(IFVs), the group decision making method based on some generalized Hamacher aggregation operators which generalized the arithmetic aggregation operators and geometric aggregation operators and extended the Algebraic aggregation operators and Einstein aggregation operators, is developed. Firstly, the generalized intuitionistic fuzzy Hamacher weighted averaging(IFGHWA) operator, intuitionistic fuzzy generalized Hamacher ordered weighted averaging(IFGHOWA) operator, and intuitionistic fuzzy generalized Hamacher hybrid weighted averaging(IFGHHWA) operator, were proposed, and some desirable properties of these operators, such as commutativity, idempotency, monotonicity and boundedness, were studied. At the same time, some special cases in these operators were analyzed. Furthermore, one method to multi-criteria group decision-making based on these operators was developed, and the operational processes were illustrated in detail. Finally, an illustrative example is given to verify the proposed methods and to demonstrate their practicality and effectiveness.
C60|A Probabilistic Approach To Setting Weights In A Weighted-Average Product Evaluation|This paper presents a procedure that estimates how markets perceive a product. A weighted-average approach is used as a model for the product perception. Since this approach requires weights to be assigned to product features, and the weights are unknown and vary among customers, the procedure focuses on estimating expected weights of the entire market – such weights are representative weights. The estimation is performed by constructing a confidence interval for the weights. Further, a more accurate location of the weights in the interval is proposed, the accuracy being based on additional information regarding inferior products. This information suggests that if a product is inferior, its value must be low. Therefore, the weights from the interval which minimize the value of an inferior product are suggested, the minimization serving as an approximation of what low value means. Since a minimization is involved, its existence and uniqueness is discussed.
C60|Does the exchange rate regime shape currency misalignments in emerging and developing countries?|Relying on a panel of 73 emerging and developing countries and on de facto exchange rate regimes classification over the 1980-2012 period, we re-examine empirically the relationship between exchange rate regimes and currency misalignments. Overall our results suggest that no exchange rate regime performs better than the others as currency misalignments do not substantially and significantly differ across exchange rate regimes. This finding is in contrast to the different arguments (both theoretical and empirical) in favor or against any particular regime and instead supports the exchange regime neutrality view.
C60|The Risk Parity Principle applied on a Corporate Bond Index using Duration Times Spread|In this paper, we apply the principle of Equal Risk Contribution (ERC) to a corporate bond index, an asset class so far left behind in this literature. Specifically, we rely on the Duration Time Spread (DTS) and demonstrate that it is an coherent metric for bond risk. We construct indexes based on sector - issuer - and bond level using structured block correlation matrices, weights being inversely proportional to DTS. Our results provide evidence that applying ERC using DTS in the index design significantly improves corporate bond index risk-adjusted returns. It appears that the higher the granularity is, the higher will be the risk adjusted performance enhancements. More generally, the ERC application we present appears to be a valuable trade-off between heuristic and more complex risk-modeling based weighting schemes.
C60|Blanchard and Kahn’s (1980) solution for a linear rational expectations model with one state variable and one control variable: the correct formula|This note corrects Blanchard and Kahn’s (1980) solution for a linear dynamic rational expectations model with one state variable and one control variable.
C60|Diminished-dimensional political economy|Economists base policy advice on models of responses by a variety of economic entities to policy adoptions. There is compelling evidence that these entities do not optimize as mainstream economics assumes. Rather, they limit decision-making to solving problems of much smaller dimensionality. We consider how political economy goes awry when ignoring diminished dimensionality, and some research avenues opened up by this realization.
C60|Red obsession: The ascent of fine wine in China|This article uses hammer prices from five global auction houses to analyse the price premium Bordeaux fine wine yielded at Hong Kong wine auctions. We find that fine wine was on average sold at a 19% premium in Hong Kong. We further observe that the Hong Kong premium is not uniform and most pronounced for wines with perfect Parker scores and the most powerful brands. The premium has declined throughout the sample period from 60% in 2008 to a level of 15% since 2012. This can be attributed to the increase in knowledge on fine wine by Chinese customers.
C60|Land requirements, feedstock haul distance, and expected profit response to land use restrictions for switchgrass production|Energy crop production has been proposed for land of poor quality to avoid competition with food production and negative indirect land use consequences. The objective of this study was to determine the land area requirements, biomass transportation distance, and expected profit consequences of restricting switchgrass biomass production, for use as biofuel feedstock, to marginal land relative to unrestricted land use. The USA soils capability classification system was used to differentiate between high quality land and land of marginal quality. Fifty years of historical weather data were used in combination with a biophysical simulation model to estimate switchgrass biomass yield distributions for land of different quality for counties in the case study region. A mathematical programming model was designed and solved to determine the economic consequences. For the levels of biofuel price considered ($0.50, $0.75 and $1.00/L), and a 262.5ML/year biorefinery modeled, restricting land use to marginally productive capability Class IV soils, increases the quantity of land optimally leased by 42 to 52%; increases biomass trucking total transportation distance by 115 to 116%; and reduces the expected net returns by $11 to $15M/year compared to when land use is unrestricted. In the absence of government restrictions, for-profit companies are not likely to limit energy crop production to land of marginal quality.
C60|CVaR constrained planning of renewable generation with consideration of system inertial response, reserve services and demand participation|Integration of renewable generation can lead to both diversification of energy sources (which can improve the overall economic performance of the power sector) and cost increase due to the need for further resources to provide flexibility and thus secure operation from unpredictable, variable and asynchronous generation. In this context, we propose a cost-risk model that can properly plan generation and determine efficient technology portfolios through balancing the benefits of energy source diversification and cost of security of supply through the provision of various generation frequency control and demand side services, including preservation of system inertia levels. We do so through a scenario-based cost minimization framework where the conditional value at risk (CVaR), associated with costs under extreme scenarios of fossil fuel prices combined with hydrological inflows, is constrained. The model can tackle problems with large data sets (e.g. 8760 hours and 1000 scenarios) since we use linear programming and propose a Benders-based method adapted to deal with CVaR constraints in the master problem. Through several analyses, including the Chilean main electricity system, we demonstrate the effects of renewables on hedging both fossil fuel and hydrological risks; effects of security of supply on costs, risks and renewable investment; and the importance of demand side services in limiting risk exposure of generation portfolios through encouraging risk mitigating renewable generation investment.
C60|The spark spread and clean spark spread option based valuation of a power plant with multiple turbines|This paper offers a novel study of two key factors that affect the valuation of a natural gas-fired power plant having multiple turbines: carbon allowance prices and the ability to switch among turbines. Amid stricter environmental rules on CO2 emissions, a power plant operator needs to be able to judge how the purchase of carbon allowances affects the plant's expected value; and whether the plant's value rises from switching among turbines. This paper presents a model analysis of a spark spread and clean spark spread option-based valuation of a power plant with multiple gas turbines — using a bivariate and a tri-variate lattice, respectively. Results demonstrate that the purchase of CO2 allowances lowers the plant's expected value. Conversely, when operations of turbines are switched in response to price movements, the plant's value increases. This outcome has implications for plant management decisions: when to switch among turbines and how the purchase of CO2 allowances affects the plant's value.
C60|North American natural gas and energy markets in transition: insights from global models|This modeling comparison exercise looks at the global consequences of increased shale gas production in the U.S. and increased gas demand from Asia. We find that differences in models' theoretical construct and assumptions can lead to divergences in their predictions about the consequences of U.S. shale gas boom. In general, models find that U.S. High Shale Gas scenario leads to increased U.S. production, lower global gas prices, and lower gas production in non-U.S. regions. Gas demand in Asia alone has little effects on U.S. production; but together with the shale gas boom, the U.S. can have a large export advantage. Overall, models find U.S. exports level range from 0.06 to 13.7 trillion cubic feet (TCF) in 2040. The comparison of supply, demand, and price changes in response to shocks reveals important differences among models. First is how the demand shocks were implemented and how the model responds to shocks: static and elastic within each time period vs. endogenous to the long-term gross domestic product (GDP) growth. Second is how the supply response is expressed through fuel/technology substitutions, particularly the flexibility of cross-fuel substitution in the power sector. Identifying these differences is important in understanding the model's insights and policy recommendations.
C60|Economics of U.S. natural gas exports: Should regulators limit U.S. LNG exports?|This study assesses the level and destination of U.S. LNG exports, using a global natural market model under a wide range of EMF 31 scenarios. The scenarios reflect different U.S. natural gas resource outlooks, market conditions, changing U.S. environmental regulations, and possible changes in geopolitical conditions that affect the global natural gas demand and supply. U.S. LNG exports respond to market conditions under each scenario and are free from any artificial limits. In the near-term, U.S. LNG exports are uncompetitive in the Reference case and in the long-run U.S. LNG exports are significant when U.S. natural gas resources are plentiful. However under demand shocks (increase demand in Asia) or supply shocks (reduction in Russian supplies) or persistence of oil-indexed pricing cases, U.S. LNG exports become competitive to varying degrees. U.S. exports depend not only on U.S. economics but also on how U.S. prices change relative to price changes in other regions of the world. We conclude that limiting U.S. LNG exports is inconsistent with simulated uncertainties, and it should be left to the market to determine the levels and destination of exports.
C60|On Economic Space notion|This paper introduces Economic Space notion to expand capacity for economic and financial modeling. Introduction of Economic Space allows defining economic variables as functions of time and coordinates and opens the way for treating economic and financial relations similar to mathematical physics equations. Economic Space allows study of economic models on discreet and continuous spaces with different dimensions. The number of risks measured simultaneously determines Economic Space dimension. We present examples of modeling on Economic Space: option pricing and derivation of Black–Scholes–Merton equation on n-dimensional Economic Space; Markov processes and derivation of Fokker–Plank Equations. Usage of Economic Space allows construing approximations of Economics and Finance similar to physical kinetics and hydrodynamics and derives Wave Equations for Economic and Financial variables.
C60|Optimal rates from eigenvalues|A financial portfolio typically pays dividend based on its value. We show that there is a unique portfolio that pays the maximum dividend rate while remaining solvent, under appropriate assumptions. We also give a characterization of both the portfolio and the optimal dividend rate.
C60|Efficiency and stability of probabilistic assignments in marriage problems|We study marriage problems where two groups of agents, men and women, match each other and probabilistic assignments are possible. When only ordinal preferences are observable, stochastic dominance efficiency (sd-efficiency) is commonly used. First, we provide a characterization of sd-efficient allocations in terms of a property of an order relation defined on the set of man–woman pairs. Then, using this characterization, we constructively prove that for each probabilistic assignment that is sd-efficient for some ordinal preferences, there is a von Neumann–Morgenstern utility profile consistent with the ordinal preferences for which the assignment is Pareto efficient. Second, we show that when the preferences are strict, for each ordinal preference profile and each ex-post stable probabilistic assignment, there is a von Neumann–Morgenstern utility profile, consistent with the ordinal preferences, for which the assignment belongs to the core of the associated transferable utility game.
C60|Modeling loss data using mixtures of distributions|In this paper, we propose an alternative approach for flexible modeling of heavy tailed, skewed insurance loss data exhibiting multimodality, such as the well-known data set on Danish Fire losses. Our approach is based on finite mixture models of univariate distributions where all K components of the mixture are assumed to be from the same parametric family. Six models are developed with components from parametric, non-Gaussian families of distributions previously used in actuarial modeling: Burr, Gamma, Inverse Burr, Inverse Gaussian, Log-normal, and Weibull. Some of these component distributions are already alone suitable to model data with heavy tails, but do not cover the case of multimodality. Estimation of the models with a fixed number of components K is proposed based on the EM algorithm using three different initialization strategies: distance-based, k-means, and random initialization. Model selection is possible using information criteria, and the fitted models can be used to estimate risk measures for the data, such as VaR and TVaR. The results of the mixture models are compared to the composite Weibull models considered in recent literature as the best models for modeling Danish Fire insurance losses. The results of this paper provide new valuable tools in the area of insurance loss modeling and risk evaluation.
C60|Unexpected shortfalls of Expected Shortfall: Extreme default profiles and regulatory arbitrage|The purpose of this paper is to dispel some common misunderstandings about capital adequacy rules based on Expected Shortfall. We establish that, from a theoretical perspective, Expected Shortfall based regulation can provide a misleading assessment of tail behavior, does not necessarily protect liability holders’ interests much better than Value-at-Risk based regulation, and may also allow for regulatory arbitrage when used as a global solvency measure. We also show that, for a value-maximizing financial institution, the benefits derived from protecting its franchise may not be sufficient to disincentivize excessive risk taking. We further interpret our results in the context of portfolio risk measurement. Our results do not invalidate the possible merits of Expected Shortfall as a risk measure but instead highlight the need for its cautious use in the context of capital adequacy regimes and of portfolio risk control.
C60|Afriat’s Theorem and Samuelson’s ‘Eternal Darkness’|Suppose that we have access to a finite set of expenditure data drawn from an individual consumer, i.e., how much of each good has been purchased and at what prices. Afriat (1967) was the first to establish necessary and sufficient conditions on such a data set for rationalizability by utility maximization. In this note, we provide a new and simple proof of Afriat’s Theorem, the explicit steps of which help to more deeply understand the driving force behind one of the more curious features of the result itself, namely that a concave rationalization is without loss of generality in a classical finite data setting. Our proof stresses the importance of the non-uniqueness of a utility representation along with the finiteness of the data set in ensuring the existence of a concave utility function that rationalizes the data.
C60|Is the refining margin stationary?|It has traditionally been assumed that the refining margin is stationary given that it is a linear combination of cointegrated time series, i.e., crude oil and its main refining products (mainly heating oil and gasoline). Following this reasoning, stationary models have been proposed to measure the refining margin. In this paper, we investigate the main empirical properties of several time series that measure the refining margin (or crack spread) using an extensive database of WTI, heating oil and unleaded gasoline futures prices traded on the NYMEX. The results show that there are serious doubts about the stationarity of the refining margin. Moreover, a non-stationary factor model is proposed and estimated to measure the refining margin, and in some cases, the model achieves better results than the traditional stationary models. This result has straightforward implications for valuation and hedging.
C60|Decision rules for allocation of finances to health systems strengthening|A key dilemma in global health is how to allocate funds between disease-specific “vertical projects” on the one hand and “horizontal programmes” which aim to strengthen the entire health system on the other. While economic evaluation provides a way of approaching the prioritisation of vertical projects, it provides less guidance on how to prioritise between horizontal and vertical spending. We approach this problem by formulating a mathematical program which captures the complementary benefits of funding both vertical projects and horizontal programmes. We show that our solution to this math program has an appealing intuitive structure. We illustrate our model by computationally solving two specialised versions of this problem, with illustrations based on the problem of allocating funding for infectious diseases in sub-Saharan Africa. We conclude by reflecting on how such a model may be developed in the future and used to guide empirical data collection and theory development.
C60|Aggregation Methods For Fuzzy Judgments|Arrow (1963) established that a group cannot always reach logically consistent collective outcome. Subsequently many developments like premise based, conclusion based and distance based methods have emerged in literature to reach group consistency. This study is focused on the judgment aggregation in fuzzy logic based setting with novel involvement of family of t-norms. We compare three distance based methods due to Miller and Osherson (2009) using Lukasiewicz and min t-norm. These methods in fuzzy logic based settings give closer results to consistency of outcome. It also broaden the set of properties and authenticity of the methods. Distance methods in our study also satisfy Arrow’s axioms in solution method.
C60|Economic Cycles and Their Synchronization: A Comparison of Cyclic Modes in Three European Countries|The present work applies singular spectrum analysis (SSA) to the study of macroeconomic fluctuations in three European countries: Italy, The Netherlands, and the United Kingdom. This advanced spectral method provides valuable spatial and frequency information for multivariate data sets and goes far beyond the classical forms of time domain analysis. In particular, SSA enables us to identify dominant cycles that characterize the deterministic behavior of each time series separately, as well as their shared behavior. We demonstrate its usefulness by analyzing several fundamental indicators of the three countries' real aggregate economy in a univariate, as well as a multivariate setting. Since business cycles are international phenomena, which show common characteristics across countries, our aim is to uncover supranational behavior within the set of representative European economies selected herein. Finally, the analysis is extended to include several indicators from the U.S. economy, in order to examine its influence on the European economies under study and their interrelationships.
C60|The Method of Leader’s Overthrow in Networks|Methods for leader’s detection and overthrow in networks are useful tools for decision-making in many real-life cases, such as criminal networks with hidden patterns or money laundering networks. In the given research, we represent the algorithms that detect and overthrow the most influential node to the weaker positions following the greedy method in terms of structural modifications. We employed the concept of Shapley value from the area of cooperative games to measure a node’s leadership and used it as the core of the developed leader’s overthrow algorithms. The approaches are illustrated based on the trivial network structures and tested on real-life networks. The results are represented in tabular and graphical formats.
C60|Specification of merger gains in the Norwegian electricity distribution industry|Electricity distribution often exhibits economies of scale. In Norway, a number of smaller distribution system operators exist and thus there is potential to restructure the industry, possibly through mergers. However, the revenue cap regulatory model in Norway does not incentivize firms to merge as merging leads to a stricter revenue cap for the merged company. Thus the regulator compensates the firms in order to create such incentives. The amount of compensation is based on the potential gains of the merger estimated using a data envelopment analysis (DEA) based frontier approach introduced by Bogetoft and Wang (Journal of Productivity Analysis, 23, 145–171, 2005). DEA is however only one of many possible frontier estimators that can be used in estimation. Furthermore, the returns to scale assumption, the operating environment of firms and the presence of stochastic noise and outlier observations are all known to affect to the estimation of production technology. In this paper we explore how varying assumption under two alternate frontier estimators shape the distribution of merger gains within the Norwegian distribution industry. Our results reveal that the restructuring policies of the industry may be significantly altered depending how potential gains from the mergers are estimated.
C60|Stochastic Electricity Dispatch: A challenge for market design|We consider an electricity market with two sequential market clearings, for instance representing a day-ahead and a real-time market. When the first market is cleared, there is uncertainty with respect to generation and/or load, while this uncertainty is resolved when the second market is cleared. We compare the outcomes of a stochastic market clearing model, i.e. a market clearing model taking into account both markets and the uncertainty, to a myopic market model where the first market is cleared based only on given bids, and not taking into account neither the uncertainty nor the bids in the second market. While the stochastic market clearing gives a solution with a higher total social welfare, it poses several challenges for market design. The stochastic dispatch may lead to a dispatch where the prices deviate from the bid curves in the first market. This can lead to incentives for selfscheduling, require producers to produce above marginal cost and consumers to pay above their marginal value in the first market. Our analysis show that the wind producer has an incentive to deviate from the system optimal plan in both the myopic and stochastic model, and this incentive is particularly strong under the myopic model. We also discuss how the total social welfare of the market outcome under stochastic market clearing depends on the quality of the information that the system operator will base the market clearing on. In particular, we show that the wind producer has an incentive to misreport the probability distribution for wind.
C60|Congestion Management in a Stochastic Dispatch Model for Electricity Markets|"We consider an electricity market organized with two settlements: one for a pre-delivery (day-ahead) market and one for real time, where uncertainty regarding production from non-dispatchable energy sources as well as variable load is resolved in the latter stage. We formulate two models to study the efficiency of this market design. In the myopic model, the day-ahead market is cleared independently of the real-time market, while in the integrated stochastic dispatch model the possible outcomes of the real-time market clearing are considered when the day-ahead market is cleared. We focus on how changes in the design of the electricity market influence the efficiency of the dispatch, measured by expected total cost or social welfare. In particular, we examine how relaxing network flow constraints and, for the stochastic dispatch model, even the balancing constraints in the day-ahead part of the dispatch models affects the overall efficiency of the system. This allows the dispatch to be infeasible day-ahead, while these infeasibilities will be handled in the real-time market. For the stochastic dispatch model we find that relaxing the network flows and balancing constraints in the dayahead part of the market provides additional flexibility that can be valuable to the system. In our examples with high up-regulation cost we find a value of ""overbooking"" that lead to lower total costs. In the myopic model the results are more ambiguous, however, leaving too many constraints to be resolved in the real-time market only, can lead to infeasibilities or high regulation cost."
C60|Integrated maritime bunker management with stochastic fuel prices and new emission regulations|Maritime bunker management (MBM) controls the procurement and consumption of the fuels used on board and therefore manages one of the most important cost drivers in the shipping industry. At the operational level, a shipping company needs to manage its fuel consumption by making optimal routing and speed decisions for each voyage. But since fuel prices are highly volatile, a shipping company sometimes also needs to do tactical fuel hedging in the forward market to control risk and cost volatility. From an operations research perspective, it is customary to think of tactical and operational decisions as tightly linked. However, the existing literature on MBM normally focuses on only one of these two levels, rather than taking an integrated point of view. This is in line with how shipping companies operate; tactical and operational bunker management decisions are made in isolation. We develop a stochastic programming model involving both tactical and operational decisions in MBM in order to minimize the total expected fuel costs, controlled for financial risk, within a planning period. This paper points out that after the latest regulation of the Sulphur Emission Control Areas (SECA) came into force in 2015, an integration of the tactical and operational levels in MBM has become important for shipping companies whose business deals with SECA. The results of the computational study shows isolated decision making on either tactical or operational level in MBM will lead to various problem. Nevertheless, the most server consequence occurs when tactical decisions are made in isolation.
C60|Pricing wind: A revenue adequate, cost recovering uniform price for electricity markets with intermittent generation|With greater penetration of renewable generation, the uncertainty faced in electricity markets has increased substantially. Conventionally, generators are assigned a pre-dispatch quantity in advance of real time, based on estimates of uncertain quantities. Expensive real time adjustments then need to be made to ensure demand is met, as uncertainty takes on a realization. We propose a new stochastic-programming market clearing mechanism to optimize pre-dispatch quantities, given the uncertainties’ probability distribution and the costs of real-time deviation. This model differs from similar mechanisms previously proposed in that pre-dispatch quantities are not subject to any network or other physical constraints; nor do they play a role in financial settlement. We establish revenue adequacy in each scenario (as opposed to “in expectation”), welfare enhancement and expected cost recovery (including deviation costs), for this market clearing mechanism. We also establish that this market clearing mechanism is social welfare optimizing.
C60|The Impact of Bunker Risk Management on CO2 Emissions in Maritime Transportation Under ECA Regulation|The shipping industry carries over 90 percent of the world’s trade, and is hence a major contributor to CO2 and other airborne emissions. As a global effort to reduce air pollution from ships, the implementation of the ECA (Emission Control Areas) regulations has given rise to the wide usage of cleaner fuels. This has led to an increased emphasis on the management and risk control of maritime bunker costs for many shipping companies. In this paper, we provide a novel view on the relationship between bunker risk management and CO2 emissions. In particular, we investigate how different actions taken in bunker risk management, based on different risk aversions and fuel hedging strategies, impact a shipping company’s CO2 emissions. We use a stochastic programming model and perform various comparison tests in a case study based on a major liner company. Our results show that a shipping company’s risk attitude on bunker costs have impacts on its CO2 emissions. We also demonstrate that, by properly designing its hedging strategies, a shipping company can sometimes achieve noticeable CO2 reduction with little financial sacrifice.
C60|Estimating output mix effectiveness: A scenario approach|The ability of public sector policy makers to prioritize has a huge impact on the effectiveness of public service provision. Public services can take the form of final outputs demanded by consumers or of intermediate outputs contributing to a process of realizing the higher goals of society. In doing the right things, policy makers choose a mix of Intermediate outputs maximizing their preference value for public service outcomes, while managers do things right when responsible for producing outputs efficiently. This distinction enables us to pinpoint important reasons for inefficiencies in the provision of public services. Taking advantage of the method of scenario based planning, a model for measuring effectiveness is developed for situations where traditional methods such as two-stage regressions fail due to long time lags and lack of variation in the variables. Scenarios take the role of outcomes in the modeling of outcome mapping functions, where each scenario represents a set of environmental variables. The model is specified for the provision of defense outcomes, where the lag between changes in input and impacts on outcomes are significant. From a sample of 12 combat units in the Norwegian Armed Forces, producing different outputs, we find that inefficiencies in output mix can explain most of the changes in overall effectiveness over a four-year period of time.
C60|Pre-evaluating technical efficiency gains from possible mergers and acquisitions: evidence from Japanese regional banks|This study focuses on bank mergers and acquisitions (M&As) and applies a DEA based procedure that allows us to pre-evaluate technical efficiency gains from possible M&As in the Japanese regional banking sector. This approach provides a strategic tool for policy-makers to pre-evaluate possible M&As decisions based on performance criteria that are measured in terms of technical efficiency gains. The results clearly show that possible M&As formed by the smaller banks performed better compared with the possible M&As formed by the larger banks. Moreover, our findings imply that small regional banks will have possible efficiency gains when they merge with neighboring banks, whereas larger banks appear to have efficiency gains from merging with distant banks. Copyright Springer Science+Business Media New York 2016
C60|Long-term petroleum product supply analysis through a robust modelling approach|Linear programming approach to economic modelling of petroleum refining has important shortcomings that make it less useful and less robust for the purposes of impact assessments of related policies. These have to do with its natural inability to calibrate observed data and obtaining jumpy responses of the decision variables to smooth exogenous shocks due to the large number of substitutions between the refining processes. Relying on positive mathematical programming literature, in this paper we propose a method that solves these issues. The main idea is that a refining model has to have a non-linear objective function via inclusion of an implicit total cost function that captures the aggregated impact of all other relevant factors that are not explicitly modelled. We discuss in some detail the issues relevant for practical implementation of the proposed approach for interested practitioners.
C60|A cobweb model with alternating demand and supply functions|In this work I present a cobweb model for markets characterized by two couples of demand and supply functions which cyclically alternate with period two, in a succession of peak and off-peak market phases. Starting from classical adaptive expectations, a new expectation formation mechanism is presented, to take into account such marketsâ€™ peculiarity. In particular, to adapt the previous in-phase expected price, agents use both in-phase and out-of-phase expectation errors, suitably weighted through a phase weight. It is shown that the resulting model is described by a non-autonomous difference equation. The local asymptotic stability of the steady state equilibrium is studied, showing that it depends on the expectation weight, the phase weight and on both the relative slopes, at the equilibrium, of the supply functions with respect to the demand functions. Several crucial differences with respect to the classical cobweb model are highlighted, showing the potentially ambiguous role of expectation weight and of relative slopes. It is shown that destabilization can occur both through a flip and a Neimark-Sacker bifurcation, which can occur for the same market conditions and different expectation weights.
C60|Local Independence, Monotonicity and Axiomatic Characterization of Price-Money Message Mechanism|To characterize money in a static economic model, it is known to be important to consider the agent- commodity double-in nity settings, i.e., the overlapping-generations framework. There does not seem to exist any papers, however, treating the axiomatic characterization problems for such monetary Walras allocations under the social choice and/or mechanism design settings. We show that the monetary Walras allocation for the economy with double in nities is characterized by weak Pareto- optimality, individual rationality, local independence or the monotonicity conditions of social choice correspondence among the allocation mechanisms with messages under the category theoretic approach in Sonnenschein (1974). We utilize Sonnenschein's market extension axiom for swamped economies that is closely related to the replica stability axiom of Thomson (1988). We can see how these conditions characterize the price-money message mechanism universally among a wide class of mechanisms, and efficiently in the sense that it has the minimal message spaces (price-money dictionary theorems). Moreover, by using the category theoretic framework, we can obtain the up-to-isomorphism uniqueness for such a dictionary object (isomorphism theorems).
C60|Eliciting the just-noticeable difference|"Abstract The evidence from psychophysics suggest that people are unable to discriminate between alternatives unless the options are significantly different. Since this assumption implies non-transitive indifferences, it can not be reconciled with utility maximisation. We provide a method of eliciting consumer preference from observable choices when the agent is incapable of discerning between similar bundles. It is well-known that the issue of noticeable differences can be modelled with semiorder maximisation. We introduce a necessary and sufficient condition under which a finite dataset of consumption bundles and corresponding budget sets can be rationalised with such a relation. The result can be thought of as an extension of Afriat's (1967) theorem to semiorders, rather than utility optimisation. Our approach is constructive and allows us to infer the just-noticeable difference that is sufficient for the agent to differentiate between bundles as well as the ""true"" preferences of the consumer (i.e., as if perfect discrimination were possible). Furthermore, we argue that the former constitutes a natural measure of how well the preference revealed in the data could be approximated by a weak order. We conclude by applying our test to household-level scanner panel data of food expenditures. Revised June 2017."
C60|"Metodología para elaborar leyes de posibilidad de retirada del cliente: una aplicación al sector del vestido || A Methodology to Elaborate Laws of Possibilities in the Retreat of a Client: An Application to the Dress Sector"|"El presente documento pone a prueba en el sector del vestido en el centro del país, una metodología basada en la teoría de la incertidumbre y los subconjuntos borrosos para construir leyes de posibilidad de retirada del cliente con la empresa, con tan sólo la opinión subjetiva de expertos. La aportación del presente trabajo permite obtener un camino alternativo cuando no es posible contar con la información requerida por los modelos identificados en la literatura basados en principios derivados de las leyes del azar, incluso métodos heurísticos. Los resultados muestran la utilidad de los conceptos borrosos en un problema donde la incertidumbre en relación a la permanencia del cliente se hace evidente, permitiendo obtener un elemento necesario (tiempo), cuando se requiera medir el valor económico del cliente (Customer Lifetime Value: CLV) en el campo de la incertidumbre. || The current work tests, in the dress sector in the center of the country, a methodology based in the theory of uncertainty and the fuzzy subsets, in order to build laws of possibilities for the retreat of clients only with the subjective opinion given by experts. The contribution of the present work allows to obtain an alternative path when it is not possible to get the required information by the models identified in the literature based in principles derived of the random laws even from heuristic methods. The results show the utility of fuzzy concepts in a problem where the uncertainty in relation to the permanence of the client is evident and allows to obtain a valuable element (time), when the Customer Lifetime Value (CLV) is required to be measured in the field of uncertainty."
C60|Sorting and Peer Effects|The effect of sorting students based on their academic performances depends not only on direct peer effects but also on indirect peer effects through teachers' efforts. Standard assumptions in the literature are insufficient to determine the effect of sorting on the performances of students and so are silent on the effect of policies such as tracking, implementing school choice, and voucher programs. We show that the effect of such policies depends on the curvature of teachers' marginal utility of effort. We characterize conditions under which sorting increases (decreases) the total effort of teachers and the average performance of students.
C60|A Rational Inattention Perspective on Equilibrium Asset Pricing under Heterogeneous Information with Structural Breaks and Market Efficiency|In this paper we present a new model of how information travels within financial markets and present empirical evidence that the concept of attention driven information efficiency is more conjugate with market data as compared to the prevailing concept of efficient markets. Augmenting our model by a shift component made it possible to explain shifts in asset prices by a lack of attention on small permanent changes in the fundamentals. This can also be seen as a micro-level explanation of the momentum effect. By a further augmentation of the model through the introduction of heterogeneous information processing capacities we are able to give a fundamental interpretation of the financial services industry as providers of information processing capacity. Moreover, the burst of the housing bubble in the US and the successful bet of John Paulson against it are shown to be prime empirical examples of our framework.
C60|The determination of the least distance to the strongly efficient frontier in Data Envelopment Analysis oriented models: Modelling and computational aspects|Determining the least distance to the efficient frontier for estimating technical inefficiency, with the consequent determination of closest targets, has been one of the relevant issues in recent Data Envelopment Analysis literature. This new paradigm contrasts with traditional approaches, which yield furthest targets. In this respect, some techniques have been proposed in order to implement the new paradigm. A group of these techniques is based on identifyiexit3b2tex.batng all the efficient faces of the polyhedral production possibility set and, therefore, is associated with the resolution of a NP-hard problem. In contrast, a second group proposes different models and particular algorithms to solve the problem avoiding the explicit identification of all these faces. These techniques have been applied more or less successfully. Nonetheless, the new paradigm is still unsatisfactory and incomplete to a certain extent. One of these challenges is that related to measuring technical inefficiency in the context of oriented models, i.e., models that aim at changing inputs or outputs but not both. In this paper, we show that existing specific techniques for determining the least distance without identifying explicitly the frontier structure for graph measures, which change inputs and outputs at the same time, do not work for oriented models. Consequently, a new methodology for satisfactorily implementing these situations is proposed. Finally, the new approach is empirically checked by using a recent PISA database consisting of 902 schools.
C60|Solving the Social Choice problem under equality constraints|Suppose that a number of equally qualified agents want to choose collectively an element from a set of alternatives defined by equality constraints. Each agent may well prefer a different element, and the social choice problem consists in deciding whether it is possible to design a rule to aggregate all the agents’ preferences into a social choice in an egalitarian way. In this paper we obtain criteria that solve this problem in terms of conditions that are explicitly computable from the constraints. As a theoretical consequence, we show that the only way to avoid running into a social choice paradox consists in designing (if possible) the set of alternatives satisfying certain optimality condition on the constraints, that is, in the natural way from the point of view of economics.
C60|Continuous time, continuous decision space prisoner’s dilemma: A bridge between game theory and economic GCD-models|General Constrained Dynamic models (GCD – models) in economics are inspired by classical mechanics with constraints. Most macroeconomic models can be understood as special cases of GCD – models. Moreover, in this paper it will be shown that not only macroeconomic models but also game theoretic models are strongly related to GCD – models. GCD models are characterized by a system of differential equations in continuous time while most game theoretical models are set up in discrete time. Therefore it is necessary to build a bridge from game theoretical models denominated in discrete time to game theoretical models using continuous time. This bridge is illustrated in the following using the example of a continuous time, continuous decision space prisoner’s dilemma. Furthermore, it is shown that the differential equations which determine other continuous game theoretic models can be understood to a certain extent as special cases of the GCD – differential equations. Well known types of continuous game theoretic models include for instance “Evolutionary Game Theory” with the replicator equation, “Adaptive Dynamics” with the canonical equation, which is nothing else than a replicator – mutator equation, and the so called “Differential Games”, which are strongly related to optimal control theory with two controls and two different objectives (goals). Most of the GCD – models are characterised by 3 key feature: - mutual influence, - Power-factors - Constraints Nowak (2006b) and Taylor & Nowak (2007) show that there are five mechanisms which, under certain conditions, can lead to the evolution of cooperation in an iterated prisoner’s dilemma. Inspired by this, we apply the 3 key features of GCD – models to the standard prisoner’s dilemma in discrete time which yields 3 additional mechanisms which enable the evolution of cooperation. The assumption or axiom of the free market economy is that an individual optimisation strategy will lead to an overall optimum by virtue of Adam Smith’s invisible hand. Without additional conditions this assumption alone is fundamentally wrong. As in prisoner’s dilemma also in economics cooperation is essential to get an overall optimum. The big question of political economy is to analyse which additional measures could guarantee that the individual optimisation strategy characterising a free market economy leads to cooperation as precondition to get an overall optimum. From this point of view the different economic theories could be characterised in terms of which measures they assume to be sufficient to guarantee an overall optimum without abandoning the principle of individual optimisation.
C60|A MS-Excel Module to Transform an Integrated Variable into Cumulative Partial Sums for Negative and Positive Components with and without Deterministic Trend Parts|Our aim is to describe how a software component called TDICPS can be used. TDICPS is a MS-Excel module developed in VBA (visual basics for applications) by the authors that transforms an integrated variable into cumulative partial sums for positive and negative components along with graphs for a potential sample size of more than one million observations. Several options are available. The variable might have both drift and trend, only drift or no deterministic trend parts. We demonstrate step by step how the stock price index of the US market can be transformed into partial components for positive and negative changes. Any other variable can also be transformed in a similar way. The transformed data can be used for implementing the asymmetric causality tests as developed by Hatemi-J (2012). It can also be used for estimating the asymmetric generalized impulse response functions and the asymmetric variance decompositions as introduced by Hatemi-J (2014). Other options are also possible. The MS-Excel code is available by e-mail from the authors.
C60|Wavelet Based Analysis Of Major Real Estate Markets|Wavelet coherence of time series provide valuable information about dynamic correlation and its impact on time scales. Here, we analyze the wavelet coherence of major real estate markets data. Our paper is the first to link co-movement in terms of wavelet coherence. Here we consider USA, Canada, Japan, China and Developed Europe real estate market prices as time series.Wavelet coherence results reveal interconnected relationships between these markets and how these relationships vary in the time-frequency space. These relationships allow us to build VARMA models of real estate data which yield forecast results with small errors.
C60|Impact of Demand Response Participation in Energy, Reserve and Capacity Markets|Demand response is capable of providing multiple services, including energy and reserve. As a consequence of providing energy, demand response is also inherently contributing to generation adequacy, and thus may be in entitled to avail of revenue from a capacity remuneration mechanism. Participation in multiple markets may result in a trade-off, thus necessitating simultaneous optimization of the demand response provision of such services. This paper uses Mixed Complementarity Problems to investigate these trade-offs and resulting market outcomes in the presence of load-shifting demand response. An approach to approximate the capacity value of the demand response resource, thereby permitting its participation in the capacity market, is also presented. It is found that, for the case study examined here, that demand response has its most significant impact on the energy market, with marginal and negligible impacts on the capacity and reserve market, respectively. The results also suggest that considerable cost savings are attainable by the DR aggregator through participation in the energy market, but that significant further cost savings are not forthcoming through participation in the reserve or capacity market.
C60|Self Image and Environmental Attitude and Behavior|In this paper, we examine some important aspects related to safeguarding environmental quality. Beginning by defining the identity of the agents considered, we conceptually describe how while they have different identities, they can be simultaneously influenced by three distinct elements: self image and environmental attitude and behavior. We define self image as the individual’s willingness to cooperate for the sake of the public good, we define environmental attitude as the individual’s concern regarding waste prevention and disposal, and the environmental behavior as the individual’s recycling behavior. Using the 1998 wave of the Multipurpose Household Survey (MHS), which is conducted annually by the Italian Central Statistical Office, and univariate probit models, we show that there is a positive relationship among these factors that is robust to the inclusion of social participation variables and to the use of a sub-sample of individuals who have no interest in environmental issues.
C60|Budget Policy And Economic Growth In Russia. Optimal Budget Rule|The article shows that actual public expenditure in the period of rapid oil prices growth of the 2000s was less than optimal level in Russia. The macroeconomic model of Russian economy is the basis of current research. The main mechanism of growth in an optimum scenario is associated with the scaling effect of public expenditure, which increase production possibilities of an economy. Adequate monetary policy allows to prevent unwinding of the inflation spiral and runs the growth spiral. Non-optimality of fiscal policy is a consequence of budget rule mechanism features, which do not take into account the influence of government expenditures on economic growth. The fiscal rule that implements the «closed loop» control and allows to design optimal economic policies for developing countries can become a basis for the system of growth management that combines universal and program planning.
C60|An element-set labelling a Cartesian product by measurable binary relations which leads to postulates of the theory of experience and chance as a theory of co~events|We introduce the set-theoretic language for the element-set labelling a Cartesian product by measurable binary relations intended for the labelling, or for the naming of parts and details of the construction that we are going to propose in the theory of experience and chance, or the theory of co~events that serve as mathematical models of events as dual pairs.
C60|Postulating the theory of experience and chance as a theory of co~events (co~beings)|The aim of the paper is the axiomatic justification of the theory of experience and chance, one of the dual halves of which is the Kolmogorov probability theory. The author’s main idea was the natural inclusion of Kolmogorov’s axiomatics of probability theory in a number of general concepts of the theory of experience and chance. The analogy between the measure of a set and the probability of an event has become clear for a long time. This analogy also allows further evolution: the measure of a set is completely analogous to the believability of an event. In order to postulate the theory of experience and chance on the basis of this analogy, you just need to add to the Kolmogorov probability theory its dual reflection — the believability theory, so that the theory of experience and chance could be postulated as the certainty (believability-probability) theory on the Cartesian product of the probability and believability spaces, and the central concept of the theory is the new notion of co~event as a measurable binary relation on the Cartesian product of sets of elementary incomes and elementary outcomes. Attempts to build the foundations of the theory of experience and chance from this general point of view are unknown to me, and the whole range of ideas presented here has not yet acquired popularity even in a narrow circle of specialists; in addition, there was still no complete system of the postulates of the theory of experience and chance free from unnecessary complications. Postulating the theory of experience and chance can be carried out in different ways, both in the choice of axioms and in the choice of basic concepts and relations. If one tries to achieve the possible simplicity of both the system of axioms and the theory constructed from it, then it is hardly possible to suggest anything other than axiomatization of concepts co~event and its certainty (believability-probability). The main result of this work is the axiom co~event, intended for the sake of constructing a theory formed by dual theories of believabilities and probabilities, each of which itself is postulated by its own Kolmogorov system of axioms. Of course, other systems of postulating the theory of experience and chance can be imagined, however, in this work, a preference is given to a system of postulates that is able to describe in the most simple manner the results of what I call an experienced-random experiment.
C60|Triangle room paradox of negative probabilities of events|Here an improved generalization of Feynman’s paradox of negative probabilities [1, 2] for observing three events is considered. This version of the paradox is directly related to the theory of quantum computing. Imagine a triangular room with three windows, where there are three chairs, on each of which a person can seat [4]. In any of the windows, an observer can see only the corresponding pair of chairs. It is known that if the observer looks at a window (to make a pairwise observation), the picture will be in the probabilistic sense the same for all windows: only one chair from the observed pair is occupied with a probability of 1/2, and there are never busy or free both chairs at once. Paradoxically, existing theories based on Kolmogorov’s probability theory do not answer the question that naturally arises after such pairs of observations of three events: «What is really happening in a triangular room, how many people are there and with what is the probability distribution they are sitting on three chairs?».
C60|The bet on a bald|A fixed company of players observes a person selected from a fixed queue. After each observation, players are asked to bet the dollar secret from others, either on the fact that person is bald or on what is not. A definite formula for the gain is suggested, such that every time after bets the gain of each player from a given company is completely determined by this formula. However, before bets player’s gain is an uncertain value. Is it possible for a given company of players and a given queue of people before bets to build a correct mathematical model of an uncertain gain of each player within the framework of Kolmogorov’s probability theory? If not, what else do you need to add to the foundations of probability theory so that before bets to be able to use this model for decision making? The paper answers these questions within the framework of the new theory of experience and of chance (the certainty theory) [1] that consists of two dual halves: the believability theory and the probability theory, and that is intended for the mathematical description of experienced-random experiments, the uncertainty in outcomes of which is generated by the observer.
C60|Proof-of-Sovereignty (PoSv) as a Method to Achieve Distributed Consensus in Crypto-Currency Networks|In this paper, a method to implement K-Y protocol using Distributed Consensus is discussed. Firstly, the various available methods are discussed. Then, Proof - Of - Sovereignty (PoSv) is proposed. Its mechanism is deliberated and its advantages are described vis-a-vis other methods of distributed consensus. Finally a summary of all the procedures involved in 'NationCoin Mining' is explained.
C60|Scenarios for sustainable heat supply in cities – case of Helsingor, Denmark|Local climate action is not only a domain of large cities, but also smaller urban areas that increasingly address climate change mitigation in their policy. The Danish municipality of Helsingør can achieve substantial CO2 emission reduction by transforming its heat supply and deploying heat savings. In the paper we model the heating system of Helsingør from a socio- and private-economic perspective, develop future scenarios, and conduct an iterative process to derive optimal mix between district heating, individual heating and heat savings. The results show that in 2030 it is cost-optimal to reduce the heating demand by 20-39% by implementing heat savings, to deploy 33%-41% of district heating and reduce heating-related CO2 emissions by up to 95% compared to now. In 2050, the cost-optimal share of district heating in Helsingør is between 38-44%. The resulting average heating costs and CO2 emissions are found to be sensitive to biomass and electricity price. Although the findings of the study are mainly applicable for Helsingør, the combined use of the Least Cost Tool and modelling with energyPRO is useful in planning of any heating and/or cooling supply and demand configuration, in any geographical region and scale
C60|Organizing Dynamic Capabilities: Exploiting Complementarities by Organizational Bundling|A firm’s ability to change is decisive for sustaining its competitive advantage in a volatile business environment. Previous research emphasizes the importance of human resources as a key determinant of a firm’s “dynamic capability”. We extend the literature by analyzing how a firm’s organizational structure is interrelated with the effectiveness of managing change projects on employee level. Building theoretically on the interplay of strategy and structure, we argue the effectiveness of strategic, coordinative, and motivational capabilities will be higher when they are bundled in one organizational unit. A management survey involving some of the most important corporations in Germany supports our hypothesis.
C60|Bayesian binomial zero-coupon bonds model|The article is devoted to construction of stochastic one-factor evolutional model for zero-coupon bond in discrete time. As the base sequence it was used an asymmetric geometric random walk. It is shown that in case of observing not only the previous values of wandering, but his condition the last time it is Markov. In this case derived formulas for the transition probability in one step, as well as for the conditional mean and variance. Based on these facts, the article describes a stochastic model of zero-coupon bonds. For this model of bond were also find explicit formulas of its volatility, risk-neutral price, temporal structure of interest rates. Results of simulation display good match with real data.
C60|Portfolio Optimzation Using of Metods Multi Objective Genetic Algorithm and Goal Programming: An Application in BIST-30|Portfolio optimization problem has become one of the related fields of financial engineering since the studies of Markowitz about modern portfolio theory. Selection process of portfolio is carried out by looking at the return and risk relationship of stocks in portfolio in order to create the best portfolio. The main purpose of a financial manager is to ensure an efficient portfolio which provides minimum risk and maximum return. For this purpose, new models and computer technology continue at an accelerated rate. Genetic algorithms are from stochastic algorithm family based on the principles of natural selection. In this study, soft closing prices data of BIST 30 stocks between the periods, 2004-2013 are used. Eight different return and risk portfolios are created by applying goal programming and multi-purpose genetic algorithm methods with Markowitz mean-variance model. Variation coefficient which is a statistical unit of measure used for selection of portfolio is used. The results obtained from the study show that the best portfolios consist of number 7 portfolio for genetic algorithm and 5 stocks of this portfolio ; number 4 portfolio for quadratic goal programming method and 8 stocks of this portfolio. It is concluded that when compared in terms of optimization techniques, quadratic goal programming gives better results than genetic algorithm.
C60|Asset Market Linkages in a Regime Switching Environment: Evidence from Commodity and Stock Markets in India|Time series models investigating the linkages between various asset markets (Commodity and Equity) in India have assumed a symmetric and linear relationship between them. They examine these interrelationships by assuming the presence of a uniform economic state. However the returns from commodity futures and stock market do not show a continuous trend but exhibit time varying behaviour i.e. the returns of stocks might be higher in a certain economic condition and it may fall as the economic environment changes due to financial crises, oil price rise, rupee depreciation etc. Similarly the return of commodities is also subject to variation with the changing economic conditions due to which the basic assumptions of stationary and linearity of time series models gets refuted. Therefore, this paper empirically examines the interrelationships between commodity futures (Energy, Metal and Agriculture) and stock markets in dynamic economic states by employing Markov Regime Switching model proposed by Hamilton (2005) which has the capability of capturing temporal asymmetries and nonlinear dynamics of time series. For each market a composite index indicating the overall movement and performance of a particular investment asset has been considered. In order to provide robust results this paper uses daily data from 2006 to 2014 which significantly represents different states in the Indian Economy. The result of the study confirms the impact of economic environment on Indian commodity and stock markets and validates the presence of two distinct regimes: a “tranquil regime” representing the state of economic expansion and a “crisis regime” representing the state of economic decline. Additionally, the result confirms that commodities and stock markets oscillate between high and low volatility regimes and this movement is different across different commodity class (Energy, Metal and Agriculture). In a portfolio, when commodity futures are combined with stock, due consideration must be given to the state varying behaviour of different asset class. However, previous studies on performance of commodity futures in context of portfolio have been done in a single time period/static context and therefore the results of the study provide interesting insights for investors and portfolio managers. By detecting the switching points in the economic states, they can rebalance their portfolio accordingly so as to reduce loss and enhance portfolio returns.
C60|Free viewpoint real video streaming system for 360° three-dimensional viewing|The goal of this study was to develop a unique free viewpoint real video technology for 360° three-dimensional (3-D) viewing of photographic subjects and to utilize it in online video transmissions. In existing multi-angle video systems, the simultaneous synchronization and playback of large volumes of video images significantly burdens the central processing unit (CPU). Because repeated play and pause processing of multiple videos must be performed, a time lag occurs with every change in viewpoint. In this context, smooth transitions between videos of multiple viewpoints are challenged.In response to this situation, the author developed a method to create a single composite image of a subject photographed in 360° using multiple cameras and display the image during transmission by partially trimming it and sliding the display positioning under the direction of the user.This method resulted in a system that enables not only smooth viewpoint switching and 360° 3-D viewing of photographic subjects; it does so at a quality and size possible for online streaming. Therefore, in 2013, a Japanese patent was granted for this technology, it was used in an advertising promotion for Sharp?s 4K television, and it was used in the television Asahi web program ?Danceta!? Additional patents were granted in 2015 in Korea, China, and the United States. The European patent is pending.
C60|Control Strategy To Trade Cryptocurrencies|The paper deals with the cryptocurrencies. First, a general introduction to crypto-currencies is given from the programmer?s point of view. It describes some basic strategies for automated trading. Also explained is the algorithm Floyd-Warshall and its modifications for automation arbitrage. An illustrative example is given and a trading algorithm is listed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C60|No arbitrage of the first kind and local martingale numéraires|Abstract A supermartingale deflator (resp. local martingale deflator) multiplicatively transforms nonnegative wealth processes into supermartingales (resp. local martingales). A supermartingale numéraire (resp. local martingale numéraire) is a wealth process whose reciprocal is a supermartingale deflator (resp. local martingale deflator). It has been established in previous works that absence of arbitrage of the first kind ( NA 1 $\mbox{NA}_{1}$ ) is equivalent to the existence of the (unique) supermartingale numéraire, and further equivalent to the existence of a strictly positive local martingale deflator; however, under NA 1 $\mbox{NA}_{1}$ , a local martingale numéraire may fail to exist. In this work, we establish that under NA 1 $\mbox{NA}_{1}$ , a supermartingale numéraire under the original probability P $P$ becomes a local martingale numéraire for equivalent probabilities arbitrarily close to P $P$ in the total variation distance.
C60|Reforming retirement age in DB and DC pension systems in an aging OLG economy with heterogenous agents|Abstract We analyze the effects of increasing the retirement age in two economies with overlapping generations and within cohort ex ante heterogeneity. The first economy has a defined benefit system, and the second economy is in transition from a defined benefit to a defined contribution. We find that if increase in the retirement age is phased in a way that allows agents to adjust, welfare is not reduced and welfare effects have a similar magnitude and between-cohort distribution in both types of the pension systems.
C60|Conditional exact law of large numbers and asymmetric information economies with aggregate uncertainty|Abstract A stochastic model with a continuum of economic agents often involves shocks at both macro and micro levels. This can be formalized by a continuum of conditionally independent random variables given the macro level shocks. Based on the framework of a Fubini extension, the results on the exact law of large numbers and its converse for a continuum of independent random variables in Sun (J Econ Theory 126:31–69, 2006) are extended to the setting with conditional independence given general macro states. It also follows from Hammond and Sun (Econ Theory 36:303–325, 2008) that the conditional independence assumption is generally satisfied. As an illustrative application, it is shown that any ex ante efficient allocation in an asymmetric information economy with general aggregate uncertainty has a (utility) equivalent allocation that is incentive compatible, which generalizes the corresponding results in Sun and Yannelis (Games Econ Behav 61:131–155, 2007) to the case with infinitely many states.
C60|Optimal Controls for a Large Insurance Under a CEV Model: Based on the Legendre Transform-Dual Method|Abstract The purpose of this paper is to consider the optimal proportional reinsurance and investment strategies for an insurance company. The insurer’s surplus process is approximated by a Brownian motion with drift. The insurance company can purchase proportional reinsurance and invest the surplus in a financial market which includes one risk-free asset and one risky asset whose price is modeled by a CEV model. The primary problem is changed to the dual problem by implying Legendre transform. When the objective of the insurance company is to maximize the expected logarithmic utility from terminal wealth, the closed-form expressions for the optimal reinsurance-investment policy which is different to the Merton case to the primal optimal problem are obtained and numerical simulations are provided to demonstrate our results. Moreover, we find an interesting result that risk exposure is non-monotonic in the cost of reinsurance.
C60|Use of simulation in controlling research: a systematic literature review for German-speaking countries|Abstract This paper provides a systematic literature review of the use of simulation in the field of “controlling”, a term synonymously used for management accounting and control in German-speaking countries. The review starts from a total of 12,102 articles published in leading, controlling-related business journals in German-speaking countries (Germany, Austria, and Switzerland) between 1980 and 2009. Thereof, 47 articles specifically refer to the use of simulation in controlling. This set of articles is analyzed along the following dimensions: development of publication volume over time, important authors, controlling tasks and instruments supported by simulation or fulfillment of minimum quality criteria concerning simulation modeling and analysis. The results indicate an increasing interest in employing simulation within controlling and its particular relevance in practice. Two areas emerge as the main application arena: planning and risk management. Despite some progress the review also shows that simulation is not yet an established branch of research on its own in controlling. A detailed analysis of the articles suggests that more transparency and standards in the application of simulations are needed to further advance this method.
C60|Groves mechanisms and communication externalities|We put forward a model of private goods with externalities. Agents derive benefit from communicating with each other. In order to communicate they need to operate on a common platform. Adopting new platforms is costly. We first provide an algorithm that determines the efficient outcome. Then we prove that no individually rational and feasible Groves mechanism exists. We provide sufficient conditions that determine when an individually rational Groves mechanism runs a deficit and we characterize the individually rational Groves mechanism that minimizes such deficit whenever it occurs. Moreover, for 2-agent economies, we single out the only feasible and symmetrical Groves mechanism that is not Pareto dominated by another strategy-proof, feasible and symmetrical mechanism. Copyright The Author(s) 2016
C60|Efficiency Study of Greek Health Units of the Public Sector using Data Envelopment Analysis Method, before and during the start of the Economic Crisis|Purpose – The purpose of this paper is to investigate the Technical Efficiency through an Efficiency Study of Health Units of the Greek Ministry of Health, before and during the start of the Economic Crisis in Greece. Design/methodology/approach – The research has been designed to collect data regarding the Health Units of the Public Sector from the Greek Statistical Authority (ELSTAT) and to process that data with the use of the Data Envelopment Analysis software. The methodology of the research extends to the application of the Efficiency Study of Decision Making Units (DMUs), the study of variations of Technical Efficiency during a number of years and the extraction of conclusions regarding variations in Technical Efficiency at a time period before and during the start of the Crisis. Data from large Health Units was used in order to achieve comparison of the results. Findings – We calculate the DEA scores based on the most common DEA models (CCR, BCC and Super Efficiency) for the Health Units of the Greek Ministry of Health. We examine the variation in Technical Efficiency of the Health Units during the extent of the time period of the study. The Efficiency Study of the Health Units leads to useful conclusions regarding the variation in the observed Efficiency of the Units and the integration of the Efficiency variation studies, as part of the initial stage of an Integrated Crisis Management. The research ranks the efficient and non-efficient units and suggests ways of improvement. Research limitations/implications – This is a study about the Health Units of the Public Sector using size as a criterion. The investigation is limited to the Public Health Sector and its conclusions cannot be extended to the Private Health Sector. There are no geographical restrictions. Originality/value – This is an innovative research which allows for further case studies in the future and completion of Efficiency studies after the end of the Economic Crisis.
C60|Empirical Pricing Performance in Long-Dated Crude Oil Derivatives: Do Models with Stochastic Interest Rates Matter?|Does modelling stochastic interest rates beyond stochastic volatility improve pricing performance on long-dated crude oil derivatives? To answer this question, we examine the empirical pricing performance of two forward price models for commodity futures and options: a deterministic interest rate - stochastic volatility model and a stochastic interest rate - stochastic volatility model. Both models allow for a correlation structure between the futures price process, the futures volatility process and the interest rate process. By estimating the model parameters from historical crude oil futures prices and option prices, we find that stochastic interest rate models improve pricing performance on long-dated crude oil derivatives, with the effect being more pronounced when the interest rate volatility is relatively high. Several results relevant to practitioners have also emerged from our empirical investigations. With regards to balancing the trade-off between precision and computational effort, we find that two-factor models would provide good fit on long-dated derivative prices thus there is no need to add more factors. We also find empirical evidence for a negative correlation between crude oil futures prices and interest rates.
C60|Hedging Futures Options with Stochastic Interest Rates|This paper presents a simulation study of hedging long-dated futures options, in the Rabinovitch (1989) model which assumes correlated dynamics between spot asset prices and interest rates. Under this model and when the maturity of the hedging instruments match the maturity of the option, forward contracts and futures contracts can hedge both the market risk and the interest rate risk of the options positions. When the hedge is rolled forward with shorter maturity hedging instruments, then bond contracts are additionally required to hedge the interest rate risk. This requirement becomes more pronounced for longer maturity contracts and amplifies as the interest rate volatility increases. Factor hedging ratios are also considered, which are suited for multi-dimensional models, and their numerical efficiency is validated.
C60|Empirical Hedging Performance on Long-Dated Crude Oil Derivatives|This paper presents an empirical study on hedging long-dated crude oil futures options with forward price models incorporating stochastic interest rates and stochastic volatility. Several hedging schemes are considered including delta, gamma, vega and interest rate hedge. Factor hedging is applied to the proposed multi-dimensional models and the corresponding hedge ratios are estimated by using historical crude oil futures prices, crude oil option prices and Treasury yields. Hedge ratios from stochastic interest rate models consistently improve hedging performance over hedge ratios from deterministic interest rate models, an improvement that becomes more pronounced over periods with high interest rate volatility, such as during the GFC. An interest rate hedge consistently improves hedging beyond delta, gamma and vega hedging, especially when shorter maturity contracts are used to roll the hedge forward. Furthermore, when the market experiences high interest rate volatility and the hedge is subject to high basis risk, adding interest rate hedge to delta hedge provides an improvement, while adding gamma and/or vega to the delta hedge worsens performance.
C60|Random categorization and bounded rationality|In this study we introduce a new stochastic choice rule that categorizes objects in order to simplify the choice procedure. At any given trial, the decision maker deliberately randomizes over mental categories and chooses the best item according to her utility function within the realized consideration set formed by the intersection of the mental category and the menu of alternatives. If no alternative is present both within the considered mental category and within the menu the decision maker picks the default option. We provide the necessary and sufficient conditions that characterize this model in a complete stochastic choice dataset in the form of an acyclicity restriction on a stochastic choice revealed preference and other regularity conditions. We recover the utility function uniquely up to a monotone transformation and the probability distribution over mental categories uniquely. This model is able to accommodate violations of IIA (independence of irrelevant alternatives), of stochastic transitivity, and of the Manzini–Mariotti menu independence notion (i-Independence).
C60|Investigating Technical Efficiency and Its Determinants by Data Envelopment Analysis: An Application in the Greek Food and Beverages Manufacturing Industry| ABSTRACT In this paper, a two‐step procedure is applied in order to investigate technical efficiency and its determinants in the Greek food and beverages manufacturing industry for the period 1984–2007. Technical efficiency scores for the industry are estimated using the data envelopment analysis approach. Moreover, bootstrapped truncated regressions and OLS regressions are applied in order to investigate the factors affecting technical efficiency. The empirical results indicate that there is a fluctuation of technical efficiency scores among the sectors of the food and beverages industry. Furthermore, the findings obtained from both the Tobit and the OLS regressions support that the factors affecting positively the level of technical efficiency are sector size, capital productivity, labor productivity, and labor intensity. The results also show that the technical efficiency of the whole industry tended to decrease during the period 1984–2007. Moreover, the present paper provides some policy recommendations that may be useful for the industry to overcome the present economic crisis. Finally, in future research, the technical efficiency of the Greek food industry will be analyzed based on the meta‐analysis approach. [JEL Classifications: C60, L60, O14].
C60|Building Computable General Equilibrium Model Of Croatia|In this paper we describe the structure of the computable general equilibrium (CGE) model and data that enables estimation of certain policy changes in Croatia. Namely, we build a 5-sector (households, firms, government, investors and foreigners) economy model while our economy is disaggregated on three highly aggregated sectors. Afterwards, we present Croatian data which enables us to simulate the model in Nadoveza, Sekur and Penava (upcoming). These data are seen as snapshot of established equilibrium in 2010 in Croatia and they represent the main input for the CGE models. Finally, we conduct the reality check of our calibrated parameters.
C60|Environmental implications of crop insurance subsidies in Southern Italy|The changing environment affects agriculture introducing sources of uncertainty. On the other hand, policies to cope with risks may have strong impacts on the environment. We evaluate the effects of public risk management programmes, such as subsidised crop insurance, fertilizer use and land allocation to crops. We implement a mathematical programming model of a representative wheat-tomato farm in Puglia, a southern Italy region. The results show that under the current crop insurance programmes, tomato productions are expected to expand and to require larger amount of fertilizer, whereas the opposite is true for wheat productions. Policy and environmental implications are discussed.
C60|Exact Methods for Path-Dependent Credit Exposure|Path dependent counterparty credit risk exposure modeling poses challenges. In this paper, we present models for consistent and accurate estimation of counterparty credit exposure involving barrier option and European swaption under the general Monte Carlo simulation framework. In particular, we discuss how to consistently estimate the pathwise swaption exercise probability and accurate monitoring of barrier crossing. We present exact formulation for standalone expected exposure and potential future exposure for swap, swaption and barrier option without monte carlo simulation. The exact formulation is of practical importance to computing standalone exposure profiles, exposure model validation and system benchmarking.
C60|Why and How to overcome General Equilibrium Theory|For more than 100 years economists have tried to describe economics in analogy to physics, more precisely to classical Newtonian mechanics. The development of the Neoclassical General Equilibrium Theory has to be understood as the result of these efforts. But there are many reasons why General Equilibrium Theory is inadequate: 1. No true dynamics. 2. The assumption of the existence of utility functions and the possibility to aggregate them to one “Master” utility function. 3. The impossibility to describe situations as in “Prisoners Dilemma”, where individual optimization does not lead to a collective optimum. This paper aims at overcoming these problems. It illustrates how not only equilibria of economic systems, but also the general dynamics of these systems can be described in close analogy to classical mechanics. To this end, this paper makes the case for an approach based on the concept of constrained dynamics, analyzing the economy from the perspective of “economic forces” and “economic power” based on the concept of physical forces and the reciprocal value of mass. Realizing that accounting identities constitute constraints in the economy, the concept of constrained dynamics, which is part of the standard models of classical mechanics, can be applied to economics. Therefore it is reasonable to denote such models as Newtonian Constraint Dynamic Models (NCD-Models). Such a framework allows understanding both Keynesian and neoclassical models as special cases of NCD-Models in which the power relationships with respect to certain variables are one-sided. As mixed power relationships occur more frequently in reality than purely one-sided power constellations, NCD-models are better suited to describe the economy than standard Keynesian or Neoclassic models. A NCD-model can be understood as “Continuous Time”, “Stock Flow Consistent”, “Agent Based Model”, where the behavior of the agents is described with a general differential equation for every agent. In the special case where the differential equations can be described with utility functions, the behavior of every agent can be understood as an individual optimization strategy. He thus seeks to maximize his utility. However, while the core assumption of neoclassical models is that due to the “invisible hand” such egoistic individual behavior leads to an optimal result for all agents, reality is often defined by “Prisoners Dilemma” situations, in which individual optimization leads to the worst outcome for all. One advantage of NCD-models over standard models is that they are able to describe also such situations, where an individual optimization strategy does not lead to an optimum result for all agents. This will be illustrated in a simple example. In conclusion, the big merit and effort of Newton was, to formalize the right terms (physical force, inertial mass, change of velocity) and to set them into the right relation. Analogously the appropriate terms of economics are force, economic power and change of flow variables. NCD-Models allow formalizing them and setting them into the right relation to each other.
C60|Wave function in economics|In this research article: 1) the new quantum macroeconomics and microeconomics theories in the quantum econophysics science are formulated, 2) the notion on the wave function in the quantum macroeconomics and microeconomics theories in the quantum econophysics science is introduced, and 3) the quantum econophysical wave equations in the quantum macroeconomics and microeconomics theories in the quantum econophysics science are derived for the first time. Authors show that there is a certain conceptual scientific analogy between 1) the wave functions in the quantum econophysical wave equations in the quantum macroeconomics and microeconomics theories in the quantum econophysics science as well as 2) the wave function in the Schrödinger quantum mechanical wave equation in the quantum mechanics science. The wave function theories are created to make: 1) the economy’s state prediction at the certain time moment, using the wave function in the quantum econophysical wave equation in the quantum macroeconomic theory in the quantum econophysics science; and 2) the firm’s state prediction at the certain time moment, using the wave function in the quantum econophysical wave equation in the quantum microeconomic theory in the quantum econophysics science. Authors use the quantum econophysical wave equations in the quantum econophysics science to develop a new software program for the application by the central / commercial / investment banks with the purpose the make the accurate characterization and forecasting of: 1) the national/global economic performance changes, including the GIP((t), GDP(t), GNP(t) dependences changes, in agreement with the quantum macroeconomics theory in the quantum econophysics science, and 2) the firm’s economic performance changes, including the EBITDA(t) dependence changes in agreement with the quantum microeconomics theory in the quantum econophysics science.
C60|Dynamische Modellierung des Cournot Oligopols mit Methoden der Regelungstechnik<BR>[Dynamic Modelling of the Cournot Oligopoly using Control Engineering Methods]|In the paper a dynamic model for the price formation process in a Cournot oligopoly market is provided. Model development is carried out using control engineering methods. At first, as common in control engineering the feedback structure of the Cournot market price mechanism is depicted as a block diagram. Then, this structure is enhanced by dynamic models reflecting an assumed time-delayed response behaviour of the market participants to changes in market price. This approach leads to a dynamic model for the Cournot oligolpolistic market which is self-evident and modularly expandable. The performance of the model is shown using simulation results for an oligopoly composed of three market participants. The simulation examples presented are related to differing response dynamics of the individual participants as well as to capacity limitations and alterations in market demand. The results show that control engineering methods can effectively be applied to dynamic modeling of economics processes as well. Im Beitrag wird ein dynamisches Modell für den Preisbildungsprozess in einem Cournot-Oligopol Markt vorgestellt. Die Modellbildung erfolgt mit ingenieur-wissenschaftlichen Methoden. Zunächst wird, wie in der Regelungstechnik üblich, die Rückkopplungsstruktur des Cournot-Preisbildungsmechanismus als Blockdiagramm dargestellt. Diese Struktur wird dann um dynamische Modelle erweitert, welche ein angenommenes, zeitlich verzögertes Reaktionsverhalten der Marktteilnehmer auf Marktpreisänderungen beschreiben. Auf diese Weise erhält man ein einfach verständliches und modular-erweiterbares dynamisches Modell des Cournot-Oligopol-Marktes. Die Leistungsfähigkeit des Modells wird anhand von Simulationsergebnissen für ein aus drei Marktteilnehmern bestehendes Oligopol veranschaulicht. Die vorgestellten Simulationsbeispiele beinhalten sowohl unterschiedliche Reaktionsdynamik der einzelnen Teilnehmer als auch Kapazitätsbeschränkungen und Änderungen der Marktnachfrage. Die Ergebnisse zeigen, dass Methoden der Regelungstechnik auch für die dynamische Modellierung ökonomischer Prozesse effektiv nutzbar sind.
C60|A decomposition for the space of games with externalities|The main goal of this paper is to present a different perspective than the more `traditional' approaches to study solutions for games with externalities. We provide a direct sum decomposition for the vector space of these games and use the basic representation theory of the symmetric group to study linear symmetric solutions. In our analysis we identify all irreducible subspaces that are relevant to the study of linear symmetric solutions and we then use such decomposition to derive some applications involving characterizations of classes of solutions.
C60|The effect of including the environment in the neoclassical growth model|This study begins with an exposition of basic principles of the theory of Optimal Control as this is used in the development of the theory of Economic Growth. Then, a brief presentation of the Neoclassical Model of Economic Growth follows and two applications are presented. In the first, optimal control techniques are used, in the context of neoclassical growth, to maximize the representative household’s total intertemporal welfare. In the second, the same problem is posed with two additional variables that affect welfare in opposing ways: pollution and abatement expenditures. In both applications, the optimal steady-state conditions are derived. This allows for a preliminary comparison of the resulting balanced growth paths under the criterion of welfare maximization with and without environmental externalities. Finally, using a balanced panel data of 43 countries and for the time period 1990-2011 we test the validity of including the environment in the neoclassical growth model approximating pollution abatement with the electricity production from renewable sources and pollution with carbon dioxide emissions. With the help of adequate econometric panel data methods we test the validity of the environmental Kuznets curve hypothesis for the full sample, as well as for the OECD and non-OECD countries
C60|Simultaneous optimization: sectorization and congolese air traffic assignment by the method of preferential reference of dominance|Air controllers encounter aeronautical problems everyday. Those problems complexity is growing as the latter problems emerge in aerial navigations sectors at the time of the air traffic assignment. As a long time as the number of aircraft in a sector is high, the controller-related load will increase in a nonlinear way. Currently, one counts, on the congolese territory, and especially in the vicinity of areas with wars, many planes movements. This would represent in a near future a difficult bulk of control for controllers. In order to avoid saturation in sectors, the congolese airspace must be divided in increasingly small sectors while distributing the workloads. To clarify the analysis, one is interested in the multicriteria optimization which deals with the case of the simultaneous presence of several objectives by the preferential reference of dominance method proposed by Joseph Okitonyumbe and Berthold Ulungu (2014). The latter method is based on a new characterization of the efficient solutions by building the probable assignments in order to minimize the load of control in the sector.
C60|Generalization of Euler and Ramanujan’s Partition Function|The theory of partitions has interested some of the best minds since the 18th century. In 1742, Leonhard Euler established the generating function of P(n). Godfrey Harold Hardy said that Srinivasa Ramanujan was the first, and up to now the only, mathematician to discover any such properties of P(n). In 1981, S. Barnard and J.M. Child stated that the different types of partitions of n in symbolic form. In this paper, different types of partitions of n are also explained with symbolic form. In 1952, E. Grosswald quoted that the linear Diophantine equation has distinct solutions; the set of solution is the number of partitions of n. This paper proves theorem 1 with the help of certain restrictions. In 1965, Godfrey Harold Hardy and E. M. Wright stated that the ‘Convergence Theorem’ converges inside the unit circle. Theorem 2 has been proved here with easier mathematical calculations. In 1853, British mathematician Norman Macleod Ferrers explained a partition graphically by an array of dots or nodes. In this paper, graphic representation of partitions, conjugate partitions and self-conjugate partitions are described with the help of examples.
C60|Portfolio Construction Based on Implied Correlation Information and Value at Risk|Valor en Riesgo (VaR) es una medida usada comunmente para establecer, dado un nivel de confianza, el peor caso de perdidas en activos. La correlacion implicita obtenida a partir de VaR es una forma alternativa del coeficiente de correlacion calculada basandose en rendimiento historico y en un pronostico de la peor perdida. En este trabajo presentamos un tratamiento accesible para estudiantes de economia, finanzas y areas afines con el objetivo de familiarizar al lector con este estimador de riesgo. Con el uso de tres estudios de caso analizamos el efecto que la correlacion implicita apartir de VaR tiene en carteras de tamaño creciente. Calculamos el VaR de cada activo asi como la media de correlacion implicita. Dicho valor es usado para ajustar las fracciones del presupuesto en la cartera original. Hacemos un seguimiento comparativo de carteras en un plazo de 50 dias para identificar tendencias entre el tipo de cartera y riesgo encontrado.
C60|CO2 abatement policies in the power sector under an oligopolistic gas market|The paper at hand examines the power system costs when a coal tax or a fixed bonus for renewables is combined with CO2 emissions trading. It explicitly accounts for the interaction between the power and the gas market and identifies three cost effects: First, a tax and a subsidy both cause deviations from the cost-efficient power market equilibrium. Second, these policies also impact the power sector's gas demand function as well as the gas market equilibrium and therefore have a feedback effect on power generation quantities indirectly via the gas price. Thirdly, by altering gas prices, a tax or a subsidy also indirectly affects the total costs of gas purchase by the power sector. However, the direction of the change in the gas price, and therefore the overall effect on power system costs, remains ambiguous. In a numerical analysis of the European power and gas market, I find using a simulation model integrating both markets that a coal tax affects gas prices ambiguously whereas a fixed bonus for renewables decreases gas prices. Furthermore, a coal tax increases power system costs, whereas a fixed bonus can decrease these costs because of the negative effect on the gas price. Lastly, the more market power that gas suppliers have, the stronger the outlined effects will be.
C60|La profundidad de mercado y el impacto cruzado de precios|En el mundo de las finanzas escuchamos muchas veces hablar de la profundidad de mercado, y del impacto cruzado de precios. Intuitivamente hablando, las ideas son claras, pero pocas veces se hacen precisas. En un contexto multidimensional en el que se realizan transacciones con más de un activo simultáneamente, tiene sentido hablar del impacto cruzado, un parámetro que tiene relación con la correlación de los valores fundamentales de los activos, pero los conceptos no son equivalentes. En esta nota se explica cómo en el modelo de Kyle [1985] es posible aterrizar las nociones intuitivas y convertirlas en conceptos precisos. En particular se demostrará la existencia del impacto cruzado sin correlación de los valores. Recíprocamente, se demostrará la existencia de correlación de los valores fundamentales y aun así no existir un efecto de impacto cruzado. / In the world of finance we often hear about the depth of the market and the cross impact of prices. Intuitively speaking, the ideas are clear but are seldom precise. In a multidimensional context in which transactions are made simultaneously with more than one asset, it makes sense to talk about cross impact, a parameter that is related to the correlation of the asset’s fundamental values, but the concepts are not equivalent. This article explains how it is possible to turn the intuitive notions into precise concepts using the Kyle model [1985]. In particular the existence of cross impact without values correlation will be demonstrated. Conversely, it will be demonstrated that the correlation of the fundamental values exists and yet there is no cross impact effect.
C60|Ekonomiczne skutki eksploatacji gazu łupkowego|Celem artykułu jest próba systematyzacji ekonomicznych skutków eksploatacji gazu łupkowego, a także omówienie różnorodnych podejść metodycznych i narzędzi stosowanych w analizach dotyczących tego zagadnienia. Podstawą analizy jest przegląd literatury przedmiotu, dokonywany pod kątem identyfikacji mechanizmów ekonomicznych, które powinny być wzięte pod uwagę w kompleksowej ocenie wpływu wydobycia na gospodarkę. Przegląd wskazuje na dużą różnorodność ujęć tematu, m.in. pod względem metodyki, zasięgu terytorialnego (region, kraj, świat), zakresu rozważanych skutków i horyzontu czasowego. Część autorów skupia się na powiązanym z działalnością inwestycyjną i wydobywczą wzroście produkcji i zatrudnienia. Inni zwracają uwagę na konieczność bardziej wszechstronnej analizy kosztów i korzyści, odwołującej się raczej do kwestii efektywności ekonomicznej niż do wpływu na poprawę koniunktury – z tej perspektywy najważniejszym potencjalnym skutkiem wydobycia gazu łupkowego jest spadek cen energii. Istotne są także – zwłaszcza na poziomie lokalnym – efekty zewnętrzne działalności wydobywczej, związanych z jej potencjalnym negatywnym wpływem na środowisko przyrodnicze i zdrowie. Wycena kosztów zewnętrznych jest jednak w praktyce trudna i obarczona dużą niepewnością. Szacunki wpływu gazu łupkowego na gospodarkę bazują najczęściej na symulacji przy wykorzystaniu wielosektorowego modelu gospodarki, modelu systemu energetycznego lub ich hybrydy. Próby empirycznej oceny ex post skutków eksploatacji podają w wątpliwość założenia niektórych analiz symulacyjnych.
C60|Forward equations for option prices in semimartingale models|We derive a forward partial integro-differential equation for prices of call options in a model where the dynamics of the underlying asset under the pricing measure is described by a—possibly discontinuous—semimartingale. This result generalizes Dupire’s forward equation to a large class of non-Markovian models with jumps. Copyright Springer-Verlag Berlin Heidelberg 2015
C60|A nonparametric approach for evaluating long-term energy policy scenarios: an application to the Greek energy system|This paper by using Long-range Energy Alternatives Planning System (LEAP) constructs four different renewable energy scenarios for the Greek transport, energy, and industry sectors. By projecting the demand for renewable energy and the associated resulting carbon dioxide emissions up to the years 2020 and 2030, the paper applies in a second stage data envelopment analysis (DEA) evaluating the Greek renewable energy policy. As a result, it provides a quantitative measure for future renewable energy policy evaluation under different renewable energy consumption scenarios. The results reveal that the main challenge for the Greek policy makers will be the energy policies associated with the renewable energy usage of the Greek industry since they are rigid toward the adoption of technologies utilizing renewable energy sources. It appears that under the four different energy policy scenarios, the Greek industry sector will not be able to meet its renewable energy targets set by the Greek government. Finally, the analysis reveals that the renewable energy targets set for 2020 and 2030 can be met for the energy sector. However, the renewable energy targets set for the transport sector can only be met for the year 2030. JEL classifications: C60; Q47; Q53; Q58 Copyright Halkos et al. 2015
C60|Use of Monte Carlo simulation: an empirical study of German, Austrian and Swiss controlling departments|This paper addresses current and future aspects of the use of Monte Carlo simulation in controlling departments and examines context as well as company-internal factors that may drive the intensity of its usage. To this end, we conducted an empirical survey that was completed by 445 participants from Germany, Austria and Switzerland. The results suggest a rather low adoption rate of Monte Carlo simulation in controlling, but at the same time, the quality of knowledge concerning Monte Carlo simulation within the companies is much higher. In addition, we identify a strong increase in the use of Monte Carlo simulation very recently, and its use is expected to increase threefold within the next 5 years. Furthermore, regression analyses indicate that the use of Monte Carlo simulation is mainly driven by company-internal factors such as its perceived relevance and years of usage. Contrary to our expectations, context factors such as perceived environmental uncertainty do not explain usage, and only company size and industry sector have significant effects. Copyright Springer-Verlag Berlin Heidelberg 2015
C60|Social evaluation functionals: a gateway to continuity in social choice|This paper develops social choice theory aggregating individual utility functions to a social utility function. Such a tool allows me to deal with a natural notion of continuity in social choice theory. In addition, and in order to have the choice problem as close as possible to its beginnings, the social evaluation functionals considered are assumed to satisfy both ordinal measurability and interpersonal non-comparability, and unanimity. I present two results concerning the characterization of projective social evaluation functionals (which means that the social utility function is exactly the utility of the dictator). The first one needs a strong form of welfarism called social state separability. The second one uses continuity in combination with a new axiom called ordinal-scale-preserving. Copyright Springer-Verlag Berlin Heidelberg 2015
C60|Long-run heterogeneity in an exchange economy with fixed-mix traders|Abstract We consider an exchange economy where agents have heterogeneous beliefs and assets are long-lived, and investigate the coupled dynamics of asset prices and agents’ wealth. We assume that agents hold fixed-mix portfolios and invest on each asset proportionally to its expected dividends. We prove the existence and uniqueness of a sequence of arbitrage-free market equilibrium prices and provide sufficient conditions for an agent, or a group of agents, to survive or dominate. Our main finding is that long-run coexistence of agents with heterogeneous beliefs, leading to asset prices endogenous fluctuations, is a generic outcome of the market selection process.
C60|Measuring Seaports' Productivity: A Malmquist Productivity Index Decomposition Approach| This paper uses three different Malmquist productivity index (MPI) decompositions to measure Greek seaports' productivity for the period 2006-10. Bootstrap techniques are applied in order for confidence intervals of MPIs and their components to be constructed, and to verify if the indicated changes are statistically significant. Finally, a second-stage nonparametric analysis is applied identifying the effect of seaports' size on their productivity levels. The results reveal that the number of terminals is a crucial determinant of seaports' productivity levels. Additionally, it appears that the high length of Greek seaports has a negative influence on their productivity levels over the years. © 2015 LSE and the University of Bath
