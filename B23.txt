B23|Working Paper 312 - Quality Homes for Sustainable Malaria Prevention in Africa|Using the Demographic and Health Surveys (DHS) data from 8 African countries, among the top 10 countries with the highest malaria cases, accounting for 87% of malaria incidence cases in Africa, we analyze the impact of housing quality and the usual malaria prevention measures on malaria incidence among children under 5 years old. First, we investigate the potential correlation between malaria incidence and the quality of housing materials. Secondly, using OLS, two-stage least squares and Poisson regression, we estimate the marginal effects of housing quality on the incidence of malaria. The results can be summarized in four points. (i) The statistical analysis results show a substantial correlation between housing quality and the incidence of malaria. We found 8 percentage points lower rate of incidence among children living in houses constructed with improved materials than those in houses with poor quality materials. (ii) We also found that it is not only the physical characteristic of homes that matters, having good sanitation is associate with lower malaria incidence, with a total difference of 10 and 4 percentage points compared to those with less improved toilet facility and poor-quality drinking water respectively. (iii) An improvement in the overall housing quality leads to a significant reduction in the incidence of malaria among children under 5 years old. Explicitly, an improvement from the first percentile measure of housing quality to the 50th percentile leads to a 32% reduction in the number of malaria cases among children under age-five. In other words, if one improves the housing quality of poorer households to the national average, and keeping other factors constant, the number of malaria cases will drop by 50%. (iv) For both groups of households, those that use mosquito bed nets and those who use insecticide as means of preventing malaria, the results show that improved housing quality complements bed nets and insecticides. As housing quality improves, the role of the two preventives become smaller and smaller. Keywords: Malaria, Housing quality, indoor intervention, IV regression, marginal plot. JEL classification: I18; P25; B23
B23|Publish and Perish: Creative Destruction and Macroeconomic Theory|A number of macroeconomic theories, very popular in the 1980s, seem to have completely disappeared and been replaced by the dynamic stochastic general equilibrium (DSGE) approach. We will argue that this replacement is due to a tacit agreement on a number of assumptions, previously seen as mutually exclusive, and not due to a settlement by ‘nature’. As opposed to econometrics and microeconomics and despite massive progress in the access to data and the use of statistical software, macroeconomic theory appears not to be a cumulative science so far. Observational equivalence of different models and the problem of identification of parameters of the models persist as will be highlighted by examining two examples: one in growth theory and a second in testing inflation persistence.
B23|The Cowles Commission and Foundation for Research in Economics: Bringing Mathematical Economics and Econometrics from the Fringes of Economics to the Mainstream|Founded in 1932 by a newspaper heir disillusioned by the failure of forecasters to predict the Great Crash, the Cowles Commission promoted the use of formal mathematical and statistical methods in economics, initially through summer research conferences in Colorado and through support of the Econometric Society (of which Alfred Cowles was secretary-treasurer for decades). After moving to the University of Chicago in 1939, the Cowles Commission sponsored works, many later honored with Nobel Prizes but at the time out of the mainstream of economics, by Haavelmo, Hurwicz and Koopmans on econometrics, Arrow and Debreu on general equilibrium, Yntema and Mosak on general equilibrium in international trade theory, Arrow on social choice, Koopmans on activity analysis, Klein on macroeconometric modelling, Lange, Marschak and Patinkin on macroeconomic theory, and Markowitz on portfolio choice, but came into intense methodological, ideological and personal conflict with the emerging “Chicago school.” This conflict led the Cowles Commission to move to Yale in 1955 as the Cowles Foundation, directed by James Tobin (who had declined to move to Chicago to direct it). The Cowles Foundation remained a leader in the more technical areas of economics, notably with Tobin’s “Yale school” of monetary theory, Scarf’s computable general equilibrium, Shubik in game theory, and later Phillips and Andrews in econometric theory but as formal methods in economic theory and econometrics pervaded the discipline of economics, Cowles (like the Econometric Society) became less distinct from the rest of economics.
B23|Australian macro-econometric models and their construction - A short history|The paper provides a short account of the major complete macroeconometric models that have been constructed in Australia. Initially these were by academics but later both the Treasury and Reserve Bank of Australia developed these for policy analysis and forecasting, so that the history focusses a good deal on what was developed in those institutions. The basic strategy of the paper is to set out the modelling themes that were occurring overseas and then to discuss the same variants in Australia. In a number of instances Australian research might be considered to have been well ahead of overseas developments.
B23|From Disequilibrium to Equilibrium Macroeconomics: Barro and Grossman's Trade-off between Rigor and Realism|During the 1970s, Keynesian macroeconomics was challenged by the New Classical Economics of Robert Lucas. This involved a battle between disequilibrium and equilibrium macroeconomics. My article contributes to explain why the equilibrium approach came to dominate. My case study is Robert Barro and Herschel Grossman. In 1971, Barro and Grossman elaborated the basic disequilibrium model. In 1976, they wrote the first book on disequilibrium macroeconomics – i.e., Money, Employment, and Inflation. However, at the end of the 1970s, they came to advocate for equilibrium models à la Lucas (1972, 1975). My article traces how and why
B23|Introduction : Malaise dans la science économique ?<BR>[Introduction: Economics and Its Discontents]|"This is the introductive chapter to the book ""Wassily Leontief and Economics"", published in February 2019 (ENS editions). Economists produce their statements and forecasts from devices articulating abstract theories with mathematical models and statistical instruments of measurement. What is the empirical significance of these theories, models and instruments? We consider this question from the reflection of Wassily Leontief, 1973 Nobel Prize winner, on the role of mathematics and statistical analysis in economics. His perspective makes it possible to reconsider why, in economics, ""the connection does not go by itself"" between the theory and the observation, according to the expression of Alain Desrosières. We reconstruct Leontief’s methodology of economics as an empirical science. From there, we show how the input-output device paves the way to empirical and disaggregated macroeconomics."
B23|Family Decision Making for Educational Expenditure, New Evidence from Survey Data for Nigeria|This study examines the determinants of educational expenditures by households in Nigeria. Data from the Nigerian General Household Survey, Panel 2012/2013, Wave 2 was used and a double-hurdle model was employed for the analysis. The results suggest household income, the age, education, gender of the household heads and urban versus rural residence have a significant impact on the decision to spend on education. Such expenditures are income elastic overall, but are very different in magnitude for low income compared to higher income families. It is found that the income elasticity of education expenditures are approximately four times greater for households in the bottom two-thirds of the income distribution than for those on the top one-third of the income distribution.
B23|The Wavering Economic Thought About The Link Between Education And Growth|The objective of this paper is twofold. On the one hand, it provides a balanced account of both theoretical and empirical debates on the link between education and growth since World War 2. We point out the lack of a clear-cut consensus.On the other hand, we question the traditional measurements of human capital, and assess their fit to various theoretical models of growth. Subsequently, we provide a new and arguably more appropriate proxy. Using it, we document crude correlations in line with the literature, pointing out that education may not be an appropriate instrument to accelerate growth.
B23|Spurious Seasonality Detection: A Non-Parametric Test Proposal|This paper offers a general and comprehensive definition of the day-of-the-week effect. Using symbolic dynamics, we develop a unique test based on ordinal patterns in order to detect it. This test uncovers the fact that the so-called “day-of-the-week” effect is partly an artifact of the hidden correlation structure of the data. We present simulations based on artificial time series as well. While time series generated with long memory are prone to exhibit daily seasonality, pure white noise signals exhibit no pattern preference. Since ours is a non-parametric test, it requires no assumptions about the distribution of returns, so that it could be a practical alternative to conventional econometric tests. We also made an exhaustive application of the here-proposed technique to 83 stock indexes around the world. Finally, the paper highlights the relevance of symbolic analysis in economic time series studies.
B23|Lawrence R. Klein and the making of large-scale macro-econometric modeling, 1938-1955|Lawrence R. Klein was the father of macro-econometric modeling, the scientific practice that dominated macroeconomics throughout the second half of the twentieth century. Therefore, understanding how Klein developed his identity as a macro-econometrician and how he conceived and forged macro-econometric modeling at the same time, is essential to draw a clear picture of the origins and subsequent development of this scientific practice in the United States. To this aim, I focus on Klein’s early trajectory as a student of economics and as an economist (from 1938-1955), and I particularly examine the extent to which the people and institutions Klein encountered helped him shape his professional identity. Klein’s experience at places like Berkeley, MIT, Cowles, and the University of Michigan, as well as his early acquaintance with people such as Griffith Evans, Paul Samuelson, and Trygve Haavelmo were decisive in the formation of his idea on how econometrics, expert knowledge, mathematical rigor, and a specific institutional configuration should enter macro-econometric modeling. Although Klein’s identity defined some of the most important characteristics of this practice, by the end of the 1950s, macro-econometric modeling became a scientific practice independent of Klein’s enthusiasm and with a “life of its own,” ready to be further developed and adapted to specific contexts by the community of macroeconomists.
B23|Del mundo del más o menos al universo de precisión: a propósito de los modelos de ciclos de los negocios de Ragnar Frisch Y Michal Kalecki|El objetivo de este trabajo es presentar de manera detallada los modelos matemáticos de Frisch y Kalecki, mostrando los problemas conceptuales y analíticos que la literatura reciente ha señalado. La linealización del sistema de ecuaciones de los modelos de Frisch y Kalecki, fortalecieron la idea de que los ciclos económicos se podrían explicar mediante un mecanismo de propagación de oscilaciones amortiguadas y uno de impulso de choques externos aleatorios, mientras que el ciclo auto-sostenido de manera endógena encontraría su fundamento en la no linealidad. ****** The aim of this paper is to present in detail the mathematical models of Frisch and Kalecki, showing the conceptual and analytical problems that recent literature has pointed out. The linearization of the system of equations of the Frisch and Kalecki models strengthened the idea that economic cycles could be explained by a mechanism of propagation of damped oscillations and one of impulse of random external shocks, while the self-sustained cycle of endogenous way would find its foundation in non-linearity.
B23|A Century of Education in Romania|Education is not just a matter of national interest, it's strategic, it's a fundamental interest. For over 100 years, the Romanian education system has been constantly changing. At present, there is still a need to modernize the national education system and to create mechanisms for integrating young graduates into the labor market according to their professional training. The year 2018 is an anniversary one in Romania. Statistics of education could reflect the social development of the nation, in term of education system evolution and structure during the last century. This paper presents a summary of data on education collected from a diversity of old statistical yearbooks, historical studies and databases provided by National Institute of Statistics.
B23|Romania at the Centenary - Population Dynamics|Romania has witnessed a new positive evolution, after 1918, through the new framework created as a result of the unification of Basarabia, Bucovina and Transylvania. In that time, Romania was one of the countries with the strongest economic, political and cultural development in Europe. This paper presents a summary of data on population collected from a diversity of old statistical yearbooks, historical studies and databases provided by National Institute of Statistics.
B23|Texas Service Sector Outlook Survey: Survey Methodology and Performance|The Texas Service Sector Outlook Survey (TSSOS) and Texas Retail Outlook Survey (TROS) are monthly surveys of service sector and retail firms in Texas conducted by the Federal Reserve Bank of Dallas. TSSOS and TROS track the Texas private services sector, including general service businesses, retailers and wholesalers. The surveys provide invaluable information on regional economic conditions—information that Dallas Fed economists and the Bank president use in the formulation of monetary policy. This paper describes the survey’s methodology and analyzes the explanatory and predictive power of TSSOS and TROS indexes with regard to Texas employment growth. Regression analysis shows that several TSSOS and TROS indexes help explain monthly variation in Texas employment. In addition, most TSSOS and TROS indexes are also useful in forecasting Texas employment growth.
B23|Statistical Inference on the Canadian Middle Class|Conventional wisdom says that the middle classes in many developed countries have recently suffered losses, in terms of both the share of the total population belonging to the middle class, and also their share in total income. Here, distribution-free methods are developed for inference on these shares, by means of deriving expressions for their asymptotic variances of sample estimates, and the covariance of the estimates. Asymptotic inference can be undertaken based on asymptotic normality. Bootstrap inference can be expected to be more reliable, and appropriate bootstrap procedures are proposed. As an illustration, samples of individual earnings drawn from Canadian census data are used to test various hypotheses about the middle-class shares, and confidence intervals for them are computed. It is found that, for the earlier censuses, sample sizes are large enough for asymptotic and bootstrap inference to be almost identical, but that, in the twenty-first century, the bootstrap fails on account of a strange phenomenon whereby many presumably different incomes in the data are rounded to one and the same value. Another difference between the centuries is the appearance of heavy right-hand tails in the income distributions of both men and women.
B23|The Wall’s Impact in the Occupied West Bank: A Bayesian Approach to Poverty Dynamics Using Repeated Cross-Sections|In 2002, the Israeli government decided to build a wall inside the occupied West Bank. The wall had a marked effect on the access to land and water resources as well as to the Israeli labour market. It is difficult to include the effect of the wall in an econometric model explaining poverty dynamics as the wall was built in the richer region of the West Bank. So a diff-in-diff strategy is needed. Using a Bayesian approach, we treat our two-period repeated cross-section data set as an incomplete data problem, explaining the income-to-needs ratio as a function of time invariant exogenous variables. This allows us to provide inference results on poverty dynamics. We then build a conditional regression model including a wall variable and state dependence to see how the wall modified the initial results on poverty dynamics. We find that the wall has increased the probability of poverty persistence by 58 percentage points and the probability of poverty entry by 18 percentage points.
B23|Recent Developments in Macro-Econometric Modeling: Theory and Applications|Developments in macro-econometrics have been evolving since the aftermath of the Second World War.[...]
B23|Econometrics and Income Inequality|It is well-known that, after decades of non-interest in the theme, economics has experienced a proper surge in inequality research in recent years. [...]
B23|Studying The Replicability Of Aggregate External Credit Assessments Using Public Information|In this paper, we examine whether an aggregate rating can be accurately predicted with publicly available information about a company’s individual characteristics. We propose an algorithm that shows how efficient and replicable an arbitrary aggregate rating is respectively to the widely used credit risk models and to what extent an aggregate rating can be extrapolated to the non-rated companies as a valid indicator of their credit risk. Using this algorithm, we empirically study the aggregate ratings constructed as a consensus of ratings assigned by seven credit rating agencies for Russian banks on a national scale and compare it with several alternatives and proxies based on the publicly available characteristics of those banks. We measure how well the aggregate (consensus) rating and the proxies are agreed in terms of ordering banks by their credit quality and predicting defaults over a one-year horizon. We show that the aggregate (consensus) rating is comparable to a standard logit default model in terms of discriminatory power, but for ordering, the former is in low agreement with the latter. We also found that using models for predicting initial credit ratings allows the building of a proxy that is in high agreement with the original aggregate rating, but the original aggregate rating outperforms the proxy in terms of discriminatory power. It was also found that greater agreement between the original aggregated rating and the proxy can be achieved on a subsample of investment grade ratings
B23|Aux prémices des humanités numériques? La première analyse automatisée d'un réseau économique ancien (Gardin Garelli, 1961). Réalisation, conceptualisation, réception<BR>[A Precursor of Digital Humanities ? The First Automated Analysis of an Ancient Economic Network (Gardin & Garelli, 1961). Implementation, Theorization, Reception]|From as early as the 1950s, J.C. Gardin's work spanned both archaeology and the emerging automation of numerical computation and documentation. In 1961, with P. Garelli, he published the first automated application of graph theory to historical materials, working from Assyrian cuneiform tablets documenting economic relations. This work was then widely ignored both in archeology and network analysis. However, in the past twenty years, socio-epistemic claims related to the growth of the Internet and computing (digital humanities, computational archaeology, etc.) have brought a surge of interest in Gardin's work, which is now regarded as pioneering. Working from archive materials and publications, this paper shows how a historical sociology of scientific writings can be relevant to the history of automation in historical sciences. The paper examines Gardin's recognition as an influential forerunner of computational archeology, showing that : 1) although Gardin had access to resources (financial, instrumental, etc.) that were rare at the time, and could have provided material for the foundation of a school or a specialty, he did not however pursue this ambition; 2) the demonstrative purposes pursued by Gardin with his study of 1961 economic networks varied between the 1960s (demonstrating the relevance of non-numerical computation) and the 1980s (legitimizing simulation in the social sciences), but were never concerned with network analysis as such.
B23|A Modest Proposal For Augmenting The Gross Domestic Product Of Italy, Allowing Greater Public Spending, Employment, And Graft|Italy’s economy is stagnating, but a fiscal stimulus is ruled out by the Maastricht-limited deficit/GDP ratio. This paper presents a modest proposal for loosening the constraint on public spending by augmenting Italy’s female labor-force participation rate and therewith Italy’s GDP. Additional public spending would be popular, as it would increase employment; it would also be politically viable, as Italy’s elected and appointed officials would welcome the opportunity for increased graft.
B23|Utility Matters: Edmond Malinvaud and growth theory in the 1950s and 1960s|The present-day standard textbook narrative on the history of growth theory usually takes Robert Solowâ€™s 1956 contribution as a key starting point, with extensions on the savings decision (done by David Cass and Tjalling Koopmans in 1965) being the next important development. However, such account is historically misleading because it organizes past developments based on theoretical concerns. Our goal is to tell a richer story about the developments of growth theory from the 1950s to the mid 1960s, in the activity analysis literature that started before Solowâ€™s model and never had him as a central reference. We stress the role played by Edmond Malinvaud, and take his travel from the French milieu of mathematical economics to the Cowles Commission in 1950-1951 and back to France as a guiding line. The rise of turnpike theory in the end of the 1950s generated a debate on the choice criteria of growth programs, opposing the productive efficiency typical of these models to the utilitarian approach supported by Malinvaud and Koopmans. The Vatican Conference of 1963, where Koopmans presented a first version of his 1965 model, was embedded in this debate. We argue that Malinvaudâ€™s (and Koopmansâ€™s) contributions were crucial to steer the activity analysis literature towards a utilitarian analysis of growth paths.
B23|Edmond MalinvaudÂ´s Criticisms of the New Classical Economics: Restoring the Nature and the Rationale of the Old KeynesiansÂ´ Opposition|Contrarily to standard accounts of the history of macroeconomics, recent research has increasingly paid attention to the Old Keynesiansâ€™ criticisms of the New Classical Economics. In this paper, I study another study case through Edmond Malinvaudâ€™s criticisms of the New Classical Economics from the 1980s onwards. I argue that his opposition was radical in the sense that it was both multi-dimensional and systematic. I show, then, that the way he opposed reveals his own conception of macroeconomics, which owed much to the methodology and the practice of macroeconometric modeling. Finally, I suggest that the study of Malinvaudâ€™s opposition to the New Classical Economics shed light on both the nature and rationale of the Old Keynesiansâ€™.
B23|"A proposal for replicating Evanschitzky, Baumgarth, Hubbard, and Armstrong's ""Replication research's disturbing trend"" (Journal of Business Research, 2007)"|"This paper is about how the author proposes to replicate Evanschitzky, Baumgarth, Hubbard, and Armstrong's ""Replication research's disturbing trend"" (Journal of Business Research, 2007). This is because estimating the incidence of published replication research and its outcomes must be continued."
B23|Game Theory and Cold War Rationality: A Review Essay|This essay reviews new histories of the role of game theory and rational decision making in shaping the social sciences, economics among them, in the postwar period. The recent books The World the Game Theorists Made by Paul Erickson and How Reason Almost Lost Its Mind: The Strange Career of Cold War Rationality by Paul Erickson, Judy Klein, Lorraine Daston, Rebecca Lemov, Thomas Sturm, and Michael Gordin raise a number of complex historical questions about the interconnections among game theory, utility theory, decision theory, optimization theory, information theory, and theories of rational choice. Moreover, the contingencies of time, place, and person call into question the usefulness of economists' linear narratives about the autonomous and progressive development of modern economics. The essay finally reflects on the challenges that these issues present for historians of recent economics.
B23|Estimation of Relationship between Inflation and Relative Price Variability: Granger Causality and ARDL Modelling Approach|The objective of this research paper is to examine the relationship between relative price variability and inflation by using consumer price index (CPI) of Pakistan. The outcomes of the research further divided into food and non-food groups too. The monthly data of CPI was taken from the Pakistan Bureau of Statistics, from August 2001 to July 2011 (with 2000-01 base) for 92 composite commodities with 12 sub-groups. We employed the Granger causality testing approach for the evaluation of any possible influence of one indicator to another. In this scenario, it is viable to state that there is a presence of causality and bidirectional feedback between the variables or the two variables are independent. The major issue is to identify a suitable statistical method that enables us to analyze the association among the variables. The findings of this study demonstrated that there is a probable relationship between inflation (DPt) and both un-weighted measures of price variability (VPt and SPt) for the whole items that have been considered for the analysis. Apart from that, this association also exists between food and non-food categories of CPI basket.
B23|CDS Rate Construction Methods by Machine Learning Techniques|Regulators require financial institutions to estimate counterparty default risks from liquid CDS quotes for the valuation and risk management of OTC derivatives. However, the vast majority of counterparties do not have liquid CDS quotes and need proxy CDS rates. Existing methods cannot account for counterparty-specific default risks; we propose to construct proxy CDS rates by associating to illiquid counterparty liquid CDS Proxy based on Machine Learning Techniques. After testing 156 classifiers from 8 most popular classifier families, we found that some classifiers achieve highly satisfactory accuracy rates. Furthermore, we have rank-ordered the performances and investigated performance variations amongst and within the 8 classifier families. This paper is, to the best of our knowledge, the first systematic study of CDS Proxy construction by Machine Learning techniques, and the first systematic classifier comparison study based entirely on financial market data. Its findings both confirm and contrast existing classifier performance literature. Given the typically highly correlated nature of financial data, we investigated the impact of correlation on classifier performance. The techniques used in this paper should be of interest for financial institutions seeking a CDS Proxy method, and can serve for proxy construction for other financial variables. Some directions for future research are indicated.
B23|Nonlinear and asymmetric pricing behaviour in the Spanish gasoline market|Over the last decades a transition from a state-own monopoly to a private business took place in the Spanish fuel sector. To figure out whether downstream prices react differently to upstream price increases than to price decreases, alternative dynamic nonlinear and asymmetric error correction models are applied to weekly price data. This paper analyse the existence of price asymmetries in the fuel market in Spain during the 2011–2016 period. In comparison with traditional asymmetric price theory literature, this paper introduces a new double threshold error correction (ECM) model (DT-ECM) and new double logistic ECM models and compares them with more common linear ECM, time varying parameter models (TV-ECM), threshold autoregressive models (T-ECM), smooth transition autoregressive (STAR) models and nonlinear error correction (Logistic-ECM) and double threshold Logistic (DT-Logistic ECM). The asymmetric results found in Spain in the oil sector, show that sophisticated bivariate short-run nonlinearities are present in the gasoline market prices and that those price reactions depend on two main aspects; whether the oil price increases or decreases and on the stage (above of below) of the prices of gasoline relative to their long-run expected crude oil prices (error correction term). Those empirical results are consistent with the economic explanations based on market power-collusion and/or with consumer having search costs.
B23|John Denis Sargan at the London School of Economics|During his period at the LSE from the early 1960s to the mid 1980s, John Denis Sargan rose to international prominence and the LSE emerged as the world’s leading centre for econometrics. Within this context, we examine the life of Denis Sargan, describe his major research accomplishments, recount the work of his many doctoral students, and track this remarkable period that constitutes the Sargan era of econometrics at the LSE.
B23|Business Cycle Estimation with High-Pass and Band-Pass Local Polynomial Regression|Filters constructed on the basis of standard local polynomial regression (LPR) methods have been used in the literature to estimate the business cycle. We provide a frequency domain interpretation of the contrast filter obtained by the difference of a series and its long-run LPR component and show that it operates as a kind of high-pass filter, so that it provides a noisy estimate of the cycle. We alternatively propose band-pass local polynomial regression methods aimed at isolating the cyclical component. Results are compared to standard high-pass and band-pass filters. Procedures are illustrated using the US GDP series.
B23|A Note on Identification of Bivariate Copulas for Discrete Count Data|Copulas have enjoyed increased usage in many areas of econometrics, including applications with discrete outcomes. However, Genest and Nešlehová (2007) present evidence that copulas for discrete outcomes are not identified, particularly when those discrete outcomes follow count distributions. This paper confirms the Genest and Nešlehová result using a series of simulation exercises. The paper then proceeds to show that those identification concerns diminish if the model has a regression structure such that the exogenous variable(s) generates additional variation in the outcomes and thus more completely covers the outcome domain.
B23|Structural Breaks, Inflation and Interest Rates: Evidence from the G7 Countries|This study reconsiders the common unit root/co-integration approach to test for the Fisher effect for the economies of the G7 countries. We first show that nominal interest and inflation rates are better represented as I(0) variables. Later, we use the Bai–Perron procedure to show the existence of structural changes in the Fisher equation. After considering these breaks, we find very limited evidence of a total Fisher effect as the transmission coefficient of the expected inflation rates to nominal interest rates is very different than one.
B23|Testing for a Structural Break in a Spatial Panel Model|We consider the problem of testing for a structural break in the spatial lag parameter in a panel model (spatial autoregressive). We propose a likelihood ratio test of the null hypothesis of no break against the alternative hypothesis of a single break. The limiting distribution of the test is derived under the null when both the number of individual units N and the number of time periods T is large or N is ﬁxed and T is large. The asymptotic critical values of the test statistic can be obtained analytically. We also propose a break-date estimator that can be employed to determine the location of the break point following evidence against the null hypothesis. We present Monte Carlo evidence to show that the proposed procedure performs well in ﬁnite samples. Finally, we consider an empirical application of the test on budget spillovers and interdependence in ﬁscal policy within the U.S. states.
B23|Goodness-of-Fit Tests for Copulas of Multivariate Time Series|In this paper, we study the asymptotic behavior of the sequential empirical process and the sequential empirical copula process, both constructed from residuals of multivariate stochastic volatility models. Applications for the detection of structural changes and specification tests of the distribution of innovations are discussed. It is also shown that if the stochastic volatility matrices are diagonal, which is the case if the univariate time series are estimated separately instead of being jointly estimated, then the empirical copula process behaves as if the innovations were observed; a remarkable property. As a by-product, one also obtains the asymptotic behavior of rank-based measures of dependence applied to residuals of these time series models.
B23|Accuracy and Efficiency of Various GMM Inference Techniques in Dynamic Micro Panel Data Models|Studies employing Arellano-Bond and Blundell-Bond generalized method of moments (GMM) estimation for linear dynamic panel data models are growing exponentially in number. However, for researchers it is hard to make a reasoned choice between many different possible implementations of these estimators and associated tests. By simulation, the effects are examined in terms of many options regarding: (i) reducing, extending or modifying the set of instruments; (ii) specifying the weighting matrix in relation to the type of heteroskedasticity; (iii) using (robustified) 1-step or (corrected) 2-step variance estimators; (iv) employing 1-step or 2-step residuals in Sargan-Hansen overall or incremental overidentification restrictions tests. This is all done for models in which some regressors may be either strictly exogenous, predetermined or endogenous. Surprisingly, particular asymptotically optimal and relatively robust weighting matrices are found to be superior in finite samples to ostensibly more appropriate versions. Most of the variants of tests for overidentification and coefficient restrictions show serious deficiencies. The variance of the individual effects is shown to be a major determinant of the poor quality of most asymptotic approximations; therefore, the accurate estimation of this nuisance parameter is investigated. A modification of GMM is found to have some potential when the cross-sectional heteroskedasticity is pronounced and the time-series dimension of the sample is not too small. Finally, all techniques are employed to actual data and lead to insights which differ considerably from those published earlier.
B23|A Simple Test for Causality in Volatility|An early development in testing for causality (technically, Granger non-causality) in the conditional variance (or volatility) associated with financial returns was the portmanteau statistic for non-causality in the variance of Cheng and Ng (1996). A subsequent development was the Lagrange Multiplier (LM) test of non-causality in the conditional variance by Hafner and Herwartz (2006), who provided simulation results to show that their LM test was more powerful than the portmanteau statistic for sample sizes of 1000 and 4000 observations. While the LM test for causality proposed by Hafner and Herwartz (2006) is an interesting and useful development, it is nonetheless arbitrary. In particular, the specification on which the LM test is based does not rely on an underlying stochastic process, so the alternative hypothesis is also arbitrary, which can affect the power of the test. The purpose of the paper is to derive a simple test for causality in volatility that provides regularity conditions arising from the underlying stochastic process, namely a random coefficient autoregressive process, and a test for which the (quasi-) maximum likelihood estimates have valid asymptotic properties under the null hypothesis of non-causality. The simple test is intuitively appealing as it is based on an underlying stochastic process, is sympathetic to Granger’s (1969, 1988) notion of time series predictability, is easy to implement, and has a regularity condition that is not available in the LM test.
B23|Regime Switching Vine Copula Models for Global Equity and Volatility Indices|For nearly every major stock market there exist equity and implied volatility indices. These play important roles within finance: be it as a benchmark, a measure of general uncertainty or a way of investing or hedging. It is well known in the academic literature that correlations and higher moments between different indices tend to vary in time. However, to the best of our knowledge, no one has yet considered a global setup including both equity and implied volatility indices of various continents, and allowing for a changing dependence structure. We aim to close this gap by applying Markov-switching R -vine models to investigate the existence of different, global dependence regimes. In particular, we identify times of “normal” and “abnormal” states within a data set consisting of North-American, European and Asian indices. Our results confirm the existence of joint points in a time at which global regime switching between two different R -vine structures takes place.
B23|Consistency of Trend Break Point Estimator with Underspecified Break Number|This paper discusses the consistency of trend break point estimators when the number of breaks is underspecified. The consistency of break point estimators in a simple location model with level shifts has been well documented by researchers under various settings, including extensions such as allowing a time trend in the model. Despite the consistency of break point estimators of level shifts, there are few papers on the consistency of trend shift break point estimators in the presence of an underspecified break number. The simulation study and asymptotic analysis in this paper show that the trend shift break point estimator does not converge to the true break points when the break number is underspecified. In the case of two trend shifts, the inconsistency problem worsens if the magnitudes of the breaks are similar and the breaks are either both positive or both negative. The limiting distribution for the trend break point estimator is developed and closely approximates the finite sample performance.
B23|Fractional Unit Root Tests Allowing for a Structural Change in Trend under Both the Null and Alternative Hypotheses|This paper considers testing procedures for the null hypothesis of a unit root process against the alternative of a fractional process, called a fractional unit root test. We extend the Lagrange Multiplier (LM) tests of Robinson (1994) and Tanaka (1999), which are locally best invariant and uniformly most powerful, to allow for a slope change in trend with or without a concurrent level shift under both the null and alternative hypotheses. We show that the limit distribution of the proposed LM tests is standard normal. Finite sample simulation experiments show that the tests have good size and power. As an empirical analysis, we apply the tests to the Consumer Price Indices of the G7 countries.
B23|Between Institutions and Global Forces: Norwegian Wage Formation Since Industrialisation|This paper reviews the development of labour market institutions in Norway, shows how labour market regulation has been related to the macroeconomic development, and presents dynamic econometric models of nominal and real wages. Single equation and multi-equation models are reported. The econometric modelling uses a new data set with historical time series of wages and prices, unemployment and labour productivity. Impulse indicator saturation is used to achieve robust estimation of focus parameters, and the breaks are interpreted in the light of the historical overview. A relatively high degree of constancy of the key parameters of the wage setting equation is documented, over a considerably longer historical time period than earlier studies have done. The evidence is consistent with the view that the evolving system of collective labour market regulation over long periods has delivered a certain necessary level of coordination of wage and price setting. Nevertheless, there is also evidence that global forces have been at work for a long time, in a way that links real wages to productivity trends in the same way as in countries with very different institutions and macroeconomic development.
B23|Acknowledgement to Reviewers of Econometrics in 2016|The editors of Econometrics would like to express their sincere gratitude to the following reviewers for assessing manuscripts in 2016.[...]
B23|Endogeneity, Time-Varying Coefficients, and Incorrect vs. Correct Ways of Specifying the Error Terms of Econometric Models|Using the net effect of all relevant regressors omitted from a model to form its error term is incorrect because the coefficients and error term of such a model are non-unique. Non-unique coefficients cannot possess consistent estimators. Uniqueness can be achieved if; instead; one uses certain “sufficient sets” of (relevant) regressors omitted from each model to represent the error term. In this case; the unique coefficient on any non-constant regressor takes the form of the sum of a bias-free component and omitted-regressor biases. Measurement-error bias can also be incorporated into this sum. We show that if our procedures are followed; accurate estimation of bias-free components is possible.
B23|A Fast Algorithm for the Computation of HAC Covariance Matrix Estimators|This paper considers the algorithmic implementation of the heteroskedasticity and autocorrelation consistent (HAC) estimation problem for covariance matrices of parameter estimators. We introduce a new algorithm, mainly based on the fast Fourier transform, and show via computer simulation that our algorithm is up to 20 times faster than well-established alternative algorithms. The cumulative effect is substantial if the HAC estimation problem has to be solved repeatedly. Moreover, the bandwidth parameter has no impact on this performance. We provide a general description of the new algorithm as well as code for a reference implementation in R .
B23|Advances in specification testing|Testing the specification of econometric models has come a long way from the t tests and F tests of the classical normal linear model. In this paper, we trace the broad outlines of the development of specification testing, along the way discussing the role of structural versus purely statistical models. Inferential procedures have had to advance in tandem with techniques of estimation, and so we discuss the generalized method of moments, non parametric inference, empirical likelihood and estimating functions. Mention is made of some recent literature, in particular, of weak instruments, non parametric identification and the bootstrap.
B23|Identification, Instruments, Omitted Variables, and Rudimentary Models: Fallacies in the ‘Experimental Approach’ to Econometrics|Since identification, instrumental variables and variables exclusion, core concepts in econometrics, are entwined, several questions arise: How is identification related to the existence of IVs? How are identification criteria related to omitted variables? Is omission/inclusion of variables from a model’s equations part of the definition of IVs? Is exogeneity a critical claim to an IV? Is ‘omitted variables’ a meaningful term for a single equation when its ‘environment’ is incompletely described? Which are the borderlines between omitted, observable variables, omitted non-modeled variables, latent variables represented by proxies or measurement error mechanisms? These are among the questions addressed in this paper, partly with reference to the conflict between ‘experimentalists’ and ‘structuralists’, specifically relating to: (i) the contrast between ‘rudimentary models’ and models for ‘limited information inference’, (ii) the distinction between exogeneity of variables and the orthogonality claim for IVs and disturbances or errors, (iii) the role of predetermined variables in selecting IVs, and (iv) the ‘omitted variables’ concept and the role of IVs in ‘handling’ such variables, when considering the ‘origin’ of the omission.
B23|Economic and Political determinants of government expenditure in the state of Jammu and Kashmir (India): A multivariate co-integration analysis|The study examine the long and short run determinants of marked expansion of government expenditure in the state of Jammu and Kashmir. Using annual time series data for the period 1984-2013 and broader data pertaining to economic and political dimension, the paper seeks to identify the significant variables from each dimension that determines the growth of government expenditure in the state. Further, an attempt is made to examine whether the short run economic and political dimensions play any significant role in correcting the disequilibrium in government expenditure over the long run. Using multivariate co-integration technique followed by VECM and OLS model, we find that that absolute level of Government expenditure is largely determined by economic and political environment in the state. From the point of view of economic dimension, the study reveals that NSDP, tax revenue and grants from center are positively significant in determining government expenditure both in long as well as short run, while rate of unemployment and Gross fiscal deficit are found to be negative but significant determinants of government expenditure both in long and short run. An analysis of political dimension exhibits that Majoritarian government leads to lesser government expenditure than coalition government or governorâs rule. Law and order found to be negative but significant determinant of government expenditure while as election cycle is insignificant. Finally, the study reveals that the speed of adjustment of economic variables is significant and any disequilibrium in government expenditure is being restored by economic variables within the period of 2 years.
B23|Competitiveness of selected countries from Central and Eastern Europe in the era of globalisation|The changes that took place in the late twentieth century led to the transformation of the political system in the countries of Central and Eastern Europe (CEE). As a result, there has been an increase in the competitiveness of some of the economies among the CEE states. Due to different priorities and goals, these countries are also characterised by different levels in socio-economic development. The aim of the article is to identify the determinants affecting the competitiveness among the selected CEE countries. Based on Eurostat data, a set of determinants affecting competitiveness was established. A number of determinants have been eliminated in relation to the variation coefficient. At the same time, a classification of the level of competitiveness among the CEE countries has been made by using the Perkal method. The analysis used 14 selected indicators, 10 of which are considered as stimulating and 4 as deteriorating the competitiveness of national economies. The result led to obtaining a synthetic level indicator of potential of the CEE countries. Following the findings of the conducted analysis, the highest economic competitiveness exists in Estonia and in the Czech Republic, while the lowest was found in Romania and Bulgaria. The results of the evaluation obtained with the Perkal method concerning the competitiveness of the CEE countries that belong to the EU are largely consistent with those presented in different global competitiveness rankings. However, the method applied in this article seems much simpler and less time-consuming, allowing at the same time an optimal choice of analytical determinants. The selected linear Pearson correlation’s coefficient confirmed that there is a strong positive relationship between the designated values of the synthetic indicator of competitiveness and the GDP per capita. This confirms the validity of test method used.
B23|CDS Rate Construction Methods by Machine Learning Techniques|Regulators require financial institutions to estimate counterparty default risks from liquid CDS quotes for the valuation and risk management of OTC derivatives. However, the vast majority of counterparties do not have liquid CDS quotes and need proxy CDS rates. Existing methods cannot account for counterparty-specific default risks; we propose to construct proxy CDS rates by associating to illiquid counterparty liquid CDS Proxy based on Machine Learning Techniques. After testing 156 classifiers from 8 most popular classifier families, we found that some classifiers achieve highly satisfactory accuracy rates. Furthermore, we have rank-ordered the performances and investigated performance variations amongst and within the 8 classifier families. This paper is, to the best of our knowledge, the first systematic study of CDS Proxy construction by Machine Learning techniques, and the first systematic classifier comparison study based entirely on financial market data. Its findings both confirm and contrast existing classifier performance literature. Given the typically highly correlated nature of financial data, we investigated the impact of correlation on classifier performance. The techniques used in this paper should be of interest for financial institutions seeking a CDS Proxy method, and can serve for proxy construction for other financial variables. Some directions for future research are indicated.
B23|Investissement public et croissance économique au Cameroun<BR>[Public Investment and Economic Growth in Cameroon]|The present study analyzes the effect of the public investment on the economic growth in Cameroon. It is a question of appreciating the direct effect of the public investment on the growth on the one hand, and of highlighting the indirect effect of this last on the economic growth through private investment in addition. The econometric regression, using a modeling by method ARDL worked out by Pesaran et al. (2001), we end to the following results: (i) the public investment impacts negatively the long-term growth; (ii) no effect of drive of the private investment by the public investment is observed, and (iii) any shock on economic growth observed within a given year is may be entirely integrated within a period of two year.
B23|Challenging Lucas: from overlapping generations to infinite-lived agent models|The canonical history of macroeconomics, one of the rival schools of thought and the great economists, gives Robert Lucas a prominent role in shaping the recent developments in the area. According to it, his followers were initially split into two camps, the “real business cycle” theorists with models of efficient fluctuations, and the “new-Keynesians” with models in which fluctuations are costly, and the government has a role to play, due to departures from the competitive equilibrium (such as nominal rigidities and imperfect competition). Later on, a consensus view emerged (the so-called new neoclassical synthesis), based on the dynamic stochastic general equilibrium (DSGE) model, which combines elements of the models developed by economists of those two groups. However, this account misses critical developments, as already pointed out by Cherrier and Saïdi (2015). As a reaction to Lucas’s 1972 policy ineffectiveness results, based on an overlapping generations (OLG) model, a group of macroeconomists realized that a competitive OLG model may have a continuum of equilibria and that this indeterminacy justified government intervention for competitive cycles that emerged even in deterministic models. We can identify here two distinct, but related, groups: one of the deterministic cycles of David Gale, David Cass, and Jean-Michel Grandmont, and another of the stochastic models and sunspots of Karl Shell, Roger Guesnerie, Roger Farmer and Costas Azariadis (Lucas’s PhD student). Here, the OLG was the workhorse model. Following from these works, a number of authors, including Michael Woodford, argued that similar results could occur in models with infinitely lived agents when there are various kinds of market imperfections. With such generalization, some of these macroeconomists saw that once these imperfections are introduced, nothing important for business cycle modeling was lost and they could therefore leave the OLG model aside as a model of business fluctuations, to the dismay of authors such as Grandmont, Robert Solow and Frank Hahn. In this paper, we scrutinize the differences between the deterministic cycles and sunspot groups and explore the many efforts of building a dynamic competitive business cycle model that implies a role for the government to play. We then assess the transformation process that took place in the late 1980s when several macroeconomists switched from OLG to infinite-lived agents models with imperfections that eventually became central to the DSGE literature. With this we hope to shed more light on the origins of new neoclassical synthesis.
B23|Coast to Coast: How MIT's students linked the Solow model and optimal growth theory|Textbook narratives usually describe the Ramsey-Cass-Koopmans model of optimal growth as an important development over Solow’s model. The constant saving rate rule of the latter is replaced by an endogenous determination of savings rates based on utility maximization behaviour of the former. However, neither Tjalling Koopmans nor David Cass were trying to upgrade Robert Solow’s modelling of savings with their contributions. Koopmans was pushing for utilitarian analysis to study economic growth within the activity analysis community, with the help of Edmond Malinvaud. Cass and his colleagues at Stanford’s graduate program were exploring the multiple applications of the maximum principle on economic growth models, influenced mostly by Hirofumi Uzawa. When the group of Stanford students graduated and moved to different departments, Karl Shell brought his interest on optimal growth and technical progress to the MIT. There, he organized a conference in 1965-66 that united his Stanford colleagues and MIT young scholars to discuss optimal growth theory. The conference represented the formation of a scientific community that consolidated optimal growth theory in the second half of the 1960 decade. The link between optimal growth theory and Solow’s model was consolidated by this community of young students. Most of them were advised by Solow. This paper plans to shed some light on the importance of the MIT in the history of optimal growth theory and to show how the standard textbook narrative is a MIT-only story.
B23|The Double Edge of Case-Studies: A Frame-Based Definition of Economic Models|Starting on the 1950s, the philosophy of science rearranged its methods. Logic gradually lost space as philosophers’ exclusive tool of analysis, whereas case-studies and historical methods emerged as viable instruments. However, the methodological transposition concealed a double-edged knife. The approximation of philosophy and scientific practice happened at the expense of exponentially widening the list of possible philosophical definitions for scientific concepts, creating numerous incomparable explanations. The present paper thus advocates in favor of the use of more objective tools for defining scientific concepts, looking to reduce incomparability of definitions. From this point of view, framing proposals are presented as suitable mechanisms for reasoned definitions, given that frames’ exemplar-dependency entail the necessity of organized selections of case-studies. In this context, community targeting understood as a limit of time and discipline for the selections of case-studies stood out as an organizational criterion. Considering this criterion, the present paper selected as case-studies, for defining the concept of an economic model, the first two cases where the term ‘model’ was utilized for referring to abstract economic reasoning. The proposed frame presented the following attributes: mathematical structure, adaptability, simplification, neutrality and purpose.
B23|Transforming the Abstract into Concrete: The Dual Semantic Roots of Economic Modelling|The existing connotation of models in economics emerged only in the XXth century, substituting the previous methodological terms: ‘scheme’, ‘diagram’ and ‘system’. This immature characteristic of economic models provokes important questions: when exactly and why schemes, diagrams, and systems became models? The purpose of the present paper is to look into the history of economic thought, searching for some explanations regarding these changes. The problem will be observed from a semantic point of view, reformulating a broad view of how the term was introduced. Considering this, the paper analyzes how the combination of cartesian and newtonian mathematics with psychological and empirical economics occurred, paving the way for the insertion of the term model into the jargon of economists in two different manners. The review shows that there were two semantic pioneers: Tinbergen (1935) and Von Neumann (1945). Both were the first economists to use the term ‘model’ for pure abstract reasoning in each of their respective methodologies.
B23|On the causal relationship between nutrition and economic Growth: Evidence from sub-Saharan Africa|No abstract is available for this item.
B23|CONVERGÊNCIA E DINÂMICA AGROPECUÁRIA: UMA ANÁLISE ESPACIAL ENTRE OS ANOS DE 1990 e 2013| The growing of agricultural has been object of many researches that highlights the importance of this sector in North region. The economic geography has showed that economics phenomenon can present spatial dependence, mainly considered the diffusion effect. The objective of this work is to verify the existence of spatial cluster of agriculture in theBrazilians microregions, with emphasis in role of the variables livestock cattle and permanent and temporary crops for de years of 1990 and 2013. The methodology used was the explorative analyze of spatial data (EASD) by calculation of I Moran index global and locally and clusters analyzes. The results show the existence of spatial autocorrelation in the agricultural among Brazilian microregions, in the variables research in this study. The also evidence of a migration of these productions from South and Southeast regions to Middle-west and North regions.The estimated econometric models show the existence of a strong spatial effect on livestock production in Brazilian micro-regions.These also show the existence of spatial convergence in the Brazilian livestock production.
B23|Determinants Of The Offer For Educational Services Ofthe Romanian Military Higher Education Institutions|In a society of changes which is dynamic, in a state of permanent change, the offerfor educational products and services is fundamented on the needs for instruction, it is flexibleand addresses to diverse types of consumers. Adapting the educational offer to the demands ofthe labor market and of the employers, adjusting depending on the qualification needs, arepriorities of the higher education institutions, of the Romanian educational policies. Beginningwith the hypothesis that the educational offer of the military higher education system depends onmacroeconomic factors, our analysis reveals that the demographic evolution does not exert asignificant influence over the number of admission places for the military higher educationinstitutions, the number of national military high school graduates having the strongest influenceover the educational offer of the military higher education system.
B23|From real business cycle and new Keynesian to DSGE Macroeconomics: facts and models in the emergence of a consensus|Macroeconomists have emphasized the force of facts in forging a consensus understanding of business cycle fluctuations. According to this view, rival economists could no longer hold disparate views on the topic because “facts have a way of not going away” (Blanchard 2009). But how can macroeconomists observe the workings of an economy? Essentially through building and manipulating models. Thus the construction of macroeconomic facts –or “stylized facts”–, empirical regularities that come to be widely accepted, opens up technical spaces where macroeconomists negotiated their theoretical commitments and eventually allowed a consensus to emerge. I argue that this is an important element in the history of the DSGE macroeconomics.
B23|Reflections and Comments on Randomness|This article is not directed to any purpose other than conveying the effort of understanding to the ones that are able to understand randomness.
B23|The New Keynesian Theory And Its Associated Model|The basic new Keynesian model rendered in this paper, as well as the analysis of the reaction of economic variables to the occurrence of a structural, monetary policy shock, strengthen the hypothesis exposed at pure theoretical level, namely the active role of Central Banks in economy, the classical dichotomy between the nominal and the real economic factors being abandoned. As reflected by the impulse-response function graphs, the model endogenous variables: output, output gap, labour hours, inflation rate, nominal interest rate and real interest rate, clearly react to the exogenous variables of the same, represented by structural shocks, returning afterwards, more or less quickly, to their initial steady state. In compliance with the literature in the matter, the monetary entity policies, although having a lower impact than the one generated by the technological changes, manifest obvious influences on the model variables, therefore affecting both the decisions of the representative agents, at microeconomic level, and the aggregate economy, as a whole.
B23|El Plan de Ordenamiento Territorial (POT) de Cali, una aproximación|En este documento se describe, analiza y evalúa la política pública de desarrollo urbanístico del Plan de Ordenamiento Territorial en la ciudad de Cali (POT, 2000). La incidencia de dicha política pública dentro de la transformación física de la urbe se puede evidenciar a través de un indicador tan elemental como es el precio del suelo urbano -en relación con el mercado de los precios de la tierra urbana, debido a que este factor señalizador de mercado determina las reglas del juego en una sociedad-. Es decir, se tiene una herramienta metodológica en términos de evaluación de estas políticas públicas específicas.
B23|Economics and Physics: A Forgotten Discussion|This paper has the objective of reviewing some of the key aspects that involve the association between physics and economics. It also invites to considerate the history behind the neoclassical model and how its physical origin is not well known. It is curious to see the close relation between these two sciences, but it is also curious how this could lead to misinterpretations and beliefs of economics being dependent on physics, which clearly is not the case.
B23|Raúl Prebisch: historia, pensamiento y vigencia de la teoría de la transformación para el desarrollo de América Latina|Raúl Prebisch fue sin duda uno de los economistas más reconocidos e influyentes de América Latina, tanto por sus aportes teóricos para el desarrollo de la región como por su participación en la creación institucional. En este trabajo nos proponemos repasar en forma paralela su vida y pensamiento, destacando los principales debates en los que se vio involucrado. Nos interesa, en particular, resaltar el legado de sus últimas obras, donde completó su teoría sobre el desarrollo latinoamericano a la luz de la vasta experiencia obtenida en su larga trayectoria académica y profesional.
B23|CoPS-style CGE modelling and analysis|CoPS is a world leader in CGE modelling. This paper describes a brief history of CoPS style of CGE modelling; from ORANI to new generation CoPS models, and discusses the distinctive features of CoPS style models. In addition, it introduces solution methods, notations, and tools that are used in CoPS style of CGE modelling and analysis.
B23|Tribute To T.W. Anderson|Professor T.W. Anderson passed away on September 17, 2016 at the age of 98 years after an astonishing career that spanned more than seven decades. Standing at the nexus of the statistics and economics professions, Ted Anderson made enormous contributions to both disciplines, playing a significant role in the birth of modern econometrics with his work on structural estimation and testing in the Cowles Commission during the 1940s, and educating successive generations through his brilliant textbook expositions of time series and multivariate analysis. This article is a tribute to his many accomplishments.<br><small>(This abstract was borrowed from another version of this item.)</small>
B23|Econometric Methods for Evaluating of Open National Innovative Systems|The urgency of the problem stated in the paper is reasoned by the fact that the rapid acceleration of the changes of the existing economic and institutional conditions raises the need to develop new theoretical-methodological and practical approaches to the problems’ solving in order to achieve sustainable growth of innovation growth. The purpose of the paper is developing of a methodology to assess the open national innovation systems through the use of econometric models. The leading approach to the study of this problem is the method of economic-mathematical modeling, allowing evaluating of the level of national innovation systems’ openness using quantitative indicators and building of innovative development’s forecasts. The article reveals the essence of open innovations, open national innovation systems, on the basis of production functions the forecast of the share of service sector’s value added in gross domestic product is built using additive and multiplicative models. Paper Submissions are of theoretical and practical significance for open innovation management models’ development, as well as for the development of the state innovation policy’s strategy.
B23|The Analysis of Regional Development on the Basis of Corporate Structures’ Activity|Modern conditions upgrade issues concerning the search of ways to develop and increase activity efficiency of large-scale industrial associations that possess a high concentration of material and scientific resources, and influence significantly both a certain economic sector or region and the country’s development in general. This paper aims to substantiate author’s techniques to define the impact of corporate entities on a regional social-economic sphere. The authors have highlighted main features of Russian corporate entities and possible forms of ownership. Types of regions and their features the consideration of which is necessary to evaluate regional development are presented on the basis of the author’s estimation procedure. The factor analysis made the foundation for a rating assessment of corporate entities impact on the region; it allowed to estimate quantitatively corporations’ activity and the level of regional development at a certain time period. The paper is intended for heads of regions, top-managers, researchers dealing with issues of corporate entities’ and regional economy development.
B23|Jackknife Bias Reduction in the Presence of a Near-Unit Root|This paper considers the specification and performance of jackknife estimators of the autoregressive coefficient in a model with a near-unit root. The limit distributions of sub-sample estimators that are used in the construction of the jackknife estimator are derived, and the joint moment generating function (MGF) of two components of these distributions is obtained and its properties explored. The MGF can be used to derive the weights for an optimal jackknife estimator that removes fully the first-order finite sample bias from the estimator. The resulting jackknife estimator is shown to perform well in finite samples and, with a suitable choice of the number of sub-samples, is shown to reduce the overall finite sample root mean squared error, as well as bias. However, the optimal jackknife weights rely on knowledge of the near-unit root parameter and a quantity that is related to the long-run variance of the disturbance process, which are typically unknown in practice, and so, this dependence is characterised fully and a discussion provided of the issues that arise in practice in the most general settings.
B23|Sequentially Adaptive Bayesian Learning for a Nonlinear Model of the Secular and Cyclical Behavior of US Real GDP|There is a one-to-one mapping between the conventional time series parameters of a third-order autoregression and the more interpretable parameters of secular half-life, cyclical half-life and cycle period. The latter parameterization is better suited to interpretation of results using both Bayesian and maximum likelihood methods and to expression of a substantive prior distribution using Bayesian methods. The paper demonstrates how to approach both problems using the sequentially adaptive Bayesian learning algorithm and sequentially adaptive Bayesian learning algorithm (SABL) software, which eliminates virtually of the substantial technical overhead required in conventional approaches and produces results quickly and reliably. The work utilizes methodological innovations in SABL including optimization of irregular and multimodal functions and production of the conventional maximum likelihood asymptotic variance matrix as a by-product.
B23|Parallelization Experience with Four Canonical Econometric Models Using ParMitISEM|This paper presents the parallel computing implementation of the MitISEM algorithm, labeled Parallel MitISEM . The basic MitISEM algorithm provides an automatic and flexible method to approximate a non-elliptical target density using adaptive mixtures of Student- t densities, where only a kernel of the target density is required. The approximation can be used as a candidate density in Importance Sampling or Metropolis Hastings methods for Bayesian inference on model parameters and probabilities. We present and discuss four canonical econometric models using a Graphics Processing Unit and a multi-core Central Processing Unit version of the MitISEM algorithm. The results show that the parallelization of the MitISEM algorithm on Graphics Processing Units and multi-core Central Processing Units is straightforward and fast to program using MATLAB. Moreover the speed performance of the Graphics Processing Unit version is much higher than the Central Processing Unit one.
B23|Evolutionary Sequential Monte Carlo Samplers for Change-Point Models|Sequential Monte Carlo (SMC) methods are widely used for non-linear filtering purposes. However, the SMC scope encompasses wider applications such as estimating static model parameters so much that it is becoming a serious alternative to Markov-Chain Monte-Carlo (MCMC) methods. Not only do SMC algorithms draw posterior distributions of static or dynamic parameters but additionally they provide an estimate of the marginal likelihood. The tempered and time (TNT) algorithm, developed in this paper, combines (off-line) tempered SMC inference with on-line SMC inference for drawing realizations from many sequential posterior distributions without experiencing a particle degeneracy problem. Furthermore, it introduces a new MCMC rejuvenation step that is generic, automated and well-suited for multi-modal distributions. As this update relies on the wide heuristic optimization literature, numerous extensions are readily available. The algorithm is notably appropriate for estimating change-point models. As an example, we compare several change-point GARCH models through their marginal log-likelihoods over time.
B23|Bayesian Nonparametric Measurement of Factor Betas and Clustering with Application to Hedge Fund Returns|We define a dynamic and self-adjusting mixture of Gaussian Graphical Models to cluster financial returns, and provide a new method for extraction of nonparametric estimates of dynamic alphas (excess return) and betas (to a choice set of explanatory factors) in a multivariate setting. This approach, as well as the outputs, has a dynamic, nonstationary and nonparametric form, which circumvents the problem of model risk and parametric assumptions that the Kalman filter and other widely used approaches rely on. The by-product of clusters, used for shrinkage and information borrowing, can be of use to determine relationships around specific events. This approach exhibits a smaller Root Mean Squared Error than traditionally used benchmarks in financial settings, which we illustrate through simulation. As an illustration, we use hedge fund index data, and find that our estimated alphas are, on average, 0.13% per month higher (1.6% per year) than alphas estimated through Ordinary Least Squares. The approach exhibits fast adaptation to abrupt changes in the parameters, as seen in our estimated alphas and betas, which exhibit high volatility, especially in periods which can be identified as times of stressful market events, a reflection of the dynamic positioning of hedge fund portfolio managers.
B23|Return and Risk of Pairs Trading Using a Simulation-Based Bayesian Procedure for Predicting Stable Ratios of Stock Prices|We investigate the direct connection between the uncertainty related to estimated stable ratios of stock prices and risk and return of two pairs trading strategies: a conditional statistical arbitrage method and an implicit arbitrage one. A simulation-based Bayesian procedure is introduced for predicting stable stock price ratios, defined in a cointegration model. Using this class of models and the proposed inferential technique, we are able to connect estimation and model uncertainty with risk and return of stock trading. In terms of methodology, we show the effect that using an encompassing prior, which is shown to be equivalent to a Jeffreys’ prior, has under an orthogonal normalization for the selection of pairs of cointegrated stock prices and further, its effect for the estimation and prediction of the spread between cointegrated stock prices. We distinguish between models with a normal and Student t distribution since the latter typically provides a better description of daily changes of prices on financial markets. As an empirical application, stocks are used that are ingredients of the Dow Jones Composite Average index. The results show that normalization has little effect on the selection of pairs of cointegrated stocks on the basis of Bayes factors. However, the results stress the importance of the orthogonal normalization for the estimation and prediction of the spread—the deviation from the equilibrium relationship—which leads to better results in terms of profit per capital engagement and risk than using a standard linear normalization.
B23|Timing Foreign Exchange Markets|To improve short-horizon exchange rate forecasts, we employ foreign exchange market risk factors as fundamentals, and Bayesian treed Gaussian process (BTGP) models to handle non-linear, time-varying relationships between these fundamentals and exchange rates. Forecasts from the BTGP model conditional on the carry and dollar factors dominate random walk forecasts on accuracy and economic criteria in the Meese-Rogoff setting. Superior market timing ability for large moves, more than directional accuracy, drives the BTGP’s success. We explain how, through a model averaging Monte Carlo scheme, the BTGP is able to simultaneously exploit smoothness and rough breaks in between-variable dynamics. Either feature in isolation is unable to consistently outperform benchmarks throughout the full span of time in our forecasting exercises. Trading strategies based on ex ante BTGP forecasts deliver the highest out-of-sample risk-adjusted returns for the median currency, as well as for both predictable, traded risk factors.
B23|The Evolving Transmission of Uncertainty Shocks in the United Kingdom|This paper investigates if the impact of uncertainty shocks on the U.K. economy has changed over time. To this end, we propose an extended time-varying VAR model that simultaneously allows the estimation of a measure of uncertainty and its time-varying impact on key macroeconomic and financial variables. We find that the impact of uncertainty shocks on these variables has declined over time. The timing of the change coincides with the introduction of inflation targeting in the U.K.
B23|Bayesian Calibration of Generalized Pools of Predictive Distributions|Decision-makers often consult different experts to build reliable forecasts on variables of interest. Combining more opinions and calibrating them to maximize the forecast accuracy is consequently a crucial issue in several economic problems. This paper applies a Bayesian beta mixture model to derive a combined and calibrated density function using random calibration functionals and random combination weights. In particular, it compares the application of linear, harmonic and logarithmic pooling in the Bayesian combination approach. The three combination schemes, i.e ., linear, harmonic and logarithmic, are studied in simulation examples with multimodal densities and an empirical application with a large database of stock data. All of the experiments show that in a beta mixture calibration framework, the three combination schemes are substantially equivalent, achieving calibration, and no clear preference for one of them appears. The financial application shows that the linear pooling together with beta mixture calibration achieves the best results in terms of calibrated forecast.
B23|Spatial Econometrics: A Rapidly Evolving Discipline|Spatial econometrics has a relatively short history in the scenario of the scientific thought. Indeed, the term “spatial econometrics” was introduced only forty years ago during the general address delivered by Jean Paelinck to the annual meeting of the Dutch Statistical Association in May 1974 (see [1]). [...]
B23|Forecasting Value-at-Risk under Different Distributional Assumptions|Financial asset returns are known to be conditionally heteroskedastic and generally non-normally distributed, fat-tailed and often skewed. These features must be taken into account to produce accurate forecasts of Value-at-Risk (VaR). We provide a comprehensive look at the problem by considering the impact that different distributional assumptions have on the accuracy of both univariate and multivariate GARCH models in out-of-sample VaR prediction. The set of analyzed distributions comprises the normal, Student, Multivariate Exponential Power and their corresponding skewed counterparts. The accuracy of the VaR forecasts is assessed by implementing standard statistical backtesting procedures used to rank the different specifications. The results show the importance of allowing for heavy-tails and skewness in the distributional assumption with the skew-Student outperforming the others across all tests and confidence levels.
B23|A Conditional Approach to Panel Data Models with Common Shocks|This paper studies the effects of common shocks on the OLS estimators of the slopes’ parameters in linear panel data models. The shocks are assumed to affect both the errors and some of the explanatory variables. In contrast to existing approaches, which rely on using results on martingale difference sequences, our method relies on conditional strong laws of large numbers and conditional central limit theorems for conditionally-heterogeneous random variables.
B23|Acknowledgement to Reviewers of Econometrics in 2015|The editors of Econometrics would like to express their sincere gratitude to the following reviewers for assessing manuscripts in 2015. [...]
B23|Functional-Coefficient Spatial Durbin Models with Nonparametric Spatial Weights: An Application to Economic Growth|This paper considers a functional-coefficient spatial Durbin model with nonparametric spatial weights. Applying the series approximation method, we estimate the unknown functional coefficients and spatial weighting functions via a nonparametric two-stage least squares (or 2SLS) estimation method. To further improve estimation accuracy, we also construct a second-step estimator of the unknown functional coefficients by a local linear regression approach. Some Monte Carlo simulation results are reported to assess the finite sample performance of our proposed estimators. We then apply the proposed model to re-examine national economic growth by augmenting the conventional Solow economic growth convergence model with unknown spatial interactive structures of the national economy, as well as country-specific Solow parameters, where the spatial weighting functions and Solow parameters are allowed to be a function of geographical distance and the countries’ openness to trade, respectively.
B23|Multiple Discrete Endogenous Variables in Weakly-Separable Triangular Models|We consider a model in which an outcome depends on two discrete treatment variables, where one treatment is given before the other. We formulate a three-equation triangular system with weak separability conditions. Without assuming assignment is random, we establish the identification of an average structural function using two-step matching. We also consider decomposing the effect of the first treatment into direct and indirect effects, which are shown to be identified by the proposed methodology. We allow for both of the treatment variables to be non-binary and do not appeal to an identification-at-infinity argument.
B23|Volatility Forecasting: Downside Risk, Jumps and Leverage Effect|We provide empirical evidence of volatility forecasting in relation to asymmetries present in the dynamics of both return and volatility processes. Using recently-developed methodologies to detect jumps from high frequency price data, we estimate the size of positive and negative jumps and propose a methodology to estimate the size of jumps in the quadratic variation. The leverage effect is separated into continuous and discontinuous effects, and past volatility is separated into “good” and “bad”, as well as into continuous and discontinuous risks. Using a long history of the S & P500 price index, we find that the continuous leverage effect lasts about one week, while the discontinuous leverage effect disappears after one day. “Good” and “bad” continuous risks both characterize the volatility persistence, while “bad” jump risk is much more informative than “good” jump risk in forecasting future volatility. The volatility forecasting model proposed is able to capture many empirical stylized facts while still remaining parsimonious in terms of the number of parameters to be estimated.
B23|Computational Complexity and Parallelization in Bayesian Econometric Analysis|Challenging statements have appeared in recent years in the literature on advances in computational procedures.[...]
B23|A Method for Measuring Treatment Effects on the Treated without Randomization|This paper contributes to the literature on the estimation of causal effects by providing an analytical formula for individual specific treatment effects and an empirical methodology that allows us to estimate these effects. We derive the formula from a general model with minimal restrictions, unknown functional form and true unobserved variables such that it is a credible model of the underlying real world relationship. Subsequently, we manipulate the model in order to put it in an estimable form. In contrast to other empirical methodologies, which derive average treatment effects, we derive an analytical formula that provides estimates of the treatment effects on each treated individual. We also provide an empirical example that illustrates our methodology.
B23|Recovering the Most Entropic Copulas from Preliminary Knowledge of Dependence|This paper provides a new approach to recover relative entropy measures of contemporaneous dependence from limited information by constructing the most entropic copula (MEC) and its canonical form, namely the most entropic canonical copula (MECC). The MECC can effectively be obtained by maximizing Shannon entropy to yield a proper copula such that known dependence structures of data (e.g., measures of association) are matched to their empirical counterparts. In fact the problem of maximizing the entropy of copulas is the dual to the problem of minimizing the Kullback-Leibler cross entropy (KLCE) of joint probability densities when the marginal probability densities are fixed. Our simulation study shows that the proposed MEC estimator can potentially outperform many other copula estimators in finite samples.
B23|Unit Root Tests: The Role of the Univariate Models Implied by Multivariate Time Series|In cointegration analysis, it is customary to test the hypothesis of unit roots separately for each single time series. In this note, we point out that this procedure may imply large size distortion of the unit root tests if the DGP is a VAR. It is well-known that univariate models implied by a VAR data generating process necessarily have a finite order MA component. This feature may explain why an MA component has often been found in univariate ARIMA models for economic time series. Thereby, it has important implications for unit root tests in univariate settings given the well-known size distortion of popular unit root test in the presence of a large negative coefficient in the MA component. In a small simulation experiment, considering several popular unit root tests and the ADF sieve bootstrap unit tests, we find that, besides the well known size distortion effect, there can be substantial differences in size distortion according to which univariate time series is tested for the presence of a unit root.
B23|Distribution of Budget Shares for Food: An Application of Quantile Regression to Food Security 1|This study examines, using quantile regression, the linkage between food security and efforts to enhance smallholder coffee producer incomes in Rwanda. Even though in Rwanda smallholder coffee producer incomes have increased, inhabitants these areas still experience stunting and wasting. This study examines whether the distribution of the income elasticity for food is the same for coffee and noncoffee growing provinces. We find that that the share of expenditures on food is statistically different in coffee growing and noncoffee growing provinces. Thus, the increase in expenditure on food is smaller for coffee growing provinces than noncoffee growing provinces.
B23|Building a Structural Model: Parameterization and Structurality|A specific concept of structural model is used as a background for discussing the structurality of its parameterization. Conditions for a structural model to be also causal are examined. Difficulties and pitfalls arising from the parameterization are analyzed. In particular, pitfalls when considering alternative parameterizations of a same model are shown to have lead to ungrounded conclusions in the literature. Discussions of observationally equivalent models related to different economic mechanisms are used to make clear the connection between an economically meaningful parameterization and an economically meaningful decomposition of a complex model. The design of economic policy is used for drawing some practical implications of the proposed analysis.
B23|Bayesian Bandwidth Selection for a Nonparametric Regression Model with Mixed Types of Regressors|This paper develops a sampling algorithm for bandwidth estimation in a nonparametric regression model with continuous and discrete regressors under an unknown error density. The error density is approximated by the kernel density estimator of the unobserved errors, while the regression function is estimated using the Nadaraya-Watson estimator admitting continuous and discrete regressors. We derive an approximate likelihood and posterior for bandwidth parameters, followed by a sampling algorithm. Simulation results show that the proposed approach typically leads to better accuracy of the resulting estimates than cross-validation, particularly for smaller sample sizes. This bandwidth estimation approach is applied to nonparametric regression model of the Australian All Ordinaries returns and the kernel density estimation of gross domestic product (GDP) growth rates among the organisation for economic co-operation and development (OECD) and non-OECD countries.
B23|Stable-GARCH Models for Financial Returns: Fast Estimation and Tests for Stability|A fast method for estimating the parameters of a stable-APARCH not requiring likelihood or iteration is proposed. Several powerful tests for the (asymmetric) stable Paretian distribution with tail index 1 < α < 2 are used for assessing the appropriateness of the stable assumption as the innovations process in stable-GARCH-type models for daily stock returns. Overall, there is strong evidence against the stable as the correct innovations assumption for all stocks and time periods, though for many stocks and windows of data, the stable hypothesis is not rejected.
B23|Removing Specification Errors from the Usual Formulation of Binary Choice Models|We develop a procedure for removing four major specification errors from the usual formulation of binary choice models. The model that results from this procedure is different from the conventional probit and logit models. This difference arises as a direct consequence of our relaxation of the usual assumption that omitted regressors constituting the error term of a latent linear regression model do not introduce omitted regressor biases into the coefficients of the included regressors.
B23|Continuous and Jump Betas: Implications for Portfolio Diversification|Using high-frequency data, we decompose the time-varying beta for stocks into beta for continuous systematic risk and beta for discontinuous systematic risk. Estimated discontinuous betas for S&P500 constituents between 2003 and 2011 generally exceed the corresponding continuous betas. We demonstrate how continuous and discontinuous betas decrease with portfolio diversification. Using an equiweighted broad market index, we assess the speed of convergence of continuous and discontinuous betas in portfolios of stocks as the number of holdings increase. We show that discontinuous risk dissipates faster with fewer stocks in a portfolio compared to its continuous counterpart.
B23|Testing Symmetry of Unknown Densities via Smoothing with the Generalized Gamma Kernels|This paper improves a kernel-smoothed test of symmetry through combining it with a new class of asymmetric kernels called the generalized gamma kernels. It is demonstrated that the improved test statistic has a normal limit under the null of symmetry and is consistent under the alternative. A test-oriented smoothing parameter selection method is also proposed to implement the test. Monte Carlo simulations indicate superior finite-sample performance of the test statistic. It is worth emphasizing that the performance is grounded on the first-order normal limit and a small number of observations, despite a nonparametric convergence rate and a sample-splitting procedure of the test.
B23|Evaluating Eigenvector Spatial Filter Corrections for Omitted Georeferenced Variables|The Ramsey regression equation specification error test (RESET) furnishes a diagnostic for omitted variables in a linear regression model specification ( i.e. , the null hypothesis is no omitted variables). Integer powers of fitted values from a regression analysis are introduced as additional covariates in a second regression analysis. The former regression model can be considered restricted, whereas the latter model can be considered unrestricted; this first model is nested within this second model. A RESET significance test is conducted with an F -test using the error sums of squares and the degrees of freedom for the two models. For georeferenced data, eigenvectors can be extracted from a modified spatial weights matrix, and included in a linear regression model specification to account for the presence of nonzero spatial autocorrelation. The intuition underlying this methodology is that these synthetic variates function as surrogates for omitted variables. Accordingly, a restricted regression model without eigenvectors should indicate an omitted variables problem, whereas an unrestricted regression model with eigenvectors should result in a failure to reject the RESET null hypothesis. This paper furnishes eleven empirical examples, covering a wide range of spatial attribute data types, that illustrate the effectiveness of eigenvector spatial filtering in addressing the omitted variables problem for georeferenced data as measured by the RESET.
B23|Estimation of Gini Index within Pre-Specified Error Bound|Gini index is a widely used measure of economic inequality. This article develops a theory and methodology for constructing a confidence interval for Gini index with a specified confidence coefficient and a specified width without assuming any specific distribution of the data. Fixed sample size methods cannot simultaneously achieve both specified confidence coefficient and fixed width. We develop a purely sequential procedure for interval estimation of Gini index with a specified confidence coefficient and a specified margin of error. Optimality properties of the proposed method, namely first order asymptotic efficiency and asymptotic consistency properties are proved under mild moment assumptions of the distribution of the data.
B23|Market Microstructure Effects on Firm Default Risk Evaluation|Default probability is a fundamental variable determining the credit worthiness of a firm and equity volatility estimation plays a key role in its evaluation. Assuming a structural credit risk modeling approach, we study the impact of choosing different non parametric equity volatility estimators on default probability evaluation, when market microstructure noise is considered. A general stochastic volatility framework with jumps for the underlying asset dynamics is defined inside a Merton-like structural model. To estimate the volatility risk component of a firm we use high-frequency equity data: market microstructure noise is introduced as a direct effect of observing noisy high-frequency equity prices. A Monte Carlo simulation analysis is conducted to (i) test the performance of alternative non-parametric equity volatility estimators in their capability of filtering out the microstructure noise and backing out the true unobservable asset volatility; (ii) study the effects of different non-parametric estimation techniques on default probability evaluation. The impact of the non-parametric volatility estimators on risk evaluation is not negligible: a sensitivity analysis defined for alternative values of the leverage parameter and average jumps size reveals that the characteristics of the dataset are crucial to determine which is the proper estimator to consider from a credit risk perspective.
B23|Measuring the Distance between Sets of ARMA Models|A distance between pairs of sets of autoregressive moving average (ARMA) processes is proposed. Its main properties are discussed. The paper also shows how the proposed distance finds application in time series analysis. In particular it can be used to evaluate the distance between portfolios of ARMA models or the distance between vector autoregressive (VAR) models.
B23|Econometrics Best Paper Award 2016|Econometrics has had a distinguished start publishing over 92 articles since 2013, with 76,475 downloads.[...]
B23|Jump Variation Estimation with Noisy High Frequency Financial Data via Wavelets|This paper develops a method to improve the estimation of jump variation using high frequency data with the existence of market microstructure noises. Accurate estimation of jump variation is in high demand, as it is an important component of volatility in finance for portfolio allocation, derivative pricing and risk management. The method has a two-step procedure with detection and estimation. In Step 1, we detect the jump locations by performing wavelet transformation on the observed noisy price processes. Since wavelet coefficients are significantly larger at the jump locations than the others, we calibrate the wavelet coefficients through a threshold and declare jump points if the absolute wavelet coefficients exceed the threshold. In Step 2 we estimate the jump variation by averaging noisy price processes at each side of a declared jump point and then taking the difference between the two averages of the jump point. Specifically, for each jump location detected in Step 1, we get two averages from the observed noisy price processes, one before the detected jump location and one after it, and then take their difference to estimate the jump variation. Theoretically, we show that the two-step procedure based on average realized volatility processes can achieve a convergence rate close to O P ( n − 4 / 9 ) , which is better than the convergence rate O P ( n − 1 / 4 ) for the procedure based on the original noisy process, where n is the sample size. Numerically, the method based on average realized volatility processes indeed performs better than that based on the price processes. Empirically, we study the distribution of jump variation using Dow Jones Industrial Average stocks and compare the results using the original price process and the average realized volatility processes.
B23|Special Issues of Econometrics: Celebrated Econometricians|Econometrics is pleased to announce the commissioning of a new series of Special Issues dedicated to celebrated econometricians of our time.[...]
B23|Nonparametric Regression with Common Shocks|This paper considers a nonparametric regression model for cross-sectional data in the presence of common shocks. Common shocks are allowed to be very general in nature; they do not need to be finite dimensional with a known (small) number of factors. I investigate the properties of the Nadaraya-Watson kernel estimator and determine how general the common shocks can be while still obtaining meaningful kernel estimates. Restrictions on the common shocks are necessary because kernel estimators typically manipulate conditional densities, and conditional densities do not necessarily exist in the present case. By appealing to disintegration theory, I provide sufficient conditions for the existence of such conditional densities and show that the estimator converges in probability to the Kolmogorov conditional expectation given the sigma-field generated by the common shocks. I also establish the rate of convergence and the asymptotic distribution of the kernel estimator.
B23|Generalized Fractional Processes with Long Memory and Time Dependent Volatility Revisited|In recent years, fractionally-differenced processes have received a great deal of attention due to their flexibility in financial applications with long-memory. This paper revisits the class of generalized fractionally-differenced processes generated by Gegenbauer polynomials and the ARMA structure (GARMA) with both the long-memory and time-dependent innovation variance. We establish the existence and uniqueness of second-order solutions. We also extend this family with innovations to follow GARCH and stochastic volatility (SV). Under certain regularity conditions, we give asymptotic results for the approximate maximum likelihood estimator for the GARMA-GARCH model. We discuss a Monte Carlo likelihood method for the GARMA-SV model and investigate finite sample properties via Monte Carlo experiments. Finally, we illustrate the usefulness of this approach using monthly inflation rates for France, Japan and the United States.
B23|Econometric Information Recovery in Behavioral Networks|In this paper, we suggest an approach to recovering behavior-related, preference-choice network information from observational data. We model the process as a self-organized behavior based random exponential network-graph system. To address the unknown nature of the sampling model in recovering behavior related network information, we use the Cressie-Read (CR) family of divergence measures and the corresponding information theoretic entropy basis, for estimation, inference, model evaluation, and prediction. Examples are included to clarify how entropy based information theoretic methods are directly applicable to recovering the behavioral network probabilities in this fundamentally underdetermined ill posed inverse recovery problem.
B23|Estimation of Dynamic Panel Data Models with Stochastic Volatility Using Particle Filters|Time-varying volatility is common in macroeconomic data and has been incorporated into macroeconomic models in recent work. Dynamic panel data models have become increasingly popular in macroeconomics to study common relationships across countries or regions. This paper estimates dynamic panel data models with stochastic volatility by maximizing an approximate likelihood obtained via Rao-Blackwellized particle filters. Monte Carlo studies reveal the good and stable performance of our particle filter-based estimator. When the volatility of volatility is high, or when regressors are absent but stochastic volatility exists, our approach can be better than the maximum likelihood estimator which neglects stochastic volatility and generalized method of moments (GMM) estimators.
B23|Editorial Announcement|I am pleased to announce that, following my retirement on the 30th September 2016, Marc Paolella will become Editor-in-Chief (EiC) of Econometrics.
B23|Oil Price and Economic Growth: A Long Story?|This study investigates changes in the relationship between oil prices and the US economy from a long-term perspective. Although neither of the two series (oil price and GDP growth rates) presents structural breaks in mean, we identify different volatility periods in both of them, separately. From a multivariate perspective, we do not observe a significant effect between changes in oil prices and GDP growth when considering the full period. However, we find a significant relationship in some subperiods by carrying out a rolling analysis and by investigating the presence of structural breaks in the multivariate framework. Finally, we obtain evidence, by means of a time-varying VAR, that the impact of the oil price shock on GDP growth has declined over time. We also observe that the negative effect is greater at the time of large oil price increases, supporting previous evidence of nonlinearity in the relationship.
B23|Social Networks and Choice Set Formation in Discrete Choice Models|The discrete choice literature has evolved from the analysis of a choice of a single item from a fixed choice set to the incorporation of a vast array of more complex representations of preferences and choice set formation processes into choice models. Modern discrete choice models include rich specifications of heterogeneity, multi-stage processing for choice set determination, dynamics, and other elements. However, discrete choice models still largely represent socially isolated choice processes —individuals are not affected by the preferences of choices of other individuals. There is a developing literature on the impact of social networks on preferences or the utility function in a random utility model but little examination of such processes for choice set formation. There is also emerging evidence in the marketplace of the influence of friends on choice sets and choices. In this paper we develop discrete choice models that incorporate formal social network structures into the choice set formation process in a two-stage random utility framework. We assess models where peers may affect not only the alternatives that individuals consider or include in their choice sets, but also consumption choices. We explore the properties of our models and evaluate the extent of “errors” in assessment of preferences, economic welfare measures and market shares if network effects are present, but are not accounted for in the econometric model. Our results shed light on the importance of the evaluation of peer or network effects on inclusion/exclusion of alternatives in a random utility choice framework.
B23|Pair-Copula Constructions for Financial Applications: A Review|This survey reviews the large and growing literature on the use of pair-copula constructions (PCCs) in financial applications. Using a PCC, multivariate data that exhibit complex patterns of dependence can be modeled using bivariate copulae as simple building blocks. Hence, this model represents a very flexible way of constructing higher-dimensional copulae. In this paper, we survey inference methods and goodness-of-fit tests for such models, as well as empirical applications of the PCCs in finance and economics.
B23|Testing Cross-Sectional Correlation in Large Panel Data Models with Serial Correlation|This paper considers the problem of testing cross-sectional correlation in large panel data models with serially-correlated errors. It finds that existing tests for cross-sectional correlation encounter size distortions with serial correlation in the errors. To control the size, this paper proposes a modification of Pesaran’s Cross-sectional Dependence (CD) test to account for serial correlation of an unknown form in the error term. We derive the limiting distribution of this test as N , T → ∞ . The test is distribution free and allows for unknown forms of serial correlation in the errors. Monte Carlo simulations show that the test has good size and power for large panels when serial correlation in the errors is present.
B23|Panel Cointegration Testing in the Presence of Linear Time Trends|We consider a class of panel tests of the null hypothesis of no cointegration and cointegration. All tests under investigation rely on single-equations estimated by least squares, and they may be residual-based or not. We focus on test statistics computed from regressions with intercept only (i.e., without detrending) and with at least one of the regressors (integrated of order 1) being dominated by a linear time trend. In such a setting, often encountered in practice, the limiting distributions and critical values provided for and applied with the situation “with intercept only” are not correct. It is demonstrated that their usage results in size distortions growing with the panel size N . Moreover, we show which are the appropriate distributions, and how correct critical values can be obtained from the literature.
B23|Generalized Information Matrix Tests for Detecting Model Misspecification|Generalized Information Matrix Tests (GIMTs) have recently been used for detecting the presence of misspecification in regression models in both randomized controlled trials and observational studies. In this paper, a unified GIMT framework is developed for the purpose of identifying, classifying, and deriving novel model misspecification tests for finite-dimensional smooth probability models. These GIMTs include previously published as well as newly developed information matrix tests. To illustrate the application of the GIMT framework, we derived and assessed the performance of new GIMTs for binary logistic regression. Although all GIMTs exhibited good level and power performance for the larger sample sizes, GIMT statistics with fewer degrees of freedom and derived using log-likelihood third derivatives exhibited improved level and power performance.
B23|Subset-Continuous-Updating GMM Estimators for Dynamic Panel Data Models|The two-step GMM estimators of Arellano and Bond (1991) and Blundell and Bond (1998) for dynamic panel data models have been widely used in empirical work; however, neither of them performs well in small samples with weak instruments. The continuous-updating GMM estimator proposed by Hansen, Heaton, and Yaron (1996) is in principle able to reduce the small-sample bias, but it involves high-dimensional optimizations when the number of regressors is large. This paper proposes a computationally feasible variation on these standard two-step GMM estimators by applying the idea of continuous-updating to the autoregressive parameter only, given the fact that the absolute value of the autoregressive parameter is less than unity as a necessary requirement for the data-generating process to be stationary. We show that our subset-continuous-updating method does not alter the asymptotic distribution of the two-step GMM estimators, and it therefore retains consistency. Our simulation results indicate that the subset-continuous-updating GMM estimators outperform their standard two-step counterparts in finite samples in terms of the estimation accuracy on the autoregressive parameter and the size of the Sargan-Hansen test.
B23|Higher Order Bias Correcting Moment Equation for M-Estimation and Its Higher Order Efficiency|This paper studies an alternative bias correction for the M-estimator, which is obtained by correcting the moment equations in the spirit of Firth (1993). In particular, this paper compares the stochastic expansions of the analytically-bias-corrected estimator and the alternative estimator and finds that the third-order stochastic expansions of these two estimators are identical. This implies that at least in terms of the third-order stochastic expansion, we cannot improve on the simple one-step bias correction by using the bias correction of moment equations. This finding suggests that the comparison between the one-step bias correction and the method of correcting the moment equations or the fully-iterated bias correction should be based on the stochastic expansions higher than the third order.
B23|Testing for the Equality of Integration Orders of Multiple Series|Testing for the equality of integration orders is an important topic in time series analysis because it constitutes an essential step in testing for (fractional) cointegration in the bivariate case. For the multivariate case, there are several versions of cointegration, and the version given in Robinson and Yajima (2002) has received much attention. In this definition, a time series vector is partitioned into several sub-vectors, and the elements in each sub-vector have the same integration order. Furthermore, this time series vector is said to be cointegrated if there exists a cointegration in any of the sub-vectors. Under such a circumstance, testing for the equality of integration orders constitutes an important problem. However, for multivariate fractionally integrated series, most tests focus on stationary and invertible series and become invalid under the presence of cointegration. Hualde (2013) overcomes these difficulties with a residual-based test for a bivariate time series. For the multivariate case, one possible extension of this test involves testing for an array of bivariate series, which becomes computationally challenging as the dimension of the time series increases. In this paper, a one-step residual-based test is proposed to deal with the multivariate case that overcomes the computational issue. Under certain regularity conditions, the test statistic has an asymptotic standard normal distribution under the null hypothesis of equal integration orders and diverges to infinity under the alternative. As reported in a Monte Carlo experiment, the proposed test possesses satisfactory sizes and powers.
B23|The Status of Bridge Principles in Applied Econometrics|The paper begins with a figurative representation of the contrast between present-day and formal applied econometrics. An explication of the status of bridge principles in applied econometrics follows. To illustrate the concepts used in the explication, the paper presents a simultaneous-equation model of the equilibrium configurations of a perfectly competitive commodity market. With artificially generated data I carry out two empirical analyses of such a market that contrast the prescriptions of formal econometrics in the tradition of Ragnar Frisch with the commands of present-day econometrics in the tradition of Trygve Haavelmo. At the end I demonstrate that the bridge principles I use in the formal-econometric analysis are valid in the Real World—that is in the world in which my data reside.
B23|Fixed- b Inference for Testing Structural Change in a Time Series Regression|This paper addresses tests for structural change in a weakly dependent time series regression. The cases of full structural change and partial structural change are considered. Heteroskedasticity-autocorrelation (HAC) robust Wald tests based on nonparametric covariance matrix estimators are explored. Fixed- b theory is developed for the HAC estimators which allows fixed- b approximations for the test statistics. For the case of the break date being known, the fixed- b limits of the statistics depend on the break fraction and the bandwidth tuning parameter as well as on the kernel. When the break date is unknown, supremum, mean and exponential Wald statistics are commonly used for testing the presence of the structural break. Fixed- b limits of these statistics are obtained and critical values are tabulated. A simulation study compares the finite sample properties of existing tests and proposed tests.
B23|On Performativity: Option Theory and the Resistance of Financial Phenomena|The issue of performativity concerns the claim that economics shape rather than merely describing the social world. This idea took hold following a paper by Donald MacKenzie and Yuval Millo entitled “Constructing a Market, Performing Theory: The Historical Sociology of a Financial Derivatives Exchange” (2003). This paper constitutes an important contribution to the history of economic thought since it provides an original way to focus on the scientific construction of the real economy. The authors discuss the empirical success of the Black-Scholes-Merton (BSM) model on the Chicago Board Options Exchange (CBOE) during the period 1973-1987. They explain this success in part as instead of discovering pre-existing price regularities, it was used by traders to anticipate options’ prices in their arbitrages. As a result, options’ prices came to correspond to the theoretical prices derived from the BSM model. In the present article I show that this is not a completely correct conclusion since the BSM model never became a self-fulfilling model. I would claim that the October 1987 stock market crash is empirical proof that the financial world never fitted with the economic theory underpinning the BSM.
B23|Econometrics as a Pluralistic Scientific Tool for Economic Planning: On Lawrence R. Klein's Econometrics|Lawrence R. Klein (1920-2013) played a major role in the construction and in the further dissemination of econometrics from the 1940s. Considered as one the main developers and practitioners of macroeconometrics, Klein's influence is reflected in his application of econometric modelling “to the analysis of economic fluctuations and economic policies” for which he was awarded the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel in 1980. The purpose of this paper is to give an account of Klein's image of econometrics focusing on his early period as an econometrician (1944-1950), and more specifically on his period as a Cowlesman (1944-1947). Independently of how short this period might appear, it contains a set of fundamental publications and events, which were decisive for Klein's conception of econometrics, and which formed Klein's unique way of doing econometrics. At least four features are worth mentioning, which characterise this uniqueness. First, Klein was the only Cowlesman who carried on the macroeconometric programme beyond the 1940s, even if the Cowles had already abandoned it. Second, his pluralistic approach in terms of economic theory allowed him not only to use the Walrasian framework appraised by the Cowles Commission and especially by T.C. Koopmans, but also the Marxian and Keynesian frameworks, enriching the process of model specification and motivating economists of different stripes to make use of the nascent econometrics. Third, Klein differentiated himself from the rigid methodology praised at Cowles; while the latter promoted the use of highly sophisticated methods of estimation, Klein was convinced that institutional reality and economic intuition would contribute more to econometrics than the sophistication of these statistical techniques. Last but not least, Klein never gave up what he thought was the political objective of econometrics: economic planning and social reform
B23|Forward or Backward Looking? The Economic Discourse and the Observed Reality|Is academic research anticipating economic shake-ups or merely reflecting the past? Exploiting the corpus of articles published in the Journal of Economics and Statistics (Jahrbücher für Nationalökonomie und Statistik) for the years 1949 to 2010, this pilot study proposes a quantitative framework for addressing these questions. The framework comprises two steps. First, methods from computational linguistics are used to identify relevant topics and their relative importance over time. In particular, Latent Dirichlet Analysis is applied to the corpus after some preparatory work. Second, for some of the topics which are closely related to specific economic indicators, the developments of topic weights and indicator values are confronted in dynamic regression and VAR models. The results indicate that for some topics of interest, the discourse in the journal leads developments in the real economy, while for other topics it is the other way round.
B23|Forward or Backward Looking? The Economic Discourse and the Observed Reality|Is academic research anticipating economic shake-ups or merely reflecting the past? Exploiting the corpus of articles published in the Journal of Economics and Statistics (Jahrbücher für Nationalökonomie und Statistik) for the years 1949 to 2010, this pilot study proposes a quantitative framework for addressing these questions. The framework comprises two steps. First, methods from computational linguistics are used to identify relevant topics and their relative importance over time. In particular, Latent Dirichlet Analysis is applied to the corpus after some preparatory work. Second, for some of the topics which are closely related to specific economic indicators, the developments of topic weights and indicator values are confronted in dynamic regression and VAR models. The results indicate that for some topics of interest, the discourse in the journal leads developments in the real economy, while for other topics it is the other way round.
B23|Testing the Efficiency of the GIPS Sovereign Debt Markets using an Asymmetrical Volatility Test|The efficient market hypothesis has been around since 1962, the theory is based on a simple rule, namely that the price of any asset must fully reflect all available information.Yet there is empirical evidence financial markets are too volatile to be efficient. The empirical evidence suggests that the reaction to events is the crucial factor, rather than the actual information. Generally, market participants react differently to negative and positive market shocks, hinting at asymmetrical effects. This paperanalyses the impact of asymmetrical effects on the efficiency of the financial market during the recent crises. We test the efficiency of the financial markets using the daily prices of the GIPS sovereign debts between June 2007 and December 2011. This allowed us to test the efficiency during the financial crisis and sovereign debt crisis periods. We used a GJR-GARCH based variance bound test based on the test derived by Fakhry & Richter (2015). Our tests provide evidence forfinancial markets being too volatile to be efficient. At the same time, the results are pointing towards bounded rationality rather than irrationality.
B23|Decision Theoretic Approaches to Experiment Design and External Validity|A modern, decision-theoretic framework can help clarify important practical questions of experimental design. Building on our recent work, this chapter begins by summarizing our framework for understanding the goals of experimenters, and applying this to re-randomization. We then use this framework to shed light on questions related to experimental registries, pre-analysis plans, and most importantly, external validity. Our framework implies that even when large samples can be collected, external decision-making remains inherently subjective. We embrace this conclusion, and argue that in order to improve external validity, experimental research needs to create a space for structured speculation.
B23|Theory and Measurement: Emergence, Consolidation and Erosion of a Consensus|We identify three separate stages in the post-World War II history of applied microeconomic research: A generally non-mathematical period; a period of consensus (from the 1960s through the early 1990s) characterized by the use of mathematical models, optimization and equilibrium to generate and test hypotheses about economic behavior; and (from the late 1990s) a partial abandonment of economic theory in applied work in the “experimentalist paradigm.” We document the changes implied by the changing paradigms in the profession by coding the content of all applied micro articles published in the “Top 5 journals” in 1951-55, 1974-75 and 2007-08. We also show that, despite the partial abandonment of theory by applied microeconomists, the labor market for economists still pays a wage premium to theorists.
B23|A Framework for Eliciting, Incorporating, and Disciplining Identification Beliefs in Linear Models|To estimate causal effects from observational data, an applied researcher must impose beliefs. The instrumental variables exclusion restriction, for example, represents the belief that the instrument has no direct effect on the outcome of interest. Yet beliefs about instrument validity do not exist in isolation. Applied researchers often discuss the likely direction of selection and the potential for measurement error in their papers but at present lack formal tools for incorporating this information into their analyses. As such they not only leave money on the table, by failing to use all relevant information, but more importantly run the risk of reasoning to a contradiction by expressing mutually incompatible beliefs. In this paper we characterize the sharp identified set relating instrument invalidity, treatment endogeneity, and measurement error in a workhorse linear model, showing how beliefs over these three dimensions are mutually constrained. We consider two cases: in the first the treatment is continuous and subject to classical measurement error; in the second it is binary and subject to non-differential measurement error. In each, we propose a formal Bayesian framework to help researchers elicit their beliefs, incorporate them into estimation, and ensure their mutual coherence. We conclude by illustrating the usefulness of our proposed methods on a variety of examples from the empirical microeconomics literature.
B23|The Misalignment of Russian Economists' Scientometric Indicators in RISC|The article is focused on the problem of noncompliance between the key scientometric indicators of Russian economists in the electronic database of the Russian Science Citation Index (RSCI). It is shown that the number of publications, number of citations and h-index don't have are not related to each other statistically, which undermines the whole system of the individual contribution assessment in science. Such a situation leads to the appearance of pseudo-leaders between economists, which enjoy undeserved prestige. In addition, the syndrome of systematic data manipulation arises; it destroys academic ethics and distorts traditional academic values. This effect is specific to RSCI, because in Western databases of scientific information there is a strong relationship between aforementioned scientometric indicators. It is still difficult to find filtration procedures for bibliometric parameters, thus it is necessary to use labor-consuming manual methods of data processing.
B23|Spatiotemporal traffic forecasting: review and proposed directions| This paper systematically reviews studies that forecast short-term traffic conditions using spatial dependence between links. We extract and synthesise 130 research papers, considering two perspectives: (1) methodological framework and (2) methods for capturing spatial information. Spatial information boosts the accuracy of prediction, particularly in congested traffic regimes and for longer horizons. Machine learning methods, which have attracted more attention in recent years, outperform the naïve statistical methods such as historical average and exponential smoothing. However, there is no guarantee of superiority when machine learning methods are compared with advanced statistical methods such as spatiotemporal autoregressive integrated moving average. As for the spatial dependency detection, a large gulf exists between the realistic spatial dependence of traffic links on a real network and the studied networks as follows: (1) studies capture spatial dependency of either adjacent or distant upstream and downstream links with the study link, (2) the spatially relevant links are selected either by prejudgment or by correlation-coefficient analysis, and (3) studies develop forecasting methods in a corridor test sample, where all links are connected sequentially together, assume a similarity between the behaviour of both parallel and adjacent links, and overlook the competitive nature of traffic links.
B23|Who are the forerunners, economists or central bankers?|Is it possible to tell whether central banks’ choices are grounded on monetary theories or whether the theories derive from what central bankers have already experimented? This study delves into this issue by adopting an approach that is novel for at least two reasons. First, it involves a lexical comparison between the textual content used by central banks and in economic articles. Second, this comparison is drawn using quantitative tools. In short, the variables measured here are words and segments of text that were submitted to a statistical analysis to identify trends and behaviors in central bankers and economists that would otherwise not be immediately apparent.
B23|Generalized Random Coefficient Estimators of Panel Data Models: Asymptotic and Small Sample Properties|This paper provides a generalized model for the random-coefficients panel data model where the errors are cross-sectional heteroskedastic and contemporaneously correlated as well as with the first-order autocorrelation of the time series errors. Of course, the conventional estimators, which used in standard random-coefficients panel data model, are not suitable for the generalized model. Therefore, the suitable estimator for this model and other alternative estimators have been provided and examined in this paper. Moreover, the efficiency comparisons for these estimators have been carried out in small samples and also we examine the asymptotic distributions of them. The Monte Carlo simulation study indicates that the new estimators are more reliable (more efficient) than the conventional estimators in small samples.
B23|Economics Is a Science of Time Saving: The First Tentative Model (2016)|Time might be the scarcest resource for every individual. Time saving could be one of a few specific starting points of Economics. According to the new attribute theory that is based on Lancaster’s attribute theory, the paper constructs a linear programming model with an objective function of time saving that is independent of any utility function. Then the demand function of the time saving good can be derived with a programming method, which is different from the parametric and typically non-parametric methods. Because some data was unavailable, the empirical research of this paper cannot directly derive the demand function from the programming model. Thus another method is used to derive the demand function and the predictive power of this function is at least as good as that of the most common econometric models. Further, as a basic human behavior, time saving could be generalized to other fundamental theories of Economics, especially the long-run growth theory. Therefore, in the ultimate meaning, the conclusion is that Economics is a science of how to save time and time saving might be a milestone that will shift Economics towards a modern science.
B23|Keynes In Italian Econometric Models During The Seventies. The Experience Of Prometeia And Confindustria|The aim of this paper is to highlight the intellectual influence of Keynes in the building of Italian econometrics and how our scholars tried to find a way out from the stagflation occurred in Italy during the Seventies. We have chosen two case studies: the model designed by PROMETEIA, the think tank belonging to the University of Bologna, and DYANMOD, the main model of Confindustria (Confederation of Italian Industries). Although the Keynesian thought crucially influenced their structure and functioning, they managed to explain also the oil-shock-induced supply side effects, showing an unexpected degree of innovation within Keynesian thought. Furthermore, these models were conceived to join the larger Project LINK by Lawrence Klein, in order to connect the economies of many OECD countries. Given our microeconomic foundations, the main conclusions traced by these models were the focus on balance of payments equilibrium, capital accumulation and public finance constraints, thus asking for economic policies that could guarantee growth and disinflation.
B23|The Econometric Analysis in Right Economy: Research of Institutional Barriers During Right Realization on the Example of Lands Distribution Processes in Moscow Region. Patterns in Neighboring Areas|This article offers econometric approach for right realization institutional barriers research in right economy. Offered approach is realized on the example of right to build receiving on a farmland in Moscow region. In this article there is the concept of right demand entered, the features of this term are explained and the econometric model for the purpose of demand factors identification with institutional barriers taken into account is offered.
B23|Causality As A Tool For Empirical Analysis In Economics|This paper deals with the causal determination of phenomena (briefly causality) as a tool for empirical analysis in economics. Although is the causality difficult to grasp, they are built on the basis of many scientific theories, including economic theory. Causality is very hot topic today, both in philosophy and economics. The causality is used in many multi-sectorial disciplines and the concept of causality is different in various disciplines. In economics, we encounter many assertions that connect cause and effect, but causal relationships are not clearly expressed. At first glance, there may be confusion between cause and effect and the phenomena studied can then be viewed in terms of causality and vice versa. The causality plays very important role in econometric and economics. The paper focused on using of causality in economics and econometric studies. The paper begins with a brief overview of theoretical definition of the causality. Then, the empirical approaches to causality in economics and econometric and selected tools of causality are presented and discussed and the case study of possible using of Granger Causality Test is shown. At the end of the paper we discuss the significance of the Grander Causality Test in economics. The aims of this paper are following: to define the different approaches to causality and describe a short history of this term, to analyse selected econometric methods in interaction with causality and to show on the example of Granger Causality Test using of causality in empirical analysis in economics.
B23|Chicago Economics in the Making, 1926-1940. A Further Look at US Interwar Pluralism|The aim of this paper is to unfold a rich body of archival material that can shed new light on the nature and evolution of interwar Chicago economics. Specifically, this paper is based on a scrutiny of the PhD qualifying exams on Economic Theory at Chicago from 1926 to 1940. The qualifying tests (supplemented by the courses’ programs) show the existence of two important turning points in the shaping of Chicago economic training. The first one is in 1927, when John M. Clark, the undisputed leader of the Chicago Department of Economics during the heyday of institutionalism, moved to Columbia, leaving open ground to the restructuring of the courses according to a different and more analytical approach already represented in the Department by Viner and, in a narrower field, by Paul Douglas. The arrival at Chicago of figures such as Knight, Schultz and Simons definitely shifted the balance toward neoclassical theory. A second turning point occurred in 1933 when the qualifying test in Economic Theory was divided into two major fields: price and distribution theory on the one side; money and business cycle on the other. This innovation reveals the importance acquired by monetary theory in economic training at a time that is commonly associated with the nurturing of what was later named as the “Chicago monetary tradition.”
B23|A profit-maximization model for a company that sells an arbitrary number of products|No abstract is available for this item.
B23|Research Ideas for the Journal of Informatics and Data Mining: Opinion|The purpose of this Opinion article is to discuss some ideas that might lead to papers that are suitable for publication in the Journal of Informatics and Data Mining. The suggestions include the analysis of citations databases, PI-BETA (Papers Ignored – By Even The Authors), model specification and testing, pre-test bias and data mining, international rankings of academic journals based on citations, international rankings of academic institutions based on citations and other factors, and case studies in numerous disciplines in the sciences and social sciences.
B23|Resurgence of the endogeneity-backed instrumental variable methods|This paper investigates the nature of the IV method for tackling endogeneity. By tracing the rise and fall of the method in macroeconometrics and its subsequent revival in microeconometrics, it pins the method down to an implicit model respecification device-breaking the circular causality of simultaneous relations by redefining it as an asymmetric one conditioning on a non-optimal conditional expectation of the assumed endogenous explanatory variable, thus rejecting that variable as a valid conditional variable. The revealed nature explains why the IV route is popular for models where endogeneity is superfluous whereas measurement errors are of the key concern.
B23|Economic paradoxism and meson economics|The structure of the paper brings together three major sections, following the general approach to the impact of paradoxes in economic theory. The first section describes a necessary investigation in the synthesized universe of paradoxes, to capitalize on Quine’s paradox taxonomy, and to reveal the importance of really paraconsistent paradoxes, defining, in a relative and innovative manner, economic paradoxism in the sense of excess of creative capitalization of paradoxes in the area of science, as initiated by mathematician and logician Florentin Smarandache. The second section turns into an original exposition of economics theory and the third section reveals the concept of meson economics and the principles of that economy, completed with some final remarks, some of which are conclusive, and some others interrogative, aligned to the paradox of knowledge, in keeping with which human beings are looking for answers, and finally find more and more questions.
B23|A New Estimator for Multivariate Binary Data| This study proposes a new estimator for multivariate binary response data. This study considers binary responses as being generated from a truncated multivariate discrete distribution. Specifically the discrete normal probability mass function, which has support on all integers, is extended to a multivariate form. Truncating this point probability mass function below zero and above one results the multivariate binary discrete normal distribution. This distribution has a number of attractive properties. Monte Carlo simulation and empirical applications are performed to show the properties of this new estimator; comparisons are made to the traditional multivariate probit model.
B23|Estimating Latent Variable Models When the Latent Variable is Observable| Logit and probit models are designed to estimate latent variable models. However, there are cases that these estimates are used, even though the latent variable is fully observable. The most prominent examples are studies about obesity, where they calculate BMI based on two observed variables: weight and height squared. They translate BMI into a binary variable (e.g. obese or not obese) and this index is used to examine factors affecting obesity. This study determines the loss in efficiency of using logit/probit models versus the conventional OLS (e.g. with unknown variance). We also compare the marginal effects between these models. The results suggest that OLS is a more efficient than the logit/probit models when estimating the true coefficients, regardless of the multicollinearity, fit of regression and cut-off probability. Likewise, OLS provided unbiased marginal effects compared to both binary response models. It is also less likely to be biased. We can conclude, that according to our Monte Carlo simulation, when the latent variable is observable, it is better to use the continous value and regress it with respect to their explanatory variable instead of converting it into a latent variable.
B23|Spatial Analysis of Market Power of Cassava Starch in Brazil| The present study aimed to investigate the spatial price transmission of starch, addressing the major producing regions. The specific objectives were to identify their long-term behavior and to verify possible asymmetries on the price transmission. The analysis of price transmission among the markets indicates that prices cointegrated themselves in the long run, leading to the Vector Error Correction Model (VECM). Through the econometric regressions it was possible to verify the application of the Law of Price for only a region under analysis. This study concluded that the markets for starch were not spatially integrated. Thus, it was possible to infer that the market for starch showed a competitive inefficiency.
B23|Structural and atheoretic approaches to micro-econometrics of public policy evaluation.(in french)|This article aims at presenting and comparing structural and atheoretic approaches to micro-econometrics of public policy evaluation. If these approaches are often opposed because they rely on different scientific methodologies, they complement each other in the lessons one can draw from them. Two illustrations are presented: the evaluation of the workweek reduction in 1998 (Crépon, Leclair, Roux [1998]) and the local effect of speed enforcement cameras on road accidents (Roux, Zamora [2013]).
B23|Is Benford’s Law a Universal Behavioral Theory?|In this paper, we consider the question and present evidence as to whether or not Benford’s exponential first significant digit (FSD) law reflects a fundamental principle behind the complex and nondeterministic nature of large-scale physical and behavioral systems. As a behavioral example, we focus on the FSD distribution of Australian micro income data and use information theoretic entropy methods to investigate the degree that corresponding empirical income distributions are consistent with Benford’s law.
B23|Trygve Haavelmo At The Cowles Commission|The article reviews the early history of the Cowles Commission, its close and intertwined relations with the Econometric Society (ES), and the influence and guidance of Ragnar Frisch. It provides much detail on the three rounds of choosing a research director, in 1937-38, in 1939 and at the end of 1942. Haavelmo’s work in the early 1940s came to play a major role for the econometric research at the Cowles Commission under Jacob Marschak as research director 1943-48. The article points to the importance of Abraham Wald and Jacob Marschak for the success of Haavelmo’s venture and its influence and tells the story of how it came about that Haavelmo’s ideas were adopted, applied and disseminated by the Cowles Commission. Thus the mention of Trygve Haavelmo in the title is referring also to his econometric ideas. The ideas themselves and their further evolvement at the CC have been a dominating theme in the history of econometrics literature, e.g. Hildreth (1986), Epstein (1987), Morgan (1990), Qin (1993) and Christ (1994). The article discusses the recruitment, the inner workings and various other concerns of the Cowles econometricians, from Marxism to Black Magic. It recounts at some length the efforts made by Marschak to recruit Abraham Wald to the University of Chicago and the Cowles Commission. The article can be read as a sequel to Bjerkholt (2005, 2007).<br><small>(This abstract was borrowed from another version of this item.)</small>
B23|The Reasons of Eurozone Sovereign Debt Crisis and an Empirical Analysis over Permanency of the Crisis|This study aims to review how the Eurozone has been formed and to analyze how it has been formed on weak roots, both in public finance and politics. Contrary to economical ideas, political grounds tested the endeavor of creating a powerful the monetary union. Hence, it was very clean to predict upcoming the debt crisis. According to those assumptions known, pre-crisis problems of the countries has been reviewed. Latter, the crisis’ permanency has been tested via PANKPSS if it’s permanent or not? Mainstream acknowledgement on the literature agrees on the crisis is permanent and might lead to dire consequences. Nevertheless, nexus between those countries which has been forged by political bounds cannot be broken by that crisis; thus, the Eurozone might not be break down.
B23|Diversified Currency Holdings and Exchange Rate Dynamics|In this study, we incorporate diversified currency holdings into the New Open Economy Macroeconomics (NOEM) model to explore the issue of exchange rates dynamics. The findings show that exchange rate overshooting occurs when diversified currency holdings are included in a two-country dynamic optimizing NOEM model, and the extent of the overshoot depends on the level of diversified currency holdings. Regarding welfare analysis, an expansionary monetary policy of the home country increases domestic welfare. Also, we find that current account plays an important role in the effect of monetary policy on exchange rates, the increase in the domestic money supply stimulates domestic consumption expenditure through the effects of current account improvements, resulting in exchange rate appreciation.
B23|Research Ideas for the Journal of Informatics and Data Mining: Opinion|The purpose of this Opinion article is to discuss some ideas that might lead to papers that are suitable for publication in the Journal of Informatics and Data Mining. The suggestions include the analysis of citations databases, PI-BETA (Papers Ignored – By Even The Authors), model specification and testing, pre-test bias and data mining, international rankings of academic journals based on citations, international rankings of academic institutions based on citations and other factors, and case studies in numerous disciplines in the sciences and social sciences.
B23|Acknowledgement to Reviewers of Econometrics in 2014|The editors of Econometrics would like to express their sincere gratitude to the following reviewers for assessing manuscripts in 2014:[...]
B23|Heteroskedasticity of Unknown Form in Spatial Autoregressive Models with a Moving Average Disturbance Term|In this study, I investigate the necessary condition for the consistency of the maximum likelihood estimator (MLE) of spatial models with a spatial moving average process in the disturbance term. I show that the MLE of spatial autoregressive and spatial moving average parameters is generally inconsistent when heteroskedasticity is not considered in the estimation. I also show that the MLE of parameters of exogenous variables is inconsistent and determine its asymptotic bias. I provide simulation results to evaluate the performance of the MLE. The simulation results indicate that the MLE imposes a substantial amount of bias on both autoregressive and moving average parameters.
B23|Two-Step Lasso Estimation of the Spatial Weights Matrix|The vast majority of spatial econometric research relies on the assumption that the spatial network structure is known a priori. This study considers a two-step estimation strategy for estimating the n(n-1) interaction effects in a spatial autoregressive panel model where the spatial dimension is potentially large. The identifying assumption is approximate sparsity of the spatial weights matrix. The proposed estimation methodology exploits the Lasso estimator and mimics two-stage least squares (2SLS) to account for endogeneity of the spatial lag. The developed two-step estimator is of more general interest. It may be used in applications where the number of endogenous regressors and the number of instrumental variables is larger than the number of observations. We derive convergence rates for the two-step Lasso estimator. Our Monte Carlo simulation results show that the two-step estimator is consistent and successfully recovers the spatial network structure for reasonable sample size, T .
B23|A Joint Chow Test for Structural Instability|The classical Chow test for structural instability requires strictly exogenous regressors and a break-point specified in advance. In this paper, we consider two generalisations, the one-step recursive Chow test (based on the sequence of studentised recursive residuals) and its supremum counterpart, which relaxes these requirements. We use results on the strong consistency of regression estimators to show that the one-step test is appropriate for stationary, unit root or explosive processes modelled in the autoregressive distributed lags (ADL) framework. We then use the results in extreme value theory to develop a new supremum version of the test, suitable for formal testing of structural instability with an unknown break-point. The test assumes the normality of errors and is intended to be used in situations where this can be either assumed nor established empirically. Simulations show that the supremum test has desirable power properties, in particular against level shifts late in the sample and against outliers. An application to U.K. GDP data is given.
B23|Modeling Autoregressive Processes with Moving-Quantiles-Implied Nonlinearity|We introduce and investigate some properties of a class of nonlinear time series models based on the moving sample quantiles in the autoregressive data generating process. We derive a test fit to detect this type of nonlinearity. Using the daily realized volatility data of Standard & Poor’s 500 (S&P 500) and several other indices, we obtained good performance using these models in an out-of-sample forecasting exercise compared with the forecasts obtained based on the usual linear heterogeneous autoregressive and other models of realized volatility.
B23|On the Interpretation of Instrumental Variables in the Presence of Specification Errors|The method of instrumental variables (IV) and the generalized method of moments (GMM), and their applications to the estimation of errors-in-variables and simultaneous equations models in econometrics, require data on a sufficient number of instrumental variables that are both exogenous and relevant. We argue that, in general, such instruments (weak or strong) cannot exist.
B23|Finding Starting-Values for the Estimation of Vector STAR Models|This paper focuses on finding starting-values for the estimation of Vector STAR models. Based on a Monte Carlo study, different procedures are evaluated. Their performance is assessed with respect to model fit and computational effort. I employ (i) grid search algorithms and (ii) heuristic optimization procedures, namely differential evolution, threshold accepting, and simulated annealing. In the equation-by-equation starting-value search approach the procedures achieve equally good results. Unless the errors are cross-correlated, equation-by-equation search followed by a derivative-based algorithm can handle such an optimization problem sufficiently well. This result holds also for higher-dimensional Vector STAR models with a slight edge for heuristic methods. For more complex Vector STAR models which require a multivariate search approach, simulated annealing and differential evolution outperform threshold accepting and the grid search.
B23|Entropy Maximization as a Basis for Information Recovery in Dynamic Economic Behavioral Systems|As a basis for information recovery in open dynamic microeconomic systems, we emphasize the connection between adaptive intelligent behavior, causal entropy maximization and self-organized equilibrium seeking behavior. This entropy-based causal adaptive behavior framework permits the use of information-theoretic methods as a solution basis for the resulting pure and stochastic inverse economic-econometric problems. We cast the information recovery problem in the form of a binary network and suggest information-theoretic methods to recover estimates of the unknown binary behavioral parameters without explicitly sampling the configuration-arrangement of the sample space.
B23|Information Recovery in a Dynamic Statistical Markov Model|Although economic processes and systems are in general simple in nature, the underlying dynamics are complicated and seldom understood. Recognizing this, in this paper we use a nonstationary-conditional Markov process model of observed aggregate data to learn about and recover causal influence information associated with the underlying dynamic micro-behavior. Estimating equations are used as a link to the data and to model the dynamic conditional Markov process. To recover the unknown transition probabilities, we use an information theoretic approach to model the data and derive a new class of conditional Markov models. A quadratic loss function is used as a basis for selecting the optimal member from the family of possible likelihood-entropy functional(s). The asymptotic properties of the resulting estimators are demonstrated, and a range of potential applications is discussed.
B23|Plug-in Bandwidth Selection for Kernel Density Estimation with Discrete Data|This paper proposes plug-in bandwidth selection for kernel density estimation with discrete data via minimization of mean summed square error. Simulation results show that the plug-in bandwidths perform well, relative to cross-validated bandwidths, in non-uniform designs. We further find that plug-in bandwidths are relatively small. Several empirical examples show that the plug-in bandwidths are typically similar in magnitude to their cross-validated counterparts.
B23|Return and Volatility Spillovers across Equity Markets in Mainland China, Hong Kong and the United States|Examinations of the dynamics of daily returns and volatility in stock markets of the U.S., Hong Kong and mainland China (Shanghai and Shenzhen) over 2 January 2001 to 8 February 2013 suggest: (1) evidence of unidirectional return spillovers from the U.S. to the other three markets; but no spillover between Hong Kong and either of the two mainland China markets; (2) evidence of unidirectional ARCH and GARCH effects from the U.S. to the other three markets; (3) correlations of returns vary across markets, with the highest correlation of 93.5% between the two Chinese markets, medium correlation of 30% between mainland China and Hong Kong markets and low correlations of 6.4% and 7.2% between the U.S. and China’s two markets; thus, international investors may benefit by allocating their assets in China’s markets; (4) the patterns of dynamic conditional correlations from the DCC model suggest an increase in correlation between China and other stock markets since the most recent financial crisis of 2007.
B23|A Pitfall in Using the Characterization of Granger Non-Causality in Vector Autoregressive Models|It is well known that in a vector autoregressive (VAR) model Granger non-causality is characterized by a set of restrictions on the VAR coefficients. This characterization has been derived under the assumption of non-singularity of the covariance matrix of the innovations. This note shows that if this assumption is violated, then the characterization of Granger non-causality in a VAR model fails to hold. In these situations Granger non-causality test results must be interpreted with caution.
B23|Detecting Location Shifts during Model Selection by Step-Indicator Saturation|To capture location shifts in the context of model selection, we propose selecting significant step indicators from a saturating set added to the union of all of the candidate variables. The null retention frequency and approximate non-centrality of a selection test are derived using a ‘split-half’ analysis, the simplest specialization of a multiple-path block-search algorithm. Monte Carlo simulations, extended to sequential reduction, confirm the accuracy of nominal significance levels under the null and show retentions when location shifts occur, improving the non-null retention frequency compared to the corresponding impulse-indicator saturation (IIS)-based method and the lasso.
B23|Nonparametric Regression Estimation for Multivariate Null Recurrent Processes|This paper discusses nonparametric kernel regression with the regressor being a \(d\)-dimensional \(\beta\)-null recurrent process in presence of conditional heteroscedasticity. We show that the mean function estimator is consistent with convergence rate \(\sqrt{n(T)h^{d}}\), where \(n(T)\) is the number of regenerations for a \(\beta\)-null recurrent process and the limiting distribution (with proper normalization) is normal. Furthermore, we show that the two-step estimator for the volatility function is consistent. The finite sample performance of the estimate is quite reasonable when the leave-one-out cross validation method is used for bandwidth selection. We apply the proposed method to study the relationship of Federal funds rate with 3-month and 5-year T-bill rates and discover the existence of nonlinearity of the relationship. Furthermore, the in-sample and out-of-sample performance of the nonparametric model is far better than the linear model.
B23|Selection Criteria in Regime Switching Conditional Volatility Models|A large number of nonlinear conditional heteroskedastic models have been proposed in the literature. Model selection is crucial to any statistical data analysis. In this article, we investigate whether the most commonly used selection criteria lead to choice of the right specification in a regime switching framework. We focus on two types of models: the Logistic Smooth Transition GARCH and the Markov-Switching GARCH models. Simulation experiments reveal that information criteria and loss functions can lead to misspecification ; BIC sometimes indicates the wrong regime switching framework. Depending on the Data Generating Process used in the experiments, great care is needed when choosing a criterion.
B23|The SAR Model for Very Large Datasets: A Reduced Rank Approach|The SAR model is widely used in spatial econometrics to model Gaussian processes on a discrete spatial lattice, but for large datasets, fitting it becomes computationally prohibitive, and hence, its usefulness can be limited. A computationally-efficient spatial model is the spatial random effects (SRE) model, and in this article, we calibrate it to the SAR model of interest using a generalisation of the Moran operator that allows for heteroskedasticity and an asymmetric SAR spatial dependence matrix. In general, spatial data have a measurement-error component, which we model, and we use restricted maximum likelihood to estimate the SRE model covariance parameters; its required computational time is only the order of the size of the dataset. Our implementation is demonstrated using mean usual weekly income data from the 2011 Australian Census.
B23|The Seasonal KPSS Test: Examining Possible Applications with Monthly Data and Additional Deterministic Terms|The literature has been notably less definitive in distinguishing between finite sample studies of seasonal stationarity than in seasonal unit root tests. Although the use of seasonal stationarity and unit root tests is advised to determine correctly the most appropriate form of the trend in a seasonal time series, such a use is rarely noted in the relevant studies on this topic. Recently, the seasonal KPSS test, with a null hypothesis of no seasonal unit roots, and based on quarterly data, has been introduced in the literature. The asymptotic theory of the seasonal KPSS test depends on whether data have been filtered by a preliminary regression. More specifically, one may proceed to extracting deterministic components, such as the mean and trend, from the data before testing. In this paper, we examine the effects of de-trending on the properties of the seasonal KPSS test in finite samples. A sketch of the test’s limit theory is subsequently provided. Moreover, a Monte Carlo study is conducted to analyze the behavior of the test for a monthly time series. The focus on this time-frequency is significant because, as we mentioned above, it was introduced for quarterly data. Overall, the results indicated that the seasonal KPSS test preserved its good size and power properties. Furthermore, our results corroborate those reported elsewhere in the literature for conventional stationarity tests. These subsequent results assumed that the nonparametric corrections of residual variances may lead to better in-sample properties of the seasonal KPSS test. Next, the seasonal KPSS test is applied to a monthly series of the United States (US) consumer price index. We were able to identify a number of seasonal unit roots in this time series. [1] [1] Table 1 in this paper is copyrighted and initially published by JMASM in 2012, Volume 11, Issue 1, pp. 69–77, ISSN: 1538–9472, JMASM Inc., PO Box 48023, Oak Park, MI 48237, USA, ea@jmasm.com.
B23|A Jackknife Correction to a Test for Cointegration Rank|This paper investigates the performance of a jackknife correction to a test for cointegration rank in a vector autoregressive system. The limiting distributions of the jackknife-corrected statistics are derived and the critical values of these distributions are tabulated. Based on these critical values the finite sample size and power properties of the jackknife-corrected tests are compared with the usual rank test statistic as well as statistics involving a small sample correction and a Bartlett correction, in addition to a bootstrap method. The simulations reveal that all of the corrected tests can provide finite sample size improvements, while maintaining power, although the bootstrap procedure is the most robust across the simulation designs considered.
B23|Asymptotic Distribution and Finite Sample Bias Correction of QML Estimators for Spatial Error Dependence Model|In studying the asymptotic and finite sample properties of quasi-maximum likelihood (QML) estimators for the spatial linear regression models, much attention has been paid to the spatial lag dependence (SLD) model; little has been given to its companion, the spatial error dependence (SED) model. In particular, the effect of spatial dependence on the convergence rate of the QML estimators has not been formally studied, and methods for correcting finite sample bias of the QML estimators have not been given. This paper fills in these gaps. Of the two, bias correction is particularly important to the applications of this model, as it leads potentially to much improved inferences for the regression coefficients. Contrary to the common perceptions, both the large and small sample behaviors of the QML estimators for the SED model can be different from those for the SLD model in terms of the rate of convergence and the magnitude of bias. Monte Carlo results show that the bias can be severe, and the proposed bias correction procedure is very effective.
B23|Strategic Interaction Model with Censored Strategies|In this paper, we develop a new model of a static game of incomplete information with a large number of players. The model has two key distinguishing features. First, the strategies are subject to threshold effects, and can be interpreted as dependent censored random variables. Second, in contrast to most of the existing literature, our inferential theory relies on a large number of players, rather than a large number of independent repetitions of the same game. We establish existence and uniqueness of the pure strategy equilibrium, and prove that the censored equilibrium strategies satisfy a near-epoch dependence property. We then show that the normal maximum likelihood and least squares estimators of this censored model are consistent and asymptotically normal. Our model can be useful in a wide variety of settings, including investment, R&D, labor supply, and social interaction applications.
B23|Bayesian Approach to Disentangling Technical and Environmental Productivity|This paper models the firm’s production process as a system of simultaneous technologies for desirable and undesirable outputs. Desirable outputs are produced by transforming inputs via the conventional transformation function, whereas (consistent with the material balance condition) undesirable outputs are by-produced via the so-called “residual generation technology”. By separating the production of undesirable outputs from that of desirable outputs, not only do we ensure that undesirable outputs are not modeled as inputs and thus satisfy costly disposability, but we are also able to differentiate between the traditional (desirable-output-oriented) technical productivity and the undesirable-output-oriented environmental, or so-called “green”, productivity. To measure the latter, we derive a Solow-type Divisia environmental productivity index which, unlike conventional productivity indices, allows crediting the ceteris paribus reduction in undesirable outputs. Our index also provides a meaningful way to decompose environmental productivity into environmental technological and efficiency changes.
B23|A New Approach to Model Verification, Falsification and Selection|This paper shows that a qualitative analysis, i.e. , an assessment of the consistency of a hypothesized sign pattern for structural arrays with the sign pattern of the estimated reduced form, can always provide decisive insight into a model’s validity both in general and compared to other models. Qualitative analysis can show that it is impossible for some models to have generated the data used to estimate the reduced form, even though standard specification tests might show the model to be adequate. A partially specified structural hypothesis can be falsified by estimating as few as one reduced form equation. Zero restrictions in the structure can themselves be falsified. It is further shown how the information content of the hypothesized structural sign patterns can be measured using a commonly applied concept of statistical entropy. The lower the hypothesized structural sign pattern’s entropy, the more a priori information it proposes about the sign pattern of the estimated reduced form. As an hypothesized structural sign pattern has a lower entropy, it is more subject to type 1 error and less subject to type 2 error. Three cases illustrate the approach taken here.
B23|Consistency in Estimation and Model Selection of Dynamic Panel Data Models with Fixed Effects|We examine the relationship between consistent parameter estimation and model selection for autoregressive panel data models with fixed effects. We find that the transformation of fixed effects proposed by Lancaster (2002) does not necessarily lead to consistent estimation of common parameters when some true exogenous regressors are excluded. We propose a data dependent way to specify the prior of the autoregressive coefficient and argue for comparing different model specifications before parameter estimation. Model selection properties of Bayes factors and Bayesian information criterion (BIC) are investigated. When model uncertainty is substantial, we recommend the use of Bayesian Model Averaging to obtain point estimators with lower root mean squared errors (RMSE). We also study the implications of different levels of inclusion probabilities by simulations.
B23|Efficient Estimation in Heteroscedastic Varying Coefficient Models|This paper considers statistical inference for the heteroscedastic varying coefficient model. We propose an efficient estimator for coefficient functions that is more efficient than the conventional local-linear estimator. We establish asymptotic normality for the proposed estimator and conduct some simulation to illustrate the performance of the proposed method.
B23|New Graphical Methods and Test Statistics for Testing Composite Normality|Several graphical methods for testing univariate composite normality from an i.i.d. sample are presented. They are endowed with correct simultaneous error bounds and yield size-correct tests. As all are based on the empirical CDF, they are also consistent for all alternatives. For one test, called the modified stabilized probability test, or MSP, a highly simplified computational method is derived, which delivers the test statistic and also a highly accurate p-value approximation, essentially instantaneously. The MSP test is demonstrated to have higher power against asymmetric alternatives than the well-known and powerful Jarque-Bera test. A further size-correct test, based on combining two test statistics, is shown to have yet higher power. The methodology employed is fully general and can be applied to any i.i.d. univariate continuous distribution setting.
B23|A Note on the Asymptotic Normality of the Kernel Deconvolution Density Estimator with Logarithmic Chi-Square Noise|This paper studies the asymptotic normality for the kernel deconvolution estimator when the noise distribution is logarithmic chi-square; both identical and independently distributed observations and strong mixing observations are considered. The dependent case of the result is applied to obtain the pointwise asymptotic distribution of the deconvolution volatility density estimator in discrete-time stochastic volatility models.
B23|A Spectral Model of Turnover Reduction|We give a simple explicit formula for turnover reduction when a large number of alphas are traded on the same execution platform and trades are crossed internally. We model turnover reduction via alpha correlations. Then, for a large number of alphas, turnover reduction is related to the largest eigenvalue and the corresponding eigenvector of the alpha correlation matrix.
B23|A Kolmogorov-Smirnov Based Test for Comparing the Predictive Accuracy of Two Sets of Forecasts|This paper introduces a complement statistical test for distinguishing between the predictive accuracy of two sets of forecasts. We propose a non-parametric test founded upon the principles of the Kolmogorov-Smirnov (KS) test, referred to as the KS Predictive Accuracy (KSPA) test. The KSPA test is able to serve two distinct purposes. Initially, the test seeks to determine whether there exists a statistically significant difference between the distribution of forecast errors, and secondly it exploits the principles of stochastic dominance to determine whether the forecasts with the lower error also reports a stochastically smaller error than forecasts from a competing model, and thereby enables distinguishing between the predictive accuracy of forecasts. We perform a simulation study for the size and power of the proposed test and report the results for different noise distributions, sample sizes and forecasting horizons. The simulation results indicate that the KSPA test is correctly sized, and robust in the face of varying forecasting horizons and sample sizes along with significant accuracy gains reported especially in the case of small sample sizes. Real world applications are also considered to illustrate the applicability of the proposed KSPA test in practice.
B23|Right on Target, or Is it? The Role of Distributional Shape in Variance Targeting|Estimation of GARCH models can be simplified by augmenting quasi-maximum likelihood (QML) estimation with variance targeting, which reduces the degree of parameterization and facilitates estimation. We compare the two approaches and investigate, via simulations, how non-normality features of the return distribution affect the quality of estimation of the volatility equation and corresponding value-at-risk predictions. We find that most GARCH coefficients and associated predictions are more precisely estimated when no variance targeting is employed. Bias properties are exacerbated for a heavier-tailed distribution of standardized returns, while the distributional asymmetry has little or moderate impact, these phenomena tending to be more pronounced under variance targeting. Some effects further intensify if one uses ML based on a leptokurtic distribution in place of normal QML. The sample size has also a more favorable effect on estimation precision when no variance targeting is used. Thus, if computational costs are not prohibitive, variance targeting should probably be avoided.
B23|A New Family of Consistent and Asymptotically-Normal Estimators for the Extremal Index|The extremal index (θ) is the key parameter for extending extreme value theory results from i.i.d. to stationary sequences. One important property of this parameter is that its inverse determines the degree of clustering in the extremes. This article introduces a novel interpretation of the extremal index as a limiting probability characterized by two Poisson processes and a simple family of estimators derived from this new characterization. Unlike most estimators for θ in the literature, this estimator is consistent, asymptotically normal and very stable across partitions of the sample. Further, we show in an extensive simulation study that this estimator outperforms in finite samples the logs, blocks and runs estimation methods. Finally, we apply this new estimator to test for clustering of extremes in monthly time series of unemployment growth and inflation rates and conclude that runs of large unemployment rates are more prolonged than periods of high inflation.
B23|On Bootstrap Inference for Quantile Regression Panel Data: A Monte Carlo Study|This paper evaluates bootstrap inference methods for quantile regression panel data models. We propose to construct confidence intervals for the parameters of interest using percentile bootstrap with pairwise resampling. We study three different bootstrapping procedures. First, the bootstrap samples are constructed by resampling only from cross-sectional units with replacement. Second, the temporal resampling is performed from the time series. Finally, a more general resampling scheme, which considers sampling from both the cross-sectional and temporal dimensions, is introduced. The bootstrap algorithms are computationally attractive and easy to use in practice. We evaluate the performance of the bootstrap confidence interval by means of Monte Carlo simulations. The results show that the bootstrap methods have good finite sample performance for both location and location-scale models.
B23|A Joint Specification Test for Response Probabilities in Unordered Multinomial Choice Models|Estimation results obtained by parametric models may be seriously misleading when the model is misspecified or poorly approximates the true model. This study proposes a test that jointly tests the specifications of multiple response probabilities in unordered multinomial choice models. The test statistic is asymptotically chi-square distributed, consistent against a fixed alternative and able to detect a local alternative approaching to the null at a rate slower than the parametric rate. We show that rejection regions can be calculated by a simple parametric bootstrap procedure, when the sample size is small. The size and power of the tests are investigated by Monte Carlo experiments.
B23|Measurement Errors Arising When Using Distances in Microeconometric Modelling and the Individuals’ Position Is Geo-Masked for Confidentiality|In many microeconometric models we use distances. For instance, in modelling the individual behavior in labor economics or in health studies, the distance from a relevant point of interest (such as a hospital or a workplace) is often used as a predictor in a regression framework. However, in order to preserve confidentiality, spatial micro-data are often geo-masked, thus reducing their quality and dramatically distorting the inferential conclusions. In particular in this case, a measurement error is introduced in the independent variable which negatively affects the properties of the estimators. This paper studies these negative effects, discusses their consequences, and suggests possible interpretations and directions to data producers, end users, and practitioners.
B23|Counterfactual Distributions in Bivariate Models—A Conditional Quantile Approach|This paper proposes a methodology to incorporate bivariate models in numerical computations of counterfactual distributions. The proposal is to extend the works of Machado and Mata (2005) and Melly (2005) using the grid method to generate pairs of random variables. This contribution allows incorporating the effect of intra-household decision making in counterfactual decompositions of changes in income distribution. An application using data from five latin american countries shows that this approach substantially improves the goodness of fit to the empirical distribution. However, the exercise of decomposition is less conclusive about the performance of the method, which essentially depends on the sample size and the accuracy of the regression model.
B23|Forecasting Interest Rates Using Geostatistical Techniques|Geostatistical spatial models are widely used in many applied fields to forecast data observed on continuous three-dimensional surfaces. We propose to extend their use to finance and, in particular, to forecasting yield curves. We present the results of an empirical application where we apply the proposed method to forecast Euro Zero Rates (2003–2014) using the Ordinary Kriging method based on the anisotropic variogram. Furthermore, a comparison with other recent methods for forecasting yield curves is proposed. The results show that the model is characterized by good levels of predictions’ accuracy and it is competitive with the other forecasting models considered.
B23|Testing in a Random Effects Panel Data Model with Spatially Correlated Error Components and Spatially Lagged Dependent Variables|We propose a random effects panel data model with both spatially correlated error components and spatially lagged dependent variables. We focus on diagnostic testing procedures and derive Lagrange multiplier (LM) test statistics for a variety of hypotheses within this model. We first construct the joint LM test for both the individual random effects and the two spatial effects (spatial error correlation and spatial lag dependence). We then provide LM tests for the individual random effects and for the two spatial effects separately. In addition, in order to guard against local model misspecification, we derive locally adjusted (robust) LM tests based on the Bera and Yoon principle (Bera and Yoon, 1993). We conduct a small Monte Carlo simulation to show the good finite sample performances of these LM test statistics and revisit the cigarette demand example in Baltagi and Levin (1992) to illustrate our testing procedures.
B23|Forecast Combination under Heavy-Tailed Errors|Forecast combination has been proven to be a very important technique to obtain accurate predictions for various applications in economics, finance, marketing and many other areas. In many applications, forecast errors exhibit heavy-tailed behaviors for various reasons. Unfortunately, to our knowledge, little has been done to obtain reliable forecast combinations for such situations. The familiar forecast combination methods, such as simple average, least squares regression or those based on the variance-covariance of the forecasts, may perform very poorly due to the fact that outliers tend to occur, and they make these methods have unstable weights, leading to un-robust forecasts. To address this problem, in this paper, we propose two nonparametric forecast combination methods. One is specially proposed for the situations in which the forecast errors are strongly believed to have heavy tails that can be modeled by a scaled Student’s t-distribution; the other is designed for relatively more general situations when there is a lack of strong or consistent evidence on the tail behaviors of the forecast errors due to a shortage of data and/or an evolving data-generating process. Adaptive risk bounds of both methods are developed. They show that the resulting combined forecasts yield near optimal mean forecast errors relative to the candidate forecasts. Simulations and a real example demonstrate their superior performance in that they indeed tend to have significantly smaller prediction errors than the previous combination methods in the presence of forecast outliers.
B23|Bootstrap Tests for Overidentification in Linear Regression Models|We study the finite-sample properties of tests for overidentifying restrictions in linear regression models with a single endogenous regressor and weak instruments. Under the assumption of Gaussian disturbances, we derive expressions for a variety of test statistics as functions of eight mutually independent random variables and two nuisance parameters. The distributions of the statistics are shown to have an ill-defined limit as the parameter that determines the strength of the instruments tends to zero and as the correlation between the disturbances of the structural and reduced-form equations tends to plus or minus one. This makes it impossible to perform reliable inference near the point at which the limit is ill-defined. Several bootstrap procedures are proposed. They alleviate the problem and allow reliable inference when the instruments are not too weak. We also study their power properties.
B23|Non-Parametric Estimation of Intraday Spot Volatility: Disentangling Instantaneous Trend and Seasonality|We provide a new framework for modeling trends and periodic patterns in high-frequency financial data. Seeking adaptivity to ever-changing market conditions, we enlarge the Fourier flexible form into a richer functional class: both our smooth trend and the seasonality are non-parametrically time-varying and evolve in real time. We provide the associated estimators and use simulations to show that they behave adequately in the presence of jumps and heteroskedastic and heavy-tailed noise. A study of exchange rate returns sampled from 2010 to 2013 suggests that failing to factor in the seasonality’s dynamic properties may lead to misestimation of the intraday spot volatility.
B23|How Credible Are Shrinking Wage Elasticities of Married Women Labour Supply?|This paper delves into the well-known phenomenon of shrinking wage elasticities for married women in the US over recent decades. The results of a novel model experimental approach via sample data ordering unveil considerable heterogeneity across different wage groups. Yet, surprisingly constant wage elasticity estimates are maintained within certain wage groups over time. In addition to those constant wage elasticity estimates, we find that the composition of working women into different wage groups has changed considerably, resulting in shrinking wage elasticity estimates at the aggregate level. These findings would be impossible to obtain had we not dismantled and discarded the instrumental variable estimation route.
B23|Interpretation and Semiparametric Efficiency in Quantile Regression under Misspecification|"Allowing for misspecification in the linear conditional quantile function, this paper provides a new interpretation and the semiparametric efficiency bound for the quantile regression parameter β ( τ ) in Koenker and Bassett (1978). The first result on interpretation shows that under a mean-squared loss function, the probability limit of the Koenker–Bassett estimator minimizes a weighted distribution approximation error, defined as \(F_{Y}(X'\beta(\tau)|X) - \tau\), i.e., the deviation of the conditional distribution function, evaluated at the linear quantile approximation, from the quantile level. The second result implies that the Koenker–Bassett estimator semiparametrically efficiently estimates the quantile regression parameter that produces parsimonious descriptive statistics for the conditional distribution. Therefore, quantile regression shares the attractive features of ordinary least squares: interpretability and semiparametric efficiency under misspecification."
B23|The Empirical Economist's Toolkit: From Models to Methods|"While historians of economics have noted the transition toward empirical work in economics since the 1970s, less understood is the shift toward \quasi-experimental"" methods in applied microeconomics. Angrist and Pischke (2010) trumpet the wide application of these methods as a \credibility revolution"" in econometrics that has nally provided persuasive answers to a diverse set of questions. Particularly in uential in the applied areas of labor, education, public, and health economics, the methods shape the knowledge produced by economists and the expertise they possess. First documenting their growth bibliometrically, this paper aims to illuminate the origins, content, and contexts of quasi-experimental research designs, which seek natural experiments to justify causal inference. To highlight lines of continuity and discontinuity in the transition, the quasi-experimental program is situated in the historical context of the Cowles econometric framework and a case study from the economics of education is used to contrast the practical implementation of the approaches. Finally, signi cant historical contexts of the paradigm shift are explored, including the marketability of quasi-experimental methods and the 1980s crisis in econometrics."
B23|Unraveling the R&D-Innovation-Productivity relationship - a study of an academic endavour|This paper accounts for the development of the academic endavour to determine the firm-level relationship between investments in R&D and productivity. The impact of 28 highly cited publications within this line of study is investigated using a combination of bibliometric techniques and citation function analysis. We show how the attention paid to this line of research broadens and deepens in parallel to the diffusion of innovation as a research theme during 2000s. Our findings also suggest that the attraction of scholarly attention is driven by combination of broadening interest in the central research question under study and boundary-pushing methodological contributions made in the key contributions.
B23|Is Self-employment a Way to Escape from Skill Mismatches?|During the last two decades, skill mismatches have become one of the most important issues of policy concern in the EU (European Commission, 2008). Hence, the literature has stressed the necessity to reduce skill mismatches. We contribute to this literature by analyzing the impact of the transition from salaried employment to self-employment on self-reported skill mismatches. To do so, we resort to the European Community Household Panel (ECHP) covering the period 1994-2001. Using panel data, we track individuals over time and measure their self-reported skill mismatch before and after the transition. Our empirical findings indicate not only that the average self-employee is less likely to declare being skill-mismatched but also that those individuals who transit from salaried employment to self-employment reduce their probability of skill mismatches after the transition.
B23|Literaturbeitrag / Review Paper. Macroeconometric Models – From “Little Science” to “Big Science”|This is a fine, useful book on the history and structure of macroeconometric models. Its perspective is “applied” and has a “positivistic bias”. It gives a good (or not so good) picture of the state of the art. The problems of the now “Big Science” deserve more attention than the modelling community (and Welfe) so far has been willing to pay. The trend towards ever larger policy-relevant models will continue. However, few of them are accessible to third parties. “Transparency”, a major goal models had once started to increase, continues to get out of sight.
B23|Effects of Trade Facilitation on Inequality: A Case Study of Sub-Sahara Africa|Inequality in the sub-Saharan Africa has been on the high side compared to other regions of the world. The policy makers in the region are aware of this and have implemented several policies to stem it. Among the solutions is inclusive growth and strong trade reforms. However, the trade position is still not encouraging even though it is rising. One major factor inhibiting trade and hence inequality is costs attached to the movement of goods across border, that is, trade facilitation. This study investigated the effectiveness of trade facilitation on inequality in a panel of 38 SSA countries spanning from 2005 to 2012. The results show that not all trade facilitation variables contribute to reduction in inequality. While reduction in time required to export significantly reduce inequality, time required to import, and to set up a new business worsened inequality. Custom efficiency is effective and has positive impact on inequality, that is, inequality is less, and the more efficient Customs are. Following these findings, authorities in the region will do well in addressing inequality issues by paying more attention to transaction challenges facing exports and custom efficiencies.
B23|Determinants Of Energy And Co2 Emission Intensities: A Study Of Manufacturing Firms In India|This paper investigates determinants of energy and emission intensities of manufacturing firms in India, from 2000 to 2014. Given that Indian manufacturing sector is one of the world’s most polluting sectors in terms of CO2 emissions; we arrive at firm level determinants of energy and carbon dioxide emission intensities from consumption of three primary sources of energy, namely (1) Coal, (2) Natural Gas and (3) Petroleum. The results of the regression analysis suggest that there are inter-firm differences in energy and emission intensity. The results indicate that smaller and larger firms are both energy and emission intensive compared to medium sized firms. Similarly, firms spending more in research and development activities are found to be energy and emission efficient, compare to others. Hence, in the global competitive business environment, Government of India should carefully formulate policies suitable for the medium sized firms to make them energy and emission efficient.
B23|La Teoria dell'Equilibrio Economico Generale: Walras, Pareto e i Neo-Walrasiani|This paper shows that two different conceptions of economic equilibrium coexist in the theory of general economic equilibrium, as put forward by Walras over the last thirty years of the nineteenth century: a conception of equilibrium as a ‘balance of forces’, on the one hand, and a conception of equilibrium as ‘compatibility of plans’, on the other. Walras, who starts from the first conception in his early theoretical drafts (1871), in developing his theoretical system strives to progressively enhance the role played by the second conception, without ever fully succeeding in his attempt; so that significant traces of the original duplicity persist even in the fourth edition of the Éléments (1900), the last to be published during Walras’s life. Pareto faithfully enough adheres to Walras’s analytic standpoint in his Cours (1896-97), paying himself a price to the ambiguous conception of equilibrium marring Walras’s Éléments. Yet, a few innovative elements, which already peep out in the Cours, will fully blossom in Pareto’s more mature masterpiece, the Manuale di economia politica (1906), whose French edition, the Manuel (1909), crowns Pareto’s theoretical achievements in economics: therein Pareto embraces the conception of equilibrium as a configuration of optimally chosen and mutually consistent plans, eventually succeeding in almost completely erasing the older conception of equilibrium as a ‘balance of forces’. The so-called ‘neo-Walrasian research program’, started by Hicks’s Value and Capital (1939) and fully developed by Arrow, Debreu and other theorists in the 1950s and the following decades, consistently endorses the conception of equilibrium as ‘compatibility of plans’: from this point of view, as well as in a number of related features, the ‘neo-Walrasian research program’ owes more to Pareto’s mature approach than to Walras’s original one, so that it should more appropriately be labelled as the ‘neo-Paretian research program’.
B23|Proving the existence of macropsychological phenomena? The Katona-Tobin controversy over the predictive value of attitudinal data|This paper analyzes the controversy between George Katona and James Tobin that happened at the end of the nineteen fifties. The central problem of the paper concerns the nature of psychological phenomena. While Tobin defends that economic behavior stems from the individual, Katona tries to develop a macropsychological approach in which the individual plays a secondary role. The controversy thus reveals the arguments that initiated the developments of microeconomic theories and the construction of microeconomic data during the nineteen fifties
B23|"""Transit Makes you Short"": On Health Impact Assessment of Transportation and the Built Environment"|The current research provides a test framework to understand whether and to what extent increasing public transit use and accessibility by transit affect health. To this end, the effect of transit mode share and accessibility by transit on general health, body mass index, and height are investigated, while controlling for socioeconomic, demographic, and physical activity factors. The coefficient-p-value-sample-size chart is created and effect size analysis are conducted to explore whether the transit use is practically significant. Building on the results of the analysis, we found that the transit mode share and accessibility by transit are not practically significant, and the power of large-sample misrepresents the effect of transit on public health. The results, also, highlight the importance of data and variable selection by portraying a significant correlation between transit use and height in a multivariate regression analysis. What becomes clear from this study is that in spite of the mushrooming interdisciplinary studies in the nexus of transportation and health arena, researchers often propose short- and long-term policies blindly, while failing to report the inherent explanatory power of variables. We show that there is a thin line between false positive and true negative results. From the weakness of p-values perspective, further, we strove to alert both researchers and practitioners to the dangerous pitfall deriving from the power of large- samples. Building the results on just significance and sign of the parameter of interest is worthless, unless the magnitude of effect size is carefully quantified post analysis.
B23|Sociological Analysis of Economic Inequalities: Methodological Aspects|The paper outlines basic elements of the sociological analysis of economic inequalities. The baseline argument of the paper is that economic inequalities are a sociological concept which carries different content in comparison with economic differentiation and heterogeneity of incomes, which in turn are most often perceived as direct proxies of economic inequalities. The theoretical analysis of the concepts of economic differentiation and economic inequality made is focused on their specific interpretation schemes and measurement strategies and methods. The main problematic areas of classic measures of economic differentiation and are outlined including an analysis of most common research findings. The focus on the sociological analysis of economic inequalities (interpreted as status distances between economic roles and resources associated with these roles) includes the principle aspects of this analysis and a comprehensive research program (including concepts and indicators). Some methodological conclusions regarding the interpretations of economic inequalities are made, based on three ISSP surveys (1992, 1999, and 2009). Also based on empirical data some of elements of the outlined research program with regards to Bulgaria are explored: economic differentiation (Gini and extraction ratio), status identification its relationship to subjective social class, etc.
B23|Estimating The Production Function In The Case Of Romania Metodology And Results|The problem of economic growth is a headline concern among economists, mathematicians and politicians. This is because of the major impact of economic growth on the entire population of a country, which has made achieving or maintaining a sustained growth rate the major objective of macroeconomic policy of any country. Thus, in order to identify present sources of economic growth for Romania in our study we used the Cobb-Douglas type production function. The basic variables of this model are represented by work factors, capital stock and the part of economic growth determined by the technical progress, the Solow residue or total productivity of production factors. To estimate this production function in the case of Romania, we used the quarter statistical data from the period between 2000 – first quarter and 2014 – fourth quarter; the source of the data was Eurostat. The Cobb-Douglas production function with the variables work and capital is valid in Romania’s case because it has the parameters of the exogenous variables significantly different from zero. This model became valid after we eliminated the autocorrelation of errors. Removing the autocorrelation of errors does not alter the structure of the production function. The adjusted R2 determination coefficient, as well as the α and β coefficients have values close to those from the first estimated equation. The regression of the GDP is characterized by marginal decreasing efficiency of the capital stock (α > 1) and decreasing efficiency of work (β
B23|Estimating The Cobb Douglas Production Function Including The Export And Openness In The Case Of Romania|Economic convergence theories are closely related to economic growth theories. The first to study the economic growth phenomenon were the classics A. Smith, D. Ricardo, Th. Malthus, whose models of so-called classic models, do not take into account the contribution of technical progress in increasing production per capita. In order to analyze the convergence process, as a result of economic growth, a series of studies have been created to check the convergent or divergent nature of economies. Thus, in order to identify present sources of economic growth for Romania in our study we have used the Cobb-Douglas type production function. The variables that are the base of this model are represented by work factors and capital stock, to which we have added two explicative variables of economic growth: export and the openness degree of the economy. The two economic growth variables have been included in the model due to their favorable influence on the Solow residue. To estimate this production function, quarterly statistical data from the period between 2000 – first quarter and 2014 – fourth quarter have been used; the source of the data was Eurostat. As to what the first estimated model is concerned, the Cobb-Douglas production function including the export variable are both valid in Romania’s case, this have the parameters of the exogenous variables significantly different from zero, while the second estimated model, which contains the openness variable, is not valid. Its independent variable coefficient is not significantly different from zero, at the level of the entire population. This shows us that the inclusion of the degree of openness of the economy variable in the model affects the significance degree of the model and in order to validate it, the variable must be eliminated. Therefore, we can state that in Romania an increase of the openness degree of the economy due to capital imports would not generate an improvement in what type of technologies are used.
