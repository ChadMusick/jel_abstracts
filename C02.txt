C02|Resuscitating the co-fractional model of Granger (1986)|We study the theoretical properties of the model for fractional cointegration proposed by Granger (1986), namely the FVECM d,b. First, we show that the stability of any discrete time stochastic system of the type II(L)Yt = E t can be assessed by means of the argument principle under mild regularity condition on II (L) where L is the lag operator. Second, we prove that, under stability, the FVECMd,b allows for a representation of the solution that demonstrates the fractional and co-fractional properties and we find a closed-form expression for the impulse response functions. Third, we prove that the model is identifed for any combination of number of lags and cointegration rank, while still being able to generate polynomial co-fractionality. Finally, we show that the asymptotic properties of the maximum likelihood estimator reconcile with those of the FCVARd,b model studied in Johansen and Nielsen (2012).
C02|Desempleo y calidad de vida laboral en las áreas metropolitanas de Barranquilla, Cartagena y Santa Marta|En este trabajo se muestra que las principales ciudades del Caribe colombiano han alcanzado en meses recientes las menores cifras en términos de desempleo en el país. Sin embargo, existen algunos aspectos por mejorar en relación con la calidad de los empleos ofrecidos ya que la tasa de informalidad es una de las más altas. Por ejemplo, en Cartagena el 58 % de los trabajadores son informales, Barranquilla alcanza una el 64 % y Santa Marta, el 66 %, mientras que en el promedio de las principales 23 ciudades esta es el 50,8 %. Este trabajo tiene como objetivo analizar la situación del mercado laboral en términos de la informalidad y la calidad del empleo, en tres de las principales ciudades del Caribe colombiano. Los resultados muestran una evolución decreciente de la informalidad en las ciudades caribeñas en el periodo 2007 hasta septiembre de 2018. Dichos aspectos repercuten sobre las condiciones de vida laboral, que en este trabajo se estudia a través del Índice Multidimensional de Calidad del Empleo (IMCE). Sobre este último se encontró que las tres ciudades presentan condiciones laborales poco atractivas, siendo menos favorable para los ocupados con bajo nivel educativo, empleados domésticos o por cuenta propia y los trabajadores de empresas pequeñas. **** ABSTRACT: We show that, in recent months, the main cities of the Colombian Caribbean have been characterized by having the best figures in terms of unemployment. However, there are some aspects to improve in relation to the quality of the jobs offered. The objective of this paper is to analyze the main characteristics of the labor market of the three main cities of the continental Caribbean, namely, Barranquilla, Cartagena and Santa Marta. We emphasize the stylized facts related to labor informality and the quality of employment. The results show a decreasing evolution of informality in the Caribbean cities in the period 2007 until the first semester of 2018, however, Cartagena maintains 58.5 % of its population in informal status, Barranquilla reaches a figure of 64 % and Santa Marta reaches 66 %, while the main 23 metropolitan areas show a figure of 50.8 %. These aspects have repercussions on working life conditions, which we study in this work by means of the multidimensional index of quality of employment (IMCE). Regarding the latter, we found that the three cities have unattractive labor conditions in terms of quality of employment, which is more critical for the employed without any educational level, domestic workers or self-employed workers, and workers in small companies.
C02|A Theory of Scenario Generation|We show how distributions can be reduced to low-dimensional scenario trees. Applied to intertemporal distributions, the scenarios and their probabilities become time-varying factors. From S&P 500 options, two or three time-varying scenarios suffice to forecast returns, implied variance or skewness on par, or better, than extant multivariate stochastic volatility jump-diffusion models, while reducing the computational effort to fractions of a second.
C02|Desempleo y calidad de vida laboral en las áreas metropolitanas de Barranquilla, Cartagena y Santa Marta|En este trabajo se muestra que las principales ciudades del Caribe colombiano han alcanzado en meses recientes las menores cifras en términos de desempleo en el país. Sin embargo, existen algunos aspectos por mejorar en relación con la calidad de los empleos ofrecidos ya que la tasa de informalidad es una de las más altas. Por ejemplo, en Cartagena el 58 % de los trabajadores son informales, Barranquilla alcanza una el 64 % y Santa Marta, el 66 %, mientras que en el promedio de las principales 23 ciudades esta es el 50,8 %. Este trabajo tiene como objetivo analizar la situación del mercado laboral en términos de la informalidad y la calidad del empleo, en tres de las principales ciudades del Caribe colombiano. Los resultados muestran una evolución decreciente de la informalidad en las ciudades caribeñas en el periodo 2007 hasta septiembre de 2018. Dichos aspectos repercuten sobre las condiciones de vida laboral, que en este trabajo se estudia a través del Índice Multidimensional de Calidad del Empleo (IMCE). Sobre este último se encontró que las tres ciudades presentan condiciones laborales poco atractivas, siendo menos favorable para los ocupados con bajo nivel educativo, empleados domésticos o por cuenta propia y los trabajadores de empresas pequeñas. **** ABSTRACT: We show that, in recent months, the main cities of the Colombian Caribbean have been characterized by having the best figures in terms of unemployment. However, there are some aspects to improve in relation to the quality of the jobs offered. The objective of this paper is to analyze the main characteristics of the labor market of the three main cities of the continental Caribbean, namely, Barranquilla, Cartagena and Santa Marta. We emphasize the stylized facts related to labor informality and the quality of employment. The results show a decreasing evolution of informality in the Caribbean cities in the period 2007 until the first semester of 2018, however, Cartagena maintains 58.5 % of its population in informal status, Barranquilla reaches a figure of 64 % and Santa Marta reaches 66 %, while the main 23 metropolitan areas show a figure of 50.8 %. These aspects have repercussions on working life conditions, which we study in this work by means of the multidimensional index of quality of employment (IMCE). Regarding the latter, we found that the three cities have unattractive labor conditions in terms of quality of employment, which is more critical for the employed without any educational level, domestic workers or self-employed workers, and workers in small companies.
C02|Necessary and Sufficient Conditions for Determinacy of Asymptotically Stationary Equilibria in Olg Models|We propose a criterion for determining whether a local policy analysis can be made in a given equilibrium in an overlapping generations model. The criterion can be applied to models with infinite past and future as well as those with a truncated past. The equilibrium is not necessarily a steady state; for example, demographic and type composition of the population or individuals’ endowments can change over time. However, asymptotically, the equilibrium should be stationary. The two limiting stationary paths at either end of the timeline do not have to be the same. If they are, conditions for local uniqueness are far more stringent for an economy with a truncated past as compared to its counterpart with an infinite past. In addition, we illustrate our main result using a textbook model with a single physical good, a two-period life-cycle and a single type of consumer. In this model we show how to calculate a response to a policy change using the implicit function theorem.
C02|The Cowles Commission and Foundation for Research in Economics: Bringing Mathematical Economics and Econometrics from the Fringes of Economics to the Mainstream|Founded in 1932 by a newspaper heir disillusioned by the failure of forecasters to predict the Great Crash, the Cowles Commission promoted the use of formal mathematical and statistical methods in economics, initially through summer research conferences in Colorado and through support of the Econometric Society (of which Alfred Cowles was secretary-treasurer for decades). After moving to the University of Chicago in 1939, the Cowles Commission sponsored works, many later honored with Nobel Prizes but at the time out of the mainstream of economics, by Haavelmo, Hurwicz and Koopmans on econometrics, Arrow and Debreu on general equilibrium, Yntema and Mosak on general equilibrium in international trade theory, Arrow on social choice, Koopmans on activity analysis, Klein on macroeconometric modelling, Lange, Marschak and Patinkin on macroeconomic theory, and Markowitz on portfolio choice, but came into intense methodological, ideological and personal conflict with the emerging “Chicago school.” This conflict led the Cowles Commission to move to Yale in 1955 as the Cowles Foundation, directed by James Tobin (who had declined to move to Chicago to direct it). The Cowles Foundation remained a leader in the more technical areas of economics, notably with Tobin’s “Yale school” of monetary theory, Scarf’s computable general equilibrium, Shubik in game theory, and later Phillips and Andrews in econometric theory but as formal methods in economic theory and econometrics pervaded the discipline of economics, Cowles (like the Econometric Society) became less distinct from the rest of economics.
C02|Diversity and its decomposition into variety, balance and disparity|Diversity is a central concept in many fields. Despite its importance, there is no unified methodological framework to measure diversity and its three components of variety, balance and disparity. Current approaches take into account disparity of the types by considering their pairwise similarities. Pairwise similarities between types do not adequately capture total disparity, since they fail to take into account in which way pairs are similar. Hence, pairwise similarities do not discriminate between similarity of types in terms of the same feature and similarity of types in terms of different features. This paper presents an alternative approach which is based similarities of features between types over the whole set. The proposed measure of diversity properly takes into account the aspects of variety, balance and disparity, and without having to set an arbitrary weight for each aspect of diversity. Based on this measure, the 'ABC decomposition' is introduced, which pro- vides separate measures for the variety, balance and disparity, allowing them to enter analysis separately. The method is illustrated by analyzing the industrial diversity from 1850 to present while taking into account the overlap in occupations they employ. Finally, the framework is extended to take into account disparity considering multiple features, providing a helpful tool in analysis of high-dimensional data.
C02|The value of flexibility in power markets|The concept of flexibility is not one you find in standard microeconomics textbooks, yet it already plays a major role in the remuneration of the resources that generate and consume electricity every day and is likely to play an even larger role with the penetration of large intermittent renewable capacities. In this paper we attempt to quantify the net revenues that can be captured by a flexible resource able to react to the short term price variations on the day-ahead and intraday markets in Germany. We find that the difference between day-ahead and intraday revenues for a flexible resource has been increasing (although the profitability has been decreasing on both markets). This difference is more pronounced once 15 mn price variations can be captured by a flexible resource. The net revenues from the local 15 mn auction (which is held 3 h after the hourly “coupled” day-ahead auction) are more than eight times higher than the day-ahead hourly auction but below the net revenues that can be captured with the high prices from the continuous market. The results of the backward-looking empirical estimations allow us to distinguish and quantify two components of flexibility: (1) the “immediacy” value as we are approaching real-time and the urgency of the delivery increases (this value is revealed during the continuous intraday process and is highly linked to the stochastic nature of power supply and demand (i.e., wind/solar forecasts, forced outages of thermal generation,…) forecast error risk), and (2) the “ramping capability” component based on the technical characteristics as a resource can react to variations of shorter granularity (15 mn vs. 60 mn). We model and quantify the ramping capability component using a geometric brownian motion with jumps.
C02|A Statistical Field Approach to Capital Accumulation|This paper presents a model of capital accumulation for a large number of heterogenous producer-consumers in an exchange space in which interactions depend on agents' positions. Each agent is described by his production, consumption, stock of capital, as well as the position he occupies in this abstract space. Each agent produces one differentiated good whose price is fixed by market clearing conditions. Production functions are Cobb-Douglas, and capital stocks follow the standard capital accumulation dynamic equation. Agents consume all goods but have a preference for goods produced by their closest neighbors. Agents in the exchange space are subject both to attractive and repulsive forces. Exchanges drive agents closer, but beyond a certain level of proximity, agents will tend to crowd out more distant agents. The present model uses a formalism based on statistical field theory developed earlier by the authors. This approach allows the analytical treatment of economic models with an arbitrary number of agents, while preserving the system's interactions and complexity at the individual level. Our results show that the dynamics of capital accumulation and agents' position in the exchange space are correlated. Interactions in the exchange space induce several phases of the system. A first phase appears when attractive forces are limited. In this phase, an initial central position in the exchange space favors capital accumulation in average and leads to a higher level of capital, while agents far from the center will experience a slower accumulation process. A high level of initial capital drives agents towards a central position, i.e. improve the terms of their exchanges: they experience a higher demand and higher prices for their product. As usual, high capital productivity favors capital accumulation, while higher rates of capital depreciation reduce capital stock. In a second phase, attractive forces are predominant. The previous results remain, but an additional threshold effect appears. Even though no restriction was imposed initially on the system, two types of agents emerge, depending on their initial stock of capital. One type of agents will remain above the capital threshold and occupy and benefit from a central position. The other type will remain below the threshold, will not be able to break it and will remain at the periphery of the exchange space. In this phase, capital distribution is less homogenous than in the first phase.
C02|CDS index options in Markov chain models|We study CDS index options in a credit risk model where the defaults times have intensities which are driven by a finite-state Markov chain representing the underlying economy. In this setting we derive compact computationally tractable formulas for the CDS index spread and the price of a CDS index option. In particular, the evaluation of the CDS index option is handled by translating the Cox-framework into a bivariate Markov chain. Due to the potentially very large, but extremely sparse matrices obtained in this reformulating, special treatment is needed to efficiently compute the matrix exponential arising from the Kolmogorov Equation. We provide details of these computational methods as well as numerical results. The finite-state Markov chain model is calibrated to data with perfect fits, and several numerical studies are performed. In particular we show that under same exogenous circumstances, the CDS index options prices in the Markov chain framework can be close to or sometimes larger than prices in models which assume that the CDS index spreads follows a log-normal process. We also study the different default risk components in the option prices generated by the Markov model, an investigation which is difficult to do in models where the CDS index spreads follows a log-normal process.
C02|DyMH_LU: a simple tool for modelling and simulating the health status of the Luxembourgish elderly in the longer run|We are facing one of the most important demographic events of the last decades in Europe: the population ageing process. This process will have significant economic effects particularly on health. As most diseases are age-related, this process might imply a proportionally higher share of individuals with declining health. Being able to forecast the health status of the population can help to deal with concerns about the financial and social sustainability of several public policies including health. In this paper, we present the DyMH_LU model, a dynamic microsimulation model focused exclusively on the health status of the Luxembourgish population. One of its major characteristics is that it simulates more than sixty different diseases and limitations in the activities of daily living. All this simulated information can be aggregated in order to compute, for each period, the overall health status of each individual, the marginal distribution of each disease among the total population and the global health status of the entire population. The starting point of the DyMH_LU model is the information collected in 2015 in the Wave 6 of the SHARE database that targets individuals aged 51 or older. The simulation period covers 2017 until 2045.
C02|The Knightian Uncertainty Hypothesis: Unforeseeable Change and Muthï¿½s Consistency Constraint in Modeling Aggregate Outcomes|This paper proposes the Knightian Uncertainty Hypothesis (KUH), a new approach to macroeconomics and finance theory. KUH rests on a novel mathematical framework that characterizes both measurable and Knightian uncertainty about economic outcomes. Relying on this framework and Muthï¿½s pathbreaking hypothesis, KUH represents participantsï¿½ forecasts to be consistent with both uncertainties. KUH thus enables models of aggregate outcomes that 1) are premised on market participantsï¿½ rationality, and 2) accord a role to both fundamental and psychological (and other non-fundamental) factors in driving outcomes. The paper also suggests how a KUH modelï¿½s quantitative predictions can be confronted with time-series data.
C02|The Degree Ratio Ranking Method for Directed Networks|One of the most famous ranking methods for digraphs is the ranking by Copeland score. The Copeland score of a node in a digraph is the difference between its outdegree (i.e. its number of outgoing arcs) and its indegree (i.e. its number of ingoing arcs). In the ranking by Copeland score, a node is ranked higher, the higher is its Copeland score. In this paper, we deal with an alternative to rank nodes according to their out– and indegree, namely ranking the nodes according to their degree ratio, i.e. the outdegree divided by the indegree. To avoid dividing by a zero indegree, we implicitly take the out– and indegree of the reflexive digraph. We provide an axiomatization of the ranking by degree ratio using a sibling neutrality axiom, which says that the entrance of a sibling (i.e. a node that is in some sense similar to the original node) does not change the ranking among the original nodes. We also provide a new axiomatization of the ranking by Copeland score using the same axioms except that this method satisfies a different sibling neutrality. Finally, we modify the ranking by degree ratio by not considering the reflexive digraph, but by definition assume nodes with indegree zero to be ranked higher than nodes with a positive indegree. We provide an axiomatization of this ranking by modified degree ratio using yet another sibling neutrality and a maximal property. In this way, we can compare the three ranking methods by their respective sibling neutrality
C02|Current and past trends in physical activity in four OECD countries: Empirical results from time use surveys in Canada, France, Germany and the United States|Physical inactivity and sedentary behaviours have been rising throughout the OECD in recent decades. Lack of physical activity and excessive sedentary behaviour are well-known risk factors for non-communicable diseases, such as heart diseases, stroke, diabetes, and osteoporosis. As such, reducing physical inactivity and sedentary behaviours and increasing daily physical activity has become a crucial public health issue. Using nationally representative time use surveys, this paper presents the trends in physical activity (PA) and sedentary behaviours over time, in Canada, France, Germany and the United States. A particular focus of this analysis is placed on sport activities. Men and women spend between 80 and 105 minutes daily in physical activities, with women spending more time in domestic physical activity, and men more time in sports. Participation in sport activities has been increasing over time, but no global trend for time spent in sports is visible; additionally, women are consistently less likely than men to report engagement in sport activities. Meanwhile, participation in active travel has been decreasing, displaying no overall trend for duration either. Education-based inequalities for sports participation are higher in men than in women, while income-based inequalities for sports are higher in women than in men. Men and women with a low level of income are more likely to report active travel in all countries. Additional MET (metabolic equivalent) hours spent in sports and non-sports leisure PA, domestic PA, and active travel are all associated with an increase in total PA, while work-related PA as well as other activities are associated with a decrease in total PA. At the individual level, an increase in time spent in all previously mentioned activities is associated with a decrease in total time spent in sedentary behaviours.
C02|Forbidden zones for the expectations of measurement data and problems of behavioral economics|A theorem, applied mathematical method and qualitative mathematical models are introduced in the present article. The method and models are based on the forbidden zones of the theorem and suppose that people decide as if there were some biases of the expectations of measurement data, e.g., under influence of noise. The article is motivated by the need for theoretical support for the practical analysis that was performed for the purposes of behavioral economics.
C02|Optimal Control of the Parameters of the Production Line|The problem of optimal control of the parameters of the production flow line - stocks (work in process) and the rate of processing of objects of labour for a technological operation is considered. The article presents a mathematical formulation of the problem of controlling the parameters of a production line with restrictions on work in progress and the speed of machining parts for each technological operation. The control program is determined by the specified quality criteria. An example of the calculation of the optimal control for the production line parameters is presented.
C02|New Essentials of Economic Theory I. Assumptions, Economic Space and Variables|This paper develops economic theory framework free from general equilibrium assumptions. We describe macroeconomics as system of economic agents under action of n risks. Economic and financial variables of agents, their expectations and transactions between agents define macroeconomic variables. Agents variables depend on transactions between agents and transactions are performed under agents expectations. Agents expectations are formed by economic variables, transactions, expectations of other agents, other factors that impact macroeconomic evolution. We use risk ratings of agents as their coordinates on economic space and approximate description of economic and financial variables, transactions and expectations of numerous separate agents by description of variables, transactions and expectations as density functions on economic space. We describe evolution of macroeconomic density functions of variables, transactions and expectations and their flows induced by motion of separate agents on economic space due to change of agents risk rating. We apply our model to description of business cycles, present models of wave propagation for disturbances of economic variables and transactions, model asset price fluctuations and argue hidden complexities of classical Black-Scholes-Merton option pricing.
C02|Распределенная Динамическая Pde-Модель Программного Управления Загрузкой Технологического Оборудования Производственной Линии<BR>[Distributed dynamic PDE-model of a program control by utilization of the technological equipment of production line]|В работе необходимо рассмотреть проектирование системы управления параметрами производственной линии для предприятия с поточным методом организации производства. Методика. Производственная ли-ния предприятия с поточным методом организации производства – это сложная динамическая распреде-ленная система. Технологический маршрут изготовления изделия для многих современных предприятий содержит несколько сотен технологических операций, в межоперационном заделе каждой из которых со-держатся тысячи изделий, ожидающих обработку. Технологические маршруты разных деталей одного вида изделий пересекаются. Это приводит к тому, что распределение предметов труда вдоль технологического маршрута оказывает значительное влияние на пропускную способность производственной линии. Для опи-сания таких систем введен новый класс моделей производственных линий (PDE-model). Модели этого клас-са используют уравнения в частных производных для описания поведения потоковых параметров произ-водственной линии. В данной статье построена PDE-модель производственной линии, потоковые парамет-ры которой зависят от величины коэффициента загрузки технологического оборудования для каждой опе-рации.
C02|Bitcoin's return behaviour: What do We know so far?|In this paper we study the daily return behavior of Bitcoin digital currency. We propose the use of generalized hyperbolic distributions (GH) to model Bitcoin's return. Our, results show that GH is a very good candidate to model this return.
C02|Shape Factor Asymptotic Analysis I|"The shape factor defined as kurtosis divided by skewness squared K/S^2 is characterized as the only choice among all factors K/〖|S|〗^α ,α>0 which is greater than or equal to 1 for all probability distributions. For a specific distribution family, there may exists α>2 such that min⁡〖K/〖|S|〗^α 〗≥1. The least upper bound of all such α is defined as the distribution’s characteristic number. The useful extreme values of the shape factor for various distributions which are found numerically before, the Beta, Kumaraswamy, Weibull, and GB2 Distribution, are derived using asymptotic analysis. The match of the numerical and the analytical results can be considered prove of each other. The characteristic numbers of these distributions are also calculated. The study of the boundary value of the shape factor, or the shape factor asymptotic analysis, help reveal properties of the original shape factor, and reveal relationship between distributions, such as between the Kumaraswamy distribution and the Weibull distribution."
C02|Monotonic Estimation for the Survival Probability over a Risk-Rated Portfolio by Discrete-Time Hazard Rate Models|Monotonic estimation for the survival probability of a loan in a risk-rated portfolio is based on the observation arising, for example, from loan pricing that a loan with a lower credit risk rating is more likely to survive than a loan with a higher credit risk rating, given the same additional risk covariates. Two probit-type discrete-time hazard rate models that generate monotonic survival probabilities are proposed in this paper. The first model calculates the discrete-time hazard rate conditional on systematic risk factors. As for the Cox proportion hazard rate model, the model formulates the discrete-time hazard rate by including a baseline component. This baseline component can be estimated outside the model in the absence of model covariates using the long-run average discrete-time hazard rate. This results in a significant reduction in the number of parameters to be otherwise estimated inside the model. The second model is a general form model where loan level factors can be included. Parameter estimation algorithms are also proposed. The models and algorithms proposed in this paper can be used for loan pricing, stress testing, expected credit loss estimation, and modeling of the probability of default term structure.
C02|An attitude of complexity: thirteen essays on the nature and construction of reality under the challenge of Zeno's Paradox|This book is about the construction of reality. The central aim of this study is to understand how gravity works and how it may be focused and manipulated. While I do not have an answer to this question, the discoveries along the way have been worth collecting into a single volume for future reference.
C02|IFRS9 Expected Credit Loss Estimation: Advanced Models for Estimating Portfolio Loss and Weighting Scenario Losses|Estimation of portfolio expected credit loss is required for IFRS9 regulatory purposes. It starts with the estimation of scenario loss at loan level, and then aggregated and summed up by scenario probability weights to obtain portfolio expected loss. This estimated loss can vary significantly, depending on the levels of loss severity generated by the IFSR9 models, and the probability weights chosen. There is a need for a quantitative approach for determining the weights for scenario losses. In this paper, we propose a model to estimate the expected portfolio losses brought by recession risk, and a quantitative approach for determining the scenario weights. The model and approach are validated by an empirical example, where we stress portfolio expected loss by recession risk, and calculate the scenario weights accordingly.
C02|A simple characterization for sustained growth|This article considers an inter-temporal optimization problem in a fairly general form and give sufficient conditions ensuring the convergence to infinity of the economy. These conditions are easy to verify and can be applied for a large class of problems in literature. As examples, some applications for different economies are also given.
C02|A simple characterization for sustained growth|This article considers an inter-temporal optimization problem in a fairly general form and give sufficient conditions ensuring the convergence to infinity of the economy. These conditions are easy to verify and can be applied for a large class of problems in literature. As examples, some applications for different economies are also given.
C02|A simple characterization for sustained growth|This article considers an inter-temporal optimization problem in a fairly general form and give sufficient conditions ensuring the convergence to infinity of the economy. These conditions are easy to verify and can be applied for a large class of problems in literature. As examples, some applications for different economies are also given.
C02|Solving the Grid-Connected Microgrid Operation by Teaching Learning Based Optimization Algorithm|In this paper, the grid-connected operation of microgrid is investigated where the microgrid can exchange power with the main grid. The proposed problem is modeled as the mixed-integer linear programming (MILP) and is solved by an evolutionary algorithm known as the teaching learning-based optimization (TLBO). Finally, the proposed model is tested on a modified IEEE 33 bus test system to show the performance of the method.
C02|Passive Control of Spar Type Floating Wind Turbine using Effective Economic Optimal Design Values|This paper studies the performance of a passive linear tuned mass damper on controlling motion of a floating wind turbine. Controlled and uncontrolled analytical models of the spar FWT is established using Newton’s second law and conservation of angular momentum theory. The aerodynamic, hydrodynamic, mooring and buoyancy forces are determined and coupled with the system. For the controlled model, the TMDs are located in different locations and are tuned to different frequency ratios to reduce the motion of the FWT in different directions. The economic optimal design values are considered for the passive controller. The control performance is evaluated by the reduction of root mean square response in each degree of freedom. The results reveals that the linear TMD tuned to the frequency of pitch degree of freedom reduces the translational motion and rotational motion by 5% and 12% respectively. However, tuning the linear tuned mass damper to the frequency of surge degree of freedom provides 8% and 6% motion reduction in translational and rotational degrees of freedom. Also, it has been shown that installing the linear TMD inside the spar is more effective than installing the TMD inside the nacelle.
C02|New Essentials of Economic Theory I. Assumptions, Economic Space and Variables|This paper develops economic theory framework free from assumptions on market equilibrium, utility functions, rational expectations and etc. We describe macroeconomics as system of economic agents under action of n risks. Economic and financial variables of agents, their expectations and transactions between agents define macroeconomic variables. Agents variables depend on transactions between agents and transactions are performed under agents expectations. Agents expectations are formed by economic variables, transactions, expectations of other agents, by all factors that impact macroeconomic evolution. We describe evolution of macroeconomic variables, transactions and expectations by systems of economic partial differential equations. We develop asset pricing model as a result of equations on transactions and expectations and derive equations that describe price dynamics. To do this we use risk ratings of economic agents as their coordinates on economic space. We approximate description of economic and financial variables, transactions and expectations of numerous separate agents by description of variables, transactions and expectations as density functions on economic space. We take into account flows of economic variables, transactions and expectations induced by motion of separate agents on economic space due to change of agents risk ratings and describe macroeconomic impact of these economic flows. We apply our model to description of business cycles, describe models of wave propagation for disturbances of economic variables and transactions, model asset price fluctuations and argue hidden complexities of classical Black-Scholes-Merton option pricing.
C02|Дискретно-Событийная Модель Расчета Продолжительного Производственного Цикла Изготовления Партии Деталей<BR>[Discrete-Eventing Model Of Calculation Of The Duration Of The Production Cycle Of Manufacturing A Part Of Products]|The method of calculating the duration of the production cycle for manufacturing a batch of parts is considered. The production cycle of manufacturing the batch of parts is one of the main characteristics of the production system. It is used to calculate the important indicators of planning the production activity of the factory. At present, the task of calculating the duration of the production cycle for unsynchronized production lines remains relevant. The task takes on special relevance in the case when the processing time of a work item in a technological operation is a random quantity. The present work is devoted to the analysis of this case. To derive the equation of motion of labour objects for technological operations, a discrete-event model of the production process is used. The structure of the processing time of an object of labour on a technological operation is considered. The source of change in the value of inter-operational stocks at each technological operation is shown. The interrelation of the trajectories of the previous and after subjects of labour is analyzed. The equation of motion of a subject of labour on technological operations is recorded, taking into account the inter-operational stocks. Methods for its solution are proposed. Conditions for the applicability of the obtained results are considered. The analysis of the machine time spent on calculating the duration of the production cycle of manufacturing the batch of products for the factory of the semiconductor industry was carried out. Prospects for research have been determined.
C02|Performance measurement and decomposition of value added|In this paper, we benchmark an investment actively managed (e.g., fund, portfolio) against a reference portfolio passively managed replicating the investment's cash flows in order to measure the value added by the active investment and decompose it according to the influence of the investment choices (i.e., selection and allocation of assets) made in the various periods. The active investment choices are reflected in the investment's returns as opposed to the benchmark returns earned by the passive strategy. We precisely quantify the impact of the holding period rates on the value added and rank them accordingly, in order to identify the most (and the least) influential ones. The analysis is performed by applying the Finite Change Sensitivity Index (FCSI) method (Borgonovo 2010a, 2010b), a recently-conceived technique of sensitivity analysis, which we refine by means of a duplication-clearing procedure which allows a perfect (i.e., with no residue) decomposition of the value added. We conduct the analysis for a given contribution-and-distribution policy, characterized by a fixed sequence of deposits and withdrawals. We show that, if the contribution-and-distribution policy changes, the effect of the investment choices made in the various periods on the value added changes as well.
C02|The accounting-and-finance of a solar photovoltaic plant: Economic efficiency of a replacement project|In this work we illustrate a simple logical framework serving the purpose of assessing the economic profitability and measuring value creation in a solar photovoltaic (PhV) project and, in general, in a replacement project where the cash-flow stream is nonnegative, with some strictly positive cash flows. We use the projected accounting data to compute the average ROI, building upon Magni (2011, 2019) (see also Magni and Marchioni 2018), which enables retrieving information on the role of the project’s economic efficiency and the role of the project scale on increasing shareholders’ wealth. The average ROI is a genuinely internal measure and does not suffer from the pitfalls of the internal rate of return (IRR), which may be particularly critical in replacement projects such as the purchase of a PhV plant aimed at replacing conventional retail supplies of electricity.
C02|Numerical Study of the Gap at the Base of the Bridge on the River Flow Parameters|In this study, substrate surface channel and bridge piers is modeled by utilization of Ansys Fluent. Continuity and Momentum equations is solved and for the pressure-velocity linkage SIMPLE algorithm is utilized. In order to model turbulence due to relatively high Reynolds value, K-ε is being used. Effect of wall shear stress, velocity magnitude and static pressure is being investigated at different heights thorough substrate longitudinal lines. It was found that different heights of longitudinal lines won’t affect static pressure, while it affects velocity magnitude and wall shear stress significantly.
C02|Human networks and toxic relationships|We devise a theoretical model to shed light on the dynamics leading to the so called toxic relationships. We want to investigate what policy interventions people could advocate to protect themselves and to reduce suffocant assuefaction so to escape to the trap of physical or psychological abuses either in family or at work. By assuming that the toxic partner's behavior is exogenous and that the main source of addiction is income or wealth, and solving a dynamical system of differential equations we find that an asympotically stable equilibrium with positive love is always possibile for enough high level of appealing unless subsides to reduces assuefaction are introduced. Also the existence of a third uncondicionally reciprocating part as a benckmark (which represents not only the real presence of another partner but also the support from family, friends and overall private organizations. These last may help victims of domestic abuses or private organizations by offering economic and psychological support as well as legal counseling to victims of bullying at workplace and placement offices which e ectively help to find soon another job)plays an important role in reducing the toxic partner's appealing. By solving our model we outline the condition for a best mixed policy where both monetary subsides and alternatives are at work
C02|The optimal control problem for output material flow on a conveyor belt with input accumulating bunker|The article is devoted to the synthesis of optimal control of the conveyor belt with the accumulating input bunker. Much attention is given to the model of the conveyor belt with a constant speed of the belt. Simulation of the conveyor belt is carried out in the one-moment approximation using partial differential equations. The conveyor belt is represented as a distributed system. The used PDE-model of the conveyor belt allows determining the state of the flow parameters for a given technological position as a function of time. We consider the optimal control problem for flow parameters of the conveyor belt. The problem consists in ensuring the minimum deviation of the output material flow from a given target amount. The control is carried out by the material flow amount, which comes from the accumulating bunker into the conveyor belt input. In the synthesis of optimal control, we take into account the limitations on the size of the accumulating bunker, as well as on both max and min amounts of control. We construct optimal control of the material flow amount coming from the accumulating bunker. Also, we determine the conditions to switch control modes and estimate time period between the moments of the switching.
C02|Similarity Maps in Various Areas of Science|The aim of the present work are similarity/dissimilarity studies in bioinformatics as well as in social and health sciences. The considered objects have very diverse characters. They are biological sequences in bioinformatics and groups of individuals and their answers to some questions in the case of health and social sciences. In particular, we present a new bioinformatics method in which the biological sequences are represented by 3D-dynamic graphs [1]. For comparison of such objects we apply values analogous to these used in the classical dynamics. This method is a generalization of the 2D method [2]. Similarity is relative: it depends on the properties being considered. Examples of similarity maps for different properties (factors) in social science [3,4], in bioinformatics [1], and unpublished results in the health sciences are shown. In this way, we can classify different objects and search for factors which influence the results of the classifications.[1] P. W??, D. Bieli?ska-W??, 3D-dynamic Representation of DNA Sequences, Journal of Molecular Modeling 20 (2014) art. ID 2141, 1-7.[2] P. W??, D. Bieli?ska-W??, A. Nandy, Descriptors of 2D-dynamic Graphs as a Classification Tool of DNA Sequences, Journal of Mathematical Chemistry 52 (2014) 132-140.[3] A. Bieli?ska, M. Majkowicz, P. W??, D. Bieli?ska-W??, Overall Quality of Life and General Health - Changes Related to the Retirement Threshold, eTELEMED 2018, The Tenth International Conference on eHealth, Telemedicine, and Social Medicine, Rome, Italy, 2018, XPS IARIA Press, eTELEMED 2018 Proceedings, eds. Y. Murata et al., pp. 1-5.[4] A. Bieli?ska, M. Majkowicz, P. W??, D. Bieli?ska-W??, Mathematical Modeling: Interdisciplinary Similarity Studies, in ?Numerical Methods and Applications?, eds. G. Nikolov et al., Lecture Notes in Computer Science vol. 11189, pp. 334-341, Springer, 2019.
C02|Policy Distortions and Aggregate Productivity with Endogenous Establishment-Level Productivity|The large differences in income per capita across countries are mostly accounted for by differences in total factor productivity (TFP). What explains these differences in TFP across countries? Evidence suggests that the (mis)allocation of factors of production across heterogenous production units is an important factor. We study factor misallocation in a model with an endogenously determined distribution of establishment-level productivity. In this framework, policy distortions not only misallocate resources across a given set of productive units, but they also worsen the distribution of establishment-level productivity. We show that in our model, compared to the model with an exogenous distribution, the quantitative effect of policy distortions is substantially amplified. Whereas empirically-plausible policy distortions in our model generate TFP that is 14 percent that of a benchmark economy with no distortions, with an exogenous distribution the same policy distortions generate TFP that is 86 percent of the benchmark, a 6-fold amplification factor.
C02|Decision Making and Games with Vector Outcomes|In this paper, we study decision making and games with vector outcomes. We provide a general framework where outcomes lie in a real topological vector space and the decision makerâ€™s preferences over outcomes are described by a preference cone, which is defined as a convex cone satisfying a continuity axiom. Further, we define a notion of utility representation and introduce a duality between outcomes and utilities. We provide conditions under which a preference cone is represented by a utility and is the dual of a set of utilities. We formulate a decision-making problem with vector outcomes and study optimal choices. We also consider games with vector outcomes and characterize the set of equilibria. Lastly, we discuss the problem of equilibrium selection based on our characterization.
C02|A quantum framework for economic science: New directions|The current paper explores the cutting-edge applications of quantum field theory and quantum information theory modelling in different areas of economic science, namely, in the behavioural modelling of agents under market uncertainty, and mathematical modelling of asset or option prices and firm theory. The paper then provides a brief discussion into a possible extension of the extant literature of quantum-like modelling based on scattering theory and statistical field theory. A statistical theory of firm based on Feynman path integral technique is also proposed very recently. The collage of new initiatives as described in the current paper will hopefully ignite still newer ideas.
C02|Correlation Investigation: The Cognitive Reflection Test and the Math National Evaluation scores|Purpose - the aim of the paper is to assess the relationship between the Cognitive Reflection Test (CRT) scores and the Math National Evaluation scores.Design/methodology/approach - statistical analyze and econometric methods, using SPSS, the Pearson correlation; critical assessment of literature review; quantitative methods.Findings - there is a significant, strong and positive correlation between the Math National Evaluation scores and the Cognitive Reflection Test scores, in both versions, the original CRT and the 10 items LCRT (Long Cognitive Reflection Test). Practical implications ? optional courses focused on logical reasoning could have a positive impact on Math National Evaluation scores.Originality/value ? to the author?s knowledge, this is the first analysis of the relationship between CRT scores and Math scores at secondary school level.Limitations - the participants were only from the Eastern part of Romania.
C02|Les impacts de la fiscalité carbone sur les ménages : les Français pas tous égaux devant les coups de pompe|La fiscalité des carburants ne peut expliquer à elle seule le mouvement social des gilets jaunes. Mais elle a fédéré le ressentiment d’une partie de la population française sur la question du pouvoir d’achat et a finalement conduit le gouvernement à renoncer à la hausse programmée de la composante carbone de la taxe intérieure sur la consommation sur les produits énergétiques (Contribution climat énergie, CCE) tout comme le rattrapage de la fiscalité du diesel sur celle de l’essence pour l’année 2019
C02|Decomposition of changes in the consumption of macronutrients in Vietnam between 2004 and 2014|Vietnam is undergoing a nutritional transition like many middle-income countries. This transition is characterized by an increase in per capita total calorie intake resulting from an increase in the consumption of fat and protein while the carbohydrate consumption decreases. This paper proposes to highlight the sociodemographic drivers of this transition over the period 2004–2014, using Vietnam Household Living Standard Survey data. We implement a method of decomposition of between-year differences in economic outcomes recently proposed in the literature. This method decomposes the between-year change in various indicators related to the outcome distribution (mean, median, quantiles, etc.) into (1) the effect due to between-year change in the conditional distribution of the outcome given sociodemographic characteristics, or “structure effect”, and (2) the effect due to the differences in sociodemographic characteristics across years, or “composition effect”. In turn, this last effect is decomposed into direct contributions of each sociodemographic characteristics and effects of their interactions. The composition effect, always positive, generally outweighs the structure effect when considering the between-year changes in distributions of per capita calorie intake or calorie intake coming from protein or fat. The effects of changes in the composition of the Vietnamese population thus overcome the effects of changes in preferences of the same population. This finding is reversed in the case of carbohydrates. Food expenditure and household size appear to be the main contributors to the composition effect. The positive effects of these two variables explain well most of the between-year shifts observed in the calorie intake distributions. Urbanization and level of education contribute negatively to the composition effect, with the noticeable exception of fat where the effect of urbanization is positive. But these two variables effects are negligible compared to those of food expenditure and household size.
C02|Measuring the progress of the timeliness childhood immunization compliance in Vietnam between 2006-2014: A decomposition analysis|Vietnam launched the national Expanded Program on Immunization in 1981. Since then, this program has contributed signi cantly to the improvement of child health and to the reduction of child mortality rate. Despite of the fact that the coverage of the national EPI keeps expanding, the number of children who complied with the recommended immunization schedule remains low. This article studies the progress of the timeliness childhood immunization compliance among children between 0-5 years of age in Vietnam from 2006 to 2014 and analyzes the socio-economic factors that account for the changes of the compliance rate during this period. The dataset is extracted from the Multiple Indicator Cluster Survey in 2006 and 2014. We rst identify the socio-economic factors that impact on the vaccination compliance rate using a logistic regression model. Next, we apply the decomposition method to determine the contribution of each factor on the evolution of the timeliness childhood immunization compliance. The progress of the timeliness childhood immunization has been positive and the major contribution comes from the structure e ect (unmeasured e ect). Rural areas show a stronger improvement as of 2014. Among the socio-economic factors, mother education and birth order are the ones that have the larger in uence on the childhood immunization compliance rate. However, these factors have di erent implications in urban and rural areas. These ndings are critical to the current context of Vietnam where the government is designing a strategy focusing on the e ectiveness rather than the traditional coverage indicator.
C02|Macronutrient balances and body mass index: a new insight using compositional data analysis with a total at various quantile orders|Vietnam launched the national Expanded Program on Immunization in 1981. Since then, this program has contributed signi cantly to the improvement of child health and to the reduction of child mortality rate. Despite of the fact that the coverage of the national EPI keeps expanding, the number of children who complied with the recommended immunization schedule remains low. This article studies the progress of the timeliness childhood immunization compliance among children between 0-5 years of age in Vietnam from 2006 to 2014 and analyzes the socio-economic factors that account for the changes of the compliance rate during this period. The dataset is extracted from the Multiple Indicator Cluster Survey in 2006 and 2014. We rst identify the socio-economic factors that impact on the vaccination compliance rate using a logistic regression model. Next, we apply the decomposition method to determine the contribution of each factor on the evolution of the timeliness childhood immunization compliance. The progress of the timeliness childhood immunization has been positive and the major contribution comes from the structure e ect (unmeasured e ect). Rural areas show a stronger improvement as of 2014. Among the socio-economic factors, mother education and birth order are the ones that have the larger in uence on the childhood immunization compliance rate. However, these factors have di erent implications in urban and rural areas. These ndings are critical to the current context of Vietnam where the government is designing a strategy focusing on the e ectiveness rather than the traditional coverage indicator.
C02|Duality in Production|The paper reviews the application of duality theory in production theory. Duality theory turns out to be a useful tool for two reasons: (i) it leads to relatively easy characterizations of the properties of systems of producer derived demand functions for inputs and producer supply functions for outputs and (ii) it facilitates the generation of flexible functional forms for producer demand and supply functions that can be estimated using econometrics. The paper focuses on describing the properties of five functional forms that have been used in the production literature: (i) the Constant Elasticity of Substitution (CES), (ii) the Generalized Leontief, (iii) the translog, (iv) the Normalized Quadratic and (v) the KonÃ¼s Byushgens Fisher functional forms. The applications of GDP functions and joint cost functions to various areas of applied economics is explained.
C02|The New Keynesian Model with Stochastically Varying Policies|The Multiplicative Ergodic Theorem provides a novel general method- ology to analyze rational expectations models with stochastically vary- ing coecients. The approach is applied for the first time to economics and analyzes the canonical New Keynesian model with a Taylor rule which switches randomly between an aggressive and a passive reaction to in ation. The paper delineates the trade-o of the central bank of being passive in some periods and aggressive in others. Moreover, it is shown how this trade-o depends on the stochastic process governing the randomness in the central bank's policy. Finally, explicit solution formulas are derived in the case of determinateness as well as inde- terminateness. In doing so he paper considerably extends the current approach.
C02|Robust policy schemes for R&D games with asymmetric information|We consider an abstract setting of the differential r&d game, where participating firms are allowed for strategic behavior. We assume the information asymmetry across those firms and the government, which seeks to support newer technologies in a socially optimal manner. We develop a general theory of robust subsidies under such one-sided uncertainty and establish results on relative optimality, duration and size of different policy tools available to the government. It turns out that there might exist multiple sets of second-best robust policies, but there always exist a naturally induced ordering across such sets, implying the optimal choice of a policy exists for the government under different uncertainty levels.
C02|Government Expenditure Ceiling and Public Debt Dynamics in a Demand-led Macromodel|This article explores some aspects of the debate about the efficacy of a fiscal rule that sets a government expenditure ceiling for the stabilisation of the public debt-to-output ratio. We develop a demand-led macromodel that assumes a closed economy operating with excess capacity and show that a fiscal rule that sets a limit for government spending, excluding the payment of interests, may not ensure a non-explosive trajectory of the public debt-to-output ratio. Our model allows us to map out different outcomes in terms of the stabilisation of the public debt stemming from the process of fiscal consolidation and conclude that the commitment of the fiscal authority to comply with the ceiling by cutting government spending is less likely to stabilise the public debt-to-output ratio in economies enduring excessively high interest rates accompanied by more regressive taxation systems. Our model also suggests that a more progressive tax structure may increase the likelihood of public debt stabilisation in the long run.
C02|The Conjunction Fallacy in Quantum Decision Theory|"The conjunction fallacy is a renowned violation of classical probability laws, which is persistently observed among decision makers. Within Quantum decision theory (QDT), such deviations are the manifestation of interference between decision modes of a given prospect. We propose a novel QDT interpretation of the conjunction fallacy, which cures some inconsistencies of a previous treatment, and incorporates the latest developments of QDT, in particular the representation of a decision-maker's state of mind with a statistical operator. Rather than focusing on the interference between choice options, our new interpretation identifies the origin of uncertainty and interference between decision modes to an entangled state of mind, whose structure determines the representation of prospects. On par with prospects, the state of mind can be a source of uncertainty and lead to interference effects, resulting in characteristic behavioral patterns. We present the first in-depth QDT-based analysis of an empirical study (the touchstone experimental investigations of Shafir et al. (1990)), which enables a data-driven exploration of its underlying theoretical construct. We link typicality judgements to probability amplitudes of the decision modes in the state of mind, and quantify the level of uncertainty and the relative contributions of prospect's interfering modes to its probability judgement. This enables inferences about the key QDT interference ""attraction'' q-factor with respect to different types of prospects - compatible versus incompatible. We propose a novel empirically motivated ""QDT indeterminacy (or uncertainty) principle,'' as a fundamental limit of the precision with which certain sets of prospects can be simultaneously known (or assessed) by a decision maker, or elicited by an experimental procedure. For any type of prospects, we observe a general tendency for the q-factor to converge to the same negative range q > (−0.3,−0.1) in the presence of high uncertainty, which motivates the hypothesis of an universal ""aversion'' q. The ""aversion'' q is independent of the (un-)attractiveness of a prospect under more certain conditions, which is the main difference with the previously considered ""QDT quarter law''. The universal ""aversion'' q substantiates the previously proposed ""QDT uncertainty aversion principle'' and clarifies its domain of application. The universal ""aversion'' q provides a theoretical basis for modelling different risk attitudes, such as aversions to uncertainty, to risk or to losses."
C02|Predicción del mercado de TES en el corto plazo|En el presente trabajo se estudia la hipótesis de caminata aleatoria para el mercado colombiano de bonos gubernamentales (TES). En el estudio se encuentra que es posible predecir el precio en el corto plazo, por lo que se rechaza la hipótesis de caminata aleatoria para dicho mercado. Lo anterior se concluye por medio de un algoritmo de trading intradía, basado en modelos de aprendizaje de máquinas, al cual se le realiza un Backtest y se concluye que los retornos esperados son superiores a los costos de transacción. A su vez, se realiza un Backtest del algoritmo sobre el mercado norteamericano y se observa que en dicho mercado los retornos esperados son significativamente menores. Es de resaltar que adicional a las implicaciones académicas, el algoritmo tiene aplicaciones empíricas ya que puede ser utilizado como una estrategia de inversión.
C02|Estimation of Factor Structured Covariance Mixed Logit Models|Mixed logit models with normally distributed random coefficients are typically estimated under the extreme assumptions that either the random coefficients are completely independent or fully correlated. A factor structured covariance provides a middle ground between these two assumptions. However, because these models are more difficult to estimate, they are not frequently used to model preference heterogeneity. This paper develops a simple expectation maximization algorithm for estimating mixed logit models when preferences are generated from a factor structured covariance. The algorithm is easy to implement for both exploratory and confirmatory factor models. The estimator is applied to stated-preference survey data from residential energy customers (Train, 2007). Comparing the fit across five different models, which differed in their assumptions on the covariance of preferences, the results show that all three factor specifications produced a better fit of the data than the fully correlated model measured by BIC and two out of three performed better in terms of AIC.
C02|Modelling the G20|World leaders have declared the G20 to be the premier forum for economic cooperation. But as its influence and policy agenda has grown, so too has the need to be able to effectively model the G20 and the implications of its policy agenda. The paper introduces the G-Cubed (G20) model: a multi-country, multi-sector, intertemporal general equilibrium model of the G20. The paper gives an overview of the model and highlights its key features through four simulated shocks, all of which relate to the G20’s goal of reducing global current account imbalances: a fiscal shock (reducing the fiscal deficit in the United States), a productivity/fiscal shock (increasing infrastructure investment in Germany), a consumption shock (increasing domestic consumption in China) and the collective impact of all three shocks occurring simultaneously. The results demonstrate that, to be effective, any model of the G20 must reflect the complex trade and financial linkages between countries, the structural differences across G20 economies and the short-term rigidities observed empirically in the data, as well as a high level of disaggregation across economies, markets and sectors. The simulations show that reducing current account imbalances through these policies often comes with a real economic cost. The results also explain some of the shifts in global current account balances observed since 2007.
C02|Shock Diffusion in Large Regular Networks: The Role of Transitive Cycles|We study how the presence of transitive cycles in the interbank network affects the extent of financial contagion. In a regular network setting, where the same pattern of links repeats for each node, we allow an external shock to propagate losses through the system of linkages (interbank network). The extent of contagion (contagiousness) of the network is measured by the limit of the losses when the initial shock is diffused into an infinitely large network. This measure indicates how a network may or may not facilitate shock diffusion in spite of other external factors.\r\nOur analysis provides two main results. First, contagiousness decreases as the length of the minimal transitive cycle increases, keeping the degree of connectivity (density) constant. Secondly, as density increases the extent of contagion can decrease or increase, because the addition of new links might decrease the length of the minimal transitive cycle. Our results provide new insights to better understand systemic risk and could be used to build complementary indicators for financial regulation.
C02|From Methodology to Practice (and Back): Georgescu-Roegen's Philosophy of Economics and the Flow-Fund Model|Despite his early contribution to the rise of mathematics in economics, Georgescu-Roegen's later methodological criticism of models has received little attention from historians and philosophers of economics. This paper attempts to fill this gap following two lines. First, I examine his explicitly methodological claims and connect them with related topics in economic methodology. Building on the distinction between dialectical and arithmomorphic concepts, I characterise his approach to theory-making as a three steps process of idealisation, isolation and arithmetisation. In this framework, models perform two functions, checking for logical consistency and facilitating understanding, which can be related to the idea of modelling as theorising. I then confront these general principles with Georgescu-Roegen's flow-fund model of production. I use the methodology as a reading grid of this theory, while examining its limits and complementary principles in practice. This shows a great deal of consistency, where idealisation provides conceptual foundations, isolation determines the relevant problems, and models are built according to structural consistency. The two functions of models are then illustrated by the logical derivation of older principles formulated by Babbage and Smith, and the understanding of the different organisational patterns of production. But some slightly different functions also appear when specific configurations of the model enable to check the conceptual consistency of other theories, or the understanding provided by the model contributes to the formation of new concepts. Hence, the consistency and the complementarity between Georgescu-Roegen's methodology and practice of theory-making provide interesting insights and a useful background for further investigations
C02|Robust comparative statics for non-monotone shocks in large aggregative games|A policy change that involves a redistribution of income or wealth is typically controversial, affecting some people positively but others negatively. In this paper we extend the “robust comparative statics” result for large aggregative games established by Acemoglu and Jensen (2010) to possibly controversial policy changes. In particular, we show that both the smallest and the largest equilibrium values of an aggregate variable increase in response to a policy change to which individuals' reactions may be mixed but the overall aggregate response is positive. We provide sufficient conditions for such a policy change in terms of distributional changes in parameters.
C02|The Intergenerational Elasticity of What? The Case for Redefining the Workhorse Measure of Economic Mobility|The intergenerational elasticity (IGE) has been assumed to refer to the expectation of children’s income when in fact it pertains to the geometric mean of children’s income. We show that mobility analyses based on the conventional IGE have been widely misinterpreted, are subject to selection bias, and cannot disentangle the “channels” underlying intergenerational persistence. The solution to these problems — estimating the IGE of expected income or earnings — returns the field to what it has long meant to estimate. Under this approach, persistence is found to be substantially higher, thus raising the possibility that the field’s stock results are misleading.
C02|Intergenerational Income Elasticities, Instrumental Variable Estimation, and Bracketing Strategies|Although the intergenerational income elasticity (IGE) has long been the workhorse measure of economic mobility, this elasticity has been widely misinterpreted as pertaining to the conditional expectation of children’s income when it actually pertains to its conditional geometric mean. This has led to a call to replace it by the IGE of the expectation, which requires developing the methodological knowledge necessary to estimate the latter with short-run measures of income. This paper contributes to this aim. It advances a “bracketing strategy” for the estimation of the IGE of the expectation that is equivalent to that used to bracket the conventional IGE with estimates obtained with the Ordinary Least Squares and Instrumental Variable (IV) estimators. The proposed bracketing strategy couples estimates generated with the Poisson Pseudo Maximum Likelihood estimator and a Generalized Method of Moments IV estimator of the Poisson or exponential regression model. To achieve its goal the paper develops two generalized error-in-variables models for the IV estimation of the IGE of the expectation, and compares them to the corresponding model underlying the IV estimation of the conventional IGE. By considering the bracketing strategies from the perspective of the partial-identification approach to inference, the paper also specifies how to construct confidence intervals for the IGEs from the bounds estimated with those strategies. Lastly, using data from the Panel Study of Income Dynamics, the paper shows that the bracketing strategies work as expected, and assesses the information they generate and how this information varies across instruments and short-run measures of parental income.
C02|Forecasting Inflation Expectations from the CESifo World Economic Survey: An Empirical Application in Inflation Targeting|The purpose of this paper is twofold. First, we evaluate the responses to the questions on inflation expectations in the World Economic Survey for sixteen inflation targeting countries. Second, we compare inflation expectation forecasts across countries by using a two-step approach that selects the most accurate linear or non-linear forecasting method for each country. Then, using Self Organizing Maps, we cluster the inflation expectations, setting June 2014 as a benchmark. At this time there was a sharp decline in oil prices and by analyzing inflation expectations in the context of this price change, we can discriminate between countries that anticipated the oil shock smoothly and those that had to significantly adjust their expectations. Our main findings from the in-sample comparison of the WES surveys suggest that expert forecasts of inflation expectations are systematically distorted in 83 percent of the countries in the sample. On the other hand, our out of sample forecast analysis indicates that Non-linear Artificial Neural Networks combined with Bayesian regularization outperform ARIMA linear models for longer forecasting horizons. This holds true for countries with both soft and brisk changes of expectations. However, when forecasting one step ahead, the performance between the two methods is similar.
C02|A static approach to the Nelson-Siegel-Svensson model: an application for several negative yield cases|The appearance of negative bond yields presents significant challenges for the fixed income markets, which mainly concern related forecasting models. The Nelson-Siegel-Svensson model (NSS) is one of the models that is most frequently used by central banks to estimate the term structure of interest rates. The objective of this study is to evaluate the application of the NSS model to fit the yield curve of a set of 20 countries, the majority from the Eurozone, which registered negative sovereign bond yields. We conclude that the model adjusted well for all countries’ yield curves, although no changes or constraints were introduced. In addition, a comparison was carried out between market instantaneous interest rate and the interest rate for the very distant future, which the model can predict, with good results for the instantaneous interest rate. An evaluation of the possible behaviour of shared debt securities (i.e. Eurobonds) was also analysed. In conclusion, the NSS model seems to remain a valuable, easy to use, and adaptable tool, to fit negative yield curves, for monetary policy institutions and market players alike.
C02|Capital Income Risk and the Dynamics of the Wealth Distribution|"In this paper, we develop and nummerically solve a model of idiosyncratic labour income and idiosyncratic interest rates to predict the evolution of a wealth dirstribution over time. Stochastic labour income follows a deterministic growth trend and it fluctuaues between a wage and unempolyment benefits. Stochastic interst rates are drawn initially (ex-ante heterogeneity) fluctuate between two values (ex-post heterogeneity) and can differ in their arrival rates (financial types). A low interest rate implies a stationary long-run wealth distribution, a high interest rate implies non-stationary wealth distribution. Our baseline model matches the evolution of the wealth distribution of the NLSY 79 cohort from 1986 to 2008 very well. When we start in 1986 and target 2008, we obtain a fit of 96.1%. The fit for non-targeted years is 77.0% on average. When targeting the evolution of wealth, the fit is 88.9%. With a more flexible interest rate distribution, the fit can even be increased to 96.7%. Comparing calibrated mean returns with data shows that the flexible interest rate distribution has empirically not convincing ""superstar states"". In the baseline model, mean returns are empirically convincing. Surprisingly, the standard deviation of model returns is an order of magnitude lower than empirical standard deviation."
C02|Capital Income Risk and the Dynamics of the Wealth Distribution|"In this paper, we develop and numerically solve a model of idiosyncratic labour income and idiosyncratic interest rates to predict the evolution of a wealth distribution over time. Stochastic labour income follows a deterministic growth trend and it fluctuates between a wage and unemployment benefits. Stochastic interest rates are drawn initially (ex-ante heterogeneity), fluctuate between two values (ex-post heterogeneity) and can differ in their arrival rates (financial types). A low interest rate implies a stationary long-run wealth distribution, a high interest rate implies non-stationary wealth distributions. Our baseline model matches the evolution of the wealth distribution of the NLSY 79 cohort from 1986 to 2008 very well. When we start in 1986 and target 2008, we obtain a fit of 96.1%: The fit for non-targeted years is 77.0% on average. When targeting the evolution of wealth, the fit is 88.9%. With a more flexible interest rate distribution, the fit can even be increased to 96.7%. Comparing calibrated mean returns with data shows that the flexible interest rate distribution has empirically not convincing ""superstar states"". In the baseline model, mean returns are empirically convincing. Surprisingly, the standard deviation of model returns is an order of magnitude lower than the empirical standard deviation."
C02|Credit Risk Analysis using Machine and Deep learning models|Due to the hyper technology associated to Big Data, data availability and computing power, most banks or lending financial institutions are renewing their business models. Credit risk predictions, monitoring, model reliability and effective loan processing are key to decision making and transparency. In this work, we build binary classifiers based on machine and deep learning models on real data in predicting loan default probability. The top 10 important features from these models are selected and then used in the modelling process to test the stability of binary classifiers by comparing performance on separate data. We observe that tree-based models are more stable than models based on multilayer artificial neural networks. This opens several questions relative to the intensive used of deep learning systems in the enterprises
C02|From Methodology to Practice (and Back): Georgescu-Roegen's Philosophy of Economics and the Flow-Fund Model|Despite his early contribution to the rise of mathematics in economics, Georgescu-Roegen's later methodological criticism of models has received little attention from historians and philosophers of economics. This paper attempts to fill this gap following two lines. First, I examine his explicitly methodological claims and connect them with related topics in economic methodology. Building on the distinction between dialectical and arithmomorphic concepts, I characterise his approach to theory-making as a three steps process of idealisation, isolation and arithmetisation. In this framework, models perform two functions, checking for logical consistency and facilitating understanding, which can be related to the idea of modelling as theorising. I then confront these general principles with Georgescu-Roegen's flow-fund model of production. I use the methodology as a reading grid of this theory, while examining its limits and complementary principles in practice. This shows a great deal of consistency, where idealisation provides conceptual foundations, isolation determines the relevant problems, and models are built according to structural consistency. The two functions of models are then illustrated by the logical derivation of older principles formulated by Babbage and Smith, and the understanding of the different organisational patterns of production. But some slightly different functions also appear when specific configurations of the model enable to check the conceptual consistency of other theories, or the understanding provided by the model contributes to the formation of new concepts. Hence, the consistency and the complementarity between Georgescu-Roegen's methodology and practice of theory-making provide interesting insights and a useful background for further investigations.
C02|The impact of occupancy in collective accommodation establishments on the selected macroeconomic variables in the Czech Republic in the period 2001 - 2015| The aim of this paper is to assess the impact of occupancy in collective accommodation establishments in the Czech Republic at an average rate of unemployment (%) to GDP (%) and the average pace of real wage growth (%) between 2001-2015. The methods are regression and correlation analysis. The output is then evaluate the impact of occupancy on the selected parameters. The most significant impact has occupancy in collective accommodation establishments at the average rate of unemployment. When we are comparing the GDP growth parameters and occupancy collective accommodation establishments is no evidence of a linear relationship. For evaluation of development depending average growth rate of real wages in total attendance collective accommodation establishments is indirect evidence of a weak linear dependence.
C02|Feedback Pareto weights in cooperative NTU differential games|This note deals with agreeability in nontransferable utility (NTU) differential games. We introduce state feedback Pareto weights to enrich the set of efficient cooperative solutions. The framework is particularly useful if constant weights fail to support agreeability, but cooperation is desired nonetheless. The concept is applied to an adverting differential game.
C02|Forbidden zones for the expectation of a random variable. New version 1|A forbidden zones theorem is deduced in the present article. Its consequences and applications are preliminary considered. The following statement is proven: if some non-zero lower bound exists for the variance of a random variable, that takes on values in a finite interval, then non-zero bounds or forbidden zones exist for its expectation near the boundaries of the interval. The article is motivated by the need of rigorous theoretical support for the practical analysis that has been performed for the influence of scattering and noise in the behavioral economics, decision sciences, utility and prospect theories. If a noise can be one of possible causes of the above lower bound on the variance, then it can cause or broaden out such forbidden zones. So the theorem can provide new possibilities for mathematical description of the influence of such a noise. The considered forbidden zones can evidently lead to some biases in measurements.
C02|Updating Probabilities for a Mineral Exploration Project|This paper describes a mineral exploration project conducted by a junior mining company within a probabilistic framework. In particular, it shows how beliefs about a project can be updated in response to a series of new pieces of information about the project associated with exploration activities using subjective probabilities. Several alternative frameworks are briefly mentioned as ways to improve on this simplified model.
C02|Simulation Framework for Economic Modeling of Mineral Resources|This paper describes an approach to include uncertainty over the commodity price when modelling the economic attributes of a mine plan for a mineral resource. The approach starts with a method to generate price paths from a broad historical set to establish a set of price paths, where the NPV is calculated for each path to generate a distribution for the NPV. It goes on to describes how to use this distribution to compare different mine plans in a manner that is similar to stress testing.
C02|Mathematics vs. Statistics in tackling Environmental Economics uncertainty|In this paper the appropriate background in Mathematics and Statistics is considered in developing methods to investigate Risk Analysis problems associated with Environmental Economics uncertainty. New senses of uncertainty are introduced and a number of sources of uncertainty are discussed and presented. The causes of uncertainty are recognized helping to understand how they affect the adopted policies and how important their management is in any decision-making process. We show Mathematical Models formulate the problem and Statistical models offer possible solutions, restricting the underlying uncertainty, given the model and the error assumptions are correct. As uncertainty is always present we suggest ways on how to handle it.
C02|Forbidden zones and biases for the expectation of a random variable. Version 2|A forbidden zones theorem is proven in the present article. If some non-zero lower bound exists for the variance of a random variable, whose support is located in a finite interval, then non-zero bounds or forbidden zones exist for its expectation near the boundaries of the interval. The article is motivated by the need of a theoretical support for the practical analysis of the influence of a noise that was performed for the purposes of behavioral economics, utility and prospect theories, decision and social sciences and psychology. The four main contributions of the present article are: the mathematical support, approach and model those are developed for this analysis and the successful uniform applications of the model in more than one domain. In particular, the approach supposes that subjects decide as if there were some biases of the expectations. Possible general consequences and applications of the theorem for a noise and biases of measurement data are preliminary considered.
C02|Shock Diffusion in Regular Networks: The Role of Transitive Cycles|We study how the presence of transitive cycles in the interbank network affects the extent of financial contagion. In a regular network setting, where the same pattern of links repeats for each node, we allow an external shock to propagate losses through the system of linkages (interbank network). The extent of contagion (contagiousness) of the network is measured by the limit of the losses when the initial shock is diffused into an infinitely large network. This measure indicates how a network may or may not facilitate shock diffusion in spite of other external factors. Our analysis highlights two main results. First, contagiousness decreases as the length of the minimal transitive cycle increases, keeping the degree of connectivity (density) constant. Second, as density increases the extent of contagion can decrease or increase, because the addition of new links might decrease the length of the minimal transitive cycle. Our results provide new insights to better understand systemic risk and could be used to build complementary indicators for financial regulation.
C02|Dynamic Beta|The phrase “Dynamic Beta” is broad and this paper describes statistical procedure for estimating regression coefficients in a way that allows for variation across relevant subsets of the data. For example, the time axis. I describe an algorithm to structure the search for variation in sets of coefficient estimates and discuss the example of a single stock versus a stock index. In the end, I suggest that a human analyst has an important role for someone who has relevant skill in pattern recognition and subject area expertise.
C02|Forbidden zones for the expectation. New mathematical results for behavioral and social sciences|A forbidden zones theorem, mathematical approach and model are proposed in the present article. In particular, the approach supposes that people decide as if there were some biases of the expectations of measurement data. The article is motivated by the need of a theoretical support for the practical analysis performed for the purposes of utility and prospect theories, behavioral economics, psychology, decision and social sciences. Possible general consequences and applications of the theorem and approach for a noise and biases of measurement data are preliminary considered as well.
C02|The factors inefficient allocation of investment between economies|The article deals with the problem of possibility of allocation of investment capital by economies. The situation, when the less effective investors find themselves in more favorable investment climate despite of rationality principle is considered, i.e. there is a situation adverse selection. The model describing investors’ behavior in the economy, characterized by an investment climate of some favorability, is developed. There was considered a behaviour of investors–maximizers and investors–satisficers. 2 cases are modeled: independence of the economic climate from the volume of available investments and limitedness of maximal volume of investments, and dependence of the economic climate from the volume of available investments and unlimitedness of maximal volume of investments. The profitableness of investments on the economic climate considered for exponential and logistic function. The analysis shows reasons for adverse selection have a fundamental behavioral basis. The given study allows concluding the investment market is not fully self-controlled and sometimes needs government intervention.
C02|A convexity result for the range of vector measures with applications to large economies|"On a Boolean algebra we consider the topology $u$ induced by a finitely additive measure $\mu$ with values in a locally convex space and formulate a condition on $u$ that is sufficient to guarantee the convexity and weak compactness of the range of $\mu$. This result à la Lyapunov extends those obtained in (Khan, Sagara 2013) to the finitely additive setting through a more direct and less involved proof. We will then give an economical interpretation of the topology $u$ in the framework of coalitional large economies to tackle the problem of measuring the bargaining power of coalitions when the commodity space is infinite dimensional and locally convex. We will show that our condition on $u$ plays the role of the ""many more agents than commodities"" condition introduced by Rustichini and Yannelis in (1991). As a consequence of the convexity theorem, we will obtain two straight generalizations of Schmeidler's and Vind's Theorems on the veto power of coalitions of arbitrary economic weight."
C02|The Business Cycle Model Beyond General Equilibrium|This paper presents the business cycle model without using assumptions of general equilibrium. We use agent-based models, risk assessments and economic space as ground for modelling business cycles. All economic agents are at risk but not for all agents risk assessments are performed. We propose that for each agent risk assessment can be performed and suggest treat risk ratings x of agents as their coordinate x on economic space. Agents fill economic domain bounded by most secure and most risky agents. Economic processes, exogenous or endogenous shocks induce evolution of agent’s risk coordinates. We show how risk motions of agents on the bounded economic domain induce the business cycle. We derive the system of economic equations that describe macroeconomic evolution and the business cycle on economic space. As example, we study simple model that describe relations between macro Assets A(t,x) and Revenue-on-Assets B(t,x). To show how economic equations describe the business cycle we obtain from them the system of ordinary differential equations that describes business cycle time fluctuations of macroeconomic Assets A(t) and Revenue-on-Assets B(t).
C02|Economic Transactions Govern Business Cycles|This paper presents the business cycle model based on treatment of economic agents as simple units of macroeconomics. Agents (banks, corporations, households, etc.) have numerous economic and financial variables like Assets, Credits, Debts, Consumption, etc. Agents perform economic and financial transactions with other agents. All agents are at risk but not for all agents risk assessments are performed now. Let’s propose that risk assessment can be made for all agents and let’s use agents risk ratings x as their coordinates. Agents coordinates for n risks define n-dimensional economic space. Economic and financial transactions between agents describe evolution of their economic and financial variables. Aggregations of economic or financial variables of agents in a unit volume at point x determine macro variables as functions of x. Aggregations of transactions between agents in unit volumes at points x and y determine macro transactions as functions of x and y. Macro transactions describe rate of change of macro variables at points x and y. We derive economic equations that describe evolution of macro transactions. We show that business cycle fluctuations are consequence of these equations. We argue that business cycle fluctuations of particular macro variable can be treated as oscillations of “mean risk coordinates” of this economic variable. As example we study the business cycle determined by model interactions between transactions CL(t,x,y) that provide Loans from Creditors at point x to Borrowers at point y and transactions LR(t,x,y) that describe repayments from Borrowers at point y to Creditors at point x. Starting with economic equations we derive the system of ordinary differential equations that describe the business cycle fluctuations of macro Credits C(t) and macro Loan-Repayments LR(t) of the entire economics.
C02|A second welfare theorem in a non-convex economy: The case of antichain-convexity|We introduce the notion of an antichain-convex set to extend Debreu (1954)’s version of the second welfare theorem to economies where either the aggregate production set or preference relations are not convex. We show that – possibly after some redistribution of individuals’ wealth – the Pareto optima of some economies which are marked by certain types of non-convexities can be spontaneously obtained as valuation quasiequilibria and equilibria: both equilibrium notions are to be understood in Debreu (1954)’s sense. From a purely structural point of view, the mathematical contribution of this work is the study of the conditions that guarantee the convexity of the Minkowski sum of finitely many possibly non-convex sets. Such a study allows us to obtain a version of the Minkowski\Hahn–Banach separation theorem which dispenses with the convexity of the sets to be separated and which can be naturally applied in standard proofs of the second welfare theorem; in addition – and equally importantly – the study allows to get a deeper understanding of the conditions on the single production sets of an economy that guarantee the convexity of their aggregate.
C02|Transition drivers and crisis signaling in stock markets|The present paper introduces an up-to-date methodology to detect Early Warning Signals of critical transitions, that manifest when distress stages in financial markets are about to take place. As a first step, we demonstrate that a high-dimensional dynamical system can be formulated in a simpler form but in an abstract phase space. Then we detect its approaching towards a critical transition by means of a set of observable variables that exhibit some particular statistical features. We name these variables the Leading Temporal Module. The impactful change in the properties of this group reflects the transition of the system from a normal to a distress state. Starting from these observations we develop an early warning indicator for determining the proximity of a financial crisis. The proposed measure is model free and the application to three different stock markets, together with the comparison with alternative systemic risk measures, highlights the usefulness in signaling upcoming distress phases. Computational results establish that the methodology we propose is effective and it may constitute a relevant decision support mechanism for macro prudential policies.
C02|Zur mathematischen Struktur der Wertformen von Karl Marx in 'Das Kapital'<BR>[About the mathematical structure of the form of value of Karl Marx in 'Das Kapital']|In the first section of Das Kapital by Karl Marx different forms of values are analysed. From a mathematical point of view one can find therein structures, which correspond to elements of the mathematical theory of categories. These are especially the limit of cones and the definition of subobjects as morphisms. Using the limit cone, the concept of money contains the categorial product of commodities. The concept of the value of a commodity contains the categorial definition of a subobject.
C02|Economic Transactions Govern Business Cycles|This paper presents the business cycle model without using assumptions of general equilibrium. All economic agents are at risk but not for all agents risk assessments are performed. We propose that risk assessment can be completed for all agents and suggest use agents risk ratings as their coordinates x. We show that macroeconomics as ABM is described on bounded economic domain of economic space. Transactions between agents describe evolution of their economic and financial variables. Aggregations of economic or financial variables of agents in a unit volume near point x determine macro variables as functions of x. Aggregations of transactions between agents in unit volumes near points x and y determine macro transactions as functions of x and y. Macro transactions describe change of macro variables near points x and y. We explain how evolution of macro transactions can be described by economic equations on economic space. We show that business cycle fluctuations are consequence of these equations. We treat the nature of the business cycle fluctuations of particular macro variable as oscillations of “mean risk” of this economic variable on bounded economic domain. As example we describe interactions between transactions CL(t,x,y) that provide Loans from Creditors at point x to Borrowers at point y and transactions LR(t,x,y) that describe repayments from Borrowers at point y to Creditors at point x. Starting with economic equations we derive the system of ordinary differential equations that describe the business cycle fluctuations of macro Credits C(t) and macro Loan-Repayments LR(t) of the entire economics.
C02|Factors for the formation of inefficient states when using tax incentive regimes|The article investigates the problem of adopting the tax incentives regime in certain industries. The general problem of tax benefits is their ineffectiveness, which often leads to results contrary to expected and also to losses in economy. So this paper aimed to define the reasons, factors and loss prevention of failures related to implementing the tax incentives regime. In order to analyze the subject area, we use the object and process modeling of it. Particularly, we use the optimization models and game-theory tools. We classified the types of tax incentives regimes in order to distinguish two targets for implementation of tax benefits: increase the government revenue and diversification the product line; and also three strategies of granting tax exemptions: overall, targeted, and individual tax incentives. We found that the strategy of granting overall tax exemptions potentially can lead to “free-rider problem”, and strategy of granting the targeted and individual ones can create conditions for arising of adverse selection mechanism. We defined the conditions leading to increase the tax revenues in process of adopting the tax incentives regime. Also the analysis of “principal-agent” model as Nash equilibrium allowed to find the conditions of arising the ineffective norm of interaction between government and investor, when the first satisfies the investor’s unjustified claim related to obtaining tax incentives. Obtained patterns, despite of their non-numerical character, can be useful in business decision-making, because the revealed ineffective norms and states define concrete threats, which should be considered by policymakers in the process of adopting the tax incentives regime. The future research can be related to extension of formalization of mechanisms of granting tax exemptions and of arising the inefficient states and norms of agents’ behavior; to development of mechanisms of prevention of inefficient states in the process of implementation of tax incentives regimes; to investigate their concrete evidence in the process of adopting the tax incentives regime in actual practice.
C02|A Path Integral Approach to Business Cycle Models with Large Number of Agents|This paper presents an analytical treatment of economic systems with an arbitrary number of agents that keeps track of the systems' interactions and agents' complexity. This formalism does not seek to aggregate agents. It rather replaces the standard optimization approach by a probabilistic description of both the entire system and agents' behaviors. This is done in two distinct steps. A first step considers an interacting system involving an arbitrary number of agents, where each agent's utility function is subject to unpredictable shocks. In such a setting, individual optimization problems need not be resolved. Each agent is described by a time-dependent probability distribution centered around his utility optimum. The entire system of agents is thus defined by a composite probability depending on time, agents' interactions and forward-looking behaviors. This dynamic system is described by a path integral formalism in an abstract space - the space of the agents' actions - and is very similar to a statistical physics or quantum mechanics system. We show that this description, applied to the space of agents' actions, reduces to the usual optimization results in simple cases. Compared to a standard optimization, such a description markedly eases the treatment of systems with small number of agents. It becomes however useless for a large number of agents. In a second step therefore, we show that for a large number of agents, the previous description is equivalent to a more compact description in terms of field theory. This yields an analytical though approximate treatment of the system. This field theory does not model the aggregation of a microeconomic system in the usual sense. It rather describes an environment of a large number of interacting agents. From this description, various phases or equilibria may be retrieved, along with individual agents' behaviors and their interactions with the environment. For illustrative purposes, this paper studies a Business Cycle model with a large number of agents.
C02|Inclusion financière, frictions financières et croissance économique<BR>[Financial inclusion, financial frictions and economic growth]|This article develops an analytical growth model that integrates the financial sector, the part of individuals acceding to financial systems, frictions related to the enforceability of contracts and the constraints related to the costs of information on production processes. Our model considers an economy with three categories of individuals. The first includes individuals excluded from the financial system. The second includes individuals included but with constraints due to the costs of researching information on the quality of projects. The individuals of the third category accede with less constraint than the second category and more chances that the financial contracts to which they subscribe will be executed. Based on the model simulation, the quantity of resources directed to firms increase when the financial system becomes inclusive. Our model is original in nature and provides analytical and empirical evidence on the negative impact of financial exclusion on economic growth, and highlighted the vital role of financial inclusion in economic growth. As for the enforceability of contracts concluded on the credit market, it stimulates the proportion of resources invested by the agents of the three categories.
C02|Inequalities and zones. New mathematical results for behavioral and social sciences|A theorem, mathematical method and model are introduced in the present article. Inequalities, allowed and forbidden zones, their relations, consequences, and applications are considered for the expectations of random variables. The method and model are based on the inequalities and zones of the theorem. The article is motivated by the need for theoretical support for the practical analysis performed for the purposes of behavioral economics.
C02|L’impact De L’investissement Des Revenus Pétroliers Sur La Croissance, L’inflation Et Le Chômage : Cas D’Algérie (2000-2015)<BR>[The Impact of Oil Revenue Investment on Growth, Inflation and Unemployment: The Case of Algeria (2000-2015)]|This paper investigates the short-run and long-run relationships between four main Algerian macroeconomic variables, the investment of oil revenues, economic growth, unemployment rate, inflation rate, using the Johansen multivariate cointegration techniques as well as VAR model for the period 2000-2015. The results indicate that there is not a long relationship between these four macroeconomic variables. The impulse functions and the variance decomposition from the stationary VAR show that the investment of oil revenues is very important to short run dynamics of the Algerian economy, when there is a shock in investment of oil revenues, GDP responds positively (13%) while the unemployment rate responds negatively (11%), in the long term. This is in line with the Algerian government's investment strategy, increasing GDP and reducing the unemployment rate.
C02|Environmental regulation and economic cycles|This paper considers economic cycles that do not depend on the exogenous economic actions. More precisely, the paper develops a positive model of government behavior in order to define the intertemporal fiscal policies that are optimal for a country, determining the optimal level of the budget and the optimal level of the rate of environmental quality, as well. For this purpose, we setup an optimal control model involving the intertemporal subsidy strategies for an authoritarian (like a central European) government. It will be shown - applying the Hopf bifurcation theorem - that cyclical strategy, i.e. waves of regulation, environmental subsidies alternating with deregulation, cuts in social programmes, etc., may be optimal strategies. In this paper we propose an extremely simple optimal control model concerning budget surplus and environmental subsidies. We investigate the cyclical subsiding policies applying one bifurcation theorem. A number of propositions are stated during the solution process.
C02|Economic and Financial Transactions Govern Business Cycles|Problem/Relevance - This paper presents new description of the business cycles that for decades remain as relevant and important economic problem. Research Objective/Questions - We propose that econometrics can provide sufficient data for assessments of risk ratings for almost all economic agents. We use risk ratings as coordinates of agents and show that the business cycles are consequences of collective change of risk coordinates of agents and their financial variables. Methodology - We aggregate similar financial variables of agents and define macro variables as functions on economic space. Economic and financial transactions between agents are the only tools that change their extensive variables. We aggregate similar transactions between agents with risk coordinates x and y and define macro transactions as functions of x and y. We derive economic equations that describe evolution of macro transactions and hence describe evolution of macro variables. Major Findings - As example we study simple model that describes interactions between Credits transactions from Creditors at x to Borrowers at y and Loan-Repayment transactions that describe refunds from Borrowers at y to Creditors at x. We show that collective motions of Creditors and Borrowers from safer to risky area and back on economic space induce frequencies of macroeconomic Credit cycles. Implications – Our model can improve forecasting of the business cycles and help increase economic sustainability and financial policy-making. That requires development of risk ratings methodologies and corporate accounting procedures that should correspond each other to enable risk assessments of economic agents
C02|Project appraisal and the Intrinsic Rate of Return|Building upon Magni (2011)’s approach, we propose a new rate of return measuring a project’s economic profitability. It is called the intrinsic rate of return (IROR). It is defined as the ratio of project return to project’s intrinsic value. The IROR approach decomposes the NPV into project scale and economic efficiency. In particular, NPV is found as the product of the project’s total invested capital and the excess rate of return, obtained as the difference between the IROR and the minimum attractive rate of return (MARR). This approach provides correct project ranking and is capable of managing time-varying costs of capital. In case of levered projects, shareholder value creation is captured by the equity IROR, which we call Intrinsic Return On Equity (IROE) (net income divided by total equity capital invested). If the project is unlevered, the IROE and the IROR lead to the same decision; if the project is levered, and the nominal value of debt is not equal to the market value of debt, the IROE should be preferred to project IROR.
C02|Investment decisions and sensitivity analysis: NPV-consistency of rates of return|Investment decisions may be evaluated via several different metrics/criteria, which are functions of a vector of value drivers. The economic significance and the reliability of a metric depend on its compatibility with the Net Present Value (NPV). Traditionally, a metric is said to be NPV-consistent if it is coherent with NPV in signaling value creation. This paper makes use of Sensitivity Analysis (SA) for measuring coherence between rates of return and NPV. In particular, it introduces a new, stronger definition of NPV-consistency that takes into account the influence of value drivers on the metric output. A metric is strongly NPV-consistent if it signals value creation and the ranking of the value drivers in terms of impact on the output is the same as that provided by the NPV. The degree of (in)coherence is calculated with Spearman (1904) correlation coefficient and Iman and Conover (1987) top-down coefficient. We focus on the class of AIRRs (Magni 2010, 2013) and show that the average Return On Investment (ROI) enjoys strong NPV-consistency under several (possibly all) methods of Sensitivity Analysis.
C02|How does the underlying affect the risk-return profiles of structured products?|Abstract Regulators of some of the major markets have adopted value at risk (VaR) as the risk measure for structured products. Under the mean-VaR framework, this paper discusses the impact of the underlying’s distribution on structured products. We expand the expected return and the VaR of a structured product with its underlying’s moments (mean, variance, skewness, and kurtosis), so that the impact of the moments can be investigated simultaneously. Results are tested by Monte Carlo and historical simulations. The findings show that for the majority of structured products, underlyings with large positive skewness are preferred. The preferences for the variance and the kurtosis of the underlying are both ambiguous.
C02|Solving the horizontal conflation problem with a constrained Delaunay triangulation|Abstract Datasets produced by different countries or organisations are seldom properly aligned and contain several discrepancies (e.g., gaps and overlaps). This problem has been so far almost exclusively tackled by snapping vertices based on a user-defined threshold. However, as we argue in this paper, this leads to invalid geometries, is error-prone, and leaves several discrepancies along the boundaries. We propose a novel algorithm to align the boundaries of adjacent datasets. It is based on a constrained Delaunay triangulation to identify and eliminate the discrepancies, and the alignment is performed without moving vertices with a snapping operator. This allows us to guarantee that the datasets have been properly conflated and that the polygons are geometrically valid. We present our algorithm, our implementation (based on the stable and fast triangulator in CGAL), and we show how it can be used it practice with different experiments with real-world datasets. Our experiments demonstrate that our approach is highly efficient and that it yields better results than snapping-based methods.
C02|Probability Measures on Product Spaces with Uniform Metrics|For a countable product of complete separable metric spaces with a topology induced by a uniform metric, the set of Borel probability measures coincides with the set of completions of probability measures on the product σ-algebra. Whereas the product space with the uniform metric is non-separable, the support of any Bofrel measure is separable, and the topology of weak convergence on the space of Borel measures is metrizable by both the Prohorov metric and the bounded Lipschitz metric.
C02|Stochastic Evolution of Distributions - Applications to CDS indices|We use mixture of percentile functions to model credit spread evolution, which allows to obtain a flexible description of credit indices and their components at the same time. We show regularity results in order to extend mixture percentile to the dynamic case. We characterise the stochastic differential equation of the flow of cumulative distribution function and we link it with the ordered list of the components of the credit index. The main application is to introduce a functional version of Bollinger bands. The crossing of bands by the spread is associated with a trading signal. Finally, we show the richness of the signals produced by functional Bollinger bands compared with standard one with a pratical example
C02|The degree measure as utility function over positions in networks|In this paper, we connect the social network theory on centrality measures to the economic theory of preferences and utility. Using the fact that networks form a special class of cooperative TU-games, we provide a foundation for the degree measure as a von Neumann-Morgenstern expected utility function reflecting preferences over being in different positions in different networks. The famous degree measure assigns to every position in a weighted network the sum of the weights of all links with its neighbours. A crucial property of a preference relation over network positions is neutrality to ordinary risk. If an expected utility function over network positions satisfies this property and some regularity properties, then it must be represented by a utility function that is a multiple of the degree centrality measure. We show this in three steps. First, we characterize the degree measure as a centrality measure for weighted networks using four natural axioms. Second, we relate these network centrality axioms to properties of preference relations over positions in networks. Third, we show that the expected utility function is equal to a multiple of the degree measure if and only if it represents a regular preference relation that is neutral to ordinary risk. Similarly, we characterize a class of affine combinations of the outdegree and indegree measure in weighted directed networks and deliver its interpretation as a von Neumann-Morgenstern expected utility function
C02|Winning Investment Strategies Based on Financial Crisis Indicators|The aim of this work is to create systematic trading strategies built upon several financial crisis indicators based on the spectral properties of market dynamics. Within the limitations of our framework and data, we will demonstrate that our systematic trading strategies are able to make money, not as a result of pure luck but, in a reproducible way and while avoiding the pitfall of over fitting, as a result of the skill of the operators and their understanding and knowledge of the financial market. Using singular value decomposition (SVD) techniques in order to compute all spectra in an efficient way, we have built two kinds of financial crisis indicators with a demonstrable power of prediction. Firstly, there are those that compare at every date the distribution of the eigenvalues of a covariance or correlation matrix to a distribution of reference representing either a calm or agitated market reference. Secondly, we have those that merely compute at every date a chosen spectral property (trace, spectral radius or Frobenius norm) of a covariance or correlation matrix. Aggregating the signals provided by all the indicators in order to minimize false positive errors, we then build systematic trading strategies based on a discrete set of rules governing the investment decisions of the investor. Finally, we compare our active strategies to a passive reference as well as to random strategies in order to prove the usefulness of our approach and the added value provided by the out-of-sample predictive power of the financial crisis indicators upon which our systematic trading strategies are built
C02|The Complexity of Bank Holding Companies: A Topological Approach|Large bank holding companies (BHCs) are structured into intricate ownership hierarchies involving hundreds or even thousands of legal entities. Each subsidiary in these hierarchies has its own legal form, assets, liabilities, managerial goals, and supervisory authorities. In the event of BHC default or insolvency, regulators may need to resolve the BHC and its constituent entities. Each entity individually will require some mix of cash infusion, outside purchase, consolidation with other subsidiaries, legal guarantees, and outright dissolution. The subsidiaries are not resolved in isolation, of course, but in the context of resolving the consolidated BHC at the top of the hierarchy. The number, diversity, and distribution of subsidiaries within the hierarchy can therefore significantly ease or complicate the resolution process. We propose a set of related metrics intended to assess the complexity of the BHC ownership graph. These proposed metrics focus on the graph quotient relative to certain well identified partitions on the set of subsidiaries, such as charter type and regulatory jurisdiction. The intended measures are mathematically grounded, intuitively sensible, and easy to implement. We illustrate the process with a case study of one large U.S. BHC.
C02|Using Wavelets In Economics. An Application On The Analysis Of Wage-Price Relation|In the last decades, more and more approaches of economic issues have used mathematical tools, and among the most recent ones, spectral and wavelet methods are to be distinguished. If in the case of spectral analysis the approaches and results are sufficiently clear, while the use of wavelet decomposition, especially in the analysis of time series, is not fully valorized. The purpose of this paper is to emphasize how these methods are useful for time series analysis. After theoretical considerations on Fourier transforms versus wavelet transforms, we have presented the methodology we have used and the results obtained by using wavelets in the analysis of wage-price relation, based on some empirical data. The data we have used is concerning the Romanian economy - the inflation and the average nominal wage denominated in US dollars, between January 1991 and May 2016, highlighting that the relation between nominal salary and prices can be revealed more accurately by use of wavelets.
C02|Higher-Order Risk Measure and (Higher-Order) Stochastic Dominance|This paper extends the theory between Kappa ratio and stochastic dominance (SD) and risk-seeking SD (RSD) by establishing several relationships between first- and higher-order risk measures and (higher-order) SD and RSD. We first show the sufficient relationship between the (n+1)-order SD and the n-order Kappa ratio. We then find that, in general, the necessary relationship between SD/RSD and the Kappa ratio cannot be established. Thereafter, we find that when the variables being compared belong to the same location-scale family or the same linear combination of location-scale families, we can get the necessary relationships between the (n+1)-order SD with the n-order Kappa ratio when we impose some conditions on the means. Our findings enable academics and practitioners to draw better decision in their analysis.
C02|Can forbidden zones for the expectation explain noise influence in behavioral economics and decision sciences?|The present article is devoted to discrete random variables that take a limited number of values in finite closed intervals. I prove that if non-zero lower bounds exist for the variances of the variables, then non-zero bounds or forbidden zones exist for their expectations near the boundaries of the intervals. This article is motivated by the need in rigorous theoretical support for the analysis of the influence of scattering and noise on data in behavioral economics and decision sciences.
C02|Multiple time-xcales analysis of global stock markets spillovers effects in African stock markets|This paper investigates the time and frequency interdependence relationship between seven African stock markets, emerging stock markets, developed stock markets and Japan) and oil prices. The spillovers are examined from 2005 to 2016 taking into account the recent financial crisis and the recent oil prices fall. We combine the generalized VAR framework proposed by Diebold and Yilmaz (2012) and the Maximal Overlap Discrete Wavelet Transform (MODWT) to obtain the spillovers at different time scales. Result show that African financial markets integration with themselves and the outside depends on the time scales, the economic relations, the world financial markets state. Relationships with global financial markets are generally weak in the short run but tend to grow in the long run. The interdependence with oil prices is strong in short and medium run but weak in long run. African stock markets could be an opportunity of capital diversification in short run.
C02|Теория Эндогенного Экономического Роста И Уравнения Математической Физики<BR>[The theory of endogenous economic growth and equations of mathematical physics]|It is given an overview of recent studies that use equations of mathematical physics, their analogs and modifications for describing endogenous evolution of technologies. A master equation is proposed that includes, as special cases, a number of known models of Schumpeterian dynamics. A scheme for constructing multifactorial models of endogenous growth is also proposed, based on a combination of different imitation rules for different performance indicators. Directions for further research are outlined.
C02|Robust modelling of the impacts of climate change on the habitat suitability of forest tree species|In Europe, forests play a strategic multifunctional role, serving economic, social and environmental purposes. However, forests are among the most complex systems and their interaction with the ongoing climate change – and the multifaceted chain of potential cascading consequences for European biodiversity, environment, society and economy – is not yet well understood. The JRC PESETA project series proposes a consistent multi-sectoral assessment of the impacts of climate change in Europe. Within the PESETA II project, a robust methodology is introduced for modelling the habitat suitability of forest tree species (2071-2100 time horizon). Abies alba (the silver fir) is selected as a case study: a main European tree species often distributed in bioclimatically complex areas, spanning over various forest types and with multiple populations adapted to different conditions. The modular modelling architecture is based on relative distance similarity (RDS) estimates which link field observations with bioclimatic patterns, projecting their change under climate scenarios into the expected potential change of suitable habitat for tree species. Robust management of uncertainty is also examined. Both technical and interpretation core aspects are presented in an integrated overview. The semantics of the array of quantities under focus and the uneven sources of uncertainty at the continental scale are discussed (following the semantic array programming paradigm), with an effort to offer some minimal guidance on terminology, meaning and methodological limitations not only of the proposed approach, but also of the broad available literature – whose heterogeneity and partial ambiguity might potentially reverberate at the science-policy interface. ► How to cite: ◄ de Rigo, D., Caudullo, G., San-Miguel-Ayanz, J, Barredo, J.I., 2017. Robust modelling of the impacts of climate change on the habitat suitability of forest tree species. Publication Office of the European Union, Luxembourg. 58 pp. ISBN:978-92-79-66704-6 , https://doi.org/10.2760/296501
C02|About the minimal magnitudes of measurement’s forbidden zones. Version 1|Suppose a random variable takes on values in an interval. The minimal distance from the expectation of the variable to the nearest boundary of the interval is considered here. One of the aims of the present article is also an analysis of the question when this minimal distance can be neglected with respect to the standard deviation. This minimal distance can determine the minimal magnitudes of forbidden zones caused by noise for results of measurements near the boundaries of the intervals. The most observed influence and problems of these forbidden zones are suffered in behavioral economics and decision sciences.
C02|New bid-ask spread estimators from daily high and low prices|Estimating trading costs in the absence of recorded data is a problem that continues to puzzle financial market researchers. We address this challenge by introducing two low frequency bid-ask spread estimators using daily high and low transaction prices.
C02|Multidimensional Rank Based Poverty Measures A Case Study: Tunisia|For a long time, poverty measurement has been based strictly on a monetary approach. Since Sen (1976), many poverty measures have been proposed based on an axiomatic foundation, like the Sen (1976) measure, the class of FGT measures (1984), the Shorrocks (1995) measure, otherwise known as the Sen-Shorrocks-Thon (SST) measure. Due to capabilities approach (first developed by Sen (1985)) and basic needs approach, we realize that the poverty of a person is not only a lack of income but an insufficiency in various attributes of well-being. For better representing the multidimensional aspect of poverty, many approaches of multidimensional poverty measurement have been proposed like the multidimensional axiomatic approach. We propose in this paper to contribute to the latter approach. For that, we use a two-stage aggregation procedure to develop classes of multidimensional poverty measures which are extension to the multidimensional context of classes of generalized SST measures developed by Chtioui and Ayadi (2013). We apply our measures on Tunisia using the bootstrap method to see the evolution of multidimensional poverty between 1994 and 2006.
C02|A Path Integral Approach to Interacting Economic Systems with Multiple Heterogeneous Agents|This paper presents an analytical treatment of economic systems with an arbitrary number of agents that keeps track of the systems' interactions and agent's complexity. The formalism does not seek to aggregate agents: it rather replaces the standard optimization approach by a probabilistic description of the agents' behaviors and of the whole system. This is done in two distinct steps. A first step considers an interacting system involving an arbitrary number of agents, where each agent's utility function is subject to unpredictable shocks. In such a setting, individual optimization problems need not be resolved. Each agent is described by a time-dependent probability distribution centered around its utility optimum. The whole system of agents is thus defined by a composite probability depending on time, agents' interactions, relations of strategic dominations, agents' information sets and expectations. This setting allows for heterogeneous agents with different utility functions, strategic domination relations, heterogeneity of information, etc. This dynamic system is described by a path integral formalism in an abstract space – the space of the agents' actions –and is very similar to a statistical physics or quantum mechanics system. We show that this description, applied to the space of agents' actions, reduces to the usual optimization results in simple cases. Compared to the standard optimization, such a description markedly eases the treatment of a system with a small number of agents. It becomes however useless for a large number of agents. In a second step therefore, we show that, for a large number of agents, the previous description is equivalent to a more compact description in terms of field theory. This yields an analytical, although approximate, treatment of the system. This field theory does not model an aggregation of microeconomic systems in the usual sense, but rather describes an environment of a large number of interacting agents. From this description, various phases or equilibria may be retrieved, as well as the individual agents' behaviors, along with their interaction with the environment. This environment does not necessarily have a unique or stable equilibrium and allows to reconstruct aggregate quantities without reducing the system to mere relations between aggregates. For illustrative purposes, this paper studies several economic models with a large number of agents, some presenting various phases. These are models of consumer/producer agents facing binding constraints , business cycle models, and psycho-economic models of interacting and possibly strategic agents.
C02|Forward Ordinal Probability Models for Point-in-Time Probability of Default Term Structure|Common ordinal models, including the ordered logit model and the continuation ratio model, are structured by a common score (i.e., a linear combination of a list of given explanatory variables) plus rank specific intercepts. Sensitivity with respect to the common score is generally not differentiated between rank outcomes. In this paper, we propose an ordinal model based on forward ordinal probabilities for rank outcomes. The forward ordinal probabilities are structured by, in addition to the common score and intercepts, the rank and rating (for a risk-rated portfolio) specific sensitivity. This rank specific sensitivity allows a risk rating to respond to its migrations to default, downgrade, stay, and upgrade accordingly. An approach for parameter estimation is proposed based on maximum likelihood for observing rank outcome frequencies. Applications of the proposed model include modeling rating migration probability for point-in-time probability of default term structure for IFRS9 expected credit loss estimation and CCAR stress testing. Unlike the rating transition model based on Merton model, which allows only one sensitivity parameter for all rank outcomes for a rating, and uses only systematic risk drivers, the proposed forward ordinal model allows sensitivity to be differentiated between outcomes and include entity specific risk drivers (e.g., downgrade history or credit quality changes for an entity in last two quarters can be included). No estimation of the asset correlation is required. As an example, the proposed model, benchmarked with the rating transition model based on Merton model, is used to estimate the rating migration probability and probability of default term structure for a commercial portfolio, where for each rating the sensitivity is differentiated between migrations to default, downgrade, stay, and upgrade. Results show that the proposed model is more robust.
C02|Informal and formal meaning of the norm and the institution|In this article the problem of estimation the performance of formal norm is examined. To this purpose the existing definitions of the notion “norm” are analyzed and summarized, their insufficient formalization is noted, whereupon with the approach, using the apparatus of theory of sets and Boolean algebra, a strict definition of the norm, which includes static and dynamic components, is proposed. The description of the norm is extended by the definition of the space of the norm – a group of notions related to the norm. It’s substantiated that these notions shouldn’t be regarded as component parts of the norm. The methods of estimation the endogenous performance of formal norm (rule), basing on an analysis of its internal characteristics, but not on the results of its application is proposed. It’s suggested to determine the quality of the norm at three ways: norm as itself; accordance of the new norm to current ones; easiness of its compliance and easiness of its control. This approach allows identifying potential problematic points of the norm that can lead to difficulties in the future. Using the proposed methods of estimation of formal norm in areas, where such norms are developed actively (e.g. jurisprudence), can increase the quality of projectable norms, thus reducing costs for their future support. For the already existing norms those methods allow determining their weaknesses. In the theoretical-economic research the apparatus of formalization of norms may be useful in examination of institutional dynamics – namely, the process of institutional change.
C02|Evaluations of endogenous efficiency of the norm|In this article the problem of evaluation the efficiency of formal norm is examined. To this purpose the existing definitions of the notion “norm” are analyzed and summarized, their insufficient formalization is noted, whereupon with the approach, using the apparatus of theory of sets and Boolean algebra, a strict definition of the norm, which includes static and dynamic components, is proposed. The description of the norm is extended by the definition of the space of the norm – a group of notions related to the norm. It’s substantiated that these notions shouldn’t be regarded as component parts of the norm. The methods of evaluation the endogenous efficiency of formal norm (rule), basing on an analysis of its internal characteristics, but not on the results of its application is proposed. It’s suggested to determine the quality of the norm at three ways: norm as itself; accordance of the new norm to current ones; easiness of its compliance and easiness of its control. This approach allows identifying potential problematic points of the norm that can lead to difficulties in the future. Using the proposed methods of evaluation of formal norm in areas, where such norms are developed actively (e.g. jurisprudence), can increase the quality of projectable norms, thus reducing costs to their further support. For the already existing norms those methods allow determining their weaknesses. In the theoretical-economic research the apparatus of formalization of norms may be useful in examination of institutional dynamics – namely, the process of institutional change.
C02|Piyasa ekonomisine geçiş süreci ve sonrasında Türkiye'de GINI katsayılarının analizi: Alternatif GINI formülü yaklaşımı<BR>[During and after the process of transition to market economy, an analysis of income distribution in Turkey: An alternative GINI formula approach]|This study considers the Gini 1 and alternative Gini 2 coefficients for households’ % income shares from 1963 to 2015 in Turkey. Throughout regarding calibrations, one might see that, despite the existence of some deviations from trend and some Gini 1 and Gini 2 coefficients’ differences, (a) there exists an improvement in the distribution of income until 2011, except for the year 2009 when the global crisis was experienced, but an average deterioration in the distribution for 2011-2015 period is experienced, (b) the geographical regions of the Turkish economy reveal different outcomes. For the period 2006-2015, it is observed that in the regions of Istanbul, West Marmara, Central Anatolia and North East Anatolia, the Lorenz curve has moved away from full equilibrium line, but in the regions of Aegean, Eastern Marmara and Eastern Black Sea, on the average, a convergence towards the equilibrium from the Lorenz curve has appeared. In other three regions, the Gini coefficients improve on average in Western Anatolia, Mediterranean and Western Black Sea regions. The Gini coefficients are increasing in the period 2006-2011 in the Middle East Anatolia and falling in 2015. In the South East Anatolia Region, there is no progress in the period 2006-2011, but, there happens to be a recovery in income distribution in 2015.
C02|Axioms for Measuring without mixing apples and Oranges|A mixture set is path-connected via a suitable collection of paths, the most common example being a convex set. Yet in many economic settings, there are pairs of prospects that are not connected by a path of mixtures. Consider the thought experiment of von Neumann and Morgenstern involving a glass of milk, a glass of tea and a cup of coffee: we are often asked to choose between convex combinations of milk and tea, yet the same cannot be said of tea and coffee. We introduce partial mixture sets (which need not be path-connected) and provide a formal extension of the well-known axiomatisation of cardinal, linear utility by Herstein and Milnor. We show that partial mixture sets encompass a variety of settings in the literature and present a novel application to cardinal, nonlinear utility on a stochastic process.
C02|Application of the Net Present Value Profile to Anaconda Mining|The idea of the NPV Profile, which shows how the net present value of a project changes over the life of the project, can be used in applied settings. For example, it can be used in situations where significant changes are made to the life-of-mine plan for a gold mine. This paper presents such an example with a description of engineering changes required to achieve the change in the mine plan based on the current situation facing Anaconda Mining, a publicly-traded gold mining company in Canada.
C02|Example of a Rising NPV Profile for a Mining Project|This paper describes a situation where the NPV Profile for a mining project is rising in the initial periods of production. This is an interesting case because mining projects are typically characterized by decreasing NPV Profiles caused by declining reserves and ever-approaching end to mine life. In this example, the NPV Profile is increasing alongside mine production levels in the first half of the project life. The parameters for this model are based on simplifying assumptions for the potential production profile of Anaconda Mining, a publicly-traded mining company.
C02|Reconciling International Trade Data|International trade data are in substantial measurement error. Data reported by some countries mean next to nothing. This study develops an index of trade data quality based on the consistency between a country’s claims on bilateral trade and the corresponding claims of the rest of the world from 1962 to 2014. The index takes the relative significance of each partner and data availability into account. We produce a more reliable set of bilateral and total international trade data using the index. Findings include (a) the actual exports of most countries with low data quality are considerably higher than self-reported. (b) Corruption and poor data quality are strongly correlated. (c) Global trade data quality has been deteriorating in the past three decades even though more countries have improved their data quality over time. This is because low-quality reporters have recently increased their share in global trade. (d) China tends to under-report exports and over-report imports. (e) There is only a trivial difference between US self-reported and reconciled data. The same applies to all high-quality reporters. We recommend future studies on trade use our reconciled data.
C02|Impact of Foreign Direct Investment on Economic of Afghanistan|The present study attempts the impact of foreign direct investment on gross domestic product (GDP) of Afghanistan by taking a time- series data for the period of 2001-2014. It is applied ordinary least square (OLS) method through simple regression. T-statistic is significant and P-value is also significant at 5% level. The F-test was also significant at 5% level so the overall model is significant. The results indicated that there is positive significance relationship between foreign direct investment and gross domestic product (GDP) in Afghanistan. The study suggests that Government may focus on to attracting foreign direct investment and improve its agriculture and mine sector latest technology and Machineries for better economic growth.
C02|On the diversification benefit of reinsurance portfolios|In this paper we compare the diversification benefit of portfolios containing excess-of-loss treaties and portfolios containing quota-share treaties, when the risk measure is the (excess) Value-at-Risk or the (excess) Expected Shortfall. In a first section we introduce the set-up under which we perform our investigations. Then we show that when the losses are continuous, independent, bounded, the cover unlimited and when the risk measure is the Expected Shortfall at a level alpha close to 1, a portfolio of n excess-of-loss treaties diversifies better than a comparable portfolio of n quota-share treaties. This result extends to the other risk measures under additional assumptions. We further provide evidence that the boundedness assumption is not crucial by deriving analytical formulas in the case of treaties with i.i.d. exponentially distributed original losses. Finally we perform the comparison in the more general setting of arbitrary continuous joint loss distributions and observe in that case that a finite cover leads to opposite results, i.e. a portfolio of n quota-share treaties diversifies better than a comparable portfolio of n excess-of-loss treaties at high quantile levels.
C02|Sub-economic impulse and consciousness with quantum chromodynamic modeling|The word “sub-economics” follows the sense of “sub-atomic” (Yang, 2012). The latter is about the smallest in matter while the former is about the deepest in mind. Both are difficult to observe, but science must zoom in to understand them. Sub-economic dynamics studies the underlying human impulse and consciousness that may affect individual behavior in the market. It touches on the elementary mental structure of the sub-economic world. However, coding such an elementary structure or modeling its dynamics is not only a necessity, but also a challenge. Quantum chromodynamics (QCD; Zee, 2010) is about strong interactions of quarks and gluons. It touches on the elementary material structure of the physical world. QCD is applied as a conceptual and instrumental tool in the development of sub-economic modeling. The results show a nearly perfect isomorphism between the two elementary structures. By utilizing gauge field theory, sub-economic dynamics shares the same gauge symmetric group, SU(3), with QCD.
C02|Behavioral economics and auto-images of distributions of random variables|Distributions of random variables defined on finite intervals were considered in connection with some problems of behavioral economics. To develop the results obtained for finite intervals, auto-image distributions of random variables defined on infinite or semi-infinite intervals are proposed in this article. The proposed auto-images are intended for constructing reference auto-image distributions for preliminary considerations and estimates near the boundaries of semi-infinite intervals and on finite intervals.
C02|Quantitative Description of Financial Transactions and Risks|This paper presents a quantitative model of financial transactions between economic agents on economic space. Risk ratings of economic agents play role of their coordinates. Aggregate amounts of agent’s financial variables at point x define macro financial variables as functions of time and coordinates. Financial transactions between agents define evolution of agent’s financial variables. Aggregate amounts of financial transactions between agents at points x and y define macro financial transactions as functions of x and y. Macro transactions determine evolution of macro financial variables. To describe dynamics and interactions of macro transactions we derive hydrodynamic-like equations. Description of macro transactions permits model evolution of macro financial variables and hence develop dynamics and forecasts of macro finance. As example for simple model interactions between macro transactions we derive hydrodynamic-like equations and obtain wave equations for their perturbations. Waves of macro transactions induce waves of macro financial variables on economic space. Diversities of financial waves of macro transactions and macro financial variables on economic space in simple models uncover internal complexity of macro financial processes. Any developments of financial models and forecast should take into account financial wave processes and their influences.
C02|Диффузионное Описание Производственного Процесса<BR>[Diffusion description of the production process]|В статье основное внимание уделяется построению стохастического уравнения для расчета производственного цикла изготовления партии изделий на предприятиях с поточным типом организации производства. В качестве базового подхода рассмотрен процесс движения предметов труда по синхронизованной поточной линии. Выполнена оценка размеров межоперационных заделов технологических операций, обеспечивающая бесперебойный режим функционирования поточной линии. Записано в канонической форме стохастическое уравнение и получено определение коэффициента диффузии.
C02|The model of the production process of the party of the subjects of labour|The article discusses the construction of the model of streaming production line with the constraints on the technological trajectory of subjects of labour. The work shows the influence of the subject of labour movement trajectory, which is related to the limited maximum capacity of the operating storage. It analyses constraint that is associated with the serial order of subjects of labour processing. The equation for the trajectory of the regulatory process is built, taking into account the constraints on the trajectory of the subjects of labour, which can be used for closing the balance equations of PDE-models of streaming production lines.
C02|Regression on intervals|In some previous papers ([3],[4]) we introduced and used a regression suitable for data series where the depended variable is not a value but a set of values. These values may be a discrete set or a continuous data. The economic correspondent of this mathematical approach is the exchange rate, where values are spread into an interval, during one day. Also, the stock exchange market is an example where indicators values are continuously variable during a day, etc.
C02|Computing deltas without derivatives|Abstract A well-known application of Malliavin calculus in mathematical finance is the probabilistic representation of option price sensitivities, the so-called Greeks, as expectation functionals that do not involve the derivative of the payoff function. This allows numerically tractable computation of the Greeks even for discontinuous payoff functions. However, while the payoff function is allowed to be irregular, the coefficients of the underlying diffusion are required to be smooth in the existing literature, which for example already excludes simple regime-switching diffusion models. The aim of this article is to generalise this application of Malliavin calculus to Itô diffusions with irregular drift coefficients, where we focus here on the computation of the delta, which is the option price sensitivity with respect to the initial value of the underlying. To this end, we first show existence, Malliavin differentiability and (Sobolev) differentiability in the initial condition for strong solutions of Itô diffusions with drift coefficients that can be decomposed into the sum of a bounded, but merely measurable, and a Lipschitz part. Furthermore, we give explicit expressions for the corresponding Malliavin and Sobolev derivatives in terms of the local time of the diffusion, respectively. We then turn to the main objective of this article and analyse the existence and probabilistic representation of the corresponding deltas for European and path-dependent options. We conclude with a small simulation study of several regime-switching examples.
C02|Quality of Work Life in Colombia: A Multidimensional Fuzzy Indicator|Abstract Quality of work (QoW) encompasses multiple objective and subjective dimensions, which may include labor income, job stability, job satisfaction, and social security. This paper follows the method proposed by Gómez-Salcedo et al. (2013) that introduces a new way of measuring QoW, which consists of (1) the use of Sen’s functioning and capabilities approach and (2) a fuzzy sets method to define membership to the sets of good job quality. Using the Gran Encuesta Integrada de Hogares, we obtain results at the national level about age, gender, educational level, firm size, and industry sector. The results are consistent with previous literature. One topic that is highlighted from our results is the existence of a “Quality of Work Life Cycle”, with higher levels of the index up to age 30, that may have critical implications for the social security system; people with lower quality jobs may not be contributing to health and pension funds, leaving many people without access to a retirement fund and implying more demands on the subsidized system to cover health expenses.
C02|On the existence of approximate equilibria and sharing rule solutions in discontinuous games|This paper studies the existence of some known equilibrium solution concepts in a large class of economic models with discontinuous payoff functions. The issue is well understood for Nash equilibria, thanks to Reny's better-reply security condition, and its recent improvements. We propose new approaches, related to Reny's work, and obtain tight conditions for the existence of an approximate equilibrium and of a sharing rule solution in pure and mixed strategies (Simon and Zame). As byproducts, we prove that many auction games with correlated types admit an approximate equilibrium, and that many competition models with discontinuous preferences have a sharing rule solution.
C02|The Degree Measure as Utility Function over Positions in Networks|In this paper, we connect the social network theory on centrality measures to the economic theory of preferences and utility. Using the fact that networks form a special class of cooperative TU-games, we provide a foundation for the degree measure as a von Neumann-Morgenstern expected utility function reflecting preferences over being in different positions in different networks. The famous degree measure assigns to every position in a weighted network the sum of the weights of all links with its neighbours. A crucial property of a preference relation over network positions is neutrality to ordinary risk. If a preference relation over network positions satisfies this property and some regularity properties, then it must be represented by a utility function that is a multiple of the degree centrality measure. We show this in three steps. First, we characterize the degree measure as a centrality measure for weighted networks using four natural axioms. Second, we relate these network centrality axioms to properties of preference relations over positions in networks. Third, we show that the expected utility function is equal to a multiple of the degree measure if and only if it represents a regular preference relation that is neutral to ordinary risk. Similarly, we characterize a class of affine combinations of the outdegree and indegree measure in weighted directed networks and deliver its interpretation as a von Neumann-Morgenstern expected utility function.
C02|A necessary and sufficient condition for a unique maximum with an application to potential games|Under regularity and boundary conditions which ensure an interior maximum, I show that there is a unique critical point which is a global maximum if and only if the Hessian determinant of the negated objective function is strictly positive at any critical point. Within the large class of Morse functions, and subject to boundary conditions, this local and ordinal condition generalizes strict concavity, and is satisfied by nearly all strictly quasiconcave functions. The result also provides a new uniqueness theorem for potential games.
C02|Impact of socioeconomic factors on nutritional diet in Vietnam from 2004 to 2014: new insights using compositional data analysis|This paper contributes to the analysis of the impact of socioeconomic factors, like food expenditure level and urbanization, on diet patterns in Vietnam, from 2004 to 2014. Contrary to the existing literature, we focus on the diet balance in terms of macronutrients consumption (protein, fat and carbohydrate) and we take into account the fact that the volumes of each macronutrient are not independent. In other words, we are interested in the shares of each macronutrient in the total calorie intake. We use the compositional data analysis (CODA) to describe the evolution of diet patterns over time, and to model the impact of household characteristics on the macronutrient shares vector. We compute food expenditure elasticities of macronutrient shares, and we compare them to classical elasticities for macronutrient volumes and total calorie intake. The compositional model highlights the important role of food expenditure, size of the household and dwelling region in the determination of diet choices. Our results are consistent with the rest of the literature, but they have the advantage to highlight the substitution effects between macronutrients in the context of nutrition transition.
C02|Time – Varying Rational Expectations Models: Solutions, Stability, Numerical Implementation|"While rational expectations models with time{varying (random) coefficients have gained some esteem, the understanding of their dy- namic properties is still in its infancy. The paper adapts results from the theory of random dynamical systems to solve and analyze the stability of rational expectations models with time{varying (random) coefficients. This theory develops a \linear algebra"" in terms of Lya- punov exponents defined as the asymptotic growth rates of trajecto- ries. They replace the eigenvalue analysis used in constant coefficient models and allow the construction of solutions in the spirit of Blan- chard and Kahn (1980). The usefulness of these methods and their numerical implementation is illustrated using a canonical New Key- nesian model with a time{varying policy rule."
C02|The Implied Volatility of Forward Starting Options: ATM Short-Time Level, Skew and Curvature|For stochastic volatility models, we study the short-time behaviour of the at-the-money implied volatility level, skew and curvature for forward-starting options. Our analysis is based on Malliavin Calculus techniques
C02|Sure Profits via Flash Strategies and the Impossibility of Predictable Jumps|In an arbitrage-free financial market, asset prices should not exhibit jumps of a predictable magnitude at predictable times. We provide a rigorous formulation of this result in a fully general setting, only allowing for buy-and-hold positions and without imposing any semimartingale restriction. We show that asset prices do not exhibit predictable jumps if and only if there is no possibility of obtaining sure profits via high-frequency limits of buy-and-hold trading strategies. Our results imply that, under minimal assumptions, price changes occurring at scheduled dates should only be due to unanticipated information releases.
C02|Hyperbolic grids and discrete random graphs|We present an efficient algorithm for computing distances in hyperbolic grids. We apply this algorithm to work efficiently with a discrete variant of the hyperbolic random graph model. This model is gaining popularity in the analysis of scale-free networks, which are ubiquitous in many fields, from social network analysis to biology. We present experimental results conducted on real world networks.
C02|Forecasting compositional risk allocations|We analyse models for panel data that arise in risk allocation problems,when a given set of sources are the cause of an aggregate risk value. We focus on the modeling and forecasting of proportional contributions to risk. Compositional data methods are proposed and the regression is flexible to incorporate external information from other variables. We guarantee that projected proportional contributions add up to 100%, and we introduce a method to generate confidence regions with the same restriction. An illustration using data from the stock exchange is provided.
C02|Innovation clusters effects on adoption of a general purpose technology under uncertainty|This paper analyzes the effect of innovation clusters on the adoption of a gen- eral purpose technology (GPT) and on firms R&D investment levels in im- perfect information situation. To do this, we developed a theoretical model of vertical relation, described as a four-step game between an upstream firm providing innovative GPT and an innovative downstream associated sector, integrator of this technology. The downstream sector ignores the quality of the GPT and we model the innovation cluster as a coordination mode of firms, improving the probability of the downstream firm to receive information about the quality of the GPT technology. Then, we determine firms equilibria (prices and technological qualities) and we showed that the effect of innovation clus- ters on the choice of qualities, the adoption behavior, levels of investment in R&D as well as that social welfare depends on the quality of R&D activities carried out before the establishment of the cluster and a threshold effect or cluster critical mass; if the critical mass in terms of information sharing and interaction is not reached, the cluster may have negative effects. In other words, the consensual idea of expected positive effects of innovation clusters must be put into perspective.
C02|A dynamic theory of economics: What are the market forces?|The main weakness in the neoclassical theory of economics is its static nature. By a static model one cannot explain the observed time paths of economic quantities, like the flows of production of firms, the flows of consumption of consumers, and the prices of goods. The error in the neoclassical framework is that economic units are assumed to be in their optimum state and thus not willing to change their behaviour. Therefore, in neoclassical models a static equilibrium prevails. In this paper, the authors change this assumption so that economic units are assumed to be willing to improve their current state that may not be the optimal one. In this way, one can explain economic dynamics where every economic unit is changing its behaviour towards improving its welfare. The authors define the economic forces acting upon the production of firms, the consumption of consumers, and the prices of goods are changing in time. They show that in this dynamic system, business cycles and bankruptcies of firms emerge in a natural way like in the real world.
C02|Pareto tails in socio-economic phenomena: A kinetic description|Various phenomena related to socio-economic aspects of our daily life exhibit equilibrium densities characterized by a power law decay. Maybe the most known example of this property is concerned with wealth distribution in a western society. In this case the polynomial decay at infinity is referred to as Pareto tails phenomenon (Pareto, Cours d'économie politique, 1964). In this note, the authors discuss a possible source of this behavior by resorting to the powerful approach of statistical mechanics, which enlightens the analogies with the classical kinetic theory of rarefied gases. Among other examples, the distribution of populations in towns and cities is illustrated and discussed.
C02|Determinants and impacts of intangible investment: Evidence from Chinese private manufacturing firms|Determinants of investment in intangibles by firms and the effects of intangible investment on firm productivity have been documented for developed economies. Evidence on these issues in emerging economies, however, is scarce. Using data from China Enterprise Survey 2012 conducted by the World Bank, this study examines the determinants and impacts of intangible investment by private manufacturing firms in China, thus shedding light on recent development of intangibles in one of the largest emerging economies in the world. It is found that more human capital, larger firm size and better institutional quality generally increase the propensity and the amount of intangible investment, and yet fiercer market competition decreases both the propensity and the amount to invest in intangibles. We also provide evidence that the three components of intangibles including research and development (R&D) investment, software investment and organization investment as well as ICT investment are positively correlated with firm productivity. Furthermore there is complementarity between software investment and organization investment. Implications for policies to enhance investment in intangibles are identified from the empirical results.
C02|Cadenas Globales de Valor: el caso de Bolivia|El presente documento analiza las Cadenas Globales de Valor (CGV) en Bolivia para 2002, 2005 y 2011, utilizando el Índice de Especialización Vertical (EV) de Hummels et al. (2001) y la medida Upstreamness de Antràs y Chor (2011) (citado en Antràs et al., 2012b) y Fally (2011). Los indicadores muestran que los patrones comerciales están representados por una concentración de las materias primas, las que también han aprovechado las ventajas derivadas de las CGV; mientras que los sectores de manufacturas han sido menos importantes en esta dinámica, e incluso han perdido su relevancia en el tiempo
C02|HJB equations in infinite dimension and optimal control of stochastic evolution equations via generalized Fukushima decomposition|A stochastic optimal control problem driven by an abstract evolution equation in a separable Hilbert space is considered. Thanks to the identification of the mild solution of the state equation as V-weak Dirichlet process, the value processes is proved to be a real weak Dirichlet process. The uniqueness of the corresponding decomposition is used to prove a verification theorem. Through that technique several of the required assumptions are milder than those employed in previous contributions about non-regular solutions of Hamilton-Jacobi-Bellman equations.
C02|Efficient asymptotic variance reduction when estimating volatility in high frequency data|This paper shows how to carry out efficient asymptotic variance reduction when estimating volatility in the presence of stochastic volatility and microstructure noise with the realized kernels (RK) from Barndorff-Nielsen et al. (2008) and the quasi-maximum likelihood estimator (QMLE) studied in Xiu (2010). To obtain such a reduction, we chop the data into B blocks, compute the RK (or QMLE) on each block, and aggregate the block estimates. The ratio of asymptotic variance over the bound of asymptotic efficiency converges as B increases to the ratio in the parametric version of the problem, i.e. 1.0025 in the case of the fastest RK Tukey-Hanning 16 and 1 for the QMLE. The impact of stochastic sampling times and jump in the price process is examined carefully. The finite sample performance of both estimators is investigated in simulations, while empirical work illustrates the gain in practice.
C02|On the existence of sure profits via flash strategies|We introduce and study the notion of sure profit via flash strategy, consisting of a high-frequency limit of buy-and-hold trading strategies. In a fully general setting, without imposing any semimartingale restriction, we prove that there are no sure profits via flash strategies if and only if asset prices do not exhibit predictable jumps. This result relies on the general theory of processes and provides the most general formulation of the well-known fact that, in an arbitrage-free financial market, asset prices (including dividends) should not exhibit jumps of a predictable direction or magnitude at predictable times. We furthermore show that any price process is always right-continuous in the absence of sure profits. Our results are robust under small transaction costs and imply that, under minimal assumptions, price changes occurring at scheduled dates should only be due to unanticipated information releases.
C02|Testing if the market microstructure noise is fully explained by the informational content of some variables from the limit order book|In this paper, we build tests for the presence of residual noise in a model where the market microstructure noise is a known parametric function of some variables from the limit order book. The tests compare two distinct quasi-maximum likelihood estimators of volatility, where the related model includes a residual noise in the market microstructure noise or not. The limit theory is investigated in a general nonparametric framework. In the presence of residual noise, we examine the central limit theory of the related quasi-maximum likelihood estimation approach.
C02|Assessing the risks of asset overvaluation: models and challenges|We propose methods to compute confidence bands for the fundamental values of stocks and corporate bonds. These methods take into account uncertainty about future cash flows and about the discount factors used to discount the cash flows. We use them to assess the current degree of under-/over-valuation of asset prices. We find no evidence of over-valuation of the stocks and corporate bonds of the major economies.
C02|A quantitative analysis of risk premia in the corporate bond market|We propose an econometric model to decompose corporate bond spreads into compensation required by investors for unpredictable future changes in the credit environment and for expected default losses. We use the model to understand whether the significant reduction in corporate bond spreads observed since the launch of the CSPP (Corporate Sector Purchase Programme) is attributable more to the fact that expansionary monetary policy measures tend to increase the risk appetite of investors and compress risk premia, or to the ability of unconventional measures to reduce expected default losses by improving investors’ expectations about the economic and financial conditions of issuers.
C02|Clustering and forecasting inflation expectations using the World Economic Survey: the case of the 2014 oil price shock on inflation targeting countries|This paper examines inflation expectations of the World Economic Survey for ten inflation targeting countries. First, by a Self Organizing Maps methodology, we cluster the trajectory of agents inflation expectations using the beginning of the oil price shock occurred in June of 2014 as a benchmark in order to discriminate between those countries that anticipated the shock smoothly and those with brisk changes in expectations. Then, the expectations are modeled by artificial neural networks forecasting models. Second, for each country we investigate the information content of the quantitative survey forecast by comparing it to the average annual inflation based on national consumer price indices. The results indicate the presence of heterogeneity among countries to anticipate inflation under the oil shock and, also different patterns of accuracy to predict average annual inflation were found depending on the observed inflation trend. Classification JEL: C02, C222, C45, C63, E27
C02|The implied volatility of forward starting options: ATM short-time level, skew and curvature|For stochastic volatility models, we study the short-time behaviour of the at-the-money implied volatility level, skew and curvature for forward-starting options. Our analysis is based on Malliavin Calculus techniques.
C02|Sensitivity of energy system investments to policy regulation changes: Application of the blue sky catastrophe|In this paper we argue, that the interaction of technology and economic policy regulations in the energy sector may be described by the so-called slow-fast class of dynamical systems. It is known that such systems may exhibit the blue sky catastrophe, a special type of bifurcation. Application of this result allows us to argue that caution is needed when updating economic policies in the energy sector to avoid the onset of catastrophic developments in the system's transformation, when energy system dynamics becomes unresponsive to policy updates.
C02|Dynamic heterogeneous R&D with cross-technologies interactions|In many countries, inducing large-scale technological changes has become an important policy objective, as in the context of climate policy or energy transitions. Such large-scale changes require the development of strongly interlinked technologies. But current economic models have little flexibility for describing such linkages. We present a model of induced technological change that covers a fairly large set of cross-technology interactions and that can describe a wide variety of long-run developments. Using this model, we analyse and compare the development induced by optimal fifrm behaviour and the socially optimal dynamics. We show that the structure of cross-technology interactions is highly important. It shapes the dynamics of technological change in the decentralised and the socially optimal solution, including the prospects of continued productivity growth. It determines whether the decentralised and the socially optimal solution have similar or qualitatively difffferent dynamics. Finally, it is highly important for the question whether simple r&d policies can induce effifficient technological change.
C02|The effects of tax coordination on the tax revenue mobilization in West African Economic and Monetary Union (WAEMU)|A main objective of the regional integration in West African Economic and Monetary Union (WAEMU) is the effective harmonization of national legislations at Community level notably tax legislation. To coordinate taxation in the zone, WAEMU Commission translates into Directives the Decisions taken by the Council of Ministers of the member states. The implementation of the Community acts by countries through tax reforms may impact on their revenue performance. This paper evaluates the impact of the Directives both in terms of coordination and revenue mobilization. It relies on a comparative case study using the synthetic control method developed by Abadie and Gardeazabal (2003) and extended by Abadie, Diamond, and Hainmueller (2010 & 2015). The main results are that the tax coordination affected the revenue mobilization in the Union but the impact is different across countries.
C02|Predicting Financial Market Crashes Using Ghost Singularities|We analyse the behaviour of a non-linear model of coupled stock and bond prices exhibiting periodically collapsing bubbles. By using the formalism of dynamical system theory, we explain what drives the bubbles and how foreshocks or aftershocks are generated. A dynamical phase space representation of that system coupled with standard multiplicative noise rationalises the log-periodic power law singularity pattern documented in many historical financial bubbles. The notion of ‘ghosts of finite-time singularities’ is introduced and used to estimate the end of an evolving bubble, using finite-time singularities of an approximate normal form near the bifurcation point. We test the forecasting skill of this method on different stochastic price realisations and compare with Monte Carlo simulations of the full system. Remarkably, the former is significantly more precise and less biased. Moreover, the method of ghosts of singularities is less sensitive to the noise realisation, thus providing more robust forecasts.
C02|Forecasting market risk of portfolios: copula-Markov switching multifractal approach| This paper proposes a new methodology for modeling and forecasting market risks of portfolios. It is based on a combination of copula functions and Markov switching multifractal (MSM) processes. We assess the performance of the copula-MSM model by computing the value at risk of a portfolio composed of the NASDAQ composite index and the S&P 500. Using the likelihood ratio (LR) test by Christoffersen [1998. “Evaluating Interval Forecasts.” International Economic Review 39: 841–862], the GMM duration-based test by Candelon et al. [2011. “Backtesting Value at Risk: A GMM Duration-based Test.” Journal of Financial Econometrics 9: 314–343] and the superior predictive ability (SPA) test by Hansen [2005. “A Test for Superior Predictive Ability.” Journal of Business and Economic Statistics 23, 365–380] we evaluate the predictive ability of the copula-MSM model and compare it to other common approaches such as historical simulation, variance–covariance, RiskMetrics, copula-GARCH and constant conditional correlation GARCH (CCC-GARCH) models. We find that the copula-MSM model is more robust, provides the best fit and outperforms the other models in terms of forecasting accuracy and VaR prediction.
C02|Kalman on dynamics and contro, Linear System Theory, Optimal Control, and Filter|Rudolf Emil Kalman (“R.E.K.”) passed away on July, 2nd, 2016. Among contemporary economists Kalman is mainly remembered for his filter, an algorithm that allows recursive estimation of unobserved time varying variables in a system. However, he has also a key part on the whole of recursive macroeconomic theory as is notably expressed by Lars Ljungqvist’s and Thomas Sargent’s book [Ljunqvist and Sargent, 2012]. Our paper is a contribution to show the links between Kalman’s works on filtering, linear quadratic optimal control, and system theory. We also provide a model on cooperative advertising to show that Kalman’s works on dynamics and control can be useful in macroeconomics as in microeconomics, a domain where his contributions seem to be unfortunately less used.
C02|HJB equations in infinite dimension and optimal control of stochastic evolution equations via generalized Fukushima decomposition|A stochastic optimal control problem driven by an abstract evolution equation in a separable Hilbert space is considered. Thanks to the identification of the mild solution of the state equation as ?-weak Dirichlet process, the value processes is proved to be a real weak Dirichlet process. The uniqueness of the corresponding decomposition is used to prove a verification theorem. Through that technique several of the required assumptions are milder than those employed in previous contributions about non-regular solutions of Hamilton-Jacobi-Bellman equations.
C02|Diversification benefits under multivariate second order regular variation|We analyze risk diversification in a portfolio of heavy-tailed risk factors under the assumption of second order multivariate regular variation. Asymptotic limits for a measure of diversification benefit are obtained when considering, for instance, the value-at-risk . The asymptotic limits are computed in a few examples exhibiting a variety of different assumptions made on marginal or joint distributions. This study ties up existing related results available in the literature under a broader umbrella.
C02|New results on the order of functions at infinity|Recently, new classes of positive and measurable functions, M(ρ) and M(±∞), have been defined in terms of their asymptotic behaviour at infinity, when normalized by a logarithm (Cadena et al., 2015, 2016, 2017). Looking for other suitable normalizing functions than logarithm seems quite natural. It is what is developed in this paper, studying new classes of functions of the type lim x→∞ log U (x)/H(x) = ρ
C02|The Time Dimension of the Links Between Loss Given Default and the Macroeconomy|Most studies focusing on the determinants of loss given default (LGD) have largely ignored possible lagged effects of the macroeconomy on LGD. We fill this gap by employing a wide set of macroeconomic covariates on a retail portfolio that represents 15% of the Czech consumer credit market over the period 2002–2012. We find an important time dimension to the links between LGD and the aggregate economy in the Czech Republic. The model that allows exclusively for contemporaneous effects includes a number of significant macroeconomic variables, some of which have non-intuitive signs. Nonetheless, a more general time structure of the LGD model makes current macroeconomic variables largely irrelevant and highlights the importance of delayed responses of LGD to the macroeconomic environment.
C02|Between hawks and doves: measuring central bank communication|We propose a Hawkish-Dovish (HD) indicator that measures the degree of ‘hawkishness’ or ‘dovishness’ of the media’s perception of the ECB’s tone at each press conference. We compare two methods to calculate the indicator: semantic orientation and Support Vector Machines text classification. We show that the latter method tends to provide more stable and accurate measurements of perception on a labelled test set. Furthermore, we demonstrate the potential use of this indicator with several applications: we perform a correlation analysis with a set of interest rates, use Latent Dirichlet Allocation to detect the dominant topics in the news articles, and estimate a set of Taylor rules. The ﬁndings provide decisive evidence in favour of using an advanced text mining classiﬁcation model to measure the medias perception and the Taylor rule application conﬁrms that communication plays a signiﬁcant role in enhancing the accuracy when trying to estimate the bank’s reaction function. JEL Classification: C02, C63, E52, E58
C02|Dynamic directed random matching|We develop a general and unified model in which a continuum of agents conduct directed random searches for counterparties. Our results provide the first probabilistic foundation for static and dynamic models of directed search (including the matching-function approach) that are common in search-based models of financial markets, monetary theory, and labor economics. The agents' types are shown to be independent discrete-time Markov processes that incorporate the effects of random mutation, random matching with match-induced type changes, and with the potential for enduring partnerships that may have randomly timed break-ups. The multi-period cross-sectional distribution of types is shown to be deterministic and is calculated using the exact law of large numbers.
C02|Estimation of integrated quadratic covariation with endogenous sampling times|When estimating high-frequency covariance (quadratic covariation) of two arbitrary assets observed asynchronously, simple assumptions, such as independence, are usually imposed on the relationship between the prices process and the observation times. In this paper, we introduce a general endogenous two-dimensional nonparametric model. Because an observation is generated whenever an auxiliary process called observation time process hits one of the two boundary processes, it is called the hitting boundary process with time process (HBT) model. We establish a central limit theorem for the Hayashi–Yoshida (HY) estimator under HBT in the case where the price process and the observation price process follow a continuous Itô process. We obtain an asymptotic bias. We provide an estimator of the latter as well as a bias-corrected HY estimator of the high-frequency covariance. In addition, we give a consistent estimator of the associated standard error.
C02|Discontinuous payoff option pricing by Mellin transform: A probabilistic approach|The Mellin transform technique is applied for solving the Black-Scholes equation with time-dependent parameters and discontinuous payoff. We show that the option pricing is equivalent to recovering a probability density function on the positive real axis based on its moments, which are integer or fractional Mellin transform values. Then the Mellin transform can be effectively inverted from a collection of appropriately chosen fractional (i.e. non-integer) moments by means of the Maximum Entropy (MaxEnt) method. An accurate option pricing is guaranteed by previous theoretical results about MaxEnt distributions constrained by fractional moments. We prove that typical drawbacks of other numerical techniques, such as Finite Difference schemes, are bypassed exploiting the Mellin transform properties. An example involving discretely monitored barrier options is illustrated and the accuracy, efficiency and time consuming are discussed.
C02|Intensity-based framework for surrender modeling in life insurance|In this paper, we propose an intensity-based framework for surrender modeling. We model the surrender decision under the assumption of stochastic intensity and use, for comparative purposes, the affine models of Vasicek and Cox–Ingersoll–Ross for deriving closed-form solutions of the policyholder’s probability of surrendering the policy. The introduction of a closed-form solution is an innovative aspect of the model we propose. We evaluate the impact of dynamic policyholders’ behavior modeling the dependence between interest rates and surrendering (affine dependence) with the assumption that mortality rates are independent of interest rates and surrendering. Finally, using experience-based decrement tables for both surrendering and mortality, we explain the calibration procedure for deriving our model’s parameters and report numerical results in terms of best estimate of liabilities for life insurance under Solvency II.
C02|Noise-induced transitions in a stochastic Goodwin-type business cycle model|We motivate and specify a stochastic Goodwin-type business cycle model. Our analysis focusses on a subset of the parameter space where several attractors coexist. Applying a semi-numerical approach based on the stochastic sensitivity function and confidence domains due to Milstein and Ryashko (1995), we study random transitions between stable attractors in the context of the Goodwin-type economy embedded in an uncertain environment. Relying on a mix of analytical considerations and simulations we demonstrate that under weak noise levels regime switching is a prominent feature in the presence of low saving rates. Moreover, we explain how increased uncertainty can induce an essentially unpredictable income process out of an apparently stable high-income level situation. All dynamic phenomena are explained in terms of key concepts constituting the stochastic sensitivity function method.
C02|Modelling a complex world: improving macro-models|Macro models have come under criticism for their ability to understand or predict major economic events such as the global financial crisis and its aftermath. Some of that criticism is warranted; but, in our view, much is not. This paper contributes to the debate over the adequacy of benchmark DSGE models by showing how three extensions, which are features that have characterized the global economy since the early 2000s, are necessary to improve our understanding of global shocks and policy insights. The three extensions are to acknowledge and model the entire global economy and the linkage through trade and capital flows; to allow for a wider range of relative price variability by moving to multiple-sector models rather than a single good model; and to allow for changes in risk perceptions which propagate through financial markets and adjustments in the real economy. These extensions add some complexity to large-scale macro-models, but without them policy models can oversimplify things, allowing misinterpretations of shocks and therefore costly policy mistakes to occur. Using over-simplified models to explain a complex world makes it more likely there will be ‘puzzles’. The usefulness of these extensions is demonstrated in two ways: first, by briefly revisiting some historical shocks to show how outcomes can be interpreted that make sense within a more complex DSGE framework; then, by making a contemporary assessment of the implications from the proposed large fiscal stimulus and the bans on immigration by the Trump administration which have both sectoral and macroeconomic implications that interact.
C02|The time dimension of the links between loss given default and the macroeconomy|Most studies focusing on the determinants of loss given default (LGD) have largely ignored possible lagged effects of the macroeconomy on LGD. We fill this gap by employing a wide set of macroeconomic covariates on a retail portfolio that represents 15% of the Czech consumer credit market over the period 2002 JEL Classification: C02, G13, G33
C02|Firm Networks and Asset Returns|This paper argues that changes in the propagation of idiosyncratic shocks along firm networks are important to understanding variations in asset returns. When calibrated to match key features of supplier-customer networks in the United States, an equilibrium model in which investors have recursive preferences and firms are interlinked via enduring relationships generates long-run consumption risks. Additionally, the model matches cross-sectional patterns of portfolio returns sorted by network centrality, a feature unaccounted for by standard asset pricing models.
C02|HJB Equations in Infinite Dimension and Optimal Control of Stochastic Evolution Equations via Generalized Fukushima Decomposition|A stochastic optimal control problem driven by an abstract evolution equation in a separable Hilbert space is considered. Thanks to the identification of the mild solution of the state equation as v-weak Dirichlet process, the value processes is proved to be a real weak Dirichlet process. The uniqueness of the corresponding decomposition is used to prove a verification theorem. Through that technique several of the required assumptions are milder than those employed in previous contributions about non-regular solutions of Hamilton-Jacobi-Bellman equations.
C02|Externalities in Economies with Endogenous Sharing Rules|Endogenous sharing rules were introduced by Simon and Zame [16] to model payoff indeterminacy in discontinuous games. They prove the existence in every compact strategic game of a mixed Nash equilibrium and an associated sharing rule. We extend their result to economies with externalities [1] where, by definition, players are restricted to pure strategies. We also provide a new interpretation of payoff indeterminacy in Simon and Zame's model in terms of preference incompleteness.
C02|Longevity, age-structure, and optimal schooling|The mechanism stating that longer life implies larger investment in human capital, is premised on the view that individual decision-making governs the relationship between longevity and education. This relationship is revisited here from the perspective of optimal period school life expectancy, obtained from the utility maximization of the whole population characterized by its age structure and its age-specific fertility and mortality. Realistic life tables such as model life tables are mandatory, because the age distribution of mortality matters, notably at infant and juvenile ages. Optimal period school life expectancy varies with life expectancy and fertility. The application to French historical data from 1806 to nowadays shows that the population age structure has indeed modified the relationship between longevity and optimal schooling.
C02|A Path Integral Approach to Interacting Economic Systems with Multiple Heterogeneous Agents|This paper presents an analytical treatment of economic systems with an arbitrary number of agents that keeps track of the systems’ interactions and complexity. The formalism does not seek to aggregate agents: it rather replaces the standard optimization approach by a probabilistic description of the agent’s behavior. This is done in two distinct steps. A first step considers an interaction system involving an arbitrary number of agents, where each agent's utility function is subject to unpredictable shocks. In such a setting, individual optimization problems need not be resolved. Each agent is described by a time-dependent probability distribution centered around its utility optimum. The whole system of agents is thus defined by a composite probability depending on time, agents' interactions, relations of strategic dominations, agents' information sets and expectations. This setting allows for heterogeneous agents with different utility functions, strategic domination relations, heterogeneity of information, etc. This dynamic system is described by a path integral formalism in an abstract space -- the space of the agents' actions -- and is very similar to a statistical physics or quantum mechanics system. We show that this description, applied to the space of agents' actions, reduces to the usual optimization results in simple cases. Compared to the standard optimization, such a description markedly eases the treatment of a system with a small number of agents. It becomes however useless for a large number of agents. In a second step therefore, we show that, for a large number of agents, the previous description is equivalent to a more compact description in terms of field theory. This yields an analytical, although approximate, treatment of the system. This field theory does not model an aggregation of microeconomic systems in the usual sense, but rather describes an environment of a large number of interacting agents. From this description, various phases or equilibria may be retrieved, as well as the individual agents’ behaviors, along with their interaction with the environment. This environment does not necessarily have a unique or stable equilibrium and allows to reconstruct aggregate quantities without reducing the system to mere relations between aggregates. For illustrative purposes, this paper studies several economic models with a large number of agents, some presenting various phases. These are models of consumer/producer agents facing binding constraints, business cycle models, and psycho-economic models of interacting and possibly strategic agents.
C02|The effects of tax coordination on the tax revenue mobilization in West African Economic and Monetary Union (WAEMU)|A main objective of the regional integration in West African Economic and Monetary Union (WAEMU) is the effective harmonization of national legislations at Community level notably tax legislation. To coordinate taxation in the zone, WAEMU Commission translates into Directives the Decisions taken by the Council of Ministers of the member states. The implementation of the Community acts by countries through tax reforms may impact on their revenue performance. This paper evaluates the impact of the Directives both in terms of coordination and revenue mobilization. It relies on a comparative case study using the synthetic control method developed by Abadie and Gardeazabal (2003) and extended by Abadie, Diamond, and Hainmueller (2010 & 2015). The main results are that the tax coordination affected the revenue mobilization in the Union but the impact is different across countries.
C02|Linear Time Iteration|This paper proposes a simple iterative method – time iteration – to solve linear rational expectation models. I prove that this method converges to the solution with the smallest eigenvalues in absolute value, and provide the conditions under which this solution is unique. In particular, if conditions similar to those of Blanchard and Kahn (1980) are met, the procedure converges to the unique stable solution. Apart from its transparency and simplicity of implementation, the method provides a straightforward approach to solving models with less standard features, such as regime switching models. For large-scale problems the method is 10-20 times faster than existing solution methods.
C02|Retrieving Risk-Neutral Densities Embedded in VIX Options: a Non-Structural Approach|We propose a non-structural pricing method to retrieve the risk-neutral density implied by options contracts on the CBOE VIX. The method is based on orthogonal polynomial expansions around a kernel density and yields the risk-neutral density of the underlying asset without the need for modeling its dynamics. The method imposes only mild regularity conditions on shape of the density. The approach can be thought of as an alternative to Hermite expansions where the kernel has positive support. .e family of Laguerre kernels is extended to include the GIG and the generalized Weibull densities, which, due to their flexible rate of decay, are better suited at modeling the density of the VIX. Based on this technique, we propose a simple and robust way to estimate the expansion coefficients by means of a principal components analysis. We show that the proposed methodology yields an accurate approximation of the risk-neutral density also when the no-arbitrage and efficient option prices are contaminated by measurement errors. A number of numerical illustrations support the adequacy and the flexibility of the proposed expansions in a large variety of cases.
C02|Avaliação Espacial Das Fontes De Crescimento De Um Conjunto De Commodities Agrícolas Brasileiras Exportáveis Entre 2003-2013| Brazil is one of the major agricultural commodities exporter, especially soybeans, wheat, corn, cotton, sugar cane, orange and coffee. Given the importance of international trade for economic growth, it is important to perform a spatial and temporal assessment of the growth sources of these agricultural commodities, which in this study was made by the shift-share model and ESDA. The results show us the importance of Area Effect and Productivity Effect for the studied cultures, which brought evidence of the existence of a land expansion process and productivity improvement for these cultures. In terms of regional results, it was possible to highlight two interesting movements : a) more intense positive changes in the production of Northern states; and b) growth process of production maintenance in the Midwest states. Moreover, from the ESDA, it was possible to evaluate the formation of clusters and determine that there is a dichotomy between the Midwest (High-High cluster) and the Northeast (Low-Low cluster) regarding the Area Effect. That is, it became clear that the Midwest region, for the studied commodities, is still an area of expansion of the agricultural area.
C02|Financial Implications of Seasonal Variability in Demand for Tourism Services (A Draft)| Using Jensen’s inequality (and its mathematical generalization), this contribution shows how increased seasonal (periodic) variability of demand for tourism services can increase the annual profit of a tourism enterprise and the producers’ surplus of a corresponding competitive segment of the tourism industry experiencing this increased variability. It identifies conditions which result in these effects being magnified and takes account of the fact that a tourism business’ supply of services is often subject to capacity utilization constraints. A novel feature is that allowance is made for the possibility that variations in the market demand for tourism services may alter the prices of factors of production.
C02|Financial Implications of Seasonal Variability in Demand for Tourism Services (Final Draft)| Using Jensen’s inequality (and its mathematical generalization), this contribution shows how increased seasonal (periodic) variability of demand for tourism services can increase the annual profit of a tourism enterprise and the producers’ surplus of a corresponding competitive segment of the tourism industry experiencing this increased variability. It identifies conditions which result in these effects being magnified and takes account of the fact that a tourism business’ supply of services is often subject to capacity utilization constraints. A novel feature is that allowance is made for the possibility that variations in the market demand for tourism services may alter the prices of factors of production. Examples of seasonal variability in the prices for tourism services are provided. Furthermore, the importance of this contribution is related to the available scholarly literature about the financial consequences of seasonal variability in the demand for tourism services.
C02|Investigations Concerning E-Government Adoption in Transition Economies|The implementation of efficient cross-border digital public services for a connected Europe, a developed e-govemment represents a priority for the European Union. There are big differences in the way e-government is adopted. Transition economies lag behind developed economies. This paper explores the e-government adoption in its multidimensionality within the EU member states. It uses 22 variables, which highlight: technological preparedness, the ability to access and absorb information and information technology, the ability to generate, adopt and spread knowledge, the social and legal environment, the government policy and vision, and consumer and business adoption and innovation. Barriers to efficient e-government adoption in transition economies are identified. Multicriteria decision analysis is used for the prioritisation of the factors with the highest overall impact on efficient implementation. The authors use the Analytical Hierarchy Process (AHP method) for prioritisation and the numerical results are obtained with Expert Choice software.
C02|Heuristic approach for determining efficient frontier portfolios with more than two assets, the case of ZSE|The goal of this paper is to exhibit computation of minimal variance portfolio and efficient portfolio frontier when there are more than two assets, by using matrix algebra applied on chosen stocks listed on Zagreb Stock Exchange.The research shows that, because of low correlation of underlying assets, it is possible to significantly reduce risks of investments by constructing portfolio of the stocks. It also shows that, if restriction on short selling are imposed this significantly reduces the possibility for diversification.
C02|Brownian Movement Of Stock Quotes Of The Companies Listed On The Bucharest Stock Exchange And Probability Ranges|This paper aims to generate evolutions in continuous time of quotes for five companies listed on the Bucharest Stock Exchange, the first category, and determining ranges of probability where you can find these quotes in the future (one month, three months, six months). In this sense, the quotes of listed shares are considered random variables of continuous type and the model used to generate evolutions is the most accepted model for equities, currencies, indices.
C02|An Anatomy of the Equity Premium|This paper introduces a decomposition of the market return in terms of higher-order realized, and option-implied risk aversion, connecting it to level, slope, and curvature of the implied volatility surface. Empirically, second-order risk aversion -- loss aversion -- explains most of the market return. Signals revealed by this risk anatomy provide predictive power out-of-sample for realized returns in particular for longer maturities. The decomposition also shows that compensation for disaster risk is not prominently featured in the market return. Furthermore it highlights that models with identically and independently distributed state variables are not suited to represent in particular longer-maturity returns.
C02|A self-organizing map analysis of survey-based agents? expectations before impending shocks for model selection: The case of the 2008 financial crisis|This paper examines the role of clustering techniques to assist in the selection of the most indicated method to model survey-based expectations. First, relying on a Self-Organizing Map (SOM) analysis and using the financial crisis of 2008 as a benchmark, we distinguish between countries that show a progressive anticipation of the crisis, and countries where sudden changes in expectations occur. We then generate predictions of survey indicators, which are usually used as explanatory variables in econometric models. We compare the forecasting performance of a multi-layer perceptron (MLP) Artificial Neural Network (ANN) model to that of three different time series models. By combining both types of analysis, we find that ANN models outperform time series models in countries in which the evolution of expectations shows brisk changes before impending shocks. Conversely, in countries where expectations follow a smooth transition towards recession, autoregressive integrated moving-average (ARIMA) models outperform neural networks.
C02|Income distribution in the Colombian economy from an econophysics perspective|Recently, in econophysics, it has been shown that it is possible to analyze economic systems as equilibrium thermodynamic models. We apply statistical thermodynamics methods to analyze income distribution in the Colombian economic system. Using the data obtained in random polls, we show that income distribution in the Colombian economic system is characterized by two specific phases. The first includes about 90% of the interviewed individuals, and is characterized by an exponential Boltzmann-Gibbs distribution. The second phase, which contains the individuals with the highest incomes, can be described by means of one or two power-law density distributions that are known as Pareto distributions. ***** En el marco de la econofísica se ha demostrado recientemente que es posible analizar sistemas económicos como modelos termodinámicos en equilibrio. En este artículo aplicamos métodos de termodinámica estadística para analizar la distribución de ingresos dentro del sistema económico colombiano. Utilizando datos de encuestas aleatorias, demostramos que la distribución de ingresos presenta dos fases particulares. La primera corresponde a cerca del 90% del grupo analizado y se caracteriza por una distribución exponencial del tipo Boltzmann-Gibbs. La segunda fase, en la que están incluidos los entrevistados con ingresos más altos, se puede describir mediante una o dos distribuciones de potencias: distribuciones de Pareto.
C02|Looking Backward and Looking Forward|Filtering has had a profound impact as a device of perceiving information and deriving agent expectations in dynamic economic models. For an abstract economic system, this paper shows that the foundation of applying the filtering method corresponds to the existence of a conditional expectation as an equilibrium process. Agent-based rational behavior of looking backward and looking forward is generalized to a conditional expectation process where the economic system is approximated by a class of models, which can be represented and estimated without information loss. The proposed framework elucidates the range of applications of a general filtering device and is not limited to a particular model class such as rational expectations.
C02|Discovering SIFIs in interbank communities|This paper proposes a new methodology based on non-negative matrix factor- ization to detect communities and to identify Systemically Important Financial In- stitutions in the interbank network as well as within communities. The method is speci cally designed for directed weighted networks and it is able to take into account exposures on both sides of banksbalance sheets, distinguishing between Systemically Important Borrowers and Lenders. Using interbank transactions data from the e-Mid platform, we show that the systemic importance associated with Italian banks decreased during the 2007-2009 nancial crisis while the opposite happened for foreign institutions. We also show that, as the transactions volume grew, the number of communities rose as well. The contrary happened during the crisis phase. Moreover results indicate that, during nancial crisis, banks strongly operate into non overlapping communities with few institutions playing the role of SIFIs. On the contrary during business as usual times banks act in several and overlapping modules.
C02|Stock prices prediction via tensor decomposition and links forecast|Many complex systems display fluctuations between alternative states in correspondence to tipping points. These critical shifts are usually associated with generic empirical phenomena such as strengthening correlations between entities composing the system. In finance, for instance, market crashes are the consequence of herding behaviors that make the units of the system strongly correlated, lowering their distances. Consequently, determining future distances between stocks can be a valuable starting point for predicting market down-turns. This is the scope of the work. It introduces a multi-way procedure for forecasting stock prices by decomposing a distance tensor. This multidimensional method avoids aggregation processes that could lead to the loss of crucial features of the system. The technique is applied to a basket of stocks composing the S&P500 composite index and to the index itself so as to demonstrate its ability to predict the large market shifts that arise in times of turbulence, such as the ongoing financial crisis.
C02|VaR as the CVaR sensitivity : applications in risk optimization|VaR minimization is a complex problem playing a critical role in many actuarial and financial applications of mathematical programming. The usual methods of convex programming do not apply due to the lack of sub-additivity. The usual methods of differentiable programming do not apply either, due to the lack of continuity. Taking into account that the CVaR may be given as an integral of VaR, one has that VaR becomes a first order mathematical derivative of CVaR. This property will enable us to give accurate approximations in VaR optimization, since the optimization VaR and CVaR will become quite closely related topics. Applications in both finance and insurance will be given.
C02|Reason-Based Choice And Context-Dependence: An Explanatory Framework|We introduce a “reason-based” framework for explaining and predicting individual choices. The key idea is that a decision-maker focuses on some but not all properties of the options and chooses an option whose “motivationally salient” properties he/she most prefers. Reason-based explanations can capture two kinds of contextdependent choice: (i) the motivationally salient properties may vary across choice contexts, and (ii) they may include “context-related” properties, not just “intrinsic” properties of the options. Our framework allows us to explain boundedly rational and sophisticated choice behaviour. Since properties can be recombined in new ways, it also offers resources for predicting choices in unobserved contexts.<br><small>(This abstract was borrowed from another version of this item.)</small>
C02|Development of Network-Ranking Model to Create the Best Production Line Value Chain: A Case Study in Textile Industry|The main reason for creating value chain is fulfilling needs and organizational resources with the least cost and highest quality. Application of most of the current techniques has merely intended to choose the best scenario. But industrial units need to build an ideal scenario as a value chain which focuses on intangible interstitial and hidden factors: good (good nature), bad (bad nature), fixed (obligatory nature) and free (not identifying their nature) and creates value. Therefore, the model presented in this article answers this issue. First of all we present a model based on the network approach of data envelopment analysis, then we assess and rank the stages based on the scenarios for the stages forming the value chain and finally, the ideal decision unit is presented. For this reason, the general efficiency is designed with two natures; 1.input-centered (concentration on the costs) and 2.output-centered (concentration on the incomes).
C02|A New Method Of Assessment Based On Fuzzy Ranking And Aggregated Weights (Afraw) For Mcdm Problems Under Type-2 Fuzzy Environment|Fuzzy multi-criteria decision-making (MCDM) methods and problems have increasingly been considered in the past years. Type-1 fuzzy sets are usually used by decision-makers (DMs) to express their evaluations in the process of decision-making. Interval type-2 fuzzy sets (IT2FSs), which are extensions of type-1 fuzzy sets, have more degrees of flexibility in modeling of uncertainty. In this research, a new ranking method to calculate the ranking values of interval type-2 fuzzy sets is proposed. A comparison is performed to show the efficiency of this ranking method. Using the proposed ranking method and the arithmetic operations of IT2FSs, a new method of Assessment based on Fuzzy Ranking and Aggregated Weights (AFRAW)is developed for multi-criteria group decision-making. To obtain more realistic and practical weights for the criteria, the subjective weights expressed by DMs and objective weights calculated based on a deviation-based method are combined, and the aggregated weights are used in the proposed method. A numerical example related to assessment of suppliers in a supply chain and selecting the best one is used to illustrate the procedure of the proposed method. Moreover, a comparison and a sensitivity analysis are performed in this study. The results of these analyses show the validity and stability of the proposed method.
C02|An Efficient Binomial Method for Pricing Asian Options|We construct an efficient tree method for pricing path-dependent Asian options. The standard tree method estimates option prices at each node of the tree, while the proposed method defines an interval about each node along the stock price axis and estimates the average option price over each interval. The proposed method can be used independently to construct a new tree method, or it can be combined with other existing tree methods to improve the accuracy. Numerical results show that the proposed schemes show superiority in accuracy to other tree methods when applied to discrete forward-starting Asian options and continuous European or American Asian options.
C02|A New Combinative Distance-Based Assessment(Codas) Method For Multi-Criteria Decision-Making|A key factor to attain success in any discipline, especially in a field which requires handling large amounts of information and knowledge, is decision making. Most real-world decision-making problems involve a great variety of factors and aspects that should be considered. Making decisions in such environments can often be a difficult operation to perform. For this reason, we need multi-criteria decision-making (MCDM) methods and techniques, which can assist us for dealing with such complex problems. The aim of this paper is to present a new COmbinative Distance-based ASsessment (CODAS) method to handle MCDM problems. To determine the desirability of an alternative, this method uses the Euclidean distance as the primary and the Taxicab distance as the secondary measure, and these distances are calculated according to the negative- ideal point. The alternative which has greater distances is more desirable in the CODAS method. Some numerical examples are used to illustrate the process of the proposed method. We also perform a comparative sensitivity analysis to examine the results of CODAS and compare it by some existing MCDM methods. These analyses show that the proposed method is efficient, and the results are stable.
C02|Machine Learning Techniques For Stock Market Prediction.Acase Study Of Omv Petrom|The research reported in the paper focuses on the stock market prediction problem, the main aim being the development of a methodology to forecast the OMV Petrom stock closing price. The methodology is based on some novel variable selection methods and an analysis of neural network and support vector machines based prediction models. Also, a hybrid approach which combines the use of the variables derived from technical and fundamental analysis of stock market indicators in order to improve prediction results of the proposed approaches is reported in this paper. Two novel variable selection methods are used to optimize the performance of prediction models. In order to identify the most informative time series to predict a stock price, both methods are essentially based on the general forecasting error minimization when a certain stock price is expressed exclusively in terms of other indicators. After the variable selection is over, the forecasting is performed in terms of the historical values of the given stock price and selected variables respectively. The performance of the proposed methodology is evaluated by a long series of tests, the results being very encouraging as compared to similar developments.
C02|A Model For Optimal Investment Project Choice Using Fuzzy Probability|In this paper we present a model for classifying exclusive investments. The model uses Bellman and Zadeh’s decision-making criterion, determining the degree of convergence when the objective is to maximize the net present value of the project under the constraint of minimizing risk. The original aspect of this work consists in incorporating uncertainty into the model by considering variables such as project life, net income and capitalization rate as uncertain in order to determine net present value and risk. The concept of a fuzzy event is used to calculate the net present value and assess the risk of each investment project. This allows us to establish the degree to which a project is a good investment, understanding this as a fuzzy event and establishing the degree to which a project has a high net present value, understood as another fuzzy event.
C02|A self-calibrating method for heavy tailed data modeling : Application in neuroscience and finance|One of the main issues in the statistical literature of extremes concerns the tail index estimation, closely linked to the determination of a threshold above which a Generalized Pareto Distribution (GPD) can be fi tted. Approaches to this estimation may be classfii ed into two classes, one using standard Peak Over Threshold (POT) methods, in which the threshold to estimate the tail is chosen graphically according to the problem, the other suggesting self-calibrating methods, where the threshold is algorithmically determined. Our approach belongs to this second class proposing a hybrid distribution for heavy tailed data modeling, which links a normal (or lognormal) distribution to a GPD via an exponential distribution that bridges the gap between mean and asymptotic behaviors. A new unsupervised algorithm is then developed for estimating the parameters of this model. The effectiveness of our self-calibrating method is studied in terms of goodness-of-fi t on simulated data. Then, it is applied to real data from neuroscience and fi nance, respectively. A comparison with other more standard extreme approaches follows.
C02|Technology Platforms as an Efficient Tool to Modernize Russia’s Economy|There is an urgent need to consider the dynamic development of the global economy from the point of view of its positive impact on competitiveness improvement in national manufacturing industries, and the best ways to modernize the country economy. The purpose of the paper is to provide with perspectives for development of instruments related to technology platforms (TP) within the framework of innovation management and adapted to the conditions of Russia’s economic reality. The major method in studying this issue is mathematical economic modeling which has made it possible to facilitate expediency in determining a TP as an effective innovation control instrument. The paper considers European and Russian experience in deploying TP, and identifies national features characteristic to the performance of the innovation management instrument. A mathematical economic model is used for justifying the efficiency of introducing TP into Russian institutional innovation system. The practical significance of results and conclusions is in its ability to improve the mechanisms of developing and implementing federal and regional innovation development programs, development of the innovation infrastructure, stimulation of the innovation activity, use of a set of TP instruments by public authorities.
C02|On interactions between remittance outflows and Saudi Arabian macroeconomy: New evidence from wavelets|The effect of workers' remittance outflows on macroeconomic variables of host countries is a controversial issue. The purpose of this paper is to study lead/lag interactions between workers' remittance outflows and macroeconomic leading variables in Saudi Arabia for 1980–2013 within a time–frequency framework. To this end, we perform three wavelet variants, namely, the wavelet power spectrum, the cross-spectrum wavelet, and the coherence wavelet. We show that remittance outflows are strongly associated with the main Saudi aggregates and that their relationships change across time scale and frequency bands. In the short- and mid-term, real output growth and government expenditures guide remittance outflows. More specifically, government expenditures positively affect remittance shares to real outputs. In addition, the wavelet analysis reveals a positive causality link from the active population to remittances over low-frequency bands. These outcomes have several prominent implications and point to practical recommendations in terms of monetary policy coordination and financial stability.
C02|Between data cleaning and inference: Pre-averaging and robust estimators of the efficient price|Pre-averaging is a popular strategy for mitigating microstructure in high frequency financial data. As the term suggests, transaction or quote data are averaged over short time periods ranging from 30 s to five min, and the resulting averages approximate the efficient price process much better than the raw data. Apart from reducing the size of the microstructure, the methodology also helps synchronise data from different securities. The procedure is robust to short term dependence in the noise.
C02|Evolution of the world crude oil market integration: A graph theory analysis|This paper investigates the evolution of the world crude oil market and the pricing power for major oil-producing and oil-consuming countries using graph theory. A minimal spanning tree for the world crude oil market is constructed and some empirical results are given. The integration of the world crude oil market is verified. Furthermore, the world crude oil market is characterised as a geographical and organisational structure. The crude oil markets of adjacent countries or regions tend to link together, while OPEC is well-integrated. We also found that the links in the South and North American region and the African region are relatively stable. The crude oil markets in the U.S., Angola and Saudi Arabia take up the core, with a higher ‘betweenness centrality’ and lower ‘farness’, whereas the markets in the East and Southeast Asian countries are on the fringe. Finally, the degree of globalisation for the world crude oil market is becoming further entrenched, verified by a decreasing normalised tree length; hence, its systemic risk may increase due to the future uncertainty of world politics.
C02|A thermodynamical view on asset pricing|The dynamics of stock market systems was analyzed from the stand point of viscoelasticity, i.e. conservative and nonconservative (or elastic and viscous) forces. Asset values were modeled as a geometric Brownian motion by generating random Wiener processes at different volatilities and drift conditions. Specifically, the relation between the return value and the Wiener noise was investigated. Using a scattering diagram, the asset values were placed into a ‘potentiality–actuality’ framework, and using Euclidean distance, the market values were transformed into vectorial forms. Depending on whether the forthcoming vector is aligned or deviated from the direction of advancement of the former vector, it is possible to split the forthcoming vector into its conservative and nonconservative components. The conservative (or in-phase, or parallel) component represents the work-like term whereas the nonconservative (or out-of-phase, or vertical) component represents heat-like term providing a treatment of asset prices in thermodynamical terms. The resistances exhibited against these components, so-called the modulus, were determined in either case. It was observed that branching occurred in the values of modulus especially in the modulus of the conservative component when it was plotted with respect to the Euclidean distance of Wiener noise, i.e. Wiener length. It was also observed that interesting patterns formed when the change of modulus was plotted with respect to the value of Wiener noise. The magnitudes of work-like and heat-like terms were calculated using the mathematical expressions. The peaks of both heat-like and work-like terms reveal around the zero value of Wiener noise and at very low magnitudes of either term. The increase of both the volatility and the drift acts in the same way, and they decrease the number of low heat-like and work-like terms and increase the number of the ones with larger magnitudes. Most interestingly, the increase either in volatility or in drift decreases the heat-like term but increases the work-like term in the overall. Finally, the observation of the golden ratio in various patterns was interpreted in terms of physical resistance to flow.
